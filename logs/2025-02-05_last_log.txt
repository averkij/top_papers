[05.02.2025 20:11] Read previous papers.
[05.02.2025 20:11] Generating top page (month).
[05.02.2025 20:11] Writing top page (month).
[05.02.2025 21:09] Read previous papers.
[05.02.2025 21:09] Get feed.
[05.02.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.02492
[05.02.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01362
[05.02.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01718
[05.02.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.02584
[05.02.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.02508
[05.02.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01941
[05.02.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.02589
[05.02.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.00674
[05.02.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.19066
[05.02.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01720
[05.02.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01839
[05.02.2025 21:09] Extract page data from URL. URL: https://huggingface.co/papers/2501.19389
[05.02.2025 21:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.02.2025 21:09] No deleted papers detected.
[05.02.2025 21:09] Downloading and parsing papers (pdf, html). Total: 12.
[05.02.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2502.02492.
[05.02.2025 21:09] Extra JSON file exists (./assets/json/2502.02492.json), skip PDF parsing.
[05.02.2025 21:09] Paper image links file exists (./assets/img_data/2502.02492.json), skip HTML parsing.
[05.02.2025 21:09] Success.
[05.02.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2502.01362.
[05.02.2025 21:09] Extra JSON file exists (./assets/json/2502.01362.json), skip PDF parsing.
[05.02.2025 21:09] Paper image links file exists (./assets/img_data/2502.01362.json), skip HTML parsing.
[05.02.2025 21:09] Success.
[05.02.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2502.01718.
[05.02.2025 21:09] Extra JSON file exists (./assets/json/2502.01718.json), skip PDF parsing.
[05.02.2025 21:09] Paper image links file exists (./assets/img_data/2502.01718.json), skip HTML parsing.
[05.02.2025 21:09] Success.
[05.02.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2502.02584.
[05.02.2025 21:09] Extra JSON file exists (./assets/json/2502.02584.json), skip PDF parsing.
[05.02.2025 21:09] Paper image links file exists (./assets/img_data/2502.02584.json), skip HTML parsing.
[05.02.2025 21:09] Success.
[05.02.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2502.02508.
[05.02.2025 21:09] Extra JSON file exists (./assets/json/2502.02508.json), skip PDF parsing.
[05.02.2025 21:09] Paper image links file exists (./assets/img_data/2502.02508.json), skip HTML parsing.
[05.02.2025 21:09] Success.
[05.02.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2502.01941.
[05.02.2025 21:09] Extra JSON file exists (./assets/json/2502.01941.json), skip PDF parsing.
[05.02.2025 21:09] Paper image links file exists (./assets/img_data/2502.01941.json), skip HTML parsing.
[05.02.2025 21:09] Success.
[05.02.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2502.02589.
[05.02.2025 21:09] Extra JSON file exists (./assets/json/2502.02589.json), skip PDF parsing.
[05.02.2025 21:09] Paper image links file exists (./assets/img_data/2502.02589.json), skip HTML parsing.
[05.02.2025 21:09] Success.
[05.02.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2502.00674.
[05.02.2025 21:09] Extra JSON file exists (./assets/json/2502.00674.json), skip PDF parsing.
[05.02.2025 21:09] Paper image links file exists (./assets/img_data/2502.00674.json), skip HTML parsing.
[05.02.2025 21:09] Success.
[05.02.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.19066.
[05.02.2025 21:09] Extra JSON file exists (./assets/json/2501.19066.json), skip PDF parsing.
[05.02.2025 21:09] Paper image links file exists (./assets/img_data/2501.19066.json), skip HTML parsing.
[05.02.2025 21:09] Success.
[05.02.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2502.01720.
[05.02.2025 21:09] Extra JSON file exists (./assets/json/2502.01720.json), skip PDF parsing.
[05.02.2025 21:09] Paper image links file exists (./assets/img_data/2502.01720.json), skip HTML parsing.
[05.02.2025 21:09] Success.
[05.02.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2502.01839.
[05.02.2025 21:09] Extra JSON file exists (./assets/json/2502.01839.json), skip PDF parsing.
[05.02.2025 21:09] Paper image links file exists (./assets/img_data/2502.01839.json), skip HTML parsing.
[05.02.2025 21:09] Success.
[05.02.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.19389.
[05.02.2025 21:09] Downloading paper 2501.19389 from http://arxiv.org/pdf/2501.19389v1...
[05.02.2025 21:09] Extracting affiliations from text.
[05.02.2025 21:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models Wenzhi Fang 1 Dong-Jun Han 2 Liangqi Yuan 1 Seyyedali Hosseinalipour 3 Christopher G. Brinton "
[05.02.2025 21:09] Response: []
[05.02.2025 21:09] Extracting affiliations from text.
[05.02.2025 21:09] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models Wenzhi Fang 1 Dong-Jun Han 2 Liangqi Yuan 1 Seyyedali Hosseinalipour 3 Christopher G. BrintonFine-tuning large language models (LLMs) on devices is attracting increasing interest. Recent works have fused low-rank adaptation (LoRA) techniques with federated fine-tuning to mitigate challenges associated with device model sizes and data scarcity. Still, the heterogeneity of computational resources remains critical bottleneck: while higher-rank modules generally enhance performance, varying device capabilities constrain LoRAs feasible rank range. Existing approaches attempting to resolve this issue either lack analytical justification or impose additional computational overhead, leaving wide gap for an efficient and theoretically-grounded solution. To address these challenges, we propose federated sketching LoRA (FSLoRA), which leverages sketching mechanism to enable devices to selectively update submatrices of global LoRA modules maintained by the server. By adjusting the sketching ratios, which determine the ranks of the submatrices on the devices, FSLoRA flexibly adapts to device-specific communication and computational constraints. We provide rigorous convergence analysis of FSLoRA that characterizes how the sketching ratios affect the convergence rate. Through comprehensive experiments on multiple datasets and LLM models, we demonstrate FSLoRAs superior performance compared to various baselines. 5 2 0 2 1 3 ] . [ 1 9 8 3 9 1 . 1 0 5 2 : r 1. Introduction On-device LLMs have recently gained significant attention as promising complement to cloud-based large language models (LLMs) (Fan et al., 2024). They align with the 1Department of Electrical and Computer Engineering, Purdue University 2Department of Computer Science and Engineering, Yonsei University 3Department of Electrical Engineering, University at Buffalo-SUNY. Correspondence to: Wenzhi Fang <fang375@purdue.edu>. 1 typical paradigm of LLMs: they are initialized with base model pre-trained on extensive datasets to capture linguistic patterns, semantics, and contextual nuances, and then finetuned on specific datasets to achieve better performance in specialized or domain-specific tasks. However, an LLM fine-tuned on single device often achieves unsatisfactory performance due to the limited data available on each device. Fortunately, federated learning (McMahan et al., 2017; Chen et al., 2023) offers an effective solution here, enabling the model to be fine-tuned across distributed group of clients within the same task domain, without any data sharing. However, federated learning imposes significant computational and memory costs, as each device must fine-tune the LLM using its local dataset and send updates to the server for model aggregation. Recently, many parameterefficient fine-tuning methods have been proposed (Lester et al., 2021; Li & Liang, 2021; Hu et al., 2021) to reduce the cost associated with model adaptation. Among them, low-rank adaptation (LoRA) (Hu et al., 2021) stands out as particularly effective approach due to its flexibility. LoRA enables efficient fine-tuning by approximating weight updates through low-rank decomposition = BA, where matrices and contain significantly fewer trainable parameters than the original weight matrix. To support distributed on-device LLM, Zhang et al. (2024); Ye et al. (2024) incorporated LoRA with FedAvg (McMahan et al., 2017), significantly reducing the fine-tuning cost by cutting down the number of parameters that need to be synchronized across distributed devices. Challenges. While integrating federated learning with LoRA reduces the number of trainable parameters through matrix decomposition, communication costs still increase linearly with the rank of the decomposition. This poses challenges when complex tasks demand higher-rank LoRA modules, particularly on resource-constrained mobile devices. Furthermore, the heterogeneity in computational and communication capabilities across distributed devices makes uniform rank inefficient: fixed rank may be too large for some devices while being too small for more powerful ones, resulting in underutilized resources. Consequently, an approach that reduces communication overhead while adapting LoRA ranks to heterogeneous device capaFederated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models bilities is highly desirable for collaborative fine-tuning of LLMs. Although some existing approaches have attempted to provide solution here (Cho et al., 2024; Bai et al., 2024; Wang et al., 2024), they either lack theoretical justification or impose additional computational overhead, leaving gap for an efficient and theoretically-grounded solution. As discussed in Section 2.2, comprehensive approach that retains LoRAs advantages while addressing heterogeneous on-device fine-tuning under tight resource constraints, with theoretical guarantees, has remained elusive. 1.1. Contributions Motivated by these limitations, this work aims to develop methodology for collaborative on-device LLM fine-tuning that (i) retains the flexibility of LoRA, (ii) provides theoretical convergence guarantees, and (iii) addresses the challenges posed by system heterogeneity and resource constraints across distributed devices. As depicted in Figure 1, our key idea is to introduce sketching-based LoRA update to the fine-tuning process, which allows devices to selectively update subset of columns and rows of the LoRA modules during each round, reducing the computation and communication consumption through the system. Additionally, our method customizes the fine-tuning process by adjusting the sparsity level of the sketching matrix, i.e., the size of the updated submatrices for each device in each iteration, as illustrated in Figure 1. As we will see, the impact of the introduced sketching mechanism on the overall optimization landscape requires careful consideration, posing additional challenges for the theoretical analysis which we investigate. Overall, we make the following contributions: We propose federated sketching LoRA (FSLoRA), which leverages sketching mechanism to enable devices to selectively update submatrices of global LoRA modules maintained by the server. By adjusting the sketching ratios, which determine the ranks of the submatrices on devices, FSLoRA effectively adapts to device-specific communication and computational constraints. We present "
[05.02.2025 21:09] Mistral response. {"id": "cb3bef8a4b09486391beb986214c31b7", "object": "chat.completion", "created": 1738789788, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Department of Electrical and Computer Engineering, Purdue University\",\n    \"Department of Computer Science and Engineering, Yonsei University\",\n    \"Department of Electrical Engineering, University at Buffalo-SUNY\"\n]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1534, "total_tokens": 1595, "completion_tokens": 61}}
[05.02.2025 21:09] Response: ```python
[
    "Department of Electrical and Computer Engineering, Purdue University",
    "Department of Computer Science and Engineering, Yonsei University",
    "Department of Electrical Engineering, University at Buffalo-SUNY"
]
```
[05.02.2025 21:09] Deleting PDF ./assets/pdf/2501.19389.pdf.
[05.02.2025 21:09] Success.
[05.02.2025 21:09] Enriching papers with extra data.
[05.02.2025 21:09] ********************************************************************************
[05.02.2025 21:09] Abstract 0. Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics. We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence....
[05.02.2025 21:09] ********************************************************************************
[05.02.2025 21:09] Abstract 1. Learning diffusion bridge models is easy; making them fast and practical is an art. Diffusion bridge models (DBMs) are a promising extension of diffusion models for applications in image-to-image translation. However, like many modern diffusion and flow models, DBMs suffer from the problem of slow i...
[05.02.2025 21:09] ********************************************************************************
[05.02.2025 21:09] Abstract 2. Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging auto...
[05.02.2025 21:09] ********************************************************************************
[05.02.2025 21:09] Abstract 3. Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annota...
[05.02.2025 21:09] ********************************************************************************
[05.02.2025 21:09] Abstract 4. Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verif...
[05.02.2025 21:09] ********************************************************************************
[05.02.2025 21:09] Abstract 5. This paper investigates an under-explored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs' fundamental capabilities. While existing methods achieve impressive compression ratios on long-context benchmarks, their effects on core model capabilities remain ...
[05.02.2025 21:09] ********************************************************************************
[05.02.2025 21:09] Abstract 6. This paper introduces the COCONut-PanCap dataset, created to enhance panoptic segmentation and grounded image captioning. Building upon the COCO dataset with advanced COCONut panoptic masks, this dataset aims to overcome limitations in existing image-text datasets that often lack detailed, scene-com...
[05.02.2025 21:09] ********************************************************************************
[05.02.2025 21:09] Abstract 7. Ensembling outputs from diverse sources is a straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multiple different Large Language Models (LLMs). This paper raises the question in the context of languag...
[05.02.2025 21:09] ********************************************************************************
[05.02.2025 21:09] Abstract 8. Despite the remarkable progress in text-to-image generative models, they are prone to adversarial attacks and inadvertently generate unsafe, unethical content. Existing approaches often rely on fine-tuning models to remove specific concepts, which is computationally expensive, lack scalability, and/...
[05.02.2025 21:09] ********************************************************************************
[05.02.2025 21:09] Abstract 9. Customization of text-to-image models enables users to insert custom concepts and generate the concepts in unseen settings. Existing methods either rely on costly test-time optimization or train encoders on single-image training datasets without multi-image supervision, leading to worse image qualit...
[05.02.2025 21:09] ********************************************************************************
[05.02.2025 21:09] Abstract 10. Sampling-based search, a simple paradigm for utilizing test-time compute, involves generating multiple candidate responses and selecting the best one -- typically by verifying each response for correctness. In this paper, we study the scaling trends governing sampling-based search. Among our finding...
[05.02.2025 21:09] ********************************************************************************
[05.02.2025 21:09] Abstract 11. Fine-tuning large language models (LLMs) on devices is attracting increasing interest. Recent works have fused low-rank adaptation (LoRA) techniques with federated fine-tuning to mitigate challenges associated with device model sizes and data scarcity. Still, the heterogeneity of computational resou...
[05.02.2025 21:09] Read previous papers.
[05.02.2025 21:09] Generating reviews via LLM API.
[05.02.2025 21:09] Using data from previous issue: {"categories": ["#video"], "emoji": "🎬", "ru": {"title": "VideoJAM: Реалистичное движение в генеративных видеомоделях", "desc": "Статья представляет VideoJAM - новый подход к генерации видео, решающий проблему недостаточной реалистичности движения в существующих моделях. Авторы предлагают обучать мо
[05.02.2025 21:09] Using data from previous issue: {"categories": ["#inference", "#optimization", "#cv", "#diffusion"], "emoji": "🚀", "ru": {"title": "Ускорение диффузионных мостов: искусство эффективной дистилляции", "desc": "Статья представляет новый метод дистилляции для моделей диффузионного моста (DBM). Этот подход позволяет ускорить вывод DBM 
[05.02.2025 21:09] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#rl"], "emoji": "🤖", "ru": {"title": "Революция в обучении моделей кода: мощь RL и автоматизированных тест-кейсов", "desc": "В этой статье представлен новый подход к обучению моделей для генерации кода с использованием обучения с подкрепле
[05.02.2025 21:09] Using data from previous issue: {"categories": ["#rlhf", "#open_source", "#inference", "#reasoning", "#agents", "#training", "#optimization"], "emoji": "🤖", "ru": {"title": "QLASS: Пошаговое обучение языковых агентов для повышения эффективности", "desc": "Статья представляет новый метод QLASS для обучения языковых агентов. QLASS и
[05.02.2025 21:09] Using data from previous issue: {"categories": ["#small_models", "#open_source", "#training", "#reasoning", "#math", "#rl"], "emoji": "🧠", "ru": {"title": "Satori: LLM с внутренним поиском для улучшенного рассуждения", "desc": "Исследователи представили новый подход к улучшению способностей больших языковых моделей (LLM) к рассужд
[05.02.2025 21:09] Using data from previous issue: {"categories": ["#optimization", "#inference", "#long_context", "#training"], "emoji": "🧠", "ru": {"title": "Сжатие KV-кэша в LLM: баланс между эффективностью и производительностью", "desc": "Статья исследует влияние методов сжатия KV-кэша на фундаментальные возможности больших языковых моделей (LLM
[05.02.2025 21:09] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multimodal"], "emoji": "🖼️", "ru": {"title": "Детальные аннотации для лучшего понимания изображений", "desc": "Статья представляет набор данных COCONut-PanCap для улучшения панорамной сегментации и привязки подписей к изображениям. Этот датасет расширяет C
[05.02.2025 21:09] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#training", "#multimodal"], "emoji": "🤖", "ru": {"title": "Один лучше многих: новый подход к ансамблированию языковых моделей", "desc": "Исследователи предложили новый метод ансамблирования под названием Self-MoA, который агрегирует выходные данные тол
[05.02.2025 21:09] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#interpretability", "#security", "#cv", "#training"], "emoji": "🎨", "ru": {"title": "Безопасное и эффективное управление концепциями в генерации изображений", "desc": "Статья представляет новый подход к манипулированию концепциями в генеративных моделях 
[05.02.2025 21:09] Using data from previous issue: {"categories": ["#synthetic", "#benchmark", "#dataset", "#architecture", "#cv", "#inference"], "emoji": "🎨", "ru": {"title": "Улучшение кастомизации моделей text-to-image с помощью синтетических данных и новой архитектуры", "desc": "Статья представляет новый подход к кастомизации моделей text-to-ima
[05.02.2025 21:09] Using data from previous issue: {"categories": ["#hallucinations", "#benchmark", "#reasoning", "#training", "#optimization"], "emoji": "🔍", "ru": {"title": "Масштабирование поиска: простота ведет к силе", "desc": "Статья исследует масштабирование поиска на основе выборки в моделях машинного обучения. Авторы обнаружили, что простое
[05.02.2025 21:09] Querying the API.
[05.02.2025 21:09] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Fine-tuning large language models (LLMs) on devices is attracting increasing interest. Recent works have fused low-rank adaptation (LoRA) techniques with federated fine-tuning to mitigate challenges associated with device model sizes and data scarcity. Still, the heterogeneity of computational resources remains a critical bottleneck: while higher-rank modules generally enhance performance, varying device capabilities constrain LoRA's feasible rank range. Existing approaches attempting to resolve this issue either lack analytical justification or impose additional computational overhead, leaving a wide gap for an efficient and theoretically-grounded solution. To address these challenges, we propose federated sketching LoRA (FSLoRA), which leverages a sketching mechanism to enable devices to selectively update submatrices of global LoRA modules maintained by the server. By adjusting the sketching ratios, which determine the ranks of the submatrices on the devices, FSLoRA flexibly adapts to device-specific communication and computational constraints. We provide a rigorous convergence analysis of FSLoRA that characterizes how the sketching ratios affect the convergence rate. Through comprehensive experiments on multiple datasets and LLM models, we demonstrate FSLoRA's superior performance compared to various baselines.
[05.02.2025 21:09] Response: {
  "desc": "Данная статья представляет новый метод под названием FSLoRA (Federated Sketching LoRA) для федеративной настройки больших языковых моделей на устройствах с ограниченными ресурсами. FSLoRA использует механизм скетчинга, позволяющий устройствам выборочно обновлять подматрицы глобальных LoRA-модулей на сервере. Метод адаптируется к вычислительным ограничениям конкретных устройств путем настройки коэффициентов скетчинга. Авторы проводят строгий анализ сходимости FSLoRA и демонстрируют превосходную производительность метода в экспериментах на различных наборах данных и моделях.",
  "emoji": "📱",
  "title": "FSLoRA: эффективная федеративная настройка больших языковых моделей на устройствах"
}
[05.02.2025 21:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Fine-tuning large language models (LLMs) on devices is attracting increasing interest. Recent works have fused low-rank adaptation (LoRA) techniques with federated fine-tuning to mitigate challenges associated with device model sizes and data scarcity. Still, the heterogeneity of computational resources remains a critical bottleneck: while higher-rank modules generally enhance performance, varying device capabilities constrain LoRA's feasible rank range. Existing approaches attempting to resolve this issue either lack analytical justification or impose additional computational overhead, leaving a wide gap for an efficient and theoretically-grounded solution. To address these challenges, we propose federated sketching LoRA (FSLoRA), which leverages a sketching mechanism to enable devices to selectively update submatrices of global LoRA modules maintained by the server. By adjusting the sketching ratios, which determine the ranks of the submatrices on the devices, FSLoRA flexibly adapts to device-specific communication and computational constraints. We provide a rigorous convergence analysis of FSLoRA that characterizes how the sketching ratios affect the convergence rate. Through comprehensive experiments on multiple datasets and LLM models, we demonstrate FSLoRA's superior performance compared to various baselines."

[05.02.2025 21:09] Response: ```python
['TRAINING', 'DATASET']
```
[05.02.2025 21:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Fine-tuning large language models (LLMs) on devices is attracting increasing interest. Recent works have fused low-rank adaptation (LoRA) techniques with federated fine-tuning to mitigate challenges associated with device model sizes and data scarcity. Still, the heterogeneity of computational resources remains a critical bottleneck: while higher-rank modules generally enhance performance, varying device capabilities constrain LoRA's feasible rank range. Existing approaches attempting to resolve this issue either lack analytical justification or impose additional computational overhead, leaving a wide gap for an efficient and theoretically-grounded solution. To address these challenges, we propose federated sketching LoRA (FSLoRA), which leverages a sketching mechanism to enable devices to selectively update submatrices of global LoRA modules maintained by the server. By adjusting the sketching ratios, which determine the ranks of the submatrices on the devices, FSLoRA flexibly adapts to device-specific communication and computational constraints. We provide a rigorous convergence analysis of FSLoRA that characterizes how the sketching ratios affect the convergence rate. Through comprehensive experiments on multiple datasets and LLM models, we demonstrate FSLoRA's superior performance compared to various baselines."

[05.02.2025 21:09] Response: ```python
["OPTIMIZATION", "TRANSFER_LEARNING"]
```
[05.02.2025 21:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces federated sketching LoRA (FSLoRA), a method for fine-tuning large language models (LLMs) on devices with varying computational resources. FSLoRA uses a sketching mechanism that allows devices to update only specific parts of the global LoRA modules, making it adaptable to different device capabilities. The method addresses the challenges of data scarcity and model size by adjusting sketching ratios, which control the ranks of the updates based on device constraints. The authors provide a detailed analysis of FSLoRA\'s convergence properties and demonstrate its effectiveness through experiments, showing it outperforms existing methods.","title":"Adaptive Fine-Tuning for Diverse Devices with FSLoRA"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc="This paper introduces federated sketching LoRA (FSLoRA), a method for fine-tuning large language models (LLMs) on devices with varying computational resources. FSLoRA uses a sketching mechanism that allows devices to update only specific parts of the global LoRA modules, making it adaptable to different device capabilities. The method addresses the challenges of data scarcity and model size by adjusting sketching ratios, which control the ranks of the updates based on device constraints. The authors provide a detailed analysis of FSLoRA's convergence properties and demonstrate its effectiveness through experiments, showing it outperforms existing methods.", title='Adaptive Fine-Tuning for Diverse Devices with FSLoRA'))
[05.02.2025 21:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新的方法，称为联邦草图LoRA（FSLoRA），旨在解决在设备上微调大型语言模型（LLMs）时的计算资源异质性问题。FSLoRA利用草图机制，使设备能够选择性地更新由服务器维护的全局LoRA模块的子矩阵。通过调整草图比例，FSLoRA能够灵活适应设备特定的通信和计算限制。实验结果表明，FSLoRA在多个数据集和LLM模型上表现优于现有的基线方法。","title":"灵活适应设备限制的联邦草图LoRA"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='本文提出了一种新的方法，称为联邦草图LoRA（FSLoRA），旨在解决在设备上微调大型语言模型（LLMs）时的计算资源异质性问题。FSLoRA利用草图机制，使设备能够选择性地更新由服务器维护的全局LoRA模块的子矩阵。通过调整草图比例，FSLoRA能够灵活适应设备特定的通信和计算限制。实验结果表明，FSLoRA在多个数据集和LLM模型上表现优于现有的基线方法。', title='灵活适应设备限制的联邦草图LoRA'))
[05.02.2025 21:10] Loading Chinese text from previous data.
[05.02.2025 21:10] Renaming data file.
[05.02.2025 21:10] Renaming previous data. hf_papers.json to ./d/2025-02-05.json
[05.02.2025 21:10] Saving new data file.
[05.02.2025 21:10] Generating page.
[05.02.2025 21:10] Renaming previous page.
[05.02.2025 21:10] Renaming previous data. index.html to ./d/2025-02-05.html
[05.02.2025 21:10] [Experimental] Generating Chinese page for reading.
[05.02.2025 21:10] Chinese vocab [{'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'}, {'word': '桥', 'pinyin': 'qiáo', 'trans': 'bridge'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '翻译', 'pinyin': 'fān yì', 'trans': 'translation'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'}, {'word': '蒸馏', 'pinyin': 'zhēng liú', 'trans': 'distillation'}, {'word': '加速', 'pinyin': 'jiā sù', 'trans': 'accelerate'}, {'word': '处理', 'pinyin': 'chǔ lǐ', 'trans': 'process'}, {'word': '有条件', 'pinyin': 'yǒu tiáo jiàn', 'trans': 'conditional'}, {'word': '无条件', 'pinyin': 'wú tiáo jiàn', 'trans': 'unconditional'}, {'word': '受损', 'pinyin': 'shòu sǔn', 'trans': 'damaged'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generation'}, {'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'}]
[05.02.2025 21:10] Renaming previous Chinese page.
[05.02.2025 21:10] Renaming previous data. zh.html to ./d/2025-02-04_zh_reading_task.html
[05.02.2025 21:10] Writing Chinese reading task.
[05.02.2025 21:10] Writing result.
[05.02.2025 21:10] Renaming log file.
[05.02.2025 21:10] Renaming previous data. log.txt to ./logs/2025-02-05_last_log.txt
