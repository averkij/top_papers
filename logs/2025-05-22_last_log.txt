[22.05.2025 04:17] Read previous papers.
[22.05.2025 04:17] Generating top page (month).
[22.05.2025 04:17] Writing top page (month).
[22.05.2025 05:12] Read previous papers.
[22.05.2025 05:12] Get feed.
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15277
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14302
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15809
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13909
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15045
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15210
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15146
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15779
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15765
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14357
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15778
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15656
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15612
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15404
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13529
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15781
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14231
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13934
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12650
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15816
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15791
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15047
[22.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14827
[22.05.2025 05:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.05.2025 05:12] No deleted papers detected.
[22.05.2025 05:12] Downloading and parsing papers (pdf, html). Total: 23.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.15277.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.15277.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.15277.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.14302.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.14302.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.14302.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.15809.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.15809.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.15809.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.13909.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.13909.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.13909.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.15045.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.15045.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.15045.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.15210.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.15210.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.15210.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.15146.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.15146.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.15146.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.15779.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.15779.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.15779.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.15765.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.15765.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.15765.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.14357.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.14357.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.14357.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.15778.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.15778.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.15778.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.15656.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.15656.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.15656.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.15612.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.15612.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.15612.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.15404.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.15404.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.15404.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.13529.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.13529.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.13529.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.15781.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.15781.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.15781.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.14231.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.14231.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.14231.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.13934.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.13934.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.13934.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.12650.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.12650.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.12650.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.15816.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.15816.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.15816.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.15791.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.15791.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.15791.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.15047.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.15047.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.15047.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.14827.
[22.05.2025 05:12] Extra JSON file exists (./assets/json/2505.14827.json), skip PDF parsing.
[22.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.14827.json), skip HTML parsing.
[22.05.2025 05:12] Success.
[22.05.2025 05:12] Enriching papers with extra data.
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 0. Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks. Yet, specialized reward models for web navigation that can be utilized during bot...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 1. Large language models (LLMs) demand substantial computational and memory resources, creating deployment challenges. Quantization-aware training (QAT) addresses these challenges by reducing model precision while maintaining performance. However, the scaling behavior of QAT, especially at 4-bit precis...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 2. We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a un...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 3. Scaling up high-quality trajectory data has long been a critical bottleneck for developing human-like computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 human-annotated co...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 4. Large language model (LLM)-based embedding models, benefiting from large scale pre-training and post-training, have begun to surpass BERT and T5-based models on general-purpose text embedding tasks such as document retrieval. However, a fundamental limitation of LLM embeddings lies in the unidirecti...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 5. Knowledge graph-based retrieval-augmented generation seeks to mitigate hallucinations in Large Language Models (LLMs) caused by insufficient or outdated knowledge. However, existing methods often fail to fully exploit the prior knowledge embedded in knowledge graphs (KGs), particularly their structu...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 6. Playing video games requires perception, memory, and planning, exactly the faculties modern large language model (LLM) agents are expected to master. We study the major challenges in using popular video games to evaluate modern LLMs and find that directly dropping LLMs into games cannot make an effe...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 7. Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, a T2I model released in February would struggle to generate a suitable poster for a movie premiering in April, because the...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 8. Acquiring detailed 3D scenes typically demands costly equipment, multi-view data, or labor-intensive modeling. Therefore, a lightweight alternative, generating complex 3D scenes from a single top-down image, plays an essential role in real-world applications. While recent 3D generative models have a...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 9. World models, which predict transitions based on history observation and action sequences, have shown great promise in improving data efficiency for sequential decision making. However, existing world models often require extensive domain-specific training and still produce low-fidelity, coarse pred...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 10. Human cognition typically involves thinking through abstract, fluid concepts rather than strictly using discrete linguistic tokens. Current reasoning models, however, are constrained to reasoning within the boundaries of human language, processing discrete token embeddings that represent fixed point...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 11. Fine-tuning on open-source Large Language Models (LLMs) with proprietary data is now a standard practice for downstream developers to obtain task-specific LLMs. Surprisingly, we reveal a new and concerning risk along with the practice: the creator of the open-source LLMs can later extract the privat...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 12. Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 13. Large Reasoning Models (LRMs) have achieved remarkable success on reasoning-intensive tasks such as mathematics and programming. However, their enhanced reasoning capabilities do not necessarily translate to improved safety performance-and in some cases, may even degrade it. This raises an important...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 14. Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with "I don't know". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 15. Diffusion Language Models (DLMs) have been seen as a promising competitor for autoregressive language models. However, diffusion language models have long been constrained by slow inference. A core challenge is that their non-autoregressive architecture and bidirectional attention preclude the key-v...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 16. Traditional visual grounding methods primarily focus on single-image scenarios with simple textual references. However, extending these methods to real-world scenarios that involve implicit and complex instructions, particularly in conjunction with multiple images, poses significant challenges, whic...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 17. World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like ...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 18. Machine learning-based interatomic potentials and force fields depend critically on accurate atomic structures, yet such data are scarce due to the limited availability of experimentally resolved crystals. Although atomic-resolution electron microscopy offers a potential source of structural data, c...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 19. Large multimodal models excel in multimodal tasks but face significant computational challenges due to excessive computation on visual tokens. Unlike token reduction methods that focus on token-level redundancy, we identify and study the computation-level redundancy on vision tokens to ensure no inf...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 20. Diffusion models have emerged as powerful generative tools across various domains, yet tailoring pre-trained models to exhibit specific desirable properties remains challenging. While reinforcement learning (RL) offers a promising solution,current methods struggle to simultaneously achieve stable, e...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 21. Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate remarkable potential for scientific discovery. Existing approaches, however, often automate scientific discovery using predefined workflows that lack rationality constraints. This often leads to aimless hypothesizing and a failur...
[22.05.2025 05:12] ********************************************************************************
[22.05.2025 05:12] Abstract 22. In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution's rich information, we propose Mixture of Inputs (MoI), a training-free method ...
[22.05.2025 05:12] Read previous papers.
[22.05.2025 05:12] Generating reviews via LLM API.
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#survey", "#benchmark", "#rl", "#dataset"], "emoji": "üß≠", "ru": {"title": "Web-Shepherd: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Web-Shepherd - –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ (PRM) –¥–ª—è –≤
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#inference"], "emoji": "üî¨", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –æ–±—É—á–µ–Ω–∏—è (QAT) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#training", "#multimodal", "#open_source", "#architecture", "#reasoning", "#diffusion", "#rl"], "emoji": "üß†", "ru": {"title": "MMaDA: –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "MMaDA - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ñ—É–Ω–¥–∞
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#synthetic", "#benchmark", "#transfer_learning", "#training", "#dataset", "#agents"], "emoji": "üñ•Ô∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PC Agent-E - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–º–ø—å—é—Ç–µ
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#diffusion", "#reasoning", "#benchmark", "#training", "#long_context", "#architecture", "#dataset"], "emoji": "üß†", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rag", "#benchmark", "#hallucinations"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –∞–ø—Ä–∏–æ—Ä–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Deliberation over Priors' (DP) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#games", "#agents", "#transfer_learning"], "emoji": "üéÆ", "ru": {"title": "–í–∏–¥–µ–æ–∏–≥—Ä—ã –∫–∞–∫ –ø–æ–ª–∏–≥–æ–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ–∏–≥—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∏ 
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#rag", "#alignment", "#dataset", "#multimodal", "#diffusion"], "emoji": "üñºÔ∏è", "ru": {"title": "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-augmented T2I: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é (T2I) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#synthetic", "#3d"], "emoji": "üèôÔ∏è", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ 3D-–≥–æ—Ä–æ–¥–∞ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "3DTown - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–∞ —Å–≤–µ—Ä—Ö—É, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –≥
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#architecture", "#games", "#robotics", "#diffusion", "#transfer_learning", "#rl", "#agents", "#video"], "emoji": "üé•", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Vid2World, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#math", "#interpretability", "#reasoning", "#benchmark", "#training"], "emoji": "üß†", "ru": {"title": "–ú—è–≥–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ —è–∑—ã–∫–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –ò–ò", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Soft Thinking', –∫–æ—Ç–æ—Ä—ã–π –∏
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#data", "#security", "#open_source", "#training", "#leakage"], "emoji": "üïµÔ∏è", "ru": {"title": "–°–∫—Ä—ã—Ç–∞—è —É–≥—Ä–æ–∑–∞: –∫–∞–∫ –æ—Ç–∫—Ä—ã—Ç—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ä–∞—Å–∫—Ä—ã—Ç—å –≤–∞—à–∏ —Å–µ–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π —Ä–∏—Å–∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#training", "#optimization"], "emoji": "üí°", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ü–µ–ø–æ—á–µ–∫ –º—ã—Å–ª–µ–π –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ LASER –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#data", "#math", "#reasoning", "#training", "#safety"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ LRM: –ø—Ä–æ—Å—Ç–æ—Ç–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–ª—é—á–æ–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–ø–æ—Å–æ–±—ã –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ú–æ–¥–µ–ª–µ–π –ö—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (LRM) —Å –ø–æ–º–æ—â—å—é –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –¢–æ–Ω–∫–æ–π –ù
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#hallucinations", "#reasoning", "#training", "#math"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ò–ò-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –æ—Å–æ–∑–Ω–∞–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –∑–Ω–∞–Ω–∏–π", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) –≤ –º–∞—Ç–µ
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#architecture", "#inference", "#optimization", "#diffusion", "#benchmark"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–≥–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (DLM) - –æ—Ç
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#training", "#multimodal", "#reasoning", "#rl", "#dataset"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UniVG-R1 - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#rlhf", "#training", "#multimodal", "#optimization", "#reasoning", "#games", "#rl", "#agents", "#video"], "emoji": "üåê", "ru": {"title": "RLVR-World: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "RLVR-World - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#data", "#multimodal", "#open_source", "#benchmark", "#dataset", "#agents", "#science"], "emoji": "üî¨", "ru": {"title": "AutoMat: –æ—Ç –º–∏–∫—Ä–æ—Å–∫–æ–ø–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫ –∞—Ç–æ–º–Ω—ã–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º", "desc": "AutoMat - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–∫–∞–Ω–∏—Ä—É—é—â–µ–π –ø—Ä
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#open_source", "#architecture", "#inference"], "emoji": "üîç", "ru": {"title": "ProxyV: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ ProxyV –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#diffusion", "#rl", "#training", "#optimization"], "emoji": "üöÄ", "ru": {"title": "VARD: –£—Å–∏–ª–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º VARD (Value-based Reinforced Diffusion) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#rl", "#agents", "#science"], "emoji": "üß™", "ru": {"title": "PiFlow: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –Ω–∞—É—á–Ω–æ–º –æ—Ç–∫—Ä—ã—Ç–∏–∏", "desc": "PiFlow - —ç—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Å 
[22.05.2025 05:12] Using data from previous issue: {"categories": ["#math", "#training", "#reasoning", "#optimization"], "emoji": "üß†", "ru": {"title": "–°–º–µ—à–∏–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–≤—Ç–æ—Ä–µ–≥—Ä–∞–¥–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Mixture of Inputs (MoI). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä
[22.05.2025 05:12] Loading Chinese text from previous data.
[22.05.2025 05:12] Renaming data file.
[22.05.2025 05:12] Renaming previous data. hf_papers.json to ./d/2025-05-22.json
[22.05.2025 05:12] Saving new data file.
[22.05.2025 05:12] Generating page.
[22.05.2025 05:12] Renaming previous page.
[22.05.2025 05:12] Renaming previous data. index.html to ./d/2025-05-22.html
[22.05.2025 05:12] [Experimental] Generating Chinese page for reading.
[22.05.2025 05:12] Chinese vocab [{'word': 'ÂºÄÊ∫ê', 'pinyin': 'kƒÅi yu√°n', 'trans': 'open source'}, {'word': 'Âü∫Á°ÄÊ®°Âûã', 'pinyin': 'jƒ´ ch«î m√≥ x√≠ng', 'trans': 'foundational model'}, {'word': 'Áªü‰∏Ä', 'pinyin': 't«íng yƒ´', 'trans': 'unify'}, {'word': 'Â§ÑÁêÜ', 'pinyin': 'ch«î l«ê', 'trans': 'process'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generate'}, {'word': 'Ê®°Âºè', 'pinyin': 'm√≥ sh√¨', 'trans': 'mode'}, {'word': 'Êï∞ÊçÆ', 'pinyin': 'sh√π j√π', 'trans': 'data'}, {'word': 'ÊñáÊú¨', 'pinyin': 'w√©n bƒõn', 'trans': 'text'}, {'word': 'ÂõæÂÉè', 'pinyin': 't√∫ xi√†ng', 'trans': 'image'}, {'word': 'ËßÜÈ¢ë', 'pinyin': 'sh√¨ p√≠n', 'trans': 'video'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πn li√†n', 'trans': 'pretrain'}, {'word': 'Â±ïÁé∞', 'pinyin': 'zh«én xi√†n', 'trans': 'demonstrate'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√π z√°', 'trans': 'complex'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reasoning'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'ability'}, {'word': 'Ë∂ÖË∂ä', 'pinyin': 'chƒÅo yu√®', 'trans': 'surpass'}, {'word': 'ÁêÜËß£', 'pinyin': 'l«ê jiƒõ', 'trans': 'understanding'}, {'word': 'ÊñπÈù¢', 'pinyin': 'fƒÅng mi√†n', 'trans': 'aspect'}, {'word': 'È´òÁ∫ß', 'pinyin': 'gƒÅo j√≠', 'trans': 'advanced'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'Âõ¢Èòü', 'pinyin': 'tu√°n du√¨', 'trans': 'team'}, {'word': 'ÂÖ¨ÂºÄ', 'pinyin': 'g≈çng kƒÅi', 'trans': 'public'}, {'word': 'ÂèëÁé∞', 'pinyin': 'fƒÅ xi√†n', 'trans': 'discovery'}, {'word': 'ÁªÜËäÇ', 'pinyin': 'x√¨ jiƒõ', 'trans': 'detail'}, {'word': 'ÂçèËÆÆ', 'pinyin': 'xi√© y√¨', 'trans': 'protocol'}, {'word': 'Ê£ÄÊü•ÁÇπ', 'pinyin': 'ji«én ch√° di«én', 'trans': 'checkpoint'}, {'word': '‰øÉËøõ', 'pinyin': 'c√π j√¨n', 'trans': 'promote'}]
[22.05.2025 05:12] Renaming previous Chinese page.
[22.05.2025 05:12] Renaming previous data. zh.html to ./d/2025-05-21_zh_reading_task.html
[22.05.2025 05:12] Writing Chinese reading task.
[22.05.2025 05:12] Writing result.
[22.05.2025 05:12] Renaming log file.
[22.05.2025 05:12] Renaming previous data. log.txt to ./logs/2025-05-22_last_log.txt
