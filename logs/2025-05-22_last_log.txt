[22.05.2025 03:39] Read previous papers.
[22.05.2025 03:39] Generating top page (month).
[22.05.2025 03:39] Writing top page (month).
[22.05.2025 04:16] Read previous papers.
[22.05.2025 04:16] Get feed.
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15277
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14302
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13909
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15809
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15045
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15210
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15146
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15765
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15778
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15404
[22.05.2025 04:16] Extract page data from URL. URL: https://huggingface.co/papers/2505.15779
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15656
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14357
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13529
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15612
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14231
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13934
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12650
[22.05.2025 04:16] Extract page data from URL. URL: https://huggingface.co/papers/2505.15781
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15816
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15791
[22.05.2025 04:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15047
[22.05.2025 04:16] Extract page data from URL. URL: https://huggingface.co/papers/2505.14827
[22.05.2025 04:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.05.2025 04:16] No deleted papers detected.
[22.05.2025 04:16] Downloading and parsing papers (pdf, html). Total: 23.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.15277.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.15277.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.15277.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.14302.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.14302.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.14302.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.13909.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.13909.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.13909.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.15809.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.15809.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.15809.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.15045.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.15045.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.15045.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.15210.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.15210.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.15210.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.15146.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.15146.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.15146.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.15765.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.15765.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.15765.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.15778.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.15778.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.15778.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.15404.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.15404.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.15404.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.15779.
[22.05.2025 04:16] Downloading paper 2505.15779 from http://arxiv.org/pdf/2505.15779v1...
[22.05.2025 04:16] Extracting affiliations from text.
[22.05.2025 04:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 9 7 7 5 1 . 5 0 5 2 : r IA-T2I: Internet-Augmented Text-to-Image Generation Chuanhao Li1*, Jianwen Sun2,5*, Yukang Feng2,5*, Mingliang Zhai3, Yifan Chang4,5, Kaipeng Zhang1,5(cid:0) 1Shanghai AI Laboratory 2Nankai University 3Beijing Institute of Technology 4University of Science and Technology of China 5Shanghai Innovation Institute "
[22.05.2025 04:16] Response: ```python
["Shanghai AI Laboratory", "Nankai University", "Beijing Institute of Technology", "University of Science and Technology of China", "Shanghai Innovation Institute"]
```
[22.05.2025 04:16] Deleting PDF ./assets/pdf/2505.15779.pdf.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.15656.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.15656.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.15656.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.14357.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.14357.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.14357.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.13529.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.13529.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.13529.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.15612.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.15612.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.15612.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.14231.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.14231.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.14231.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.13934.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.13934.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.13934.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.12650.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.12650.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.12650.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.15781.
[22.05.2025 04:16] Downloading paper 2505.15781 from http://arxiv.org/pdf/2505.15781v1...
[22.05.2025 04:16] Extracting affiliations from text.
[22.05.2025 04:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 1 8 7 5 1 . 5 0 5 2 : r dKV-Cache: The Cache for Diffusion Language Models Xinyin Ma Runpeng Yu Gongfan Fang Xinchao Wang National University of Singapore maxinyin@u.nus.edu, xinchao@nus.edu.sg "
[22.05.2025 04:16] Response: ```python
["National University of Singapore"]
```
[22.05.2025 04:16] Deleting PDF ./assets/pdf/2505.15781.pdf.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.15816.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.15816.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.15816.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.15791.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.15791.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.15791.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.15047.
[22.05.2025 04:16] Extra JSON file exists (./assets/json/2505.15047.json), skip PDF parsing.
[22.05.2025 04:16] Paper image links file exists (./assets/img_data/2505.15047.json), skip HTML parsing.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2505.14827.
[22.05.2025 04:16] Downloading paper 2505.14827 from http://arxiv.org/pdf/2505.14827v1...
[22.05.2025 04:16] Extracting affiliations from text.
[22.05.2025 04:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 2 ] . [ 1 7 2 8 4 1 . 5 0 5 2 : r a Yufan Zhuang1, Liyuan Liu2, Chandan Singh2, Jingbo Shang1, and Jianfeng Gao2 1UC San Diego 2Microsoft Research "
[22.05.2025 04:16] Response: ```python
["UC San Diego", "Microsoft Research"]
```
[22.05.2025 04:16] Deleting PDF ./assets/pdf/2505.14827.pdf.
[22.05.2025 04:16] Success.
[22.05.2025 04:16] Enriching papers with extra data.
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 0. Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks. Yet, specialized reward models for web navigation that can be utilized during bot...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 1. Large language models (LLMs) demand substantial computational and memory resources, creating deployment challenges. Quantization-aware training (QAT) addresses these challenges by reducing model precision while maintaining performance. However, the scaling behavior of QAT, especially at 4-bit precis...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 2. Scaling up high-quality trajectory data has long been a critical bottleneck for developing human-like computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 human-annotated co...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 3. We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a un...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 4. Large language model (LLM)-based embedding models, benefiting from large scale pre-training and post-training, have begun to surpass BERT and T5-based models on general-purpose text embedding tasks such as document retrieval. However, a fundamental limitation of LLM embeddings lies in the unidirecti...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 5. Knowledge graph-based retrieval-augmented generation seeks to mitigate hallucinations in Large Language Models (LLMs) caused by insufficient or outdated knowledge. However, existing methods often fail to fully exploit the prior knowledge embedded in knowledge graphs (KGs), particularly their structu...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 6. Playing video games requires perception, memory, and planning, exactly the faculties modern large language model (LLM) agents are expected to master. We study the major challenges in using popular video games to evaluate modern LLMs and find that directly dropping LLMs into games cannot make an effe...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 7. Acquiring detailed 3D scenes typically demands costly equipment, multi-view data, or labor-intensive modeling. Therefore, a lightweight alternative, generating complex 3D scenes from a single top-down image, plays an essential role in real-world applications. While recent 3D generative models have a...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 8. Human cognition typically involves thinking through abstract, fluid concepts rather than strictly using discrete linguistic tokens. Current reasoning models, however, are constrained to reasoning within the boundaries of human language, processing discrete token embeddings that represent fixed point...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 9. Large Reasoning Models (LRMs) have achieved remarkable success on reasoning-intensive tasks such as mathematics and programming. However, their enhanced reasoning capabilities do not necessarily translate to improved safety performance-and in some cases, may even degrade it. This raises an important...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 10. Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, a T2I model released in February would struggle to generate a suitable poster for a movie premiering in April, because the...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 11. Fine-tuning on open-source Large Language Models (LLMs) with proprietary data is now a standard practice for downstream developers to obtain task-specific LLMs. Surprisingly, we reveal a new and concerning risk along with the practice: the creator of the open-source LLMs can later extract the privat...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 12. World models, which predict transitions based on history observation and action sequences, have shown great promise in improving data efficiency for sequential decision making. However, existing world models often require extensive domain-specific training and still produce low-fidelity, coarse pred...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 13. Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with "I don't know". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 14. Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 15. Traditional visual grounding methods primarily focus on single-image scenarios with simple textual references. However, extending these methods to real-world scenarios that involve implicit and complex instructions, particularly in conjunction with multiple images, poses significant challenges, whic...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 16. World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like ...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 17. Machine learning-based interatomic potentials and force fields depend critically on accurate atomic structures, yet such data are scarce due to the limited availability of experimentally resolved crystals. Although atomic-resolution electron microscopy offers a potential source of structural data, c...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 18. Diffusion Language Models (DLMs) have been seen as a promising competitor for autoregressive language models. However, diffusion language models have long been constrained by slow inference. A core challenge is that their non-autoregressive architecture and bidirectional attention preclude the key-v...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 19. Large multimodal models excel in multimodal tasks but face significant computational challenges due to excessive computation on visual tokens. Unlike token reduction methods that focus on token-level redundancy, we identify and study the computation-level redundancy on vision tokens to ensure no inf...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 20. Diffusion models have emerged as powerful generative tools across various domains, yet tailoring pre-trained models to exhibit specific desirable properties remains challenging. While reinforcement learning (RL) offers a promising solution,current methods struggle to simultaneously achieve stable, e...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 21. Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate remarkable potential for scientific discovery. Existing approaches, however, often automate scientific discovery using predefined workflows that lack rationality constraints. This often leads to aimless hypothesizing and a failur...
[22.05.2025 04:16] ********************************************************************************
[22.05.2025 04:16] Abstract 22. In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution's rich information, we propose Mixture of Inputs (MoI), a training-free method ...
[22.05.2025 04:16] Read previous papers.
[22.05.2025 04:16] Generating reviews via LLM API.
[22.05.2025 04:16] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#survey", "#benchmark", "#rl", "#dataset"], "emoji": "üß≠", "ru": {"title": "Web-Shepherd: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Web-Shepherd - –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ (PRM) –¥–ª—è –≤
[22.05.2025 04:16] Using data from previous issue: {"categories": ["#training", "#optimization", "#inference"], "emoji": "üî¨", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –æ–±—É—á–µ–Ω–∏—è (QAT) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä
[22.05.2025 04:16] Using data from previous issue: {"categories": ["#synthetic", "#benchmark", "#transfer_learning", "#training", "#dataset", "#agents"], "emoji": "üñ•Ô∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PC Agent-E - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–º–ø—å—é—Ç–µ
[22.05.2025 04:16] Using data from previous issue: {"categories": ["#training", "#multimodal", "#open_source", "#architecture", "#reasoning", "#diffusion", "#rl"], "emoji": "üß†", "ru": {"title": "MMaDA: –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "MMaDA - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ñ—É–Ω–¥–∞
[22.05.2025 04:16] Using data from previous issue: {"categories": ["#diffusion", "#reasoning", "#benchmark", "#training", "#long_context", "#architecture", "#dataset"], "emoji": "üß†", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥
[22.05.2025 04:16] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rag", "#benchmark", "#hallucinations"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –∞–ø—Ä–∏–æ—Ä–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Deliberation over Priors' (DP) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç
[22.05.2025 04:16] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#games", "#agents", "#transfer_learning"], "emoji": "üéÆ", "ru": {"title": "–í–∏–¥–µ–æ–∏–≥—Ä—ã –∫–∞–∫ –ø–æ–ª–∏–≥–æ–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ–∏–≥—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∏ 
[22.05.2025 04:16] Using data from previous issue: {"categories": ["#synthetic", "#3d"], "emoji": "üèôÔ∏è", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ 3D-–≥–æ—Ä–æ–¥–∞ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "3DTown - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–∞ —Å–≤–µ—Ä—Ö—É, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –≥
[22.05.2025 04:16] Using data from previous issue: {"categories": ["#math", "#interpretability", "#reasoning", "#benchmark", "#training"], "emoji": "üß†", "ru": {"title": "–ú—è–≥–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ —è–∑—ã–∫–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –ò–ò", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Soft Thinking', –∫–æ—Ç–æ—Ä—ã–π –∏
[22.05.2025 04:16] Using data from previous issue: {"categories": ["#data", "#math", "#reasoning", "#training", "#safety"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ LRM: –ø—Ä–æ—Å—Ç–æ—Ç–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–ª—é—á–æ–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–ø–æ—Å–æ–±—ã –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ú–æ–¥–µ–ª–µ–π –ö—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (LRM) —Å –ø–æ–º–æ—â—å—é –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –¢–æ–Ω–∫–æ–π –ù
[22.05.2025 04:16] Querying the API.
[22.05.2025 04:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, a T2I model released in February would struggle to generate a suitable poster for a movie premiering in April, because the character designs and styles are uncertain to the model. To solve this problem, we propose an Internet-Augmented text-to-image generation (IA-T2I) framework to compel T2I models clear about such uncertain knowledge by providing them with reference images. Specifically, an active retrieval module is designed to determine whether a reference image is needed based on the given text prompt; a hierarchical image selection module is introduced to find the most suitable image returned by an image search engine to enhance the T2I model; a self-reflection mechanism is presented to continuously evaluate and refine the generated image to ensure faithful alignment with the text prompt. To evaluate the proposed framework's performance, we collect a dataset named Img-Ref-T2I, where text prompts include three types of uncertain knowledge: (1) known but rare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt to guide GPT-4o in making preference evaluation, which has been shown to have an evaluation accuracy similar to that of human preference evaluation. Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4o by about 30% in human evaluation.
[22.05.2025 04:16] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é (T2I) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Ä–µ—Å—É—Ä—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ IA-T2I, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∑–Ω–∞–Ω–∏–π –≤ –ø—Ä–æ–º–ø—Ç–∞—Ö –ø—É—Ç–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥—É–ª—å –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–µ—Ö–∞–Ω–∏–∑–º —Å–∞–º–æ–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ Img-Ref-T2I –∏ –æ—Ü–µ–Ω–∫–æ–π —Å –ø–æ–º–æ—â—å—é GPT-4.",
  "emoji": "üñºÔ∏è",
  "title": "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-augmented T2I: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
}
[22.05.2025 04:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, a T2I model released in February would struggle to generate a suitable poster for a movie premiering in April, because the character designs and styles are uncertain to the model. To solve this problem, we propose an Internet-Augmented text-to-image generation (IA-T2I) framework to compel T2I models clear about such uncertain knowledge by providing them with reference images. Specifically, an active retrieval module is designed to determine whether a reference image is needed based on the given text prompt; a hierarchical image selection module is introduced to find the most suitable image returned by an image search engine to enhance the T2I model; a self-reflection mechanism is presented to continuously evaluate and refine the generated image to ensure faithful alignment with the text prompt. To evaluate the proposed framework's performance, we collect a dataset named Img-Ref-T2I, where text prompts include three types of uncertain knowledge: (1) known but rare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt to guide GPT-4o in making preference evaluation, which has been shown to have an evaluation accuracy similar to that of human preference evaluation. Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4o by about 30% in human evaluation."

[22.05.2025 04:16] Response: ```python
['DATASET', 'RAG', 'MULTIMODAL']
```
[22.05.2025 04:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, a T2I model released in February would struggle to generate a suitable poster for a movie premiering in April, because the character designs and styles are uncertain to the model. To solve this problem, we propose an Internet-Augmented text-to-image generation (IA-T2I) framework to compel T2I models clear about such uncertain knowledge by providing them with reference images. Specifically, an active retrieval module is designed to determine whether a reference image is needed based on the given text prompt; a hierarchical image selection module is introduced to find the most suitable image returned by an image search engine to enhance the T2I model; a self-reflection mechanism is presented to continuously evaluate and refine the generated image to ensure faithful alignment with the text prompt. To evaluate the proposed framework's performance, we collect a dataset named Img-Ref-T2I, where text prompts include three types of uncertain knowledge: (1) known but rare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt to guide GPT-4o in making preference evaluation, which has been shown to have an evaluation accuracy similar to that of human preference evaluation. Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4o by about 30% in human evaluation."

[22.05.2025 04:16] Response: ```python
["ALIGNMENT", "DIFFUSION"]
```
[22.05.2025 04:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces the Internet-Augmented text-to-image generation (IA-T2I) framework, which enhances traditional text-to-image (T2I) models by addressing uncertainties in text prompts. The framework includes an active retrieval module to assess the need for reference images, a hierarchical image selection module to find the best matching images, and a self-reflection mechanism for continuous improvement of generated images. A new dataset, Img-Ref-T2I, is created to test the framework, featuring prompts with various types of uncertain knowledge. Experimental results show that IA-T2I significantly improves the quality of generated images, outperforming existing models in human evaluations.","title":"Enhancing T2I Models with Internet-Augmented Knowledge"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces the Internet-Augmented text-to-image generation (IA-T2I) framework, which enhances traditional text-to-image (T2I) models by addressing uncertainties in text prompts. The framework includes an active retrieval module to assess the need for reference images, a hierarchical image selection module to find the best matching images, and a self-reflection mechanism for continuous improvement of generated images. A new dataset, Img-Ref-T2I, is created to test the framework, featuring prompts with various types of uncertain knowledge. Experimental results show that IA-T2I significantly improves the quality of generated images, outperforming existing models in human evaluations.', title='Enhancing T2I Models with Internet-Augmented Knowledge'))
[22.05.2025 04:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ÂΩìÂâçÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊ®°ÂûãÂú®Â§ÑÁêÜÊñáÊú¨ÊèêÁ§∫‰∏≠ÈöêÂê´ÁöÑ‰∏çÁ°ÆÂÆöÁü•ËØÜÊó∂Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∫íËÅîÁΩëÂ¢ûÂº∫ÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊ°ÜÊû∂ÔºàIA-T2IÔºâÔºåÈÄöËøáÊèê‰æõÂèÇËÄÉÂõæÂÉèÊù•Â∏ÆÂä©Ê®°ÂûãÁêÜËß£‰∏çÁ°ÆÂÆöÁöÑÁü•ËØÜ„ÄÇËØ•Ê°ÜÊû∂ÂåÖÊã¨‰∏ªÂä®Ê£ÄÁ¥¢Ê®°Âùó„ÄÅÂàÜÂ±ÇÂõæÂÉèÈÄâÊã©Ê®°ÂùóÂíåËá™ÊàëÂèçÊÄùÊú∫Âà∂Ôºå‰ª•ÊèêÈ´òÁîüÊàêÂõæÂÉèÁöÑË¥®ÈáèÂíå‰∏éÊñáÊú¨ÊèêÁ§∫ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ°ÜÊû∂Âú®ÊÄßËÉΩ‰∏ä‰ºò‰∫éÁé∞ÊúâÊ®°ÂûãÔºåÁâπÂà´ÊòØÂú®Â§ÑÁêÜÂ§çÊùÇÂíå‰∏çÁ°ÆÂÆöÁöÑÊñáÊú¨ÊèêÁ§∫Êó∂„ÄÇ","title":"‰∫íËÅîÁΩëÂ¢ûÂº∫ÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊ°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ÂΩìÂâçÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊ®°ÂûãÂú®Â§ÑÁêÜÊñáÊú¨ÊèêÁ§∫‰∏≠ÈöêÂê´ÁöÑ‰∏çÁ°ÆÂÆöÁü•ËØÜÊó∂Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∫íËÅîÁΩëÂ¢ûÂº∫ÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊ°ÜÊû∂ÔºàIA-T2IÔºâÔºåÈÄöËøáÊèê‰æõÂèÇËÄÉÂõæÂÉèÊù•Â∏ÆÂä©Ê®°ÂûãÁêÜËß£‰∏çÁ°ÆÂÆöÁöÑÁü•ËØÜ„ÄÇËØ•Ê°ÜÊû∂ÂåÖÊã¨‰∏ªÂä®Ê£ÄÁ¥¢Ê®°Âùó„ÄÅÂàÜÂ±ÇÂõæÂÉèÈÄâÊã©Ê®°ÂùóÂíåËá™ÊàëÂèçÊÄùÊú∫Âà∂Ôºå‰ª•ÊèêÈ´òÁîüÊàêÂõæÂÉèÁöÑË¥®ÈáèÂíå‰∏éÊñáÊú¨ÊèêÁ§∫ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ°ÜÊû∂Âú®ÊÄßËÉΩ‰∏ä‰ºò‰∫éÁé∞ÊúâÊ®°ÂûãÔºåÁâπÂà´ÊòØÂú®Â§ÑÁêÜÂ§çÊùÇÂíå‰∏çÁ°ÆÂÆöÁöÑÊñáÊú¨ÊèêÁ§∫Êó∂„ÄÇ', title='‰∫íËÅîÁΩëÂ¢ûÂº∫ÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊ°ÜÊû∂'))
[22.05.2025 04:17] Using data from previous issue: {"categories": ["#data", "#security", "#open_source", "#training", "#leakage"], "emoji": "üïµÔ∏è", "ru": {"title": "–°–∫—Ä—ã—Ç–∞—è —É–≥—Ä–æ–∑–∞: –∫–∞–∫ –æ—Ç–∫—Ä—ã—Ç—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ä–∞—Å–∫—Ä—ã—Ç—å –≤–∞—à–∏ —Å–µ–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π —Ä–∏—Å–∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞
[22.05.2025 04:17] Using data from previous issue: {"categories": ["#architecture", "#games", "#robotics", "#diffusion", "#transfer_learning", "#rl", "#agents", "#video"], "emoji": "üé•", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Vid2World, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω
[22.05.2025 04:17] Using data from previous issue: {"categories": ["#hallucinations", "#reasoning", "#training", "#math"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ò–ò-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –æ—Å–æ–∑–Ω–∞–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –∑–Ω–∞–Ω–∏–π", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) –≤ –º–∞—Ç–µ
[22.05.2025 04:17] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#training", "#optimization"], "emoji": "üí°", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ü–µ–ø–æ—á–µ–∫ –º—ã—Å–ª–µ–π –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ LASER –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ
[22.05.2025 04:17] Using data from previous issue: {"categories": ["#training", "#multimodal", "#reasoning", "#rl", "#dataset"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UniVG-R1 - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è
[22.05.2025 04:17] Using data from previous issue: {"categories": ["#rlhf", "#training", "#multimodal", "#optimization", "#reasoning", "#games", "#rl", "#agents", "#video"], "emoji": "üåê", "ru": {"title": "RLVR-World: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "RLVR-World - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫
[22.05.2025 04:17] Using data from previous issue: {"categories": ["#data", "#multimodal", "#open_source", "#benchmark", "#dataset", "#agents", "#science"], "emoji": "üî¨", "ru": {"title": "AutoMat: –æ—Ç –º–∏–∫—Ä–æ—Å–∫–æ–ø–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫ –∞—Ç–æ–º–Ω—ã–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º", "desc": "AutoMat - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–∫–∞–Ω–∏—Ä—É—é—â–µ–π –ø—Ä
[22.05.2025 04:17] Querying the API.
[22.05.2025 04:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Diffusion Language Models (DLMs) have been seen as a promising competitor for autoregressive language models. However, diffusion language models have long been constrained by slow inference. A core challenge is that their non-autoregressive architecture and bidirectional attention preclude the key-value cache that accelerates decoding. We address this bottleneck by proposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising process of DLMs. Our approach is motivated by the observation that different tokens have distinct representation dynamics throughout the diffusion process. Accordingly, we propose a delayed and conditioned caching strategy for key and value states. We design two complementary variants to cache key and value step-by-step: (1) dKV-Cache-Decode, which provides almost lossless acceleration, and even improves performance on long sequences, suggesting that existing DLMs may under-utilise contextual information during inference. (2) dKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving higher speed-ups with quadratic time complexity at the cost of some performance degradation. dKV-Cache, in final, achieves from 2-10x speedup in inference, largely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on several benchmarks, delivering acceleration across general language understanding, mathematical, and code-generation benchmarks. Experiments demonstrate that cache can also be used in DLMs, even in a training-free manner from current DLMs.
[22.05.2025 04:17] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (DLM) - –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–µ KV-–∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ (dKV-Cache). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞: dKV-Cache-Decode, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö, –∏ dKV-Cache-Greedy –¥–ª—è –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å 2-10-–∫—Ä–∞—Ç–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ DLM, —Å–æ–∫—Ä–∞—â–∞—è —Ä–∞–∑—Ä—ã–≤ —Å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞, –º–∞—Ç–µ–º–∞—Ç–∏–∫—É –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞.",
  "emoji": "üöÄ",
  "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–≥–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"
}
[22.05.2025 04:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Diffusion Language Models (DLMs) have been seen as a promising competitor for autoregressive language models. However, diffusion language models have long been constrained by slow inference. A core challenge is that their non-autoregressive architecture and bidirectional attention preclude the key-value cache that accelerates decoding. We address this bottleneck by proposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising process of DLMs. Our approach is motivated by the observation that different tokens have distinct representation dynamics throughout the diffusion process. Accordingly, we propose a delayed and conditioned caching strategy for key and value states. We design two complementary variants to cache key and value step-by-step: (1) dKV-Cache-Decode, which provides almost lossless acceleration, and even improves performance on long sequences, suggesting that existing DLMs may under-utilise contextual information during inference. (2) dKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving higher speed-ups with quadratic time complexity at the cost of some performance degradation. dKV-Cache, in final, achieves from 2-10x speedup in inference, largely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on several benchmarks, delivering acceleration across general language understanding, mathematical, and code-generation benchmarks. Experiments demonstrate that cache can also be used in DLMs, even in a training-free manner from current DLMs."

[22.05.2025 04:17] Response: ```python
['INFERENCE', 'BENCHMARK', 'ARCHITECTURE']
```
[22.05.2025 04:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Diffusion Language Models (DLMs) have been seen as a promising competitor for autoregressive language models. However, diffusion language models have long been constrained by slow inference. A core challenge is that their non-autoregressive architecture and bidirectional attention preclude the key-value cache that accelerates decoding. We address this bottleneck by proposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising process of DLMs. Our approach is motivated by the observation that different tokens have distinct representation dynamics throughout the diffusion process. Accordingly, we propose a delayed and conditioned caching strategy for key and value states. We design two complementary variants to cache key and value step-by-step: (1) dKV-Cache-Decode, which provides almost lossless acceleration, and even improves performance on long sequences, suggesting that existing DLMs may under-utilise contextual information during inference. (2) dKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving higher speed-ups with quadratic time complexity at the cost of some performance degradation. dKV-Cache, in final, achieves from 2-10x speedup in inference, largely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on several benchmarks, delivering acceleration across general language understanding, mathematical, and code-generation benchmarks. Experiments demonstrate that cache can also be used in DLMs, even in a training-free manner from current DLMs."

[22.05.2025 04:17] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```
[22.05.2025 04:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new mechanism called delayed KV-Cache to improve the inference speed of Diffusion Language Models (DLMs), which traditionally suffer from slow decoding due to their non-autoregressive nature. The authors identify that different tokens exhibit unique representation dynamics during the diffusion process, leading to the development of a caching strategy that optimizes key and value states. They propose two variants of the caching mechanism: dKV-Cache-Decode, which enhances performance on long sequences, and dKV-Cache-Greedy, which prioritizes speed at the expense of some accuracy. Overall, the proposed dKV-Cache achieves a significant speedup of 2-10x in inference, making DLMs more competitive with autoregressive models.","title":"Accelerating Diffusion Language Models with Delayed KV-Cache"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a new mechanism called delayed KV-Cache to improve the inference speed of Diffusion Language Models (DLMs), which traditionally suffer from slow decoding due to their non-autoregressive nature. The authors identify that different tokens exhibit unique representation dynamics during the diffusion process, leading to the development of a caching strategy that optimizes key and value states. They propose two variants of the caching mechanism: dKV-Cache-Decode, which enhances performance on long sequences, and dKV-Cache-Greedy, which prioritizes speed at the expense of some accuracy. Overall, the proposed dKV-Cache achieves a significant speedup of 2-10x in inference, making DLMs more competitive with autoregressive models.', title='Accelerating Diffusion Language Models with Delayed KV-Cache'))
[22.05.2025 04:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÔºàDLMsÔºâË¢´ËßÜ‰∏∫Ëá™ÂõûÂΩíËØ≠Ë®ÄÊ®°ÂûãÁöÑÊúâÂäõÁ´û‰∫âËÄÖÔºå‰ΩÜÂÖ∂Êé®ÁêÜÈÄüÂ∫¶ËæÉÊÖ¢ÊòØ‰∏Ä‰∏™‰∏ªË¶ÅÈóÆÈ¢ò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂª∂ËøüÈîÆÂÄºÁºìÂ≠òÔºàdKV-CacheÔºâÊú∫Âà∂Ôºå‰ª•Ëß£ÂÜ≥DLMsÂú®ÂéªÂô™ËøáÁ®ã‰∏≠ÁöÑÁì∂È¢à„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏§Áßç‰∫íË°•ÁöÑÁºìÂ≠òÂèò‰ΩìÔºåÂàÜÂà´‰∏∫dKV-Cache-DecodeÂíådKV-Cache-GreedyÔºåÂâçËÄÖÂú®ÈïøÂ∫èÂàó‰∏äÂá†‰πéÊó†ÊçüÂä†ÈÄüÔºåÂêéËÄÖÂàô‰ª•Êõ¥È´òÁöÑÈÄüÂ∫¶ÂÆûÁé∞‰∫Ü‰∫åÊ¨°Êó∂Èó¥Â§çÊùÇÂ∫¶ÁöÑÂä†ÈÄü„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºådKV-CacheÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜÈÄüÂ∫¶ÔºåÁº©Â∞è‰∫ÜËá™ÂõûÂΩíÊ®°Âûã‰∏éÊâ©Êï£ËØ≠Ë®ÄÊ®°Âûã‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ","title":"Âä†ÈÄüÊâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÈÄüÂ∫¶"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÔºàDLMsÔºâË¢´ËßÜ‰∏∫Ëá™ÂõûÂΩíËØ≠Ë®ÄÊ®°ÂûãÁöÑÊúâÂäõÁ´û‰∫âËÄÖÔºå‰ΩÜÂÖ∂Êé®ÁêÜÈÄüÂ∫¶ËæÉÊÖ¢ÊòØ‰∏Ä‰∏™‰∏ªË¶ÅÈóÆÈ¢ò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂª∂ËøüÈîÆÂÄºÁºìÂ≠òÔºàdKV-CacheÔºâÊú∫Âà∂Ôºå‰ª•Ëß£ÂÜ≥DLMsÂú®ÂéªÂô™ËøáÁ®ã‰∏≠ÁöÑÁì∂È¢à„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏§Áßç‰∫íË°•ÁöÑÁºìÂ≠òÂèò‰ΩìÔºåÂàÜÂà´‰∏∫dKV-Cache-DecodeÂíådKV-Cache-GreedyÔºåÂâçËÄÖÂú®ÈïøÂ∫èÂàó‰∏äÂá†‰πéÊó†ÊçüÂä†ÈÄüÔºåÂêéËÄÖÂàô‰ª•Êõ¥È´òÁöÑÈÄüÂ∫¶ÂÆûÁé∞‰∫Ü‰∫åÊ¨°Êó∂Èó¥Â§çÊùÇÂ∫¶ÁöÑÂä†ÈÄü„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºådKV-CacheÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜÈÄüÂ∫¶ÔºåÁº©Â∞è‰∫ÜËá™ÂõûÂΩíÊ®°Âûã‰∏éÊâ©Êï£ËØ≠Ë®ÄÊ®°Âûã‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ', title='Âä†ÈÄüÊâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÈÄüÂ∫¶'))
[22.05.2025 04:17] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#open_source", "#architecture", "#inference"], "emoji": "üîç", "ru": {"title": "ProxyV: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ ProxyV –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã
[22.05.2025 04:17] Using data from previous issue: {"categories": ["#diffusion", "#rl", "#training", "#optimization"], "emoji": "üöÄ", "ru": {"title": "VARD: –£—Å–∏–ª–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º VARD (Value-based Reinforced Diffusion) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ
[22.05.2025 04:17] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#rl", "#agents", "#science"], "emoji": "üß™", "ru": {"title": "PiFlow: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –Ω–∞—É—á–Ω–æ–º –æ—Ç–∫—Ä—ã—Ç–∏–∏", "desc": "PiFlow - —ç—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Å 
[22.05.2025 04:17] Querying the API.
[22.05.2025 04:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution's rich information, we propose Mixture of Inputs (MoI), a training-free method for autoregressive generation. After generating a token following the standard paradigm, we construct a new input that blends the generated discrete token with the previously discarded token distribution. Specifically, we employ a Bayesian estimation method that treats the token distribution as the prior, the sampled token as the observation, and replaces the conventional one-hot vector with the continuous posterior expectation as the new model input. MoI allows the model to maintain a richer internal representation throughout the generation process, resulting in improved text quality and reasoning capabilities. On mathematical reasoning, code generation, and PhD-level QA tasks, MoI consistently improves performance across multiple models including QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional training and negligible computational overhead.
[22.05.2025 04:17] Response: {
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–≤—Ç–æ—Ä–µ–≥—Ä–∞–¥–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Mixture of Inputs (MoI). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, MoI —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Ç–æ–∫–µ–Ω–æ–≤, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∞–π–µ—Å–æ–≤—Å–∫—É—é –æ—Ü–µ–Ω–∫—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ –≤—Ö–æ–¥–∞ –º–æ–¥–µ–ª–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–µ–≥–æ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π. MoI –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –±–æ–ª–µ–µ –±–æ–≥–∞—Ç–æ–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —á—Ç–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞ –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.",

  "emoji": "üß†",

  "title": "–°–º–µ—à–∏–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[22.05.2025 04:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution's rich information, we propose Mixture of Inputs (MoI), a training-free method for autoregressive generation. After generating a token following the standard paradigm, we construct a new input that blends the generated discrete token with the previously discarded token distribution. Specifically, we employ a Bayesian estimation method that treats the token distribution as the prior, the sampled token as the observation, and replaces the conventional one-hot vector with the continuous posterior expectation as the new model input. MoI allows the model to maintain a richer internal representation throughout the generation process, resulting in improved text quality and reasoning capabilities. On mathematical reasoning, code generation, and PhD-level QA tasks, MoI consistently improves performance across multiple models including QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional training and negligible computational overhead."

[22.05.2025 04:17] Response: ```python
["TRAINING", "MATH"]
```
[22.05.2025 04:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution's rich information, we propose Mixture of Inputs (MoI), a training-free method for autoregressive generation. After generating a token following the standard paradigm, we construct a new input that blends the generated discrete token with the previously discarded token distribution. Specifically, we employ a Bayesian estimation method that treats the token distribution as the prior, the sampled token as the observation, and replaces the conventional one-hot vector with the continuous posterior expectation as the new model input. MoI allows the model to maintain a richer internal representation throughout the generation process, resulting in improved text quality and reasoning capabilities. On mathematical reasoning, code generation, and PhD-level QA tasks, MoI consistently improves performance across multiple models including QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional training and negligible computational overhead."

[22.05.2025 04:17] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[22.05.2025 04:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a novel method called Mixture of Inputs (MoI) for enhancing autoregressive generation in large language models (LLMs). Instead of discarding the token distribution after sampling a token, MoI combines the generated token with the previously discarded distribution to create a richer input representation. By using Bayesian estimation, MoI treats the token distribution as a prior and the sampled token as an observation, allowing for a continuous posterior expectation to be used as input. This approach leads to improved performance in tasks such as mathematical reasoning, code generation, and PhD-level question answering without requiring additional training.","title":"Enhancing Autoregressive Generation with Mixture of Inputs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a novel method called Mixture of Inputs (MoI) for enhancing autoregressive generation in large language models (LLMs). Instead of discarding the token distribution after sampling a token, MoI combines the generated token with the previously discarded distribution to create a richer input representation. By using Bayesian estimation, MoI treats the token distribution as a prior and the sampled token as an observation, allowing for a continuous posterior expectation to be used as input. This approach leads to improved performance in tasks such as mathematical reasoning, code generation, and PhD-level question answering without requiring additional training.', title='Enhancing Autoregressive Generation with Mixture of Inputs'))
[22.05.2025 04:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Âú®Ê†áÂáÜÁöÑËá™ÂõûÂΩíÁîüÊàê‰∏≠ÔºåËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑÊµã‰∏ã‰∏Ä‰∏™Ê†áËÆ∞ÁöÑÂàÜÂ∏ÉÔºåÈááÊ†∑‰∏Ä‰∏™Á¶ªÊï£Ê†áËÆ∞ÔºåÁÑ∂Âêé‰∏¢ÂºÉËØ•ÂàÜÂ∏ÉÔºå‰ªÖÂ∞ÜÈááÊ†∑ÁöÑÊ†áËÆ∞‰Ωú‰∏∫Êñ∞ËæìÂÖ•„ÄÇ‰∏∫‰∫Ü‰øùÁïôËøô‰∏™ÂàÜÂ∏ÉÁöÑ‰∏∞ÂØå‰ø°ÊÅØÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Ê∑∑ÂêàËæìÂÖ•ÔºàMoIÔºâÁöÑÊñπÊ≥ïÔºåÂÆÉ‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉ„ÄÇËØ•ÊñπÊ≥ïÂú®ÁîüÊàêÊ†áËÆ∞ÂêéÔºåÊûÑÂª∫‰∏Ä‰∏™Êñ∞ÁöÑËæìÂÖ•ÔºåÂ∞ÜÁîüÊàêÁöÑÁ¶ªÊï£Ê†áËÆ∞‰∏é‰πãÂâç‰∏¢ÂºÉÁöÑÊ†áËÆ∞ÂàÜÂ∏ÉÊ∑∑Âêà„ÄÇMoI‰ΩøÊ®°ÂûãÂú®ÁîüÊàêËøáÁ®ã‰∏≠‰øùÊåÅÊõ¥‰∏∞ÂØåÁöÑÂÜÖÈÉ®Ë°®Á§∫Ôºå‰ªéËÄåÊèêÈ´òÊñáÊú¨Ë¥®ÈáèÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇ","title":"Ê∑∑ÂêàËæìÂÖ•ÔºöÊèêÂçáËá™ÂõûÂΩíÁîüÊàêÁöÑË¥®Èáè‰∏éÊé®ÁêÜËÉΩÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Âú®Ê†áÂáÜÁöÑËá™ÂõûÂΩíÁîüÊàê‰∏≠ÔºåËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑÊµã‰∏ã‰∏Ä‰∏™Ê†áËÆ∞ÁöÑÂàÜÂ∏ÉÔºåÈááÊ†∑‰∏Ä‰∏™Á¶ªÊï£Ê†áËÆ∞ÔºåÁÑ∂Âêé‰∏¢ÂºÉËØ•ÂàÜÂ∏ÉÔºå‰ªÖÂ∞ÜÈááÊ†∑ÁöÑÊ†áËÆ∞‰Ωú‰∏∫Êñ∞ËæìÂÖ•„ÄÇ‰∏∫‰∫Ü‰øùÁïôËøô‰∏™ÂàÜÂ∏ÉÁöÑ‰∏∞ÂØå‰ø°ÊÅØÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Ê∑∑ÂêàËæìÂÖ•ÔºàMoIÔºâÁöÑÊñπÊ≥ïÔºåÂÆÉ‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉ„ÄÇËØ•ÊñπÊ≥ïÂú®ÁîüÊàêÊ†áËÆ∞ÂêéÔºåÊûÑÂª∫‰∏Ä‰∏™Êñ∞ÁöÑËæìÂÖ•ÔºåÂ∞ÜÁîüÊàêÁöÑÁ¶ªÊï£Ê†áËÆ∞‰∏é‰πãÂâç‰∏¢ÂºÉÁöÑÊ†áËÆ∞ÂàÜÂ∏ÉÊ∑∑Âêà„ÄÇMoI‰ΩøÊ®°ÂûãÂú®ÁîüÊàêËøáÁ®ã‰∏≠‰øùÊåÅÊõ¥‰∏∞ÂØåÁöÑÂÜÖÈÉ®Ë°®Á§∫Ôºå‰ªéËÄåÊèêÈ´òÊñáÊú¨Ë¥®ÈáèÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇ', title='Ê∑∑ÂêàËæìÂÖ•ÔºöÊèêÂçáËá™ÂõûÂΩíÁîüÊàêÁöÑË¥®Èáè‰∏éÊé®ÁêÜËÉΩÂäõ'))
[22.05.2025 04:17] Loading Chinese text from previous data.
[22.05.2025 04:17] Renaming data file.
[22.05.2025 04:17] Renaming previous data. hf_papers.json to ./d/2025-05-22.json
[22.05.2025 04:17] Saving new data file.
[22.05.2025 04:17] Generating page.
[22.05.2025 04:17] Renaming previous page.
[22.05.2025 04:17] Renaming previous data. index.html to ./d/2025-05-22.html
[22.05.2025 04:17] [Experimental] Generating Chinese page for reading.
[22.05.2025 04:17] Chinese vocab [{'word': 'ÂºÄÊ∫ê', 'pinyin': 'kƒÅi yu√°n', 'trans': 'open source'}, {'word': 'Âü∫Á°ÄÊ®°Âûã', 'pinyin': 'jƒ´ ch«î m√≥ x√≠ng', 'trans': 'foundational model'}, {'word': 'Áªü‰∏Ä', 'pinyin': 't«íng yƒ´', 'trans': 'unify'}, {'word': 'Â§ÑÁêÜ', 'pinyin': 'ch«î l«ê', 'trans': 'process'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generate'}, {'word': 'Ê®°Âºè', 'pinyin': 'm√≥ sh√¨', 'trans': 'mode'}, {'word': 'Êï∞ÊçÆ', 'pinyin': 'sh√π j√π', 'trans': 'data'}, {'word': 'ÊñáÊú¨', 'pinyin': 'w√©n bƒõn', 'trans': 'text'}, {'word': 'ÂõæÂÉè', 'pinyin': 't√∫ xi√†ng', 'trans': 'image'}, {'word': 'ËßÜÈ¢ë', 'pinyin': 'sh√¨ p√≠n', 'trans': 'video'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πn li√†n', 'trans': 'pretrain'}, {'word': 'Â±ïÁé∞', 'pinyin': 'zh«én xi√†n', 'trans': 'demonstrate'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√π z√°', 'trans': 'complex'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reasoning'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'ability'}, {'word': 'Ë∂ÖË∂ä', 'pinyin': 'chƒÅo yu√®', 'trans': 'surpass'}, {'word': 'ÁêÜËß£', 'pinyin': 'l«ê jiƒõ', 'trans': 'understanding'}, {'word': 'ÊñπÈù¢', 'pinyin': 'fƒÅng mi√†n', 'trans': 'aspect'}, {'word': 'È´òÁ∫ß', 'pinyin': 'gƒÅo j√≠', 'trans': 'advanced'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'Âõ¢Èòü', 'pinyin': 'tu√°n du√¨', 'trans': 'team'}, {'word': 'ÂÖ¨ÂºÄ', 'pinyin': 'g≈çng kƒÅi', 'trans': 'public'}, {'word': 'ÂèëÁé∞', 'pinyin': 'fƒÅ xi√†n', 'trans': 'discovery'}, {'word': 'ÁªÜËäÇ', 'pinyin': 'x√¨ jiƒõ', 'trans': 'detail'}, {'word': 'ÂçèËÆÆ', 'pinyin': 'xi√© y√¨', 'trans': 'protocol'}, {'word': 'Ê£ÄÊü•ÁÇπ', 'pinyin': 'ji«én ch√° di«én', 'trans': 'checkpoint'}, {'word': '‰øÉËøõ', 'pinyin': 'c√π j√¨n', 'trans': 'promote'}]
[22.05.2025 04:17] Renaming previous Chinese page.
[22.05.2025 04:17] Renaming previous data. zh.html to ./d/2025-05-21_zh_reading_task.html
[22.05.2025 04:17] Writing Chinese reading task.
[22.05.2025 04:17] Writing result.
[22.05.2025 04:17] Renaming log file.
[22.05.2025 04:17] Renaming previous data. log.txt to ./logs/2025-05-22_last_log.txt
