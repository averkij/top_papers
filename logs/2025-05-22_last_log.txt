[22.05.2025 15:12] Read previous papers.
[22.05.2025 15:12] Generating top page (month).
[22.05.2025 15:12] Writing top page (month).
[22.05.2025 16:14] Read previous papers.
[22.05.2025 16:14] Get feed.
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15277
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14302
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15809
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14231
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15045
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13909
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14766
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15612
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15400
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14357
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15146
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15779
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15765
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15210
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15404
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15817
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15781
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13934
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15778
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15776
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15656
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13529
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14827
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12650
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15791
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15524
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15047
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15034
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15816
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15406
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14818
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14157
[22.05.2025 16:14] Extract page data from URL. URL: https://huggingface.co/papers/2505.15801
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14336
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14101
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11454
[22.05.2025 16:14] Extract page data from URL. URL: https://huggingface.co/papers/2505.15141
[22.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14990
[22.05.2025 16:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.05.2025 16:14] No deleted papers detected.
[22.05.2025 16:14] Downloading and parsing papers (pdf, html). Total: 38.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15277.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15277.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15277.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.14302.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.14302.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.14302.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15809.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15809.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15809.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.14231.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.14231.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.14231.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15045.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15045.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15045.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.13909.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.13909.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.13909.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.14766.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.14766.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.14766.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15612.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15612.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15612.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15400.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15400.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15400.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.14357.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.14357.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.14357.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15146.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15146.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15146.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15779.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15779.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15779.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15765.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15765.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15765.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15210.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15210.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15210.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15404.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15404.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15404.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15817.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15817.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15817.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15781.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15781.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15781.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.13934.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.13934.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.13934.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15778.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15778.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15778.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15776.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15776.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15776.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15656.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15656.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15656.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.13529.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.13529.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.13529.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.14827.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.14827.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.14827.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.12650.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.12650.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.12650.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15791.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15791.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15791.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15524.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15524.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15524.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15047.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15047.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15047.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15034.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15034.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15034.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15816.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15816.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15816.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15406.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.15406.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.15406.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.14818.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.14818.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.14818.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.14157.
[22.05.2025 16:14] Extra JSON file exists (./assets/json/2505.14157.json), skip PDF parsing.
[22.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.14157.json), skip HTML parsing.
[22.05.2025 16:14] Success.
[22.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.15801.
[22.05.2025 16:15] Downloading paper 2505.15801 from http://arxiv.org/pdf/2505.15801v1...
[22.05.2025 16:15] Extracting affiliations from text.
[22.05.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models Yuchen Yan1,2,*, Jin Jiang2,3, Zhenbang Ren1,4, Yijun Li1, Xudong Cai1,5, Yang Liu2, Xin Xu6, Mengdi Zhang2, Jian Shao1,, Yongliang Shen1,, Jun Xiao1, Yueting Zhuang1 1Zhejiang University 2Meituan Group 4University of Electronic Science and Technology of China 5Beijing University of Posts and Telecommunications 6The Hong Kong University of Science and Technology {yanyuchen, syl, jshao}@zju.edu.cn 3Peking University 5 2 0 2 1 ] . [ 1 1 0 8 5 1 . 5 0 5 2 : r a "
[22.05.2025 16:15] Response: ```python
[
    "Zhejiang University",
    "Meituan Group",
    "University of Electronic Science and Technology of China",
    "Beijing University of Posts and Telecommunications",
    "The Hong Kong University of Science and Technology",
    "Peking University"
]
```
[22.05.2025 16:15] Deleting PDF ./assets/pdf/2505.15801.pdf.
[22.05.2025 16:15] Success.
[22.05.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2505.14336.
[22.05.2025 16:15] Extra JSON file exists (./assets/json/2505.14336.json), skip PDF parsing.
[22.05.2025 16:15] Paper image links file exists (./assets/img_data/2505.14336.json), skip HTML parsing.
[22.05.2025 16:15] Success.
[22.05.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2505.14101.
[22.05.2025 16:15] Extra JSON file exists (./assets/json/2505.14101.json), skip PDF parsing.
[22.05.2025 16:15] Paper image links file exists (./assets/img_data/2505.14101.json), skip HTML parsing.
[22.05.2025 16:15] Success.
[22.05.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2505.11454.
[22.05.2025 16:15] Extra JSON file exists (./assets/json/2505.11454.json), skip PDF parsing.
[22.05.2025 16:15] Paper image links file exists (./assets/img_data/2505.11454.json), skip HTML parsing.
[22.05.2025 16:15] Success.
[22.05.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2505.15141.
[22.05.2025 16:15] Downloading paper 2505.15141 from http://arxiv.org/pdf/2505.15141v1...
[22.05.2025 16:15] Extracting affiliations from text.
[22.05.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms Yunlong Hou * 1 Fengzhuo Zhang * 1 Cunxiao Du * 2 Xuan Zhang * 3 Jiachun Pan 1 Tianyu Pang 2 Chao Du 2 Vincent Y. F. Tan 1 Zhuoran Yang 4 Abstract Speculative decoding has emerged as popular method to accelerate the inference of Large Language Models (LLMs) while retaining their superior text generation performance. Previous methods either adopt fixed speculative decoding configuration regardless of the prefix tokens, or train draft models in an offline or online manner to align them with the context. This paper proposes training-free online learning framework to adaptively choose the configuration of the hyperparameters for speculative decoding as text is being generated. We first formulate this hyperparameter selection problem as Multi-Armed Bandit problem and provide general speculative decoding framework BANDITSPEC. Furthermore, two bandit-based hyperparameter selection algorithms, UCBSPEC and EXP3SPEC, are designed and analyzed in terms of novel quantity, the stopping time regret. We upper bound this regret under both stochastic and adversarial reward settings. By deriving an information-theoretic impossibility result, it is shown that the regret performance of UCBSPEC is optimal up to universal constants. Finally, extensive empirical experiments with LLaMA3 and Qwen2 demonstrate that our algorithms are effective compared to existing methods, and the throughput is close to the oracle best hyperparameter in simulated real-life LLM serving scenarios with diverse input prompts. 5 2 0 2 1 2 ] . [ 1 1 4 1 5 1 . 5 0 5 2 : r 1. Introduction Large Language Model (LLM) is trained to predict the probability of the next token conditioned on all previous *Equal contribution Work done as an associate member at Sea AI Lab Project Lead 1National University of Singapore 2Sea AI Lab 3Singapore Management University 4Yale University. Correspondence to: Cunxiao Du <cnsdunm@gmail.com>, Zhuoran Yang <zhuoran.yang@yale."
[22.05.2025 16:15] Response: ```python
[
    "National University of Singapore",
    "Sea AI Lab",
    "Singapore Management University",
    "Yale University"
]
```
[22.05.2025 16:15] Deleting PDF ./assets/pdf/2505.15141.pdf.
[22.05.2025 16:15] Success.
[22.05.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2505.14990.
[22.05.2025 16:15] Extra JSON file exists (./assets/json/2505.14990.json), skip PDF parsing.
[22.05.2025 16:15] Paper image links file exists (./assets/img_data/2505.14990.json), skip HTML parsing.
[22.05.2025 16:15] Success.
[22.05.2025 16:15] Enriching papers with extra data.
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 0. Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks. Yet, specialized reward models for web navigation that can be utilized during bot...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 1. Large language models (LLMs) demand substantial computational and memory resources, creating deployment challenges. Quantization-aware training (QAT) addresses these challenges by reducing model precision while maintaining performance. However, the scaling behavior of QAT, especially at 4-bit precis...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 2. We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a un...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 3. Traditional visual grounding methods primarily focus on single-image scenarios with simple textual references. However, extending these methods to real-world scenarios that involve implicit and complex instructions, particularly in conjunction with multiple images, poses significant challenges, whic...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 4. Large language model (LLM)-based embedding models, benefiting from large scale pre-training and post-training, have begun to surpass BERT and T5-based models on general-purpose text embedding tasks such as document retrieval. However, a fundamental limitation of LLM embeddings lies in the unidirecti...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 5. Scaling up high-quality trajectory data has long been a critical bottleneck for developing human-like computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 human-annotated co...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 6. We introduce Toto, a time series forecasting foundation model with 151 million parameters. Toto uses a modern decoder-only architecture coupled with architectural innovations designed to account for specific challenges found in multivariate observability time series data. Toto's pre-training corpus ...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 7. Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 8. Large reasoning models (LRMs) achieve remarkable performance via long reasoning chains, but often incur excessive computational overhead due to redundant reasoning, especially on simple tasks. In this work, we systematically quantify the upper bounds of LRMs under both Long-Thinking and No-Thinking ...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 9. World models, which predict transitions based on history observation and action sequences, have shown great promise in improving data efficiency for sequential decision making. However, existing world models often require extensive domain-specific training and still produce low-fidelity, coarse pred...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 10. Playing video games requires perception, memory, and planning, exactly the faculties modern large language model (LLM) agents are expected to master. We study the major challenges in using popular video games to evaluate modern LLMs and find that directly dropping LLMs into games cannot make an effe...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 11. Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, a T2I model released in February would struggle to generate a suitable poster for a movie premiering in April, because the...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 12. Acquiring detailed 3D scenes typically demands costly equipment, multi-view data, or labor-intensive modeling. Therefore, a lightweight alternative, generating complex 3D scenes from a single top-down image, plays an essential role in real-world applications. While recent 3D generative models have a...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 13. Knowledge graph-based retrieval-augmented generation seeks to mitigate hallucinations in Large Language Models (LLMs) caused by insufficient or outdated knowledge. However, existing methods often fail to fully exploit the prior knowledge embedded in knowledge graphs (KGs), particularly their structu...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 14. Large Reasoning Models (LRMs) have achieved remarkable success on reasoning-intensive tasks such as mathematics and programming. However, their enhanced reasoning capabilities do not necessarily translate to improved safety performance-and in some cases, may even degrade it. This raises an important...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 15. Human beings naturally utilize multiple reasoning modalities to learn and solve logical problems, i.e., different representational formats such as natural language, code, and symbolic logic. In contrast, most existing LLM-based approaches operate with a single reasoning modality during training, typ...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 16. Diffusion Language Models (DLMs) have been seen as a promising competitor for autoregressive language models. However, diffusion language models have long been constrained by slow inference. A core challenge is that their non-autoregressive architecture and bidirectional attention preclude the key-v...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 17. World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like ...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 18. Human cognition typically involves thinking through abstract, fluid concepts rather than strictly using discrete linguistic tokens. Current reasoning models, however, are constrained to reasoning within the boundaries of human language, processing discrete token embeddings that represent fixed point...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 19. Conversational search systems require effective handling of context-dependent queries that often contain ambiguity, omission, and coreference. Conversational Query Reformulation (CQR) addresses this challenge by transforming these queries into self-contained forms suitable for off-the-shelf retrieve...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 20. Fine-tuning on open-source Large Language Models (LLMs) with proprietary data is now a standard practice for downstream developers to obtain task-specific LLMs. Surprisingly, we reveal a new and concerning risk along with the practice: the creator of the open-source LLMs can later extract the privat...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 21. Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with "I don't know". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 22. In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution's rich information, we propose Mixture of Inputs (MoI), a training-free method ...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 23. Machine learning-based interatomic potentials and force fields depend critically on accurate atomic structures, yet such data are scarce due to the limited availability of experimentally resolved crystals. Although atomic-resolution electron microscopy offers a potential source of structural data, c...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 24. Diffusion models have emerged as powerful generative tools across various domains, yet tailoring pre-trained models to exhibit specific desirable properties remains challenging. While reinforcement learning (RL) offers a promising solution,current methods struggle to simultaneously achieve stable, e...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 25. Bias in Large Language Models (LLMs) significantly undermines their reliability and fairness. We focus on a common form of bias: when two reference concepts in the model's concept space, such as sentiment polarities (e.g., "positive" and "negative"), are asymmetrically correlated with a third, targe...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 26. Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate remarkable potential for scientific discovery. Existing approaches, however, often automate scientific discovery using predefined workflows that lack rationality constraints. This often leads to aimless hypothesizing and a failur...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 27. Reinforcement learning (RL) has recently emerged as a compelling approach for enhancing the reasoning capabilities of large language models (LLMs), where an LLM generator serves as a policy guided by a verifier (reward model). However, current RL post-training methods for LLMs typically use verifier...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 28. Large multimodal models excel in multimodal tasks but face significant computational challenges due to excessive computation on visual tokens. Unlike token reduction methods that focus on token-level redundancy, we identify and study the computation-level redundancy on vision tokens to ensure no inf...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 29. The rise of Large Audio Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks a systematic, quantitative evaluation of LAM safety especially against jailbreak attacks, which are challenging due to the ...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 30. Robustly evaluating the long-form storytelling capabilities of Large Language Models (LLMs) remains a significant challenge, as existing benchmarks often lack the necessary scale, diversity, or objective measures. To address this, we introduce WebNovelBench, a novel benchmark specifically designed f...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 31. This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shapin...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 32. Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved remarkable performance in the domain of reasoning. A key component of their training is the incorporation of verifiable rewards within reinforcement learning (RL). However, existing reward benchmarks do not evaluate reference-bas...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 33. Llama-SMoP, an efficient multimodal LLM incorporating Sparse Mixture of Projectors, enhances AVSR performance without increasing inference costs through modality-specific routers and experts.  					AI-generated summary 				 Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy environm...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 34. A multilingual, multihop benchmark using knowledge graphs for evaluating and mitigating hallucinations in large language models.  					AI-generated summary 				 Large Language Models (LLMs) have inherent limitations of faithfulness and factuality, commonly referred to as hallucinations. Several benc...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 35. HumaniBench evaluates state-of-the-art LMMs on seven human-centered AI principles using 32K real-world image-question pairs to ensure fairness, ethics, empathy, and inclusivity.  					AI-generated summary 				 Large multimodal models (LMMs) now excel on many vision language benchmarks, however, they...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 36. Speculative decoding has emerged as a popular method to accelerate the inference of Large Language Models (LLMs) while retaining their superior text generation performance. Previous methods either adopt a fixed speculative decoding configuration regardless of the prefix tokens, or train draft models...
[22.05.2025 16:15] ********************************************************************************
[22.05.2025 16:15] Abstract 37. Code-switching is a common phenomenon of alternating between different languages in the same utterance, thought, or conversation. We posit that humans code-switch because they feel more comfortable talking about certain topics and domains in one language than another. With the rise of knowledge-inte...
[22.05.2025 16:15] Read previous papers.
[22.05.2025 16:15] Generating reviews via LLM API.
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#survey", "#benchmark", "#rl", "#dataset"], "emoji": "ðŸ§­", "ru": {"title": "Web-Shepherd: ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð²ÐµÐ±-Ð½Ð°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ð¸", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Web-Shepherd - Ð¿ÐµÑ€Ð²ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ° (PRM) Ð´Ð»Ñ Ð²
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#training", "#optimization", "#inference"], "emoji": "ðŸ”¬", "ru": {"title": "ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…: Ð½Ð¾Ð²Ñ‹Ðµ Ð³Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚Ñ‹ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÐ¼Ð¾ÑÑ‚ÑŒ ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ (QAT) Ð´Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM). ÐÐ²Ñ‚Ð¾Ñ€
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#training", "#multimodal", "#open_source", "#architecture", "#reasoning", "#diffusion", "#rl"], "emoji": "ðŸ§ ", "ru": {"title": "MMaDA: Ð£Ð½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸", "desc": "MMaDA - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ ÐºÐ»Ð°ÑÑ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ñ„ÑƒÐ½Ð´Ð°
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#training", "#multimodal", "#reasoning", "#rl", "#dataset"], "emoji": "ðŸ§ ", "ru": {"title": "Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð¿Ñ€Ð¸Ð²ÑÐ·ÐºÐ¸ Ñ‡ÐµÑ€ÐµÐ· Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ UniVG-R1 - Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð±Ð¾Ð»ÑŒÑˆÑƒÑŽ ÑÐ·Ñ‹ÐºÐ¾Ð²ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð¿Ñ€Ð¸Ð²Ñ
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#diffusion", "#reasoning", "#benchmark", "#training", "#long_context", "#architecture", "#dataset"], "emoji": "ðŸ§ ", "ru": {"title": "Ð”Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸: Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¼ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð°Ð¼", "desc": "Ð’ ÑÑ‚Ð¾Ð¹ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#synthetic", "#benchmark", "#transfer_learning", "#training", "#dataset", "#agents"], "emoji": "ðŸ–¥ï¸", "ru": {"title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð¼Ð¿ÑŒÑŽÑ‚ÐµÑ€Ð½Ñ‹Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð½Ð° Ð¼Ð°Ð»Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ PC Agent-E - ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½ÑƒÑŽ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ ÐºÐ¾Ð¼Ð¿ÑŒÑŽÑ‚Ðµ
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#small_models", "#synthetic", "#architecture"], "emoji": "ðŸ“ˆ", "ru": {"title": "Toto: Ñ€ÐµÐ²Ð¾Ð»ÑŽÑ†Ð¸Ñ Ð² Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ð¸ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ñ€ÑÐ´Ð¾Ð²", "desc": "ÐŸÑ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ Toto - Ñ„ÑƒÐ½Ð´Ð°Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ñ€ÑÐ´Ð¾Ð² Ñ 151 Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð¾Ð¼ Ð¿Ð°Ñ€
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#training", "#optimization"], "emoji": "ðŸ’¡", "ru": {"title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ðµ: Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ†ÐµÐ¿Ð¾Ñ‡ÐµÐº Ð¼Ñ‹ÑÐ»ÐµÐ¹ Ð² ÐºÑ€ÑƒÐ¿Ð½Ñ‹Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ LASER Ð´Ð»Ñ Ð¿Ð¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð² ÐºÑ€ÑƒÐ¿Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ… Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡Ðµ
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training", "#reasoning", "#benchmark"], "emoji": "ðŸ§ ", "ru": {"title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ðµ: Ð¼ÐµÐ½ÑŒÑˆÐµ Ð´ÑƒÐ¼Ð°Ð¹, Ð±Ð¾Ð»ÑŒÑˆÐµ Ð´ÐµÐ»Ð°Ð¹", "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ (LRM). ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ð¼ÐµÑ‚
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#architecture", "#games", "#robotics", "#diffusion", "#transfer_learning", "#rl", "#agents", "#video"], "emoji": "ðŸŽ¥", "ru": {"title": "ÐŸÑ€ÐµÐ²Ñ€Ð°Ñ‰ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð² Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¼Ð¸Ñ€Ð°", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½ Ð¼ÐµÑ‚Ð¾Ð´ Vid2World, Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÑŽÑ‰Ð¸Ð¹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€ÐµÐ´Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#games", "#agents", "#transfer_learning"], "emoji": "ðŸŽ®", "ru": {"title": "Ð’Ð¸Ð´ÐµÐ¾Ð¸Ð³Ñ€Ñ‹ ÐºÐ°Ðº Ð¿Ð¾Ð»Ð¸Ð³Ð¾Ð½ Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¸Ð·ÑƒÑ‡Ð°ÑŽÑ‚ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð²Ð¸Ð´ÐµÐ¾Ð¸Ð³Ñ€ Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM). ÐžÐ½Ð¸ 
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#rag", "#alignment", "#dataset", "#multimodal", "#diffusion"], "emoji": "ðŸ–¼ï¸", "ru": {"title": "Ð˜Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚-augmented T2I: Ð¿Ñ€ÐµÐ¾Ð´Ð¾Ð»ÐµÐ½Ð¸Ðµ Ð½ÐµÐ¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸ Ð² Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ð¾Ð¼Ñƒ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸ÑŽ (T2I) Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#synthetic", "#3d"], "emoji": "ðŸ™ï¸", "ru": {"title": "Ð ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ðµ 3D-Ð³Ð¾Ñ€Ð¾Ð´Ð° Ð¸Ð· Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð±ÐµÐ· Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ", "desc": "3DTown - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ€ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ñ… Ñ‚Ñ€ÐµÑ…Ð¼ÐµÑ€Ð½Ñ‹Ñ… ÑÑ†ÐµÐ½ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÐµÐ´Ð¸Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð²Ð¸Ð´Ð° ÑÐ²ÐµÑ€Ñ…Ñƒ, Ð½Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‰Ð¸Ð¹ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ. ÐœÐµÑ‚Ð¾Ð´ Ð¾ÑÐ½Ð¾Ð²Ð°Ð½ Ð½Ð° Ð³
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rag", "#benchmark", "#hallucinations"], "emoji": "ðŸ§ ", "ru": {"title": "ÐŸÐ¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ðµ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ‡ÐµÑ€ÐµÐ· Ð¾ÑÐ¼Ñ‹ÑÐ»ÐµÐ½Ð¸Ðµ Ð°Ð¿Ñ€Ð¸Ð¾Ñ€Ð½Ñ‹Ñ… Ð·Ð½Ð°Ð½Ð¸Ð¹", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ 'Deliberation over Priors' (DP) Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ñ€Ð°Ð±Ð¾Ñ‚
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#data", "#math", "#reasoning", "#training", "#safety"], "emoji": "ðŸ›¡ï¸", "ru": {"title": "ÐŸÐ¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ðµ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸ LRM: Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ñ‚Ð° Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ ÐºÐ»ÑŽÑ‡Ð¾Ð¼", "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ ÑÐ¿Ð¾ÑÐ¾Ð±Ñ‹ Ð¿Ð¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ñ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸ ÐœÐ¾Ð´ÐµÐ»ÐµÐ¹ ÐšÑ€ÑƒÐ¿Ð½Ð¾Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð½Ð¾Ð³Ð¾ Ð Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ (LRM) Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÐšÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ð¸Ñ€ÑƒÐµÐ¼Ð¾Ð¹ Ð¢Ð¾Ð½ÐºÐ¾Ð¹ Ð
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#training", "#multimodal", "#benchmark", "#reasoning"], "emoji": "ðŸ§ ", "ru": {"title": "Ð¡Ð¼ÐµÑˆÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÐµÐ¹ Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð˜Ð˜", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÑŽ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ - Mixture-of-Thought (MoT). MoT Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð¼ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ð¼ Ð¼Ð¾Ð´
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#architecture", "#inference", "#optimization", "#diffusion", "#benchmark"], "emoji": "ðŸš€", "ru": {"title": "Ð£ÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÑƒÐ¼Ð½Ð¾Ð³Ð¾ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ Ð²Ñ‹Ð²Ð¾Ð´Ð° Ð´Ð»Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (DLM) - Ð¾Ñ‚
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#rlhf", "#training", "#multimodal", "#optimization", "#reasoning", "#games", "#rl", "#agents", "#video"], "emoji": "ðŸŒ", "ru": {"title": "RLVR-World: Ð ÐµÐ²Ð¾Ð»ÑŽÑ†Ð¸Ñ Ð² Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ð¼Ð¸Ñ€Ð¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "RLVR-World - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÑŽ Ð¼Ð¸Ñ€Ð¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð¸Ð¹ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´Ðº
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#math", "#interpretability", "#reasoning", "#benchmark", "#training"], "emoji": "ðŸ§ ", "ru": {"title": "ÐœÑÐ³ÐºÐ¾Ðµ Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ðµ: Ð¿Ñ€ÐµÐ¾Ð´Ð¾Ð»ÐµÐ½Ð¸Ðµ Ð³Ñ€Ð°Ð½Ð¸Ñ† Ð´Ð¸ÑÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ¾Ð²Ð¾Ð³Ð¾ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð² Ð˜Ð˜", "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ 'Soft Thinking', ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¸
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#training", "#dataset", "#rag", "#rl", "#reasoning"], "emoji": "ðŸ”", "ru": {"title": "Ð¡Ð°Ð¼Ð¾Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰Ð°ÑÑÑ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð¿ÐµÑ€ÐµÑ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð´Ð»Ñ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð¾Ð²Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ConvSearch-R1 - Ð¿ÐµÑ€Ð²ÑƒÑŽ ÑÐ°Ð¼Ð¾ÑƒÐ¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ÑƒÑŽ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÑ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#data", "#security", "#open_source", "#training", "#leakage"], "emoji": "ðŸ•µï¸", "ru": {"title": "Ð¡ÐºÑ€Ñ‹Ñ‚Ð°Ñ ÑƒÐ³Ñ€Ð¾Ð·Ð°: ÐºÐ°Ðº Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¼Ð¾Ð³ÑƒÑ‚ Ñ€Ð°ÑÐºÑ€Ñ‹Ñ‚ÑŒ Ð²Ð°ÑˆÐ¸ ÑÐµÐºÑ€ÐµÑ‚Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ", "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ñ€Ð°ÑÐºÑ€Ñ‹Ð²Ð°ÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ñ€Ð¸ÑÐº Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸ Ð¿Ñ€Ð¸ Ð´Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ð½Ð°
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#hallucinations", "#reasoning", "#training", "#math"], "emoji": "ðŸ§ ", "ru": {"title": "ÐŸÐ¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ðµ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸ Ð˜Ð˜-Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ñ‡ÐµÑ€ÐµÐ· Ð¾ÑÐ¾Ð·Ð½Ð°Ð½Ð¸Ðµ Ð³Ñ€Ð°Ð½Ð¸Ñ† Ð·Ð½Ð°Ð½Ð¸Ð¹", "desc": "Ð­Ñ‚Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÑÐ²ÑÑ‰ÐµÐ½Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ðµ Ñ‡Ñ€ÐµÐ·Ð¼ÐµÑ€Ð½Ð¾Ð¹ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸ Ð¸ Ð½ÐµÑ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ (LRM) Ð² Ð¼Ð°Ñ‚Ðµ
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#math", "#training", "#reasoning", "#optimization"], "emoji": "ðŸ§ ", "ru": {"title": "Ð¡Ð¼ÐµÑˆÐ¸Ð²Ð°Ð½Ð¸Ðµ Ð²Ñ…Ð¾Ð´Ð¾Ð² Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸Ð»Ð¸ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð°Ð²Ñ‚Ð¾Ñ€ÐµÐ³Ñ€Ð°Ð´Ð¸Ð²Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ‚ÐµÐºÑÑ‚Ð° Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ Mixture of Inputs (MoI). Ð’ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð¸Ðµ Ð¾Ñ‚ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#data", "#multimodal", "#open_source", "#benchmark", "#dataset", "#agents", "#science"], "emoji": "ðŸ”¬", "ru": {"title": "AutoMat: Ð¾Ñ‚ Ð¼Ð¸ÐºÑ€Ð¾ÑÐºÐ¾Ð¿Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ðº Ð°Ñ‚Ð¾Ð¼Ð½Ñ‹Ð¼ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð°Ð¼", "desc": "AutoMat - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ ÐºÐ¾Ð½Ð²ÐµÐ¹ÐµÑ€ Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ ÑÐºÐ°Ð½Ð¸Ñ€ÑƒÑŽÑ‰ÐµÐ¹ Ð¿Ñ€
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#diffusion", "#rl", "#training", "#optimization"], "emoji": "ðŸš€", "ru": {"title": "VARD: Ð£ÑÐ¸Ð»ÐµÐ½Ð¸Ðµ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ VARD (Value-based Reinforced Diffusion) Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»Ðµ
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#ethics", "#multimodal", "#benchmark", "#interpretability", "#data"], "emoji": "ðŸ”", "ru": {"title": "BiasLens: ÐÐ¾Ð²Ñ‹Ð¹ Ð²Ð·Ð³Ð»ÑÐ´ Ð½Ð° Ð¿Ñ€ÐµÐ´Ð²Ð·ÑÑ‚Ð¾ÑÑ‚ÑŒ Ð² ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ BiasLens - Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¿Ñ€ÐµÐ´Ð²Ð·ÑÑ‚Ð¾ÑÑ‚Ð¸ Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ… (LLM) Ð±ÐµÐ· Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#rl", "#agents", "#science"], "emoji": "ðŸ§ª", "ru": {"title": "PiFlow: Ð ÐµÐ²Ð¾Ð»ÑŽÑ†Ð¸Ñ Ð² Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¼ Ð½Ð°ÑƒÑ‡Ð½Ð¾Ð¼ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ð¸", "desc": "PiFlow - ÑÑ‚Ð¾ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾-Ñ‚ÐµÐ¾Ñ€ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð½Ð°ÑƒÑ‡Ð½Ð¾Ð³Ð¾ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ñ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¼Ð½Ð¾Ð³Ð¾Ð°Ð³ÐµÐ½Ñ‚Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼ Ñ 
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#math", "#rlhf", "#reasoning", "#rl", "#training", "#optimization"], "emoji": "ðŸ•º", "ru": {"title": "Tango: Ð¢Ð°Ð½ÐµÑ† Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ LLM", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Tango Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ðº Ñ€Ð°ÑÑÑƒÐ¶
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#open_source", "#architecture", "#inference"], "emoji": "ðŸ”", "ru": {"title": "ProxyV: ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð² Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ ProxyV Ð´Ð»Ñ Ð¿Ð¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#ethics", "#dataset", "#benchmark", "#security", "#audio"], "emoji": "ðŸŽ™ï¸", "ru": {"title": "AJailBench: ÐÐ¾Ð²Ñ‹Ð¹ Ñ€ÑƒÐ±ÐµÐ¶ Ð² Ð¾Ñ†ÐµÐ½ÐºÐµ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸ Ð°ÑƒÐ´Ð¸Ð¾-Ð˜Ð˜", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ AJailBench - Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑƒÑÐ·Ð²Ð¸Ð¼Ð¾ÑÑ‚ÐµÐ¹ Ðº Ð°Ñ‚Ð°ÐºÐ°Ð¼ Ð´Ð¶ÐµÐ¹Ð»Ð±Ñ€ÐµÐ¹ÐºÐ° Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð°ÑƒÐ´Ð¸Ð¾-ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#story_generation", "#long_context", "#dataset", "#benchmark"], "emoji": "ðŸ“š", "ru": {"title": "WebNovelBench: Ð½Ð¾Ð²Ñ‹Ð¹ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… Ñ‚ÐµÐºÑÑ‚Ð¾Ð² ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸", "desc": "WebNovelBench - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ð³ÐµÐ½ÐµÑ€
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rl", "#benchmark"], "emoji": "ðŸ§ ", "ru": {"title": "ÐŸÑ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¸Ð½Ð¶ÐµÐ½ÐµÑ€Ð¸Ñ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð¾Ð²: Ð½Ð¾Ð²Ñ‹Ð¹ Ñ€ÑƒÐ±ÐµÐ¶ Ð² Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "Ð­Ñ‚Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ€Ð°ÑÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°ÐµÑ‚ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¸Ð½Ð¶ÐµÐ½ÐµÑ€Ð¸Ð¸ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð¾Ð² (pPE) Ð½Ð° Ð¾
[22.05.2025 16:15] Querying the API.
[22.05.2025 16:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved remarkable performance in the domain of reasoning. A key component of their training is the incorporation of verifiable rewards within reinforcement learning (RL). However, existing reward benchmarks do not evaluate reference-based reward systems, leaving researchers with limited understanding of the accuracy of verifiers used in RL. In this paper, we introduce two benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the performance of reference-based reward systems. These benchmarks are constructed through meticulous data collection and curation, followed by careful human annotation to ensure high quality. Current models still show considerable room for improvement on both VerifyBench and VerifyBench-Hard, especially smaller-scale models. Furthermore, we conduct a thorough and comprehensive analysis of evaluation results, offering insights for understanding and developing reference-based reward systems. Our proposed benchmarks serve as effective tools for guiding the development of verifier accuracy and the reasoning capabilities of models trained via RL in reasoning tasks.
[22.05.2025 16:15] Response: {
  "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð´Ð²Ð° Ð½Ð¾Ð²Ñ‹Ñ… Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ° Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¸ÑÑ‚ÐµÐ¼ Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ñ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÑÑ‚Ð°Ð»Ð¾Ð½Ð¾Ð² Ð² Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ (RL) Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹. VerifyBench Ð¸ VerifyBench-Hard Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ñ‹ Ð´Ð»Ñ Ð¸Ð·Ð¼ÐµÑ€ÐµÐ½Ð¸Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð²ÐµÑ€Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð¾Ð², Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼Ñ‹Ñ… Ð² RL. Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸, Ð¾ÑÐ¾Ð±ÐµÐ½Ð½Ð¾ Ð½ÐµÐ±Ð¾Ð»ÑŒÑˆÐ¾Ð³Ð¾ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð°, Ð¸Ð¼ÐµÑŽÑ‚ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð» Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð½Ð° ÑÑ‚Ð¸Ñ… Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ°Ñ…. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€Ð¾Ð²Ð¾Ð´ÑÑ‚ Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð¾Ñ†ÐµÐ½ÐºÐ¸, Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÑÑ Ñ†ÐµÐ½Ð½Ñ‹Ðµ Ð²Ñ‹Ð²Ð¾Ð´Ñ‹ Ð´Ð»Ñ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÑÐ¸ÑÑ‚ÐµÐ¼ Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ñ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÑÑ‚Ð°Ð»Ð¾Ð½Ð¾Ð².",
  "emoji": "ðŸ§ ",
  "title": "ÐÐ¾Ð²Ñ‹Ðµ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ¸ Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð²ÐµÑ€Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð¾Ð² Ð² Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼"
}
[22.05.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved remarkable performance in the domain of reasoning. A key component of their training is the incorporation of verifiable rewards within reinforcement learning (RL). However, existing reward benchmarks do not evaluate reference-based reward systems, leaving researchers with limited understanding of the accuracy of verifiers used in RL. In this paper, we introduce two benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the performance of reference-based reward systems. These benchmarks are constructed through meticulous data collection and curation, followed by careful human annotation to ensure high quality. Current models still show considerable room for improvement on both VerifyBench and VerifyBench-Hard, especially smaller-scale models. Furthermore, we conduct a thorough and comprehensive analysis of evaluation results, offering insights for understanding and developing reference-based reward systems. Our proposed benchmarks serve as effective tools for guiding the development of verifier accuracy and the reasoning capabilities of models trained via RL in reasoning tasks."

[22.05.2025 16:15] Response: ```python
['BENCHMARK', 'DATASET', 'DATA', 'RL']
```
[22.05.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved remarkable performance in the domain of reasoning. A key component of their training is the incorporation of verifiable rewards within reinforcement learning (RL). However, existing reward benchmarks do not evaluate reference-based reward systems, leaving researchers with limited understanding of the accuracy of verifiers used in RL. In this paper, we introduce two benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the performance of reference-based reward systems. These benchmarks are constructed through meticulous data collection and curation, followed by careful human annotation to ensure high quality. Current models still show considerable room for improvement on both VerifyBench and VerifyBench-Hard, especially smaller-scale models. Furthermore, we conduct a thorough and comprehensive analysis of evaluation results, offering insights for understanding and developing reference-based reward systems. Our proposed benchmarks serve as effective tools for guiding the development of verifier accuracy and the reasoning capabilities of models trained via RL in reasoning tasks."

[22.05.2025 16:15] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[22.05.2025 16:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the development of two new benchmarks, VerifyBench and VerifyBench-Hard, aimed at evaluating reference-based reward systems in reinforcement learning (RL). These benchmarks are created through careful data collection and human annotation to ensure their quality and effectiveness. The authors highlight that current reasoning models, including smaller-scale ones, still have significant room for improvement when tested against these benchmarks. The paper provides insights into enhancing verifier accuracy and the reasoning abilities of models trained with RL, ultimately guiding future research in this area.","title":"Enhancing Reasoning Models with New Reward Benchmarks"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the development of two new benchmarks, VerifyBench and VerifyBench-Hard, aimed at evaluating reference-based reward systems in reinforcement learning (RL). These benchmarks are created through careful data collection and human annotation to ensure their quality and effectiveness. The authors highlight that current reasoning models, including smaller-scale ones, still have significant room for improvement when tested against these benchmarks. The paper provides insights into enhancing verifier accuracy and the reasoning abilities of models trained with RL, ultimately guiding future research in this area.', title='Enhancing Reasoning Models with New Reward Benchmarks'))
[22.05.2025 16:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡ä»‹ç»äº†å¤§åž‹æŽ¨ç†æ¨¡åž‹åœ¨æŽ¨ç†é¢†åŸŸçš„å‡ºè‰²è¡¨çŽ°ï¼Œç‰¹åˆ«æ˜¯OpenAI o1å’ŒDeepSeek-R1ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ï¼ŒVerifyBenchå’ŒVerifyBench-Hardï¼Œç”¨äºŽè¯„ä¼°åŸºäºŽå‚è€ƒçš„å¥–åŠ±ç³»ç»Ÿçš„æ€§èƒ½ã€‚é€šè¿‡ç²¾å¿ƒçš„æ•°æ®æ”¶é›†å’Œäººå·¥æ ‡æ³¨ï¼Œè¿™äº›åŸºå‡†ç¡®ä¿äº†é«˜è´¨é‡çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜Žï¼Œå½“å‰æ¨¡åž‹åœ¨è¿™ä¸¤ä¸ªåŸºå‡†ä¸Šä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ï¼Œå°¤å…¶æ˜¯å°è§„æ¨¡æ¨¡åž‹ã€‚","title":"æå‡æŽ¨ç†æ¨¡åž‹çš„å¥–åŠ±ç³»ç»Ÿè¯„ä¼°"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡ä»‹ç»äº†å¤§åž‹æŽ¨ç†æ¨¡åž‹åœ¨æŽ¨ç†é¢†åŸŸçš„å‡ºè‰²è¡¨çŽ°ï¼Œç‰¹åˆ«æ˜¯OpenAI o1å’ŒDeepSeek-R1ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ï¼ŒVerifyBenchå’ŒVerifyBench-Hardï¼Œç”¨äºŽè¯„ä¼°åŸºäºŽå‚è€ƒçš„å¥–åŠ±ç³»ç»Ÿçš„æ€§èƒ½ã€‚é€šè¿‡ç²¾å¿ƒçš„æ•°æ®æ”¶é›†å’Œäººå·¥æ ‡æ³¨ï¼Œè¿™äº›åŸºå‡†ç¡®ä¿äº†é«˜è´¨é‡çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜Žï¼Œå½“å‰æ¨¡åž‹åœ¨è¿™ä¸¤ä¸ªåŸºå‡†ä¸Šä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ï¼Œå°¤å…¶æ˜¯å°è§„æ¨¡æ¨¡åž‹ã€‚', title='æå‡æŽ¨ç†æ¨¡åž‹çš„å¥–åŠ±ç³»ç»Ÿè¯„ä¼°'))
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#audio", "#multimodal", "#inference", "#architecture", "#low_resource"], "emoji": "ðŸ—£ï¸", "ru": {"title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð¾Ðµ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÑ‡Ð¸ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ñ€Ð°Ð·Ñ€ÐµÐ¶ÐµÐ½Ð½Ñ‹Ñ… ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð¾Ð²", "desc": "Llama-SMoP - ÑÑ‚Ð¾ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð°Ñ ÑÐ·Ñ‹ÐºÐ¾Ð²Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽ
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#hallucinations", "#benchmark", "#dataset", "#graphs", "#multilingual", "#data", "#rag"], "emoji": "ðŸ§ ", "ru": {"title": "Ð“Ñ€Ð°Ñ„Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð¿Ñ€Ð¾Ñ‚Ð¸Ð² Ð³Ð°Ð»Ð»ÑŽÑ†Ð¸Ð½Ð°Ñ†Ð¸Ð¹ Ð˜Ð˜", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼Ð½Ð¾Ð³Ð¾ÑÐ·Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº MultiHal Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¸ ÑÐ½Ð¸Ð¶ÐµÐ½Ð¸Ñ Ð³Ð°Ð»Ð»ÑŽÑ†Ð¸Ð½Ð°Ñ†Ð¸Ð¹ Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´Ðµ
[22.05.2025 16:15] Using data from previous issue: {"categories": ["#alignment", "#ethics", "#multimodal", "#dataset", "#benchmark"], "emoji": "ðŸ§ ", "ru": {"title": "ÐžÑ†ÐµÐ½ÐºÐ° Ð˜Ð˜ Ð¿Ð¾ Ñ‡ÐµÐ»Ð¾Ð²ÐµÑ‡ÐµÑÐºÐ¸Ð¼ Ð¼ÐµÑ€ÐºÐ°Ð¼", "desc": "HumaniBench - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LMM) Ð¿Ð¾ ÑÐµÐ¼Ð¸ Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ð°Ð¼ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ¾Ñ†ÐµÐ½Ñ‚Ñ€Ð¸Ñ‡Ð½Ð¾Ð³Ð¾ Ð˜Ð˜. ÐžÐ½ Ð²ÐºÐ»
[22.05.2025 16:15] Querying the API.
[22.05.2025 16:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Speculative decoding has emerged as a popular method to accelerate the inference of Large Language Models (LLMs) while retaining their superior text generation performance. Previous methods either adopt a fixed speculative decoding configuration regardless of the prefix tokens, or train draft models in an offline or online manner to align them with the context. This paper proposes a training-free online learning framework to adaptively choose the configuration of the hyperparameters for speculative decoding as text is being generated. We first formulate this hyperparameter selection problem as a Multi-Armed Bandit problem and provide a general speculative decoding framework BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms, UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity, the stopping time regret. We upper bound this regret under both stochastic and adversarial reward settings. By deriving an information-theoretic impossibility result, it is shown that the regret performance of UCBSpec is optimal up to universal constants. Finally, extensive empirical experiments with LLaMA3 and Qwen2 demonstrate that our algorithms are effective compared to existing methods, and the throughput is close to the oracle best hyperparameter in simulated real-life LLM serving scenarios with diverse input prompts.
[22.05.2025 16:16] Response: {
  "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸ÑŽ ÑÐ¿ÐµÐºÑƒÐ»ÑÑ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ Ð´ÐµÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM). ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð²Ñ‹Ð±Ð¾Ñ€Ð° Ð³Ð¸Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð´Ð»Ñ ÑÐ¿ÐµÐºÑƒÐ»ÑÑ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ Ð´ÐµÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð½Ð° Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ð°Ñ… Ð¼Ð½Ð¾Ð³Ð¾Ñ€ÑƒÐºÐ¸Ñ… Ð±Ð°Ð½Ð´Ð¸Ñ‚Ð¾Ð². Ð Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ñ‹ Ð´Ð²Ð° Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ð° - UCBSpec Ð¸ EXP3Spec, Ð´Ð»Ñ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… Ð¿Ñ€Ð¾Ð²ÐµÐ´ÐµÐ½ Ñ‚ÐµÐ¾Ñ€ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· Ð¸ Ð´Ð¾ÐºÐ°Ð·Ð°Ð½Ð° Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ UCBSpec. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑÑ… LLaMA3 Ð¸ Qwen2 Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð¸ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð° Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¼Ð¸ Ð¼ÐµÑ‚Ð¾Ð´Ð°Ð¼Ð¸.",
  "emoji": "ðŸš€",
  "title": "ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð¾Ðµ ÑÐ¿ÐµÐºÑƒÐ»ÑÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ð´ÐµÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ: ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ LLM Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð¼Ð½Ð¾Ð³Ð¾Ñ€ÑƒÐºÐ¸Ñ… Ð±Ð°Ð½Ð´Ð¸Ñ‚Ð¾Ð²"
}
[22.05.2025 16:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Speculative decoding has emerged as a popular method to accelerate the inference of Large Language Models (LLMs) while retaining their superior text generation performance. Previous methods either adopt a fixed speculative decoding configuration regardless of the prefix tokens, or train draft models in an offline or online manner to align them with the context. This paper proposes a training-free online learning framework to adaptively choose the configuration of the hyperparameters for speculative decoding as text is being generated. We first formulate this hyperparameter selection problem as a Multi-Armed Bandit problem and provide a general speculative decoding framework BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms, UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity, the stopping time regret. We upper bound this regret under both stochastic and adversarial reward settings. By deriving an information-theoretic impossibility result, it is shown that the regret performance of UCBSpec is optimal up to universal constants. Finally, extensive empirical experiments with LLaMA3 and Qwen2 demonstrate that our algorithms are effective compared to existing methods, and the throughput is close to the oracle best hyperparameter in simulated real-life LLM serving scenarios with diverse input prompts."

[22.05.2025 16:16] Response: ```python
["INFERENCE", "TRAINING", "MATH"]
```
[22.05.2025 16:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Speculative decoding has emerged as a popular method to accelerate the inference of Large Language Models (LLMs) while retaining their superior text generation performance. Previous methods either adopt a fixed speculative decoding configuration regardless of the prefix tokens, or train draft models in an offline or online manner to align them with the context. This paper proposes a training-free online learning framework to adaptively choose the configuration of the hyperparameters for speculative decoding as text is being generated. We first formulate this hyperparameter selection problem as a Multi-Armed Bandit problem and provide a general speculative decoding framework BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms, UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity, the stopping time regret. We upper bound this regret under both stochastic and adversarial reward settings. By deriving an information-theoretic impossibility result, it is shown that the regret performance of UCBSpec is optimal up to universal constants. Finally, extensive empirical experiments with LLaMA3 and Qwen2 demonstrate that our algorithms are effective compared to existing methods, and the throughput is close to the oracle best hyperparameter in simulated real-life LLM serving scenarios with diverse input prompts."

[22.05.2025 16:16] Response: ```python
["OPTIMIZATION"]
```
[22.05.2025 16:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new method called BanditSpec for improving the efficiency of Large Language Models (LLMs) during text generation. Instead of using a fixed approach for speculative decoding, it dynamically selects hyperparameters based on the context of the text being generated. The authors frame this selection process as a Multi-Armed Bandit problem and propose two algorithms, UCBSpec and EXP3Spec, to optimize performance. Their experiments show that these algorithms significantly enhance throughput and adapt well to various input prompts, achieving results close to the best possible configurations.","title":"Dynamic Hyperparameter Selection for Efficient Text Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a new method called BanditSpec for improving the efficiency of Large Language Models (LLMs) during text generation. Instead of using a fixed approach for speculative decoding, it dynamically selects hyperparameters based on the context of the text being generated. The authors frame this selection process as a Multi-Armed Bandit problem and propose two algorithms, UCBSpec and EXP3Spec, to optimize performance. Their experiments show that these algorithms significantly enhance throughput and adapt well to various input prompts, achieving results close to the best possible configurations.', title='Dynamic Hyperparameter Selection for Efficient Text Generation'))
[22.05.2025 16:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åœ¨çº¿å­¦ä¹ æ¡†æž¶ï¼Œç”¨äºŽè‡ªé€‚åº”é€‰æ‹©å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰æŽ¨ç†è¿‡ç¨‹ä¸­çš„è¶…å‚æ•°é…ç½®ï¼Œä»¥åŠ é€ŸæŽ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬å°†è¶…å‚æ•°é€‰æ‹©é—®é¢˜å»ºæ¨¡ä¸ºå¤šè‡‚èµŒåšæœºé—®é¢˜ï¼Œå¹¶æå‡ºäº†BanditSpecæ¡†æž¶ã€‚æ–‡ä¸­è®¾è®¡äº†ä¸¤ç§åŸºäºŽèµŒåšæœºçš„è¶…å‚æ•°é€‰æ‹©ç®—æ³•UCBSpecå’ŒEXP3Specï¼Œå¹¶åˆ†æžäº†å®ƒä»¬åœ¨éšæœºå’Œå¯¹æŠ—å¥–åŠ±è®¾ç½®ä¸‹çš„åœæ­¢æ—¶é—´é—æ†¾ã€‚é€šè¿‡å®žéªŒè¯æ˜Žï¼Œè¿™äº›ç®—æ³•åœ¨å¤„ç†å¤šæ ·åŒ–è¾“å…¥æç¤ºæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæé«˜æŽ¨ç†æ•ˆçŽ‡ï¼ŒæŽ¥è¿‘æœ€ä½³è¶…å‚æ•°çš„è¡¨çŽ°ã€‚","title":"è‡ªé€‚åº”è¶…å‚æ•°é€‰æ‹©ï¼ŒåŠ é€Ÿå¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åœ¨çº¿å­¦ä¹ æ¡†æž¶ï¼Œç”¨äºŽè‡ªé€‚åº”é€‰æ‹©å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰æŽ¨ç†è¿‡ç¨‹ä¸­çš„è¶…å‚æ•°é…ç½®ï¼Œä»¥åŠ é€ŸæŽ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬å°†è¶…å‚æ•°é€‰æ‹©é—®é¢˜å»ºæ¨¡ä¸ºå¤šè‡‚èµŒåšæœºé—®é¢˜ï¼Œå¹¶æå‡ºäº†BanditSpecæ¡†æž¶ã€‚æ–‡ä¸­è®¾è®¡äº†ä¸¤ç§åŸºäºŽèµŒåšæœºçš„è¶…å‚æ•°é€‰æ‹©ç®—æ³•UCBSpecå’ŒEXP3Specï¼Œå¹¶åˆ†æžäº†å®ƒä»¬åœ¨éšæœºå’Œå¯¹æŠ—å¥–åŠ±è®¾ç½®ä¸‹çš„åœæ­¢æ—¶é—´é—æ†¾ã€‚é€šè¿‡å®žéªŒè¯æ˜Žï¼Œè¿™äº›ç®—æ³•åœ¨å¤„ç†å¤šæ ·åŒ–è¾“å…¥æç¤ºæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæé«˜æŽ¨ç†æ•ˆçŽ‡ï¼ŒæŽ¥è¿‘æœ€ä½³è¶…å‚æ•°çš„è¡¨çŽ°ã€‚', title='è‡ªé€‚åº”è¶…å‚æ•°é€‰æ‹©ï¼ŒåŠ é€Ÿå¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†'))
[22.05.2025 16:16] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#alignment", "#benchmark", "#open_source", "#inference", "#low_resource", "#multilingual"], "emoji": "ðŸŒ", "ru": {"title": "Ð Ð°ÑÐºÑ€Ñ‹Ñ‚Ð¸Ðµ Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð»Ð° Ð¼Ð½Ð¾Ð³Ð¾ÑÐ·Ñ‹Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð² Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð¼ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚Ðµ", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¸Ð·ÑƒÑ‡Ð°ÑŽÑ‚ Ñ„ÐµÐ½Ð¾Ð¼ÐµÐ½ ÑÐ·Ñ‹ÐºÐ¾Ð²Ð¾-ÑÐ¿ÐµÑ†Ð¸Ñ„Ð¸Ñ‡Ð½Ñ‹Ñ… Ð·Ð½Ð°Ð½Ð¸Ð¹ (LS
[22.05.2025 16:16] Loading Chinese text from previous data.
[22.05.2025 16:16] Renaming data file.
[22.05.2025 16:16] Renaming previous data. hf_papers.json to ./d/2025-05-22.json
[22.05.2025 16:16] Saving new data file.
[22.05.2025 16:16] Generating page.
[22.05.2025 16:16] Renaming previous page.
[22.05.2025 16:16] Renaming previous data. index.html to ./d/2025-05-22.html
[22.05.2025 16:16] [Experimental] Generating Chinese page for reading.
[22.05.2025 16:16] Chinese vocab [{'word': 'è®¨è®º', 'pinyin': 'tÇŽo lÃ¹n', 'trans': 'discuss'}, {'word': 'ç½‘é¡µå¯¼èˆª', 'pinyin': 'wÇŽng yÃ¨ dÇŽo hÃ¡ng', 'trans': 'web navigation'}, {'word': 'è‡ªåŠ¨åŒ–', 'pinyin': 'zÃ¬ dÃ²ng huÃ ', 'trans': 'automation'}, {'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨n wu', 'trans': 'task'}, {'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'}, {'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'}, {'word': 'å¤§è¯­è¨€æ¨¡åž‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'}, {'word': 'å¥–åŠ±æ¨¡åž‹', 'pinyin': 'jiÇŽng lÃ¬ mÃ³ xÃ­ng', 'trans': 'reward model'}, {'word': 'é™åˆ¶', 'pinyin': 'xiÃ n zhÃ¬', 'trans': 'limit'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'è¿‡ç¨‹å¥–åŠ±æ¨¡åž‹', 'pinyin': 'guÃ² chÃ©ng jiÇŽng lÃ¬ mÃ³ xÃ­ng', 'trans': 'process reward model'}, {'word': 'é€æ­¥', 'pinyin': 'zhuÃ³ bÃ¹', 'trans': 'step-by-step'}, {'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluate'}, {'word': 'è·¯å¾„', 'pinyin': 'lÃ¹ jÃ¬ng', 'trans': 'path'}, {'word': 'åˆ›å»º', 'pinyin': 'chuÃ ng jiÃ n', 'trans': 'create'}, {'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹ jÃ¹ jÃ­', 'trans': 'dataset'}, {'word': 'è¯„ä¼°åŸºå‡†', 'pinyin': 'pÃ­ng gÅ« jÄ« zhÇ”n', 'trans': 'evaluation benchmark'}, {'word': 'å®žéªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'è¯æ˜Ž', 'pinyin': 'zhÃ¨ng mÃ­ng', 'trans': 'prove'}, {'word': 'æœ‰æ•ˆæ€§', 'pinyin': 'yÇ’u xiÃ o xÃ¬ng', 'trans': 'effectiveness'}, {'word': 'æˆæœ¬æ•ˆç›Š', 'pinyin': 'chÃ©ng bÄ›n xiÃ o yÃ¬', 'trans': 'cost-effectiveness'}, {'word': 'å…¬å¼€å¯ç”¨', 'pinyin': 'gÅng kÄi kÄ› yÃ²ng', 'trans': 'publicly available'}]
[22.05.2025 16:16] Renaming previous Chinese page.
[22.05.2025 16:16] Renaming previous data. zh.html to ./d/2025-05-21_zh_reading_task.html
[22.05.2025 16:16] Writing Chinese reading task.
[22.05.2025 16:16] Writing result.
[22.05.2025 16:16] Renaming log file.
[22.05.2025 16:16] Renaming previous data. log.txt to ./logs/2025-05-22_last_log.txt
