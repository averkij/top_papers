[22.05.2025 08:18] Read previous papers.
[22.05.2025 08:18] Generating top page (month).
[22.05.2025 08:18] Writing top page (month).
[22.05.2025 09:12] Read previous papers.
[22.05.2025 09:12] Get feed.
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15277
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14302
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15809
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14231
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13909
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15045
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15612
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15400
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15210
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15779
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14357
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15146
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15765
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15404
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15817
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15781
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15776
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15656
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13934
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13529
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15778
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14827
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12650
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15816
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15791
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15047
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15034
[22.05.2025 09:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.14157
[22.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14990
[22.05.2025 09:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.05.2025 09:12] No deleted papers detected.
[22.05.2025 09:12] Downloading and parsing papers (pdf, html). Total: 29.
[22.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.15277.
[22.05.2025 09:12] Extra JSON file exists (./assets/json/2505.15277.json), skip PDF parsing.
[22.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.15277.json), skip HTML parsing.
[22.05.2025 09:12] Success.
[22.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.14302.
[22.05.2025 09:12] Extra JSON file exists (./assets/json/2505.14302.json), skip PDF parsing.
[22.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.14302.json), skip HTML parsing.
[22.05.2025 09:12] Success.
[22.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.15809.
[22.05.2025 09:12] Extra JSON file exists (./assets/json/2505.15809.json), skip PDF parsing.
[22.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.15809.json), skip HTML parsing.
[22.05.2025 09:12] Success.
[22.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.14231.
[22.05.2025 09:12] Extra JSON file exists (./assets/json/2505.14231.json), skip PDF parsing.
[22.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.14231.json), skip HTML parsing.
[22.05.2025 09:12] Success.
[22.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.13909.
[22.05.2025 09:12] Extra JSON file exists (./assets/json/2505.13909.json), skip PDF parsing.
[22.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.13909.json), skip HTML parsing.
[22.05.2025 09:12] Success.
[22.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.15045.
[22.05.2025 09:12] Extra JSON file exists (./assets/json/2505.15045.json), skip PDF parsing.
[22.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.15045.json), skip HTML parsing.
[22.05.2025 09:12] Success.
[22.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.15612.
[22.05.2025 09:12] Extra JSON file exists (./assets/json/2505.15612.json), skip PDF parsing.
[22.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.15612.json), skip HTML parsing.
[22.05.2025 09:12] Success.
[22.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.15400.
[22.05.2025 09:12] Extra JSON file exists (./assets/json/2505.15400.json), skip PDF parsing.
[22.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.15400.json), skip HTML parsing.
[22.05.2025 09:12] Success.
[22.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.15210.
[22.05.2025 09:12] Extra JSON file exists (./assets/json/2505.15210.json), skip PDF parsing.
[22.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.15210.json), skip HTML parsing.
[22.05.2025 09:12] Success.
[22.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.15779.
[22.05.2025 09:12] Extra JSON file exists (./assets/json/2505.15779.json), skip PDF parsing.
[22.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.15779.json), skip HTML parsing.
[22.05.2025 09:12] Success.
[22.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.14357.
[22.05.2025 09:12] Extra JSON file exists (./assets/json/2505.14357.json), skip PDF parsing.
[22.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.14357.json), skip HTML parsing.
[22.05.2025 09:12] Success.
[22.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.15146.
[22.05.2025 09:12] Extra JSON file exists (./assets/json/2505.15146.json), skip PDF parsing.
[22.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.15146.json), skip HTML parsing.
[22.05.2025 09:12] Success.
[22.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.15765.
[22.05.2025 09:12] Extra JSON file exists (./assets/json/2505.15765.json), skip PDF parsing.
[22.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.15765.json), skip HTML parsing.
[22.05.2025 09:12] Success.
[22.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.15404.
[22.05.2025 09:12] Extra JSON file exists (./assets/json/2505.15404.json), skip PDF parsing.
[22.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.15404.json), skip HTML parsing.
[22.05.2025 09:12] Success.
[22.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.15817.
[22.05.2025 09:13] Downloading paper 2505.15817 from http://arxiv.org/pdf/2505.15817v1...
[22.05.2025 09:13] Extracting affiliations from text.
[22.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 7 1 8 5 1 . 5 0 5 2 : r Learning to Reason via Mixture-of-Thought for Logical Reasoning Tong Zheng1*, Lichang Chen1*, Simeng Han2, R. Thomas McCoy3, and Heng Huang *These authors contributed equally. 1Dept. of Computer Science, UMD, College Park, MD 20742 2Dept. of Computer Science, Yale University, New Haven, CT 06520 3Dept. of Linguistics, Yale University, New Haven, CT "
[22.05.2025 09:13] Response: ```python
[
    "Dept. of Computer Science, UMD, College Park, MD 20742",
    "Dept. of Computer Science, Yale University, New Haven, CT 06520",
    "Dept. of Linguistics, Yale University, New Haven, CT"
]
```
[22.05.2025 09:13] Deleting PDF ./assets/pdf/2505.15817.pdf.
[22.05.2025 09:13] Success.
[22.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.15781.
[22.05.2025 09:13] Extra JSON file exists (./assets/json/2505.15781.json), skip PDF parsing.
[22.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.15781.json), skip HTML parsing.
[22.05.2025 09:13] Success.
[22.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.15776.
[22.05.2025 09:13] Extra JSON file exists (./assets/json/2505.15776.json), skip PDF parsing.
[22.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.15776.json), skip HTML parsing.
[22.05.2025 09:13] Success.
[22.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.15656.
[22.05.2025 09:13] Extra JSON file exists (./assets/json/2505.15656.json), skip PDF parsing.
[22.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.15656.json), skip HTML parsing.
[22.05.2025 09:13] Success.
[22.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.13934.
[22.05.2025 09:13] Extra JSON file exists (./assets/json/2505.13934.json), skip PDF parsing.
[22.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.13934.json), skip HTML parsing.
[22.05.2025 09:13] Success.
[22.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.13529.
[22.05.2025 09:13] Extra JSON file exists (./assets/json/2505.13529.json), skip PDF parsing.
[22.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.13529.json), skip HTML parsing.
[22.05.2025 09:13] Success.
[22.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.15778.
[22.05.2025 09:13] Extra JSON file exists (./assets/json/2505.15778.json), skip PDF parsing.
[22.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.15778.json), skip HTML parsing.
[22.05.2025 09:13] Success.
[22.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.14827.
[22.05.2025 09:13] Extra JSON file exists (./assets/json/2505.14827.json), skip PDF parsing.
[22.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.14827.json), skip HTML parsing.
[22.05.2025 09:13] Success.
[22.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.12650.
[22.05.2025 09:13] Extra JSON file exists (./assets/json/2505.12650.json), skip PDF parsing.
[22.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.12650.json), skip HTML parsing.
[22.05.2025 09:13] Success.
[22.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.15816.
[22.05.2025 09:13] Extra JSON file exists (./assets/json/2505.15816.json), skip PDF parsing.
[22.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.15816.json), skip HTML parsing.
[22.05.2025 09:13] Success.
[22.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.15791.
[22.05.2025 09:13] Extra JSON file exists (./assets/json/2505.15791.json), skip PDF parsing.
[22.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.15791.json), skip HTML parsing.
[22.05.2025 09:13] Success.
[22.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.15047.
[22.05.2025 09:13] Extra JSON file exists (./assets/json/2505.15047.json), skip PDF parsing.
[22.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.15047.json), skip HTML parsing.
[22.05.2025 09:13] Success.
[22.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.15034.
[22.05.2025 09:13] Downloading paper 2505.15034 from http://arxiv.org/pdf/2505.15034v1...
[22.05.2025 09:13] Extracting affiliations from text.
[22.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning Kaiwen Zha1, Zhengqi Gao1, Maohao Shen1 Zhang-Wei Hong2 Duane S. Boning1 Dina Katabi1 1MIT 2MIT-IBM Watson AI Lab "
[22.05.2025 09:13] Response: ```python
["MIT", "MIT-IBM Watson AI Lab"]
```
[22.05.2025 09:13] Deleting PDF ./assets/pdf/2505.15034.pdf.
[22.05.2025 09:13] Success.
[22.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.14157.
[22.05.2025 09:13] Downloading paper 2505.14157 from http://arxiv.org/pdf/2505.14157v1...
[22.05.2025 09:13] Extracting affiliations from text.
[22.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 2 ] . [ 1 7 5 1 4 1 . 5 0 5 2 : r Prior Prompt Engineering for Reinforcement Fine-Tuning Pittawat Taveekitworachai1, Potsawee Manakul1, Sarana Nutanong2, Kunat Pipatanakul 1SCB 10X R&D, SCB 10X, SCBX Group, Thailand 2School of Information Science and Technology, Vidyasirimedhi Institute of Science and Technology, Thailand pittawat@scb10x.com, kunat@scb10x.com, potsawee@scb10x.com, snutanon@vistec.ac.th "
[22.05.2025 09:14] Response: ```python
[
    "SCB 10X R&D, SCB 10X, SCBX Group, Thailand",
    "School of Information Science and Technology, Vidyasirimedhi Institute of Science and Technology, Thailand"
]
```
[22.05.2025 09:14] Deleting PDF ./assets/pdf/2505.14157.pdf.
[22.05.2025 09:14] Success.
[22.05.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2505.14990.
[22.05.2025 09:14] Extra JSON file exists (./assets/json/2505.14990.json), skip PDF parsing.
[22.05.2025 09:14] Paper image links file exists (./assets/img_data/2505.14990.json), skip HTML parsing.
[22.05.2025 09:14] Success.
[22.05.2025 09:14] Enriching papers with extra data.
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 0. Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks. Yet, specialized reward models for web navigation that can be utilized during bot...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 1. Large language models (LLMs) demand substantial computational and memory resources, creating deployment challenges. Quantization-aware training (QAT) addresses these challenges by reducing model precision while maintaining performance. However, the scaling behavior of QAT, especially at 4-bit precis...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 2. We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a un...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 3. Traditional visual grounding methods primarily focus on single-image scenarios with simple textual references. However, extending these methods to real-world scenarios that involve implicit and complex instructions, particularly in conjunction with multiple images, poses significant challenges, whic...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 4. Scaling up high-quality trajectory data has long been a critical bottleneck for developing human-like computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 human-annotated co...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 5. Large language model (LLM)-based embedding models, benefiting from large scale pre-training and post-training, have begun to surpass BERT and T5-based models on general-purpose text embedding tasks such as document retrieval. However, a fundamental limitation of LLM embeddings lies in the unidirecti...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 6. Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 7. Large reasoning models (LRMs) achieve remarkable performance via long reasoning chains, but often incur excessive computational overhead due to redundant reasoning, especially on simple tasks. In this work, we systematically quantify the upper bounds of LRMs under both Long-Thinking and No-Thinking ...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 8. Knowledge graph-based retrieval-augmented generation seeks to mitigate hallucinations in Large Language Models (LLMs) caused by insufficient or outdated knowledge. However, existing methods often fail to fully exploit the prior knowledge embedded in knowledge graphs (KGs), particularly their structu...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 9. Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, a T2I model released in February would struggle to generate a suitable poster for a movie premiering in April, because the...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 10. World models, which predict transitions based on history observation and action sequences, have shown great promise in improving data efficiency for sequential decision making. However, existing world models often require extensive domain-specific training and still produce low-fidelity, coarse pred...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 11. Playing video games requires perception, memory, and planning, exactly the faculties modern large language model (LLM) agents are expected to master. We study the major challenges in using popular video games to evaluate modern LLMs and find that directly dropping LLMs into games cannot make an effe...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 12. Acquiring detailed 3D scenes typically demands costly equipment, multi-view data, or labor-intensive modeling. Therefore, a lightweight alternative, generating complex 3D scenes from a single top-down image, plays an essential role in real-world applications. While recent 3D generative models have a...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 13. Large Reasoning Models (LRMs) have achieved remarkable success on reasoning-intensive tasks such as mathematics and programming. However, their enhanced reasoning capabilities do not necessarily translate to improved safety performance-and in some cases, may even degrade it. This raises an important...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 14. Human beings naturally utilize multiple reasoning modalities to learn and solve logical problems, i.e., different representational formats such as natural language, code, and symbolic logic. In contrast, most existing LLM-based approaches operate with a single reasoning modality during training, typ...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 15. Diffusion Language Models (DLMs) have been seen as a promising competitor for autoregressive language models. However, diffusion language models have long been constrained by slow inference. A core challenge is that their non-autoregressive architecture and bidirectional attention preclude the key-v...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 16. Conversational search systems require effective handling of context-dependent queries that often contain ambiguity, omission, and coreference. Conversational Query Reformulation (CQR) addresses this challenge by transforming these queries into self-contained forms suitable for off-the-shelf retrieve...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 17. Fine-tuning on open-source Large Language Models (LLMs) with proprietary data is now a standard practice for downstream developers to obtain task-specific LLMs. Surprisingly, we reveal a new and concerning risk along with the practice: the creator of the open-source LLMs can later extract the privat...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 18. World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like ...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 19. Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with "I don't know". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 20. Human cognition typically involves thinking through abstract, fluid concepts rather than strictly using discrete linguistic tokens. Current reasoning models, however, are constrained to reasoning within the boundaries of human language, processing discrete token embeddings that represent fixed point...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 21. In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution's rich information, we propose Mixture of Inputs (MoI), a training-free method ...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 22. Machine learning-based interatomic potentials and force fields depend critically on accurate atomic structures, yet such data are scarce due to the limited availability of experimentally resolved crystals. Although atomic-resolution electron microscopy offers a potential source of structural data, c...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 23. Large multimodal models excel in multimodal tasks but face significant computational challenges due to excessive computation on visual tokens. Unlike token reduction methods that focus on token-level redundancy, we identify and study the computation-level redundancy on vision tokens to ensure no inf...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 24. Diffusion models have emerged as powerful generative tools across various domains, yet tailoring pre-trained models to exhibit specific desirable properties remains challenging. While reinforcement learning (RL) offers a promising solution,current methods struggle to simultaneously achieve stable, e...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 25. Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate remarkable potential for scientific discovery. Existing approaches, however, often automate scientific discovery using predefined workflows that lack rationality constraints. This often leads to aimless hypothesizing and a failur...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 26. Reinforcement learning (RL) has recently emerged as a compelling approach for enhancing the reasoning capabilities of large language models (LLMs), where an LLM generator serves as a policy guided by a verifier (reward model). However, current RL post-training methods for LLMs typically use verifier...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 27. This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shapin...
[22.05.2025 09:14] ********************************************************************************
[22.05.2025 09:14] Abstract 28. Code-switching is a common phenomenon of alternating between different languages in the same utterance, thought, or conversation. We posit that humans code-switch because they feel more comfortable talking about certain topics and domains in one language than another. With the rise of knowledge-inte...
[22.05.2025 09:14] Read previous papers.
[22.05.2025 09:14] Generating reviews via LLM API.
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#survey", "#benchmark", "#rl", "#dataset"], "emoji": "üß≠", "ru": {"title": "Web-Shepherd: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Web-Shepherd - –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ (PRM) –¥–ª—è –≤
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#training", "#optimization", "#inference"], "emoji": "üî¨", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –æ–±—É—á–µ–Ω–∏—è (QAT) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#training", "#multimodal", "#open_source", "#architecture", "#reasoning", "#diffusion", "#rl"], "emoji": "üß†", "ru": {"title": "MMaDA: –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "MMaDA - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ñ—É–Ω–¥–∞
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#training", "#multimodal", "#reasoning", "#rl", "#dataset"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UniVG-R1 - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#synthetic", "#benchmark", "#transfer_learning", "#training", "#dataset", "#agents"], "emoji": "üñ•Ô∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PC Agent-E - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–º–ø—å—é—Ç–µ
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#diffusion", "#reasoning", "#benchmark", "#training", "#long_context", "#architecture", "#dataset"], "emoji": "üß†", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#training", "#optimization"], "emoji": "üí°", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ü–µ–ø–æ—á–µ–∫ –º—ã—Å–ª–µ–π –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ LASER –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training", "#reasoning", "#benchmark"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –º–µ–Ω—å—à–µ –¥—É–º–∞–π, –±–æ–ª—å—à–µ –¥–µ–ª–∞–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rag", "#benchmark", "#hallucinations"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –∞–ø—Ä–∏–æ—Ä–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Deliberation over Priors' (DP) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#rag", "#alignment", "#dataset", "#multimodal", "#diffusion"], "emoji": "üñºÔ∏è", "ru": {"title": "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-augmented T2I: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é (T2I) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#architecture", "#games", "#robotics", "#diffusion", "#transfer_learning", "#rl", "#agents", "#video"], "emoji": "üé•", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Vid2World, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#games", "#agents", "#transfer_learning"], "emoji": "üéÆ", "ru": {"title": "–í–∏–¥–µ–æ–∏–≥—Ä—ã –∫–∞–∫ –ø–æ–ª–∏–≥–æ–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ–∏–≥—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∏ 
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#synthetic", "#3d"], "emoji": "üèôÔ∏è", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ 3D-–≥–æ—Ä–æ–¥–∞ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "3DTown - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–∞ —Å–≤–µ—Ä—Ö—É, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –≥
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#data", "#math", "#reasoning", "#training", "#safety"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ LRM: –ø—Ä–æ—Å—Ç–æ—Ç–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–ª—é—á–æ–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–ø–æ—Å–æ–±—ã –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ú–æ–¥–µ–ª–µ–π –ö—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (LRM) —Å –ø–æ–º–æ—â—å—é –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –¢–æ–Ω–∫–æ–π –ù
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#training", "#multimodal", "#benchmark", "#reasoning"], "emoji": "üß†", "ru": {"title": "–°–º–µ—à–µ–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - Mixture-of-Thought (MoT). MoT –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#architecture", "#inference", "#optimization", "#diffusion", "#benchmark"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–≥–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (DLM) - –æ—Ç
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#optimization", "#training", "#dataset", "#rag", "#rl", "#reasoning"], "emoji": "üîç", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–∞—é—â–∞—è—Å—è —Å–∏—Å—Ç–µ–º–∞ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ConvSearch-R1 - –ø–µ—Ä–≤—É—é —Å–∞–º–æ—É–ø—Ä–∞–≤–ª—è–µ–º—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#data", "#security", "#open_source", "#training", "#leakage"], "emoji": "üïµÔ∏è", "ru": {"title": "–°–∫—Ä—ã—Ç–∞—è —É–≥—Ä–æ–∑–∞: –∫–∞–∫ –æ—Ç–∫—Ä—ã—Ç—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ä–∞—Å–∫—Ä—ã—Ç—å –≤–∞—à–∏ —Å–µ–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π —Ä–∏—Å–∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#rlhf", "#training", "#multimodal", "#optimization", "#reasoning", "#games", "#rl", "#agents", "#video"], "emoji": "üåê", "ru": {"title": "RLVR-World: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "RLVR-World - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#hallucinations", "#reasoning", "#training", "#math"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ò–ò-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –æ—Å–æ–∑–Ω–∞–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –∑–Ω–∞–Ω–∏–π", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) –≤ –º–∞—Ç–µ
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#math", "#interpretability", "#reasoning", "#benchmark", "#training"], "emoji": "üß†", "ru": {"title": "–ú—è–≥–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ —è–∑—ã–∫–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –ò–ò", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Soft Thinking', –∫–æ—Ç–æ—Ä—ã–π –∏
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#math", "#training", "#reasoning", "#optimization"], "emoji": "üß†", "ru": {"title": "–°–º–µ—à–∏–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–≤—Ç–æ—Ä–µ–≥—Ä–∞–¥–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Mixture of Inputs (MoI). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#data", "#multimodal", "#open_source", "#benchmark", "#dataset", "#agents", "#science"], "emoji": "üî¨", "ru": {"title": "AutoMat: –æ—Ç –º–∏–∫—Ä–æ—Å–∫–æ–ø–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫ –∞—Ç–æ–º–Ω—ã–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º", "desc": "AutoMat - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–∫–∞–Ω–∏—Ä—É—é—â–µ–π –ø—Ä
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#open_source", "#architecture", "#inference"], "emoji": "üîç", "ru": {"title": "ProxyV: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ ProxyV –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#diffusion", "#rl", "#training", "#optimization"], "emoji": "üöÄ", "ru": {"title": "VARD: –£—Å–∏–ª–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º VARD (Value-based Reinforced Diffusion) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#rl", "#agents", "#science"], "emoji": "üß™", "ru": {"title": "PiFlow: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –Ω–∞—É—á–Ω–æ–º –æ—Ç–∫—Ä—ã—Ç–∏–∏", "desc": "PiFlow - —ç—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Å 
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#math", "#rlhf", "#reasoning", "#rl", "#training", "#optimization"], "emoji": "üï∫", "ru": {"title": "Tango: –¢–∞–Ω–µ—Ü –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Tango –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ä–∞—Å—Å—É–∂
[22.05.2025 09:14] Querying the API.
[22.05.2025 09:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shaping, and data curation, the design of the prior prompt--the instructions prepended to queries during training to elicit behaviors such as step-by-step reasoning--remains underexplored. We investigate whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT. Inspired by inference-time prompt engineering (iPE), we translate five representative iPE strategies--reasoning, planning, code-based reasoning, knowledge recall, and null-example utilization--into corresponding pPE approaches. We experiment with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and GPQA-Diamond). Our results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE approach achieving the largest average performance gain and the highest improvement on AIME2024 and GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by adapting a behavior-classification framework, we demonstrate that different pPE strategies instill distinct behavioral styles in the resulting models. These findings position pPE as a powerful yet understudied axis for RFT.
[22.05.2025 09:14] Response: {
  "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ (pPE) –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–ª–∏ –ø—è—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ pPE. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –≤—Å–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ —Å pPE, –ø—Ä–µ–≤–∑–æ—à–ª–∏ —Å–≤–æ–∏ –∞–Ω–∞–ª–æ–≥–∏ —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞, –ø—Ä–∏—á–µ–º –ø–æ–¥—Ö–æ–¥ —Å –Ω—É–ª–µ–≤—ã–º –ø—Ä–∏–º–µ—Ä–æ–º –ø–æ–∫–∞–∑–∞–ª –Ω–∞–∏–±–æ–ª—å—à–∏–π –ø—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–æ, —á—Ç–æ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ pPE —Ñ–æ—Ä–º–∏—Ä—É—é—Ç —É –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ —Å—Ç–∏–ª–∏.",
  "emoji": "üß†",
  "title": "–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω–∂–µ–Ω–µ—Ä–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º"
}
[22.05.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shaping, and data curation, the design of the prior prompt--the instructions prepended to queries during training to elicit behaviors such as step-by-step reasoning--remains underexplored. We investigate whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT. Inspired by inference-time prompt engineering (iPE), we translate five representative iPE strategies--reasoning, planning, code-based reasoning, knowledge recall, and null-example utilization--into corresponding pPE approaches. We experiment with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and GPQA-Diamond). Our results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE approach achieving the largest average performance gain and the highest improvement on AIME2024 and GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by adapting a behavior-classification framework, we demonstrate that different pPE strategies instill distinct behavioral styles in the resulting models. These findings position pPE as a powerful yet understudied axis for RFT."

[22.05.2025 09:14] Response: ```python
['RL', 'TRAINING', 'BENCHMARK']
```
[22.05.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shaping, and data curation, the design of the prior prompt--the instructions prepended to queries during training to elicit behaviors such as step-by-step reasoning--remains underexplored. We investigate whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT. Inspired by inference-time prompt engineering (iPE), we translate five representative iPE strategies--reasoning, planning, code-based reasoning, knowledge recall, and null-example utilization--into corresponding pPE approaches. We experiment with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and GPQA-Diamond). Our results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE approach achieving the largest average performance gain and the highest improvement on AIME2024 and GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by adapting a behavior-classification framework, we demonstrate that different pPE strategies instill distinct behavioral styles in the resulting models. These findings position pPE as a powerful yet understudied axis for RFT."

[22.05.2025 09:14] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[22.05.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores the concept of prior prompt engineering (pPE) in reinforcement fine-tuning (RFT) for language models (LMs). It highlights how the design of prompts, which guide the model\'s behavior during training, can significantly influence performance. The authors translate various inference-time prompt engineering strategies into prior prompt engineering methods and test them on a model called Qwen2.5-7B. The results indicate that models trained with pPE outperform those using traditional methods, particularly showing notable improvements in specific benchmarks.","title":"Unlocking Performance: The Power of Prior Prompt Engineering in Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper explores the concept of prior prompt engineering (pPE) in reinforcement fine-tuning (RFT) for language models (LMs). It highlights how the design of prompts, which guide the model's behavior during training, can significantly influence performance. The authors translate various inference-time prompt engineering strategies into prior prompt engineering methods and test them on a model called Qwen2.5-7B. The results indicate that models trained with pPE outperform those using traditional methods, particularly showing notable improvements in specific benchmarks.", title='Unlocking Performance: The Power of Prior Prompt Engineering in Language Models'))
[22.05.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂú®Âº∫ÂåñÂæÆË∞ÉÔºàRFTÔºâËÉåÊôØ‰∏ãÁöÑÂÖàÂâçÊèêÁ§∫Â∑•Á®ãÔºàpPEÔºâÔºåÊó®Âú®ÈÄöËøáÂ•ñÂä±‰ø°Âè∑ÂºïÂØºËØ≠Ë®ÄÊ®°ÂûãÔºàLMÔºâË°®Áé∞Âá∫ÊúÄÂ§ßÂåñÊÄßËÉΩÁöÑË°å‰∏∫„ÄÇÂ∞ΩÁÆ°Áé∞ÊúâÁöÑRFTÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÁÆóÊ≥ï„ÄÅÂ•ñÂä±Â°ëÈÄ†ÂíåÊï∞ÊçÆÊï¥ÁêÜ‰∏äÔºå‰ΩÜÂÖàÂâçÊèêÁ§∫ÁöÑËÆæËÆ°‰ªçÁÑ∂Êú™Ë¢´ÂÖÖÂàÜÊé¢ËÆ®„ÄÇÊàë‰ª¨Â∞Ü‰∫îÁßç‰ª£Ë°®ÊÄßÁöÑÊé®ÁêÜÊó∂Èó¥ÊèêÁ§∫Â∑•Á®ãÔºàiPEÔºâÁ≠ñÁï•ËΩ¨Âåñ‰∏∫Áõ∏Â∫îÁöÑpPEÊñπÊ≥ïÔºåÂπ∂Âú®Qwen2.5-7BÊ®°Âûã‰∏äËøõË°åÂÆûÈ™å„ÄÇÁªìÊûúË°®ÊòéÔºåÊâÄÊúâpPEËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏äÂùá‰ºò‰∫éiPEÊèêÁ§∫ÁöÑÊ®°ÂûãÔºåÂ∞§ÂÖ∂ÊòØnull-example pPEÊñπÊ≥ïÂú®AIME2024ÂíåGPQA-Diamond‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ","title":"ÂÖàÂâçÊèêÁ§∫Â∑•Á®ãÔºöÂº∫ÂåñÂæÆË∞ÉÁöÑÊñ∞ÊñπÂêë"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂú®Âº∫ÂåñÂæÆË∞ÉÔºàRFTÔºâËÉåÊôØ‰∏ãÁöÑÂÖàÂâçÊèêÁ§∫Â∑•Á®ãÔºàpPEÔºâÔºåÊó®Âú®ÈÄöËøáÂ•ñÂä±‰ø°Âè∑ÂºïÂØºËØ≠Ë®ÄÊ®°ÂûãÔºàLMÔºâË°®Áé∞Âá∫ÊúÄÂ§ßÂåñÊÄßËÉΩÁöÑË°å‰∏∫„ÄÇÂ∞ΩÁÆ°Áé∞ÊúâÁöÑRFTÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÁÆóÊ≥ï„ÄÅÂ•ñÂä±Â°ëÈÄ†ÂíåÊï∞ÊçÆÊï¥ÁêÜ‰∏äÔºå‰ΩÜÂÖàÂâçÊèêÁ§∫ÁöÑËÆæËÆ°‰ªçÁÑ∂Êú™Ë¢´ÂÖÖÂàÜÊé¢ËÆ®„ÄÇÊàë‰ª¨Â∞Ü‰∫îÁßç‰ª£Ë°®ÊÄßÁöÑÊé®ÁêÜÊó∂Èó¥ÊèêÁ§∫Â∑•Á®ãÔºàiPEÔºâÁ≠ñÁï•ËΩ¨Âåñ‰∏∫Áõ∏Â∫îÁöÑpPEÊñπÊ≥ïÔºåÂπ∂Âú®Qwen2.5-7BÊ®°Âûã‰∏äËøõË°åÂÆûÈ™å„ÄÇÁªìÊûúË°®ÊòéÔºåÊâÄÊúâpPEËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏äÂùá‰ºò‰∫éiPEÊèêÁ§∫ÁöÑÊ®°ÂûãÔºåÂ∞§ÂÖ∂ÊòØnull-example pPEÊñπÊ≥ïÂú®AIME2024ÂíåGPQA-Diamond‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ', title='ÂÖàÂâçÊèêÁ§∫Â∑•Á®ãÔºöÂº∫ÂåñÂæÆË∞ÉÁöÑÊñ∞ÊñπÂêë'))
[22.05.2025 09:14] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#alignment", "#benchmark", "#open_source", "#inference", "#low_resource", "#multilingual"], "emoji": "üåê", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏ –≤ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç —Ñ–µ–Ω–æ–º–µ–Ω —è–∑—ã–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π (LS
[22.05.2025 09:14] Trying to get texts in Chinese.
[22.05.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks. Yet, specialized reward models for web navigation that can be utilized during both training and test-time have been absent until now. Despite the importance of speed and cost-effectiveness, prior works have utilized MLLMs as reward models, which poses significant constraints for real-world deployment. To address this, in this work, we propose the first process reward model (PRM) called Web-Shepherd which could assess web navigation trajectories in a step-level. To achieve this, we first construct the WebPRM Collection, a large-scale dataset with 40K step-level preference pairs and annotated checklists spanning diverse domains and difficulty levels. Next, we also introduce the WebRewardBench, the first meta-evaluation benchmark for evaluating PRMs. In our experiments, we observe that our Web-Shepherd achieves about 30 points better accuracy compared to using GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by using GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve 10.9 points better performance, in 10 less cost compared to using GPT-4o-mini as the verifier. Our model, dataset, and code are publicly available at LINK.
[22.05.2025 09:14] Mistral response. {"id": "3986ad7cb19b4a56b790934361785c9d", "object": "chat.completion", "created": 1747905252, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u7f51\u9875\u5bfc\u822a\u7684\u81ea\u52a8\u5316\u4efb\u52a1\u3002\u8fc7\u53bb\u7684\u7814\u7a76\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\uff0c\u4f46\u8fd9\u5bf9\u5b9e\u9645\u5e94\u7528\u6709\u9650\u5236\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\uff0c\u79f0\u4e3aWeb-Shepherd\uff0c\u7528\u4e8e\u9010\u6b65\u8bc4\u4f30\u7f51\u9875\u5bfc\u822a\u8def\u5f84\u3002\u4ed6\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86Web-Shepherd\u7684\u6709\u6548\u6027\u548c\u6210\u672c\u6548\u76ca\u3002\u6240\u6709\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u4ee3\u7801\u90fd\u516c\u5f00\u53ef\u7528\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 357, "total_tokens": 519, "completion_tokens": 162}}
[22.05.2025 09:14] Response: ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÁΩëÈ°µÂØºËà™ÁöÑËá™Âä®Âåñ‰ªªÂä°„ÄÇËøáÂéªÁöÑÁ†îÁ©∂‰ΩøÁî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâ‰Ωú‰∏∫Â•ñÂä±Ê®°ÂûãÔºå‰ΩÜËøôÂØπÂÆûÈôÖÂ∫îÁî®ÊúâÈôêÂà∂„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫ÜÁ¨¨‰∏Ä‰∏™ËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMÔºâÔºåÁß∞‰∏∫Web-ShepherdÔºåÁî®‰∫éÈÄêÊ≠•ËØÑ‰º∞ÁΩëÈ°µÂØºËà™Ë∑ØÂæÑ„ÄÇ‰ªñ‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜÂíåËØÑ‰º∞Âü∫ÂáÜÔºåÂπ∂Âú®ÂÆûÈ™å‰∏≠ËØÅÊòé‰∫ÜWeb-ShepherdÁöÑÊúâÊïàÊÄßÂíåÊàêÊú¨ÊïàÁõä„ÄÇÊâÄÊúâÊ®°Âûã„ÄÅÊï∞ÊçÆÈõÜÂíå‰ª£Á†ÅÈÉΩÂÖ¨ÂºÄÂèØÁî®„ÄÇ
[22.05.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÁΩëÈ°µÂØºËà™ÁöÑËá™Âä®Âåñ‰ªªÂä°„ÄÇËøáÂéªÁöÑÁ†îÁ©∂‰ΩøÁî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâ‰Ωú‰∏∫Â•ñÂä±Ê®°ÂûãÔºå‰ΩÜËøôÂØπÂÆûÈôÖÂ∫îÁî®ÊúâÈôêÂà∂„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫ÜÁ¨¨‰∏Ä‰∏™ËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMÔºâÔºåÁß∞‰∏∫Web-ShepherdÔºåÁî®‰∫éÈÄêÊ≠•ËØÑ‰º∞ÁΩëÈ°µÂØºËà™Ë∑ØÂæÑ„ÄÇ‰ªñ‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜÂíåËØÑ‰º∞Âü∫ÂáÜÔºåÂπ∂Âú®ÂÆûÈ™å‰∏≠ËØÅÊòé‰∫ÜWeb-ShepherdÁöÑÊúâÊïàÊÄßÂíåÊàêÊú¨ÊïàÁõä„ÄÇÊâÄÊúâÊ®°Âûã„ÄÅÊï∞ÊçÆÈõÜÂíå‰ª£Á†ÅÈÉΩÂÖ¨ÂºÄÂèØÁî®„ÄÇ
[22.05.2025 09:15] Mistral response. {"id": "086e238b0a4e469183dc76118ff30cac", "object": "chat.completion", "created": 1747905270, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u7f51\u9875\u5bfc\u822a\u7684\u81ea\u52a8\u5316\u4efb\u52a1\u3002\u8fc7\u53bb\u7684\u7814\u7a76\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\uff0c\u4f46\u8fd9\u5bf9\u5b9e\u9645\u5e94\u7528\u6709\u9650\u5236\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\uff0c\u79f0\u4e3aWeb-Shepherd\uff0c\u7528\u4e8e\u9010\u6b65\u8bc4\u4f30\u7f51\u9875\u5bfc\u822a\u8def\u5f84\u3002\u4ed6\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86Web-Shepherd\u7684\u6709\u6548\u6027\u548c\u6210\u672c\u6548\u76ca\u3002\u6240\u6709\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u4ee3\u7801\u90fd\u516c\u5f00\u53ef\u7528\u3002\n\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng t\u01ceol\u00f9n le w\u01cengy\u00e8 d\u01ceoh\u00e1ng de z\u00ecd\u00f2nghu\u00e0 r\u00e8nw\u00f9. Gu\u00f2q\u00f9 de y\u00e1nji\u016b sh\u01d0y\u00f2ng du\u014d m\u00f3sh\u00ec d\u00e0 y\u01d4y\u00e1n m\u00f3x\u00edng (MLLM) zu\u00f2w\u00e9i ji\u01cengl\u00ec m\u00f3x\u00edng, d\u00e0n zh\u00e8 du\u00ec sh\u00edj\u00ec y\u00ecngy\u00f2ng y\u01d2u xi\u00e0nzh\u00ec. Zu\u00f2zh\u011b t\u00edch\u016b le d\u00ec-y\u012bg\u00e8 gu\u00f2ch\u00e9ng ji\u01cengl\u00ec m\u00f3x\u00edng (PRM), ch\u0113ngw\u00e9i Web-Shepherd, y\u00f2ngy\u00fa zh\u00fab\u00f9 p\u00edngji\u00e0 w\u01cengy\u00e8 d\u01ceoh\u00e1ng l\u00f9j\u00ecng. T\u0101men chu\u00e0ngji\u00e0n le y\u012bg\u00e8 d\u00e0 gu\u012bm\u00f3 sh\u00f9j\u00f9j\u00ed h\u00e9 p\u00edngji\u00e0 j\u012bzh\u01d4n, b\u00ecng z\u00e0i sh\u00ecy\u00e0n zh\u014dng zh\u00e8ngm\u00edng le Web-Shepherd de y\u01d2uxi\u00e0ox\u00ecng h\u00e9 ch\u00e9ngb\u011bn xi\u00e0oy\u00ec. Su\u01d2y\u01d2u m\u00f3x\u00edng, sh\u00f9j\u00f9j\u00ed h\u00e9 d\u00e0im\u01ce d\u014du g\u014dngk\u0101i k\u011by\u00f2ng."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 176, "total_tokens": 667, "completion_tokens": 491}}
[22.05.2025 09:15] Response: ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÁΩëÈ°µÂØºËà™ÁöÑËá™Âä®Âåñ‰ªªÂä°„ÄÇËøáÂéªÁöÑÁ†îÁ©∂‰ΩøÁî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâ‰Ωú‰∏∫Â•ñÂä±Ê®°ÂûãÔºå‰ΩÜËøôÂØπÂÆûÈôÖÂ∫îÁî®ÊúâÈôêÂà∂„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫ÜÁ¨¨‰∏Ä‰∏™ËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMÔºâÔºåÁß∞‰∏∫Web-ShepherdÔºåÁî®‰∫éÈÄêÊ≠•ËØÑ‰º∞ÁΩëÈ°µÂØºËà™Ë∑ØÂæÑ„ÄÇ‰ªñ‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜÂíåËØÑ‰º∞Âü∫ÂáÜÔºåÂπ∂Âú®ÂÆûÈ™å‰∏≠ËØÅÊòé‰∫ÜWeb-ShepherdÁöÑÊúâÊïàÊÄßÂíåÊàêÊú¨ÊïàÁõä„ÄÇÊâÄÊúâÊ®°Âûã„ÄÅÊï∞ÊçÆÈõÜÂíå‰ª£Á†ÅÈÉΩÂÖ¨ÂºÄÂèØÁî®„ÄÇ

Zh√® piƒÅn w√©nzhƒÅng t«éol√πn le w«éngy√® d«éoh√°ng de z√¨d√≤nghu√† r√®nw√π. Gu√≤q√π de y√°nji≈´ sh«êy√≤ng du≈ç m√≥sh√¨ d√† y«îy√°n m√≥x√≠ng (MLLM) zu√≤w√©i ji«éngl√¨ m√≥x√≠ng, d√†n zh√® du√¨ sh√≠j√¨ y√¨ngy√≤ng y«íu xi√†nzh√¨. Zu√≤zhƒõ t√≠ch≈´ le d√¨-yƒ´g√® gu√≤ch√©ng ji«éngl√¨ m√≥x√≠ng (PRM), chƒìngw√©i Web-Shepherd, y√≤ngy√∫ zh√∫b√π p√≠ngji√† w«éngy√® d«éoh√°ng l√πj√¨ng. TƒÅmen chu√†ngji√†n le yƒ´g√® d√† guƒ´m√≥ sh√πj√πj√≠ h√© p√≠ngji√† jƒ´zh«în, b√¨ng z√†i sh√¨y√†n zh≈çng zh√®ngm√≠ng le Web-Shepherd de y«íuxi√†ox√¨ng h√© ch√©ngbƒõn xi√†oy√¨. Su«íy«íu m√≥x√≠ng, sh√πj√πj√≠ h√© d√†im«é d≈çu g≈çngkƒÅi kƒõy√≤ng.
[22.05.2025 09:15] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÁΩëÈ°µÂØºËà™ÁöÑËá™Âä®Âåñ‰ªªÂä°„ÄÇËøáÂéªÁöÑÁ†îÁ©∂‰ΩøÁî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâ‰Ωú‰∏∫Â•ñÂä±Ê®°ÂûãÔºå‰ΩÜËøôÂØπÂÆûÈôÖÂ∫îÁî®ÊúâÈôêÂà∂„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫ÜÁ¨¨‰∏Ä‰∏™ËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMÔºâÔºåÁß∞‰∏∫Web-ShepherdÔºåÁî®‰∫éÈÄêÊ≠•ËØÑ‰º∞ÁΩëÈ°µÂØºËà™Ë∑ØÂæÑ„ÄÇ‰ªñ‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜÂíåËØÑ‰º∞Âü∫ÂáÜÔºåÂπ∂Âú®ÂÆûÈ™å‰∏≠ËØÅÊòé‰∫ÜWeb-ShepherdÁöÑÊúâÊïàÊÄßÂíåÊàêÊú¨ÊïàÁõä„ÄÇÊâÄÊúâÊ®°Âûã„ÄÅÊï∞ÊçÆÈõÜÂíå‰ª£Á†ÅÈÉΩÂÖ¨ÂºÄÂèØÁî®„ÄÇ
[22.05.2025 09:15] Mistral response. {"id": "e8d685165c93465ca913d3c24e05cb02", "object": "chat.completion", "created": 1747905338, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u8ba8\u8bba\", \"pinyin\": \"t\u01ceo l\u00f9n\", \"trans\": \"discuss\"},\n    {\"word\": \"\u7f51\u9875\u5bfc\u822a\", \"pinyin\": \"w\u01ceng y\u00e8 d\u01ceo h\u00e1ng\", \"trans\": \"web navigation\"},\n    {\"word\": \"\u81ea\u52a8\u5316\", \"pinyin\": \"z\u00ec d\u00f2ng hu\u00e0\", \"trans\": \"automation\"},\n    {\"word\": \"\u4efb\u52a1\", \"pinyin\": \"r\u00e8n wu\", \"trans\": \"task\"},\n    {\"word\": \"\u7814\u7a76\", \"pinyin\": \"y\u00e1n ji\u016b\", \"trans\": \"research\"},\n    {\"word\": \"\u591a\u6a21\u6001\", \"pinyin\": \"du\u014d m\u00f3 t\u00e0i\", \"trans\": \"multimodal\"},\n    {\"word\": \"\u5927\u8bed\u8a00\u6a21\u578b\", \"pinyin\": \"d\u00e0 y\u01d4 y\u00e1n m\u00f3 x\u00edng\", \"trans\": \"large language model\"},\n    {\"word\": \"\u5956\u52b1\u6a21\u578b\", \"pinyin\": \"ji\u01ceng l\u00ec m\u00f3 x\u00edng\", \"trans\": \"reward model\"},\n    {\"word\": \"\u9650\u5236\", \"pinyin\": \"xi\u00e0n zh\u00ec\", \"trans\": \"limit\"},\n    {\"word\": \"\u63d0\u51fa\", \"pinyin\": \"t\u00ed ch\u016b\", \"trans\": \"propose\"},\n    {\"word\": \"\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\", \"pinyin\": \"gu\u00f2 ch\u00e9ng ji\u01ceng l\u00ec m\u00f3 x\u00edng\", \"trans\": \"process reward model\"},\n    {\"word\": \"\u9010\u6b65\", \"pinyin\": \"zhu\u00f3 b\u00f9\", \"trans\": \"step-by-step\"},\n    {\"word\": \"\u8bc4\u4f30\", \"pinyin\": \"p\u00edng g\u016b\", \"trans\": \"evaluate\"},\n    {\"word\": \"\u8def\u5f84\", \"pinyin\": \"l\u00f9 j\u00ecng\", \"trans\": \"path\"},\n    {\"word\": \"\u521b\u5efa\", \"pinyin\": \"chu\u00e0ng ji\u00e0n\", \"trans\": \"create\"},\n    {\"word\": \"\u6570\u636e\u96c6\", \"pinyin\": \"sh\u00f9 j\u00f9 j\u00ed\", \"trans\": \"dataset\"},\n    {\"word\": \"\u8bc4\u4f30\u57fa\u51c6\", \"pinyin\": \"p\u00edng g\u016b j\u012b zh\u01d4n\", \"trans\": \"evaluation benchmark\"},\n    {\"word\": \"\u5b9e\u9a8c\", \"pinyin\": \"sh\u00ed y\u00e0n\", \"trans\": \"experiment\"},\n    {\"word\": \"\u8bc1\u660e\", \"pinyin\": \"zh\u00e8ng m\u00edng\", \"trans\": \"prove\"},\n    {\"word\": \"\u6709\u6548\u6027\", \"pinyin\": \"y\u01d2u xi\u00e0o x\u00ecng\", \"trans\": \"effectiveness\"},\n    {\"word\": \"\u6210\u672c\u6548\u76ca\", \"pinyin\": \"ch\u00e9ng b\u011bn xi\u00e0o y\u00ec\", \"trans\": \"cost-effectiveness\"},\n    {\"word\": \"\u516c\u5f00\u53ef\u7528\", \"pinyin\": \"g\u014dng k\u0101i k\u011b y\u00f2ng\", \"trans\": \"publicly available\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 208, "total_tokens": 922, "completion_tokens": 714}}
[22.05.2025 09:15] Response: [
    {"word": "ËÆ®ËÆ∫", "pinyin": "t«éo l√πn", "trans": "discuss"},
    {"word": "ÁΩëÈ°µÂØºËà™", "pinyin": "w«éng y√® d«éo h√°ng", "trans": "web navigation"},
    {"word": "Ëá™Âä®Âåñ", "pinyin": "z√¨ d√≤ng hu√†", "trans": "automation"},
    {"word": "‰ªªÂä°", "pinyin": "r√®n wu", "trans": "task"},
    {"word": "Á†îÁ©∂", "pinyin": "y√°n ji≈´", "trans": "research"},
    {"word": "Â§öÊ®°ÊÄÅ", "pinyin": "du≈ç m√≥ t√†i", "trans": "multimodal"},
    {"word": "Â§ßËØ≠Ë®ÄÊ®°Âûã", "pinyin": "d√† y«î y√°n m√≥ x√≠ng", "trans": "large language model"},
    {"word": "Â•ñÂä±Ê®°Âûã", "pinyin": "ji«éng l√¨ m√≥ x√≠ng", "trans": "reward model"},
    {"word": "ÈôêÂà∂", "pinyin": "xi√†n zh√¨", "trans": "limit"},
    {"word": "ÊèêÂá∫", "pinyin": "t√≠ ch≈´", "trans": "propose"},
    {"word": "ËøáÁ®ãÂ•ñÂä±Ê®°Âûã", "pinyin": "gu√≤ ch√©ng ji«éng l√¨ m√≥ x√≠ng", "trans": "process reward model"},
    {"word": "ÈÄêÊ≠•", "pinyin": "zhu√≥ b√π", "trans": "step-by-step"},
    {"word": "ËØÑ‰º∞", "pinyin": "p√≠ng g≈´", "trans": "evaluate"},
    {"word": "Ë∑ØÂæÑ", "pinyin": "l√π j√¨ng", "trans": "path"},
    {"word": "ÂàõÂª∫", "pinyin": "chu√†ng ji√†n", "trans": "create"},
    {"word": "Êï∞ÊçÆÈõÜ", "pinyin": "sh√π j√π j√≠", "trans": "dataset"},
    {"word": "ËØÑ‰º∞Âü∫ÂáÜ", "pinyin": "p√≠ng g≈´ jƒ´ zh«în", "trans": "evaluation benchmark"},
    {"word": "ÂÆûÈ™å", "pinyin": "sh√≠ y√†n", "trans": "experiment"},
    {"word": "ËØÅÊòé", "pinyin": "zh√®ng m√≠ng", "trans": "prove"},
    {"word": "ÊúâÊïàÊÄß", "pinyin": "y«íu xi√†o x√¨ng", "trans": "effectiveness"},
    {"word": "ÊàêÊú¨ÊïàÁõä", "pinyin": "ch√©ng bƒõn xi√†o y√¨", "trans": "cost-effectiveness"},
    {"word": "ÂÖ¨ÂºÄÂèØÁî®", "pinyin": "g≈çng kƒÅi kƒõ y√≤ng", "trans": "publicly available"}
]
[22.05.2025 09:15] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÁΩëÈ°µÂØºËà™ÁöÑËá™Âä®Âåñ‰ªªÂä°„ÄÇËøáÂéªÁöÑÁ†îÁ©∂‰ΩøÁî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâ‰Ωú‰∏∫Â•ñÂä±Ê®°ÂûãÔºå‰ΩÜËøôÂØπÂÆûÈôÖÂ∫îÁî®ÊúâÈôêÂà∂„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫ÜÁ¨¨‰∏Ä‰∏™ËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMÔºâÔºåÁß∞‰∏∫Web-ShepherdÔºåÁî®‰∫éÈÄêÊ≠•ËØÑ‰º∞ÁΩëÈ°µÂØºËà™Ë∑ØÂæÑ„ÄÇ‰ªñ‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜÂíåËØÑ‰º∞Âü∫ÂáÜÔºåÂπ∂Âú®ÂÆûÈ™å‰∏≠ËØÅÊòé‰∫ÜWeb-ShepherdÁöÑÊúâÊïàÊÄßÂíåÊàêÊú¨ÊïàÁõä„ÄÇÊâÄÊúâÊ®°Âûã„ÄÅÊï∞ÊçÆÈõÜÂíå‰ª£Á†ÅÈÉΩÂÖ¨ÂºÄÂèØÁî®„ÄÇ
[22.05.2025 09:15] Mistral response. {"id": "30c5814817be46cbb25914b89fadb9cf", "object": "chat.completion", "created": 1747905358, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article discusses the automation of web page navigation tasks. Previous research has used multimodal large language models (MLLMs) as reward models, but this has limitations for practical applications. The authors propose the first process reward model (PRM), called Web-Shepherd, for step-by-step evaluation of web page navigation paths. They created a large-scale dataset and evaluation benchmark, and demonstrated the effectiveness and cost-efficiency of Web-Shepherd in experiments. All models, datasets, and code are publicly available."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 174, "total_tokens": 290, "completion_tokens": 116}}
[22.05.2025 09:15] Response: This article discusses the automation of web page navigation tasks. Previous research has used multimodal large language models (MLLMs) as reward models, but this has limitations for practical applications. The authors propose the first process reward model (PRM), called Web-Shepherd, for step-by-step evaluation of web page navigation paths. They created a large-scale dataset and evaluation benchmark, and demonstrated the effectiveness and cost-efficiency of Web-Shepherd in experiments. All models, datasets, and code are publicly available.
[22.05.2025 09:15] Renaming data file.
[22.05.2025 09:15] Renaming previous data. hf_papers.json to ./d/2025-05-22.json
[22.05.2025 09:15] Saving new data file.
[22.05.2025 09:15] Generating page.
[22.05.2025 09:15] Renaming previous page.
[22.05.2025 09:15] Renaming previous data. index.html to ./d/2025-05-22.html
[22.05.2025 09:15] [Experimental] Generating Chinese page for reading.
[22.05.2025 09:15] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'ÁΩëÈ°µÂØºËà™', 'pinyin': 'w«éng y√® d«éo h√°ng', 'trans': 'web navigation'}, {'word': 'Ëá™Âä®Âåñ', 'pinyin': 'z√¨ d√≤ng hu√†', 'trans': 'automation'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®n wu', 'trans': 'task'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': 'Â•ñÂä±Ê®°Âûã', 'pinyin': 'ji«éng l√¨ m√≥ x√≠ng', 'trans': 'reward model'}, {'word': 'ÈôêÂà∂', 'pinyin': 'xi√†n zh√¨', 'trans': 'limit'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'ËøáÁ®ãÂ•ñÂä±Ê®°Âûã', 'pinyin': 'gu√≤ ch√©ng ji«éng l√¨ m√≥ x√≠ng', 'trans': 'process reward model'}, {'word': 'ÈÄêÊ≠•', 'pinyin': 'zhu√≥ b√π', 'trans': 'step-by-step'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ng g≈´', 'trans': 'evaluate'}, {'word': 'Ë∑ØÂæÑ', 'pinyin': 'l√π j√¨ng', 'trans': 'path'}, {'word': 'ÂàõÂª∫', 'pinyin': 'chu√†ng ji√†n', 'trans': 'create'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√π j√π j√≠', 'trans': 'dataset'}, {'word': 'ËØÑ‰º∞Âü∫ÂáÜ', 'pinyin': 'p√≠ng g≈´ jƒ´ zh«în', 'trans': 'evaluation benchmark'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ËØÅÊòé', 'pinyin': 'zh√®ng m√≠ng', 'trans': 'prove'}, {'word': 'ÊúâÊïàÊÄß', 'pinyin': 'y«íu xi√†o x√¨ng', 'trans': 'effectiveness'}, {'word': 'ÊàêÊú¨ÊïàÁõä', 'pinyin': 'ch√©ng bƒõn xi√†o y√¨', 'trans': 'cost-effectiveness'}, {'word': 'ÂÖ¨ÂºÄÂèØÁî®', 'pinyin': 'g≈çng kƒÅi kƒõ y√≤ng', 'trans': 'publicly available'}]
[22.05.2025 09:15] Renaming previous Chinese page.
[22.05.2025 09:15] Renaming previous data. zh.html to ./d/2025-05-21_zh_reading_task.html
[22.05.2025 09:15] Writing Chinese reading task.
[22.05.2025 09:15] Writing result.
[22.05.2025 09:15] Renaming log file.
[22.05.2025 09:15] Renaming previous data. log.txt to ./logs/2025-05-22_last_log.txt
