[22.05.2025 10:12] Read previous papers.
[22.05.2025 10:12] Generating top page (month).
[22.05.2025 10:12] Writing top page (month).
[22.05.2025 11:10] Read previous papers.
[22.05.2025 11:10] Get feed.
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15277
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14302
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14231
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15809
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13909
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15045
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15612
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15400
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15210
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15779
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14357
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15146
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15765
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15404
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15817
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15781
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15776
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15656
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13934
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13529
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15778
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14827
[22.05.2025 11:10] Extract page data from URL. URL: https://huggingface.co/papers/2505.14766
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12650
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15816
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15791
[22.05.2025 11:10] Extract page data from URL. URL: https://huggingface.co/papers/2505.15524
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15047
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15034
[22.05.2025 11:10] Extract page data from URL. URL: https://huggingface.co/papers/2505.15406
[22.05.2025 11:10] Extract page data from URL. URL: https://huggingface.co/papers/2505.14818
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14157
[22.05.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14990
[22.05.2025 11:10] Extract page data from URL. URL: https://huggingface.co/papers/2505.11454
[22.05.2025 11:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.05.2025 11:10] No deleted papers detected.
[22.05.2025 11:10] Downloading and parsing papers (pdf, html). Total: 34.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.15277.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.15277.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.15277.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.14302.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.14302.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.14302.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.14231.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.14231.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.14231.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.15809.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.15809.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.15809.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.13909.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.13909.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.13909.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.15045.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.15045.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.15045.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.15612.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.15612.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.15612.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.15400.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.15400.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.15400.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.15210.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.15210.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.15210.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.15779.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.15779.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.15779.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.14357.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.14357.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.14357.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.15146.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.15146.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.15146.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.15765.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.15765.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.15765.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.15404.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.15404.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.15404.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.15817.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.15817.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.15817.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.15781.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.15781.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.15781.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.15776.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.15776.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.15776.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.15656.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.15656.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.15656.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.13934.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.13934.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.13934.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.13529.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.13529.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.13529.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.15778.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.15778.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.15778.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.14827.
[22.05.2025 11:10] Extra JSON file exists (./assets/json/2505.14827.json), skip PDF parsing.
[22.05.2025 11:10] Paper image links file exists (./assets/img_data/2505.14827.json), skip HTML parsing.
[22.05.2025 11:10] Success.
[22.05.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2505.14766.
[22.05.2025 11:10] Downloading paper 2505.14766 from http://arxiv.org/pdf/2505.14766v1...
[22.05.2025 11:11] Extracting affiliations from text.
[22.05.2025 11:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 2 ] . [ 1 6 6 7 4 1 . 5 0 5 2 : r This Time is Different: An Observability Perspective on Time Series Foundation Models Ben Cohen Emaad Khwaja Youssef Doubli Salahidine Lemaachi Chris Lettieri Charles Masson Hugo Miccinilli Elise Rame Qiqi Ren Afshin Rostamizadeh Jean Ogier du Terrail Anna-Monica Toon Kan Wang Stephan Xie Zongzhe Xu David Asker Ameet Talwalkar Othmane Abou-Amal {ben.cohen, emaad, ameet, othmane}@datadoghq.com "
[22.05.2025 11:11] Response: ```python
["datadoghq.com"]
```
[22.05.2025 11:11] Deleting PDF ./assets/pdf/2505.14766.pdf.
[22.05.2025 11:11] Success.
[22.05.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2505.12650.
[22.05.2025 11:11] Extra JSON file exists (./assets/json/2505.12650.json), skip PDF parsing.
[22.05.2025 11:11] Paper image links file exists (./assets/img_data/2505.12650.json), skip HTML parsing.
[22.05.2025 11:11] Success.
[22.05.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2505.15816.
[22.05.2025 11:11] Extra JSON file exists (./assets/json/2505.15816.json), skip PDF parsing.
[22.05.2025 11:11] Paper image links file exists (./assets/img_data/2505.15816.json), skip HTML parsing.
[22.05.2025 11:11] Success.
[22.05.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2505.15791.
[22.05.2025 11:11] Extra JSON file exists (./assets/json/2505.15791.json), skip PDF parsing.
[22.05.2025 11:11] Paper image links file exists (./assets/img_data/2505.15791.json), skip HTML parsing.
[22.05.2025 11:11] Success.
[22.05.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2505.15524.
[22.05.2025 11:11] Downloading paper 2505.15524 from http://arxiv.org/pdf/2505.15524v1...
[22.05.2025 11:11] Extracting affiliations from text.
[22.05.2025 11:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 4 2 5 5 1 . 5 0 5 2 : r Evaluate Bias without Manual Test Sets: Concept Representation Perspective for LLMs Lang Gao12 Kaiyang Wan1 Wei Liu2 Chenxi Wang1 Zirui Song1 Zixiang Xu1 Yanbo Wang1 Veselin Stoyanov1 Xiuying Chen1 1MBZUAI 2Huazhong University of Science and Technology {Lang.Gao, Xiuying.Chen}@mbzuai.ac.ae "
[22.05.2025 11:11] Response: ```python
["MBZUAI", "Huazhong University of Science and Technology"]
```
[22.05.2025 11:11] Deleting PDF ./assets/pdf/2505.15524.pdf.
[22.05.2025 11:11] Success.
[22.05.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2505.15047.
[22.05.2025 11:11] Extra JSON file exists (./assets/json/2505.15047.json), skip PDF parsing.
[22.05.2025 11:11] Paper image links file exists (./assets/img_data/2505.15047.json), skip HTML parsing.
[22.05.2025 11:11] Success.
[22.05.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2505.15034.
[22.05.2025 11:11] Extra JSON file exists (./assets/json/2505.15034.json), skip PDF parsing.
[22.05.2025 11:11] Paper image links file exists (./assets/img_data/2505.15034.json), skip HTML parsing.
[22.05.2025 11:11] Success.
[22.05.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2505.15406.
[22.05.2025 11:11] Downloading paper 2505.15406 from http://arxiv.org/pdf/2505.15406v1...
[22.05.2025 11:11] Extracting affiliations from text.
[22.05.2025 11:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 6 0 4 5 1 . 5 0 5 2 : r Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models Zirui Song1 Qian Jiang 1 Mingxuan Cui1 Mingzhe Li2 Lang Gao 1 Zeyu Zhang 3 Zixiang Xu 1 Yanbo Wang 1 Chenxi Wang 1 Guangxian Ouyang 1 Zhenhao Chen 1 Xiuying Chen 1 1 Mohamed bin Zayed University of Artificial Intelligence 2 ByteDance 3 Australia National University "
[22.05.2025 11:11] Response: ```python
["Mohamed bin Zayed University of Artificial Intelligence", "ByteDance", "Australia National University"]
```
[22.05.2025 11:11] Deleting PDF ./assets/pdf/2505.15406.pdf.
[22.05.2025 11:11] Success.
[22.05.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2505.14818.
[22.05.2025 11:11] Downloading paper 2505.14818 from http://arxiv.org/pdf/2505.14818v1...
[22.05.2025 11:11] Extracting affiliations from text.
[22.05.2025 11:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 2 ] . [ 1 8 1 8 4 1 . 5 0 5 2 : r WebNovelBench: Placing LLM Novelists on the Web Novel Distribution Leon Lin1, Jun Zheng2, Haidong Wang2 1Nanyang Technological University, 2Sun Yat-Sen University liangtao.lin@ntu.edu.sg, {zhengj98, wanghd7}@mail2.sysu.edu.cn https://github.com/OedonLestrange42/webnovelbench https://huggingface.co/datasets/Oedon42/webnovelbench "
[22.05.2025 11:11] Response: ```python
["Nanyang Technological University", "Sun Yat-Sen University"]
```
[22.05.2025 11:11] Deleting PDF ./assets/pdf/2505.14818.pdf.
[22.05.2025 11:11] Success.
[22.05.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2505.14157.
[22.05.2025 11:11] Extra JSON file exists (./assets/json/2505.14157.json), skip PDF parsing.
[22.05.2025 11:11] Paper image links file exists (./assets/img_data/2505.14157.json), skip HTML parsing.
[22.05.2025 11:11] Success.
[22.05.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2505.14990.
[22.05.2025 11:11] Extra JSON file exists (./assets/json/2505.14990.json), skip PDF parsing.
[22.05.2025 11:11] Paper image links file exists (./assets/img_data/2505.14990.json), skip HTML parsing.
[22.05.2025 11:11] Success.
[22.05.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2505.11454.
[22.05.2025 11:11] Downloading paper 2505.11454 from http://arxiv.org/pdf/2505.11454v1...
[22.05.2025 11:12] Extracting affiliations from text.
[22.05.2025 11:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 1 4 5 4 1 1 . 5 0 5 2 : r HumaniBench: Human-Centric Framework for Large Multimodal Models Evaluation Shaina Raza1 Aravind Narayanan1 Vahid Reza Khazaie1 Ashmal Vayani2 Mukund S. Chettiar1 Amandeep Singh1 Mubarak Shah2 Deval Pandya1 2University of Central Florida, Orlando, USA 1Vector Institute, Toronto, Canada "
[22.05.2025 11:12] Response: ```python
["University of Central Florida, Orlando, USA", "Vector Institute, Toronto, Canada"]
```
[22.05.2025 11:12] Deleting PDF ./assets/pdf/2505.11454.pdf.
[22.05.2025 11:12] Success.
[22.05.2025 11:12] Enriching papers with extra data.
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 0. Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks. Yet, specialized reward models for web navigation that can be utilized during bot...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 1. Large language models (LLMs) demand substantial computational and memory resources, creating deployment challenges. Quantization-aware training (QAT) addresses these challenges by reducing model precision while maintaining performance. However, the scaling behavior of QAT, especially at 4-bit precis...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 2. Traditional visual grounding methods primarily focus on single-image scenarios with simple textual references. However, extending these methods to real-world scenarios that involve implicit and complex instructions, particularly in conjunction with multiple images, poses significant challenges, whic...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 3. We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a un...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 4. Scaling up high-quality trajectory data has long been a critical bottleneck for developing human-like computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 human-annotated co...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 5. Large language model (LLM)-based embedding models, benefiting from large scale pre-training and post-training, have begun to surpass BERT and T5-based models on general-purpose text embedding tasks such as document retrieval. However, a fundamental limitation of LLM embeddings lies in the unidirecti...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 6. Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 7. Large reasoning models (LRMs) achieve remarkable performance via long reasoning chains, but often incur excessive computational overhead due to redundant reasoning, especially on simple tasks. In this work, we systematically quantify the upper bounds of LRMs under both Long-Thinking and No-Thinking ...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 8. Knowledge graph-based retrieval-augmented generation seeks to mitigate hallucinations in Large Language Models (LLMs) caused by insufficient or outdated knowledge. However, existing methods often fail to fully exploit the prior knowledge embedded in knowledge graphs (KGs), particularly their structu...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 9. Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, a T2I model released in February would struggle to generate a suitable poster for a movie premiering in April, because the...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 10. World models, which predict transitions based on history observation and action sequences, have shown great promise in improving data efficiency for sequential decision making. However, existing world models often require extensive domain-specific training and still produce low-fidelity, coarse pred...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 11. Playing video games requires perception, memory, and planning, exactly the faculties modern large language model (LLM) agents are expected to master. We study the major challenges in using popular video games to evaluate modern LLMs and find that directly dropping LLMs into games cannot make an effe...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 12. Acquiring detailed 3D scenes typically demands costly equipment, multi-view data, or labor-intensive modeling. Therefore, a lightweight alternative, generating complex 3D scenes from a single top-down image, plays an essential role in real-world applications. While recent 3D generative models have a...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 13. Large Reasoning Models (LRMs) have achieved remarkable success on reasoning-intensive tasks such as mathematics and programming. However, their enhanced reasoning capabilities do not necessarily translate to improved safety performance-and in some cases, may even degrade it. This raises an important...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 14. Human beings naturally utilize multiple reasoning modalities to learn and solve logical problems, i.e., different representational formats such as natural language, code, and symbolic logic. In contrast, most existing LLM-based approaches operate with a single reasoning modality during training, typ...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 15. Diffusion Language Models (DLMs) have been seen as a promising competitor for autoregressive language models. However, diffusion language models have long been constrained by slow inference. A core challenge is that their non-autoregressive architecture and bidirectional attention preclude the key-v...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 16. Conversational search systems require effective handling of context-dependent queries that often contain ambiguity, omission, and coreference. Conversational Query Reformulation (CQR) addresses this challenge by transforming these queries into self-contained forms suitable for off-the-shelf retrieve...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 17. Fine-tuning on open-source Large Language Models (LLMs) with proprietary data is now a standard practice for downstream developers to obtain task-specific LLMs. Surprisingly, we reveal a new and concerning risk along with the practice: the creator of the open-source LLMs can later extract the privat...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 18. World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like ...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 19. Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with "I don't know". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 20. Human cognition typically involves thinking through abstract, fluid concepts rather than strictly using discrete linguistic tokens. Current reasoning models, however, are constrained to reasoning within the boundaries of human language, processing discrete token embeddings that represent fixed point...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 21. In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution's rich information, we propose Mixture of Inputs (MoI), a training-free method ...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 22. We introduce Toto, a time series forecasting foundation model with 151 million parameters. Toto uses a modern decoder-only architecture coupled with architectural innovations designed to account for specific challenges found in multivariate observability time series data. Toto's pre-training corpus ...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 23. Machine learning-based interatomic potentials and force fields depend critically on accurate atomic structures, yet such data are scarce due to the limited availability of experimentally resolved crystals. Although atomic-resolution electron microscopy offers a potential source of structural data, c...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 24. Large multimodal models excel in multimodal tasks but face significant computational challenges due to excessive computation on visual tokens. Unlike token reduction methods that focus on token-level redundancy, we identify and study the computation-level redundancy on vision tokens to ensure no inf...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 25. Diffusion models have emerged as powerful generative tools across various domains, yet tailoring pre-trained models to exhibit specific desirable properties remains challenging. While reinforcement learning (RL) offers a promising solution,current methods struggle to simultaneously achieve stable, e...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 26. Bias in Large Language Models (LLMs) significantly undermines their reliability and fairness. We focus on a common form of bias: when two reference concepts in the model's concept space, such as sentiment polarities (e.g., "positive" and "negative"), are asymmetrically correlated with a third, targe...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 27. Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate remarkable potential for scientific discovery. Existing approaches, however, often automate scientific discovery using predefined workflows that lack rationality constraints. This often leads to aimless hypothesizing and a failur...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 28. Reinforcement learning (RL) has recently emerged as a compelling approach for enhancing the reasoning capabilities of large language models (LLMs), where an LLM generator serves as a policy guided by a verifier (reward model). However, current RL post-training methods for LLMs typically use verifier...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 29. The rise of Large Audio Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks a systematic, quantitative evaluation of LAM safety especially against jailbreak attacks, which are challenging due to the ...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 30. Robustly evaluating the long-form storytelling capabilities of Large Language Models (LLMs) remains a significant challenge, as existing benchmarks often lack the necessary scale, diversity, or objective measures. To address this, we introduce WebNovelBench, a novel benchmark specifically designed f...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 31. This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shapin...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 32. Code-switching is a common phenomenon of alternating between different languages in the same utterance, thought, or conversation. We posit that humans code-switch because they feel more comfortable talking about certain topics and domains in one language than another. With the rise of knowledge-inte...
[22.05.2025 11:12] ********************************************************************************
[22.05.2025 11:12] Abstract 33. HumaniBench evaluates state-of-the-art LMMs on seven human-centered AI principles using 32K real-world image-question pairs to ensure fairness, ethics, empathy, and inclusivity.  					AI-generated summary 				 Large multimodal models (LMMs) now excel on many vision language benchmarks, however, they...
[22.05.2025 11:12] Read previous papers.
[22.05.2025 11:12] Generating reviews via LLM API.
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#survey", "#benchmark", "#rl", "#dataset"], "emoji": "üß≠", "ru": {"title": "Web-Shepherd: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Web-Shepherd - –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ (PRM) –¥–ª—è –≤
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#inference"], "emoji": "üî¨", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –æ–±—É—á–µ–Ω–∏—è (QAT) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#training", "#multimodal", "#reasoning", "#rl", "#dataset"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UniVG-R1 - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#training", "#multimodal", "#open_source", "#architecture", "#reasoning", "#diffusion", "#rl"], "emoji": "üß†", "ru": {"title": "MMaDA: –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "MMaDA - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ñ—É–Ω–¥–∞
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#synthetic", "#benchmark", "#transfer_learning", "#training", "#dataset", "#agents"], "emoji": "üñ•Ô∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PC Agent-E - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–º–ø—å—é—Ç–µ
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#diffusion", "#reasoning", "#benchmark", "#training", "#long_context", "#architecture", "#dataset"], "emoji": "üß†", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#training", "#optimization"], "emoji": "üí°", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ü–µ–ø–æ—á–µ–∫ –º—ã—Å–ª–µ–π –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ LASER –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training", "#reasoning", "#benchmark"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –º–µ–Ω—å—à–µ –¥—É–º–∞–π, –±–æ–ª—å—à–µ –¥–µ–ª–∞–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rag", "#benchmark", "#hallucinations"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –∞–ø—Ä–∏–æ—Ä–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Deliberation over Priors' (DP) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#rag", "#alignment", "#dataset", "#multimodal", "#diffusion"], "emoji": "üñºÔ∏è", "ru": {"title": "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-augmented T2I: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é (T2I) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#architecture", "#games", "#robotics", "#diffusion", "#transfer_learning", "#rl", "#agents", "#video"], "emoji": "üé•", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Vid2World, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#games", "#agents", "#transfer_learning"], "emoji": "üéÆ", "ru": {"title": "–í–∏–¥–µ–æ–∏–≥—Ä—ã –∫–∞–∫ –ø–æ–ª–∏–≥–æ–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ–∏–≥—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∏ 
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#synthetic", "#3d"], "emoji": "üèôÔ∏è", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ 3D-–≥–æ—Ä–æ–¥–∞ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "3DTown - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–∞ —Å–≤–µ—Ä—Ö—É, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –≥
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#data", "#math", "#reasoning", "#training", "#safety"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ LRM: –ø—Ä–æ—Å—Ç–æ—Ç–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–ª—é—á–æ–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–ø–æ—Å–æ–±—ã –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ú–æ–¥–µ–ª–µ–π –ö—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (LRM) —Å –ø–æ–º–æ—â—å—é –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –¢–æ–Ω–∫–æ–π –ù
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#training", "#multimodal", "#benchmark", "#reasoning"], "emoji": "üß†", "ru": {"title": "–°–º–µ—à–µ–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - Mixture-of-Thought (MoT). MoT –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#architecture", "#inference", "#optimization", "#diffusion", "#benchmark"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–≥–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (DLM) - –æ—Ç
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#dataset", "#rag", "#rl", "#reasoning"], "emoji": "üîç", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–∞—é—â–∞—è—Å—è —Å–∏—Å—Ç–µ–º–∞ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ConvSearch-R1 - –ø–µ—Ä–≤—É—é —Å–∞–º–æ—É–ø—Ä–∞–≤–ª—è–µ–º—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#data", "#security", "#open_source", "#training", "#leakage"], "emoji": "üïµÔ∏è", "ru": {"title": "–°–∫—Ä—ã—Ç–∞—è —É–≥—Ä–æ–∑–∞: –∫–∞–∫ –æ—Ç–∫—Ä—ã—Ç—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ä–∞—Å–∫—Ä—ã—Ç—å –≤–∞—à–∏ —Å–µ–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π —Ä–∏—Å–∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#rlhf", "#training", "#multimodal", "#optimization", "#reasoning", "#games", "#rl", "#agents", "#video"], "emoji": "üåê", "ru": {"title": "RLVR-World: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "RLVR-World - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#hallucinations", "#reasoning", "#training", "#math"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ò–ò-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –æ—Å–æ–∑–Ω–∞–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –∑–Ω–∞–Ω–∏–π", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) –≤ –º–∞—Ç–µ
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#math", "#interpretability", "#reasoning", "#benchmark", "#training"], "emoji": "üß†", "ru": {"title": "–ú—è–≥–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ —è–∑—ã–∫–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –ò–ò", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Soft Thinking', –∫–æ—Ç–æ—Ä—ã–π –∏
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#math", "#training", "#reasoning", "#optimization"], "emoji": "üß†", "ru": {"title": "–°–º–µ—à–∏–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–≤—Ç–æ—Ä–µ–≥—Ä–∞–¥–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Mixture of Inputs (MoI). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä
[22.05.2025 11:12] Querying the API.
[22.05.2025 11:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We introduce Toto, a time series forecasting foundation model with 151 million parameters. Toto uses a modern decoder-only architecture coupled with architectural innovations designed to account for specific challenges found in multivariate observability time series data. Toto's pre-training corpus is a mixture of observability data, open datasets, and synthetic data, and is 4-10times larger than those of leading time series foundation models. Additionally, we introduce BOOM, a large-scale benchmark consisting of 350 million observations across 2,807 real-world time series. For both Toto and BOOM, we source observability data exclusively from Datadog's own telemetry and internal observability metrics. Extensive evaluations demonstrate that Toto achieves state-of-the-art performance on both BOOM and on established general purpose time series forecasting benchmarks. Toto's model weights, inference code, and evaluation scripts, as well as BOOM's data and evaluation code, are all available as open source under the Apache 2.0 License available at https://huggingface.co/Datadog/Toto-Open-Base-1.0 and https://github.com/DataDog/toto.
[22.05.2025 11:12] Response: {
  "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Toto - —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å 151 –º–∏–ª–ª–∏–æ–Ω–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. Toto –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–µ–∫–æ–¥–µ—Ä–∞ —Å –∏–Ω–Ω–æ–≤–∞—Ü–∏—è–º–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç–∏, –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –≤ 4-10 —Ä–∞–∑ –±–æ–ª—å—à–µ, —á–µ–º —É –≤–µ–¥—É—â–∏—Ö –º–æ–¥–µ–ª–µ–π –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤. Toto –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–∞–∫ –Ω–∞ –Ω–æ–≤–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ BOOM, —Ç–∞–∫ –∏ –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.",
  "emoji": "üìà",
  "title": "Toto: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"
}
[22.05.2025 11:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce Toto, a time series forecasting foundation model with 151 million parameters. Toto uses a modern decoder-only architecture coupled with architectural innovations designed to account for specific challenges found in multivariate observability time series data. Toto's pre-training corpus is a mixture of observability data, open datasets, and synthetic data, and is 4-10times larger than those of leading time series foundation models. Additionally, we introduce BOOM, a large-scale benchmark consisting of 350 million observations across 2,807 real-world time series. For both Toto and BOOM, we source observability data exclusively from Datadog's own telemetry and internal observability metrics. Extensive evaluations demonstrate that Toto achieves state-of-the-art performance on both BOOM and on established general purpose time series forecasting benchmarks. Toto's model weights, inference code, and evaluation scripts, as well as BOOM's data and evaluation code, are all available as open source under the Apache 2.0 License available at https://huggingface.co/Datadog/Toto-Open-Base-1.0 and https://github.com/DataDog/toto."

[22.05.2025 11:12] Response: ```python
["DATASET", "BENCHMARK", "ARCHITECTURE", "SMALL_MODELS"]
```
[22.05.2025 11:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce Toto, a time series forecasting foundation model with 151 million parameters. Toto uses a modern decoder-only architecture coupled with architectural innovations designed to account for specific challenges found in multivariate observability time series data. Toto's pre-training corpus is a mixture of observability data, open datasets, and synthetic data, and is 4-10times larger than those of leading time series foundation models. Additionally, we introduce BOOM, a large-scale benchmark consisting of 350 million observations across 2,807 real-world time series. For both Toto and BOOM, we source observability data exclusively from Datadog's own telemetry and internal observability metrics. Extensive evaluations demonstrate that Toto achieves state-of-the-art performance on both BOOM and on established general purpose time series forecasting benchmarks. Toto's model weights, inference code, and evaluation scripts, as well as BOOM's data and evaluation code, are all available as open source under the Apache 2.0 License available at https://huggingface.co/Datadog/Toto-Open-Base-1.0 and https://github.com/DataDog/toto."

[22.05.2025 11:12] Response: ```python
['SYNTHETIC', 'OPEN_SOURCE']
```
[22.05.2025 11:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Toto is a new foundation model designed for time series forecasting, featuring 151 million parameters and a decoder-only architecture. It addresses challenges in multivariate observability time series data through innovative design choices. The model is trained on a diverse dataset that is significantly larger than those used by existing models, combining real and synthetic observability data. Evaluations show that Toto outperforms other models on both a new benchmark called BOOM and established forecasting tasks, with all resources made available as open source.","title":"Toto: Revolutionizing Time Series Forecasting with State-of-the-Art Performance"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Toto is a new foundation model designed for time series forecasting, featuring 151 million parameters and a decoder-only architecture. It addresses challenges in multivariate observability time series data through innovative design choices. The model is trained on a diverse dataset that is significantly larger than those used by existing models, combining real and synthetic observability data. Evaluations show that Toto outperforms other models on both a new benchmark called BOOM and established forecasting tasks, with all resources made available as open source.', title='Toto: Revolutionizing Time Series Forecasting with State-of-the-Art Performance'))
[22.05.2025 11:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫ÜTotoÔºå‰∏Ä‰∏™ÂÖ∑Êúâ1.51‰∫øÂèÇÊï∞ÁöÑÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÂü∫Á°ÄÊ®°Âûã„ÄÇTotoÈááÁî®Áé∞‰ª£ÁöÑËß£Á†ÅÂô®Êû∂ÊûÑÔºåÂπ∂ÈíàÂØπÂ§öÂèòÈáèÂèØËßÇÊµãÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆÁöÑÁâπÂÆöÊåëÊàòËøõË°å‰∫ÜÊû∂ÊûÑÂàõÊñ∞„ÄÇÂÖ∂È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁî±ÂèØËßÇÊµãÊï∞ÊçÆ„ÄÅÂºÄÊîæÊï∞ÊçÆÈõÜÂíåÂêàÊàêÊï∞ÊçÆÊ∑∑ÂêàËÄåÊàêÔºåËßÑÊ®°ÊòØÈ¢ÜÂÖàÊó∂Èó¥Â∫èÂàóÂü∫Á°ÄÊ®°ÂûãÁöÑ4Âà∞10ÂÄç„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáËøò‰ªãÁªç‰∫ÜBOOMÔºå‰∏Ä‰∏™ÂåÖÂê´2,807‰∏™ÁúüÂÆû‰∏ñÁïåÊó∂Èó¥Â∫èÂàóÁöÑ350Áôæ‰∏áËßÇÊµãÂÄºÁöÑÂ§ßËßÑÊ®°Âü∫ÂáÜ„ÄÇ","title":"TotoÔºöÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÁöÑÊñ∞Âü∫ÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫ÜTotoÔºå‰∏Ä‰∏™ÂÖ∑Êúâ1.51‰∫øÂèÇÊï∞ÁöÑÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÂü∫Á°ÄÊ®°Âûã„ÄÇTotoÈááÁî®Áé∞‰ª£ÁöÑËß£Á†ÅÂô®Êû∂ÊûÑÔºåÂπ∂ÈíàÂØπÂ§öÂèòÈáèÂèØËßÇÊµãÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆÁöÑÁâπÂÆöÊåëÊàòËøõË°å‰∫ÜÊû∂ÊûÑÂàõÊñ∞„ÄÇÂÖ∂È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁî±ÂèØËßÇÊµãÊï∞ÊçÆ„ÄÅÂºÄÊîæÊï∞ÊçÆÈõÜÂíåÂêàÊàêÊï∞ÊçÆÊ∑∑ÂêàËÄåÊàêÔºåËßÑÊ®°ÊòØÈ¢ÜÂÖàÊó∂Èó¥Â∫èÂàóÂü∫Á°ÄÊ®°ÂûãÁöÑ4Âà∞10ÂÄç„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáËøò‰ªãÁªç‰∫ÜBOOMÔºå‰∏Ä‰∏™ÂåÖÂê´2,807‰∏™ÁúüÂÆû‰∏ñÁïåÊó∂Èó¥Â∫èÂàóÁöÑ350Áôæ‰∏áËßÇÊµãÂÄºÁöÑÂ§ßËßÑÊ®°Âü∫ÂáÜ„ÄÇ', title='TotoÔºöÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÁöÑÊñ∞Âü∫ÂáÜ'))
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#data", "#multimodal", "#open_source", "#benchmark", "#dataset", "#agents", "#science"], "emoji": "üî¨", "ru": {"title": "AutoMat: –æ—Ç –º–∏–∫—Ä–æ—Å–∫–æ–ø–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫ –∞—Ç–æ–º–Ω—ã–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º", "desc": "AutoMat - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–∫–∞–Ω–∏—Ä—É—é—â–µ–π –ø—Ä
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#open_source", "#architecture", "#inference"], "emoji": "üîç", "ru": {"title": "ProxyV: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ ProxyV –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#diffusion", "#rl", "#training", "#optimization"], "emoji": "üöÄ", "ru": {"title": "VARD: –£—Å–∏–ª–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º VARD (Value-based Reinforced Diffusion) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ
[22.05.2025 11:12] Querying the API.
[22.05.2025 11:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Bias in Large Language Models (LLMs) significantly undermines their reliability and fairness. We focus on a common form of bias: when two reference concepts in the model's concept space, such as sentiment polarities (e.g., "positive" and "negative"), are asymmetrically correlated with a third, target concept, such as a reviewing aspect, the model exhibits unintended bias. For instance, the understanding of "food" should not skew toward any particular sentiment. Existing bias evaluation methods assess behavioral differences of LLMs by constructing labeled data for different social groups and measuring model responses across them, a process that requires substantial human effort and captures only a limited set of social concepts. To overcome these limitations, we propose BiasLens, a test-set-free bias analysis framework based on the structure of the model's vector space. BiasLens combines Concept Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract interpretable concept representations, and quantifies bias by measuring the variation in representational similarity between the target concept and each of the reference concepts. Even without labeled data, BiasLens shows strong agreement with traditional bias evaluation metrics (Spearman correlation r > 0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect using existing methods. For example, in simulated clinical scenarios, a patient's insurance status can cause the LLM to produce biased diagnostic assessments. Overall, BiasLens offers a scalable, interpretable, and efficient paradigm for bias discovery, paving the way for improving fairness and transparency in LLMs.
[22.05.2025 11:12] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç BiasLens - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö. BiasLens –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä—ã –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π (CAV) –∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã (SAE) –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∏–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –∏ —Å–ø–æ—Å–æ–±–µ–Ω –≤—ã—è–≤–ª—è—Ç—å —Ñ–æ—Ä–º—ã –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏, —Ç—Ä—É–¥–Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º—ã–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. BiasLens –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ LLM, —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—è —É–ª—É—á—à–µ–Ω–∏—é –∏—Ö —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏.",
  "emoji": "üîç",
  "title": "BiasLens: –ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö"
}
[22.05.2025 11:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Bias in Large Language Models (LLMs) significantly undermines their reliability and fairness. We focus on a common form of bias: when two reference concepts in the model's concept space, such as sentiment polarities (e.g., "positive" and "negative"), are asymmetrically correlated with a third, target concept, such as a reviewing aspect, the model exhibits unintended bias. For instance, the understanding of "food" should not skew toward any particular sentiment. Existing bias evaluation methods assess behavioral differences of LLMs by constructing labeled data for different social groups and measuring model responses across them, a process that requires substantial human effort and captures only a limited set of social concepts. To overcome these limitations, we propose BiasLens, a test-set-free bias analysis framework based on the structure of the model's vector space. BiasLens combines Concept Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract interpretable concept representations, and quantifies bias by measuring the variation in representational similarity between the target concept and each of the reference concepts. Even without labeled data, BiasLens shows strong agreement with traditional bias evaluation metrics (Spearman correlation r > 0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect using existing methods. For example, in simulated clinical scenarios, a patient's insurance status can cause the LLM to produce biased diagnostic assessments. Overall, BiasLens offers a scalable, interpretable, and efficient paradigm for bias discovery, paving the way for improving fairness and transparency in LLMs."

[22.05.2025 11:12] Response: ```python
["DATA", "BENCHMARK", "MULTIMODAL"]
```
[22.05.2025 11:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Bias in Large Language Models (LLMs) significantly undermines their reliability and fairness. We focus on a common form of bias: when two reference concepts in the model's concept space, such as sentiment polarities (e.g., "positive" and "negative"), are asymmetrically correlated with a third, target concept, such as a reviewing aspect, the model exhibits unintended bias. For instance, the understanding of "food" should not skew toward any particular sentiment. Existing bias evaluation methods assess behavioral differences of LLMs by constructing labeled data for different social groups and measuring model responses across them, a process that requires substantial human effort and captures only a limited set of social concepts. To overcome these limitations, we propose BiasLens, a test-set-free bias analysis framework based on the structure of the model's vector space. BiasLens combines Concept Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract interpretable concept representations, and quantifies bias by measuring the variation in representational similarity between the target concept and each of the reference concepts. Even without labeled data, BiasLens shows strong agreement with traditional bias evaluation metrics (Spearman correlation r > 0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect using existing methods. For example, in simulated clinical scenarios, a patient's insurance status can cause the LLM to produce biased diagnostic assessments. Overall, BiasLens offers a scalable, interpretable, and efficient paradigm for bias discovery, paving the way for improving fairness and transparency in LLMs."

[22.05.2025 11:12] Response: ```python
['ETHICS', 'INTERPRETABILITY']
```
[22.05.2025 11:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the issue of bias in Large Language Models (LLMs), particularly focusing on how certain concepts can be unfairly correlated with others, leading to skewed outputs. The authors introduce BiasLens, a novel framework that analyzes bias without the need for labeled data, using the model\'s vector space structure. By employing Concept Activation Vectors (CAVs) and Sparse Autoencoders (SAEs), BiasLens quantifies bias through representational similarity, demonstrating strong alignment with traditional evaluation methods. This approach not only enhances the detection of subtle biases but also promotes fairness and transparency in LLMs.","title":"Uncovering Bias in Language Models with BiasLens"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper addresses the issue of bias in Large Language Models (LLMs), particularly focusing on how certain concepts can be unfairly correlated with others, leading to skewed outputs. The authors introduce BiasLens, a novel framework that analyzes bias without the need for labeled data, using the model's vector space structure. By employing Concept Activation Vectors (CAVs) and Sparse Autoencoders (SAEs), BiasLens quantifies bias through representational similarity, demonstrating strong alignment with traditional evaluation methods. This approach not only enhances the detection of subtle biases but also promotes fairness and transparency in LLMs.", title='Uncovering Bias in Language Models with BiasLens'))
[22.05.2025 11:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑÂÅèËßÅÈóÆÈ¢òÔºåÊåáÂá∫ËøôÁßçÂÅèËßÅ‰ºöÂΩ±ÂìçÊ®°ÂûãÁöÑÂèØÈù†ÊÄßÂíåÂÖ¨Âπ≥ÊÄß„ÄÇÊàë‰ª¨ÂÖ≥Ê≥®‰∏ÄÁßçÂ∏∏ËßÅÁöÑÂÅèËßÅÂΩ¢ÂºèÔºåÂç≥Ê®°ÂûãÊ¶ÇÂøµÁ©∫Èó¥‰∏≠‰∏§‰∏™ÂèÇËÄÉÊ¶ÇÂøµÔºàÂ¶ÇÊÉÖÊÑüÊûÅÊÄßÔºâ‰∏éÁõÆÊ†áÊ¶ÇÂøµÔºàÂ¶ÇËØÑËÆ∫ÊñπÈù¢Ôºâ‰πãÈó¥ÁöÑ‰∏çÂØπÁß∞Áõ∏ÂÖ≥ÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Áé∞ÊúâÂÅèËßÅËØÑ‰º∞ÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄßÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜBiasLensÊ°ÜÊû∂ÔºåÂÆÉÁªìÂêà‰∫ÜÊ¶ÇÂøµÊøÄÊ¥ªÂêëÈáèÔºàCAVsÔºâÂíåÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSAEsÔºâÔºåÊó†ÈúÄÊ†áÊ≥®Êï∞ÊçÆÂç≥ÂèØËøõË°åÂÅèËßÅÂàÜÊûê„ÄÇBiasLensËÉΩÂ§üÊúâÊïàÈáèÂåñÂÅèËßÅÔºåÂπ∂Êè≠Á§∫‰º†ÁªüÊñπÊ≥ïÈöæ‰ª•Ê£ÄÊµãÁöÑÂÅèËßÅÂΩ¢ÂºèÔºå‰ªéËÄå‰∏∫ÊèêÈ´òLLMsÁöÑÂÖ¨Âπ≥ÊÄßÂíåÈÄèÊòéÊÄßÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ","title":"Êè≠Á§∫Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂÅèËßÅÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑÂÅèËßÅÈóÆÈ¢òÔºåÊåáÂá∫ËøôÁßçÂÅèËßÅ‰ºöÂΩ±ÂìçÊ®°ÂûãÁöÑÂèØÈù†ÊÄßÂíåÂÖ¨Âπ≥ÊÄß„ÄÇÊàë‰ª¨ÂÖ≥Ê≥®‰∏ÄÁßçÂ∏∏ËßÅÁöÑÂÅèËßÅÂΩ¢ÂºèÔºåÂç≥Ê®°ÂûãÊ¶ÇÂøµÁ©∫Èó¥‰∏≠‰∏§‰∏™ÂèÇËÄÉÊ¶ÇÂøµÔºàÂ¶ÇÊÉÖÊÑüÊûÅÊÄßÔºâ‰∏éÁõÆÊ†áÊ¶ÇÂøµÔºàÂ¶ÇËØÑËÆ∫ÊñπÈù¢Ôºâ‰πãÈó¥ÁöÑ‰∏çÂØπÁß∞Áõ∏ÂÖ≥ÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Áé∞ÊúâÂÅèËßÅËØÑ‰º∞ÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄßÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜBiasLensÊ°ÜÊû∂ÔºåÂÆÉÁªìÂêà‰∫ÜÊ¶ÇÂøµÊøÄÊ¥ªÂêëÈáèÔºàCAVsÔºâÂíåÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSAEsÔºâÔºåÊó†ÈúÄÊ†áÊ≥®Êï∞ÊçÆÂç≥ÂèØËøõË°åÂÅèËßÅÂàÜÊûê„ÄÇBiasLensËÉΩÂ§üÊúâÊïàÈáèÂåñÂÅèËßÅÔºåÂπ∂Êè≠Á§∫‰º†ÁªüÊñπÊ≥ïÈöæ‰ª•Ê£ÄÊµãÁöÑÂÅèËßÅÂΩ¢ÂºèÔºå‰ªéËÄå‰∏∫ÊèêÈ´òLLMsÁöÑÂÖ¨Âπ≥ÊÄßÂíåÈÄèÊòéÊÄßÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ', title='Êè≠Á§∫Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂÅèËßÅÊñ∞ÊñπÊ≥ï'))
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#rl", "#agents", "#science"], "emoji": "üß™", "ru": {"title": "PiFlow: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –Ω–∞—É—á–Ω–æ–º –æ—Ç–∫—Ä—ã—Ç–∏–∏", "desc": "PiFlow - —ç—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Å 
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#math", "#rlhf", "#reasoning", "#rl", "#training", "#optimization"], "emoji": "üï∫", "ru": {"title": "Tango: –¢–∞–Ω–µ—Ü –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Tango –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ä–∞—Å—Å—É–∂
[22.05.2025 11:12] Querying the API.
[22.05.2025 11:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The rise of Large Audio Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks a systematic, quantitative evaluation of LAM safety especially against jailbreak attacks, which are challenging due to the temporal and semantic nature of speech. To bridge this gap, we introduce AJailBench, the first benchmark specifically designed to evaluate jailbreak vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, converted from textual jailbreak attacks using realistic text to speech synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and reveal that none exhibit consistent robustness across attacks. To further strengthen jailbreak testing and simulate more realistic attack conditions, we propose a method to generate dynamic adversarial variants. Our Audio Perturbation Toolkit (APT) applies targeted distortions across time, frequency, and amplitude domains. To preserve the original jailbreak intent, we enforce a semantic consistency constraint and employ Bayesian optimization to efficiently search for perturbations that are both subtle and highly effective. This results in AJailBench-APT, an extended dataset of optimized adversarial audio samples. Our findings demonstrate that even small, semantically preserved perturbations can significantly reduce the safety performance of leading LAMs, underscoring the need for more robust and semantically aware defense mechanisms.
[22.05.2025 11:12] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AJailBench - –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –∫ –∞—Ç–∞–∫–∞–º –¥–∂–µ–π–ª–±—Ä–µ–π–∫–∞ –≤ –±–æ–ª—å—à–∏—Ö –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LAM). –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö AJailBench-Base –∏–∑ 1495 –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã—Ö –∞—É–¥–∏–æ-–∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ü–µ–Ω–∏–ª–∏ –Ω–∞ –Ω–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LAM. –¢–∞–∫–∂–µ –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ä–∏–π Audio Perturbation Toolkit –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∞—Ç–∞–∫ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏–∫–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –Ω–µ–±–æ–ª—å—à–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –≤–æ–∑–º—É—â–µ–Ω–∏—è –º–æ–≥—É—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∑–∏—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –≤–µ–¥—É—â–∏—Ö LAM.",
  "emoji": "üéôÔ∏è",
  "title": "AJailBench: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∞—É–¥–∏–æ-–ò–ò"
}
[22.05.2025 11:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The rise of Large Audio Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks a systematic, quantitative evaluation of LAM safety especially against jailbreak attacks, which are challenging due to the temporal and semantic nature of speech. To bridge this gap, we introduce AJailBench, the first benchmark specifically designed to evaluate jailbreak vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, converted from textual jailbreak attacks using realistic text to speech synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and reveal that none exhibit consistent robustness across attacks. To further strengthen jailbreak testing and simulate more realistic attack conditions, we propose a method to generate dynamic adversarial variants. Our Audio Perturbation Toolkit (APT) applies targeted distortions across time, frequency, and amplitude domains. To preserve the original jailbreak intent, we enforce a semantic consistency constraint and employ Bayesian optimization to efficiently search for perturbations that are both subtle and highly effective. This results in AJailBench-APT, an extended dataset of optimized adversarial audio samples. Our findings demonstrate that even small, semantically preserved perturbations can significantly reduce the safety performance of leading LAMs, underscoring the need for more robust and semantically aware defense mechanisms."

[22.05.2025 11:12] Response: ```python
['DATASET', 'BENCHMARK', 'AUDIO']
```
[22.05.2025 11:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The rise of Large Audio Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks a systematic, quantitative evaluation of LAM safety especially against jailbreak attacks, which are challenging due to the temporal and semantic nature of speech. To bridge this gap, we introduce AJailBench, the first benchmark specifically designed to evaluate jailbreak vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, converted from textual jailbreak attacks using realistic text to speech synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and reveal that none exhibit consistent robustness across attacks. To further strengthen jailbreak testing and simulate more realistic attack conditions, we propose a method to generate dynamic adversarial variants. Our Audio Perturbation Toolkit (APT) applies targeted distortions across time, frequency, and amplitude domains. To preserve the original jailbreak intent, we enforce a semantic consistency constraint and employ Bayesian optimization to efficiently search for perturbations that are both subtle and highly effective. This results in AJailBench-APT, an extended dataset of optimized adversarial audio samples. Our findings demonstrate that even small, semantically preserved perturbations can significantly reduce the safety performance of leading LAMs, underscoring the need for more robust and semantically aware defense mechanisms."

[22.05.2025 11:12] Response: ```python
['SECURITY', 'ETHICS']
```
[22.05.2025 11:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the safety concerns associated with Large Audio Language Models (LAMs), particularly their vulnerability to jailbreak attacks. It introduces AJailBench, a benchmark designed to systematically evaluate these vulnerabilities using a dataset of adversarial audio prompts. The study reveals that current LAMs lack consistent robustness against various attacks, highlighting the need for improved defenses. Additionally, the authors propose an Audio Perturbation Toolkit (APT) to create subtle yet effective adversarial audio samples that maintain semantic integrity, demonstrating the significant impact of small perturbations on LAM safety.","title":"Evaluating and Enhancing Safety in Large Audio Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the safety concerns associated with Large Audio Language Models (LAMs), particularly their vulnerability to jailbreak attacks. It introduces AJailBench, a benchmark designed to systematically evaluate these vulnerabilities using a dataset of adversarial audio prompts. The study reveals that current LAMs lack consistent robustness against various attacks, highlighting the need for improved defenses. Additionally, the authors propose an Audio Perturbation Toolkit (APT) to create subtle yet effective adversarial audio samples that maintain semantic integrity, demonstrating the significant impact of small perturbations on LAM safety.', title='Evaluating and Enhancing Safety in Large Audio Language Models'))
[22.05.2025 11:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Â§ßÂûãÈü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÔºàLAMsÔºâÁöÑÂÖ¥Ëµ∑Â∏¶Êù•‰∫ÜÊΩúÂäõÂíåÈ£éÈô©ÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÁöÑÈü≥È¢ëËæìÂá∫ÂèØËÉΩÂåÖÂê´ÊúâÂÆ≥Êàñ‰∏çÈÅìÂæ∑ÁöÑÂÜÖÂÆπ„ÄÇÁõÆÂâçÁöÑÁ†îÁ©∂Áº∫‰πèÂØπLAMÂÆâÂÖ®ÊÄßÁöÑÁ≥ªÁªüÊÄßÂÆöÈáèËØÑ‰º∞ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈíàÂØπË∂äÁã±ÊîªÂáªÊñπÈù¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜAJailBenchÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™‰∏ìÈó®ËÆæËÆ°Áî®‰∫éËØÑ‰º∞LAMË∂äÁã±ÊºèÊ¥ûÁöÑÂü∫ÂáÜÊµãËØï„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂç≥‰ΩøÊòØÂæÆÂ∞èÁöÑ„ÄÅ‰øùÊåÅËØ≠‰πâ‰∏ÄËá¥ÁöÑÊâ∞Âä®‰πüËÉΩÊòæËëóÈôç‰ΩéÈ¢ÜÂÖàLAMÁöÑÂÆâÂÖ®ÊÄßËÉΩÔºåÂº∫Ë∞É‰∫ÜÈúÄË¶ÅÊõ¥Âº∫Â§ßÂíåÊõ¥ÂÖ∑ËØ≠‰πâÊÑèËØÜÁöÑÈò≤Âæ°Êú∫Âà∂„ÄÇ","title":"ËØÑ‰º∞Â§ßÂûãÈü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄß‰∏éÊºèÊ¥û"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Â§ßÂûãÈü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÔºàLAMsÔºâÁöÑÂÖ¥Ëµ∑Â∏¶Êù•‰∫ÜÊΩúÂäõÂíåÈ£éÈô©ÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÁöÑÈü≥È¢ëËæìÂá∫ÂèØËÉΩÂåÖÂê´ÊúâÂÆ≥Êàñ‰∏çÈÅìÂæ∑ÁöÑÂÜÖÂÆπ„ÄÇÁõÆÂâçÁöÑÁ†îÁ©∂Áº∫‰πèÂØπLAMÂÆâÂÖ®ÊÄßÁöÑÁ≥ªÁªüÊÄßÂÆöÈáèËØÑ‰º∞ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈíàÂØπË∂äÁã±ÊîªÂáªÊñπÈù¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜAJailBenchÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™‰∏ìÈó®ËÆæËÆ°Áî®‰∫éËØÑ‰º∞LAMË∂äÁã±ÊºèÊ¥ûÁöÑÂü∫ÂáÜÊµãËØï„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂç≥‰ΩøÊòØÂæÆÂ∞èÁöÑ„ÄÅ‰øùÊåÅËØ≠‰πâ‰∏ÄËá¥ÁöÑÊâ∞Âä®‰πüËÉΩÊòæËëóÈôç‰ΩéÈ¢ÜÂÖàLAMÁöÑÂÆâÂÖ®ÊÄßËÉΩÔºåÂº∫Ë∞É‰∫ÜÈúÄË¶ÅÊõ¥Âº∫Â§ßÂíåÊõ¥ÂÖ∑ËØ≠‰πâÊÑèËØÜÁöÑÈò≤Âæ°Êú∫Âà∂„ÄÇ', title='ËØÑ‰º∞Â§ßÂûãÈü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄß‰∏éÊºèÊ¥û'))
[22.05.2025 11:12] Querying the API.
[22.05.2025 11:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Robustly evaluating the long-form storytelling capabilities of Large Language Models (LLMs) remains a significant challenge, as existing benchmarks often lack the necessary scale, diversity, or objective measures. To address this, we introduce WebNovelBench, a novel benchmark specifically designed for evaluating long-form novel generation. WebNovelBench leverages a large-scale dataset of over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story generation task. We propose a multi-faceted framework encompassing eight narrative quality dimensions, assessed automatically via an LLM-as-Judge approach. Scores are aggregated using Principal Component Analysis and mapped to a percentile rank against human-authored works. Our experiments demonstrate that WebNovelBench effectively differentiates between human-written masterpieces, popular web novels, and LLM-generated content. We provide a comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling abilities and offering insights for future development. This benchmark provides a scalable, replicable, and data-driven methodology for assessing and advancing LLM-driven narrative generation.
[22.05.2025 11:12] Response: {
  "desc": "WebNovelBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ä–æ–º–∞–Ω—ã. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–æ–ª–µ–µ —á–µ–º 4000 –∫–∏—Ç–∞–π—Å–∫–∏—Ö –≤–µ–±-–Ω–æ–≤–µ–ª–ª –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ –≤–æ—Å—å–º–∏ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º –∏–∑–º–µ—Ä–µ–Ω–∏—è–º. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å –ø–æ–º–æ—â—å—é –ø–æ–¥—Ö–æ–¥–∞ LLM-as-Judge, –∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–≥—Ä–µ–≥–∏—Ä—É—é—Ç—Å—è –º–µ—Ç–æ–¥–æ–º –∞–Ω–∞–ª–∏–∑–∞ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç. –ë–µ–Ω—á–º–∞—Ä–∫ —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑–ª–∏—á–∞–µ—Ç —à–µ–¥–µ–≤—Ä—ã, –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã–µ –ª—é–¥—å–º–∏, –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –≤–µ–±-–Ω–æ–≤–µ–ª–ª—ã –∏ –∫–æ–Ω—Ç–µ–Ω—Ç, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LLM.",

  "emoji": "üìö",

  "title": "WebNovelBench: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏"
}
[22.05.2025 11:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Robustly evaluating the long-form storytelling capabilities of Large Language Models (LLMs) remains a significant challenge, as existing benchmarks often lack the necessary scale, diversity, or objective measures. To address this, we introduce WebNovelBench, a novel benchmark specifically designed for evaluating long-form novel generation. WebNovelBench leverages a large-scale dataset of over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story generation task. We propose a multi-faceted framework encompassing eight narrative quality dimensions, assessed automatically via an LLM-as-Judge approach. Scores are aggregated using Principal Component Analysis and mapped to a percentile rank against human-authored works. Our experiments demonstrate that WebNovelBench effectively differentiates between human-written masterpieces, popular web novels, and LLM-generated content. We provide a comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling abilities and offering insights for future development. This benchmark provides a scalable, replicable, and data-driven methodology for assessing and advancing LLM-driven narrative generation."

[22.05.2025 11:12] Response: ```python
["DATASET", "BENCHMARK"]
```
[22.05.2025 11:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Robustly evaluating the long-form storytelling capabilities of Large Language Models (LLMs) remains a significant challenge, as existing benchmarks often lack the necessary scale, diversity, or objective measures. To address this, we introduce WebNovelBench, a novel benchmark specifically designed for evaluating long-form novel generation. WebNovelBench leverages a large-scale dataset of over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story generation task. We propose a multi-faceted framework encompassing eight narrative quality dimensions, assessed automatically via an LLM-as-Judge approach. Scores are aggregated using Principal Component Analysis and mapped to a percentile rank against human-authored works. Our experiments demonstrate that WebNovelBench effectively differentiates between human-written masterpieces, popular web novels, and LLM-generated content. We provide a comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling abilities and offering insights for future development. This benchmark provides a scalable, replicable, and data-driven methodology for assessing and advancing LLM-driven narrative generation."

[22.05.2025 11:12] Response: ```python
["STORY_GENERATION", "LONG_CONTEXT"]
```
[22.05.2025 11:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents WebNovelBench, a new benchmark for evaluating the storytelling abilities of Large Language Models (LLMs) in generating long-form narratives. It utilizes a dataset of over 4,000 Chinese web novels and frames the evaluation as a task of generating stories from synopses. The framework assesses narrative quality across eight dimensions using an LLM-as-Judge approach, with results analyzed through Principal Component Analysis. The findings show that WebNovelBench can effectively distinguish between human-written and LLM-generated stories, providing valuable insights for improving LLM storytelling capabilities.","title":"WebNovelBench: A New Standard for Evaluating LLM Storytelling"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents WebNovelBench, a new benchmark for evaluating the storytelling abilities of Large Language Models (LLMs) in generating long-form narratives. It utilizes a dataset of over 4,000 Chinese web novels and frames the evaluation as a task of generating stories from synopses. The framework assesses narrative quality across eight dimensions using an LLM-as-Judge approach, with results analyzed through Principal Component Analysis. The findings show that WebNovelBench can effectively distinguish between human-written and LLM-generated stories, providing valuable insights for improving LLM storytelling capabilities.', title='WebNovelBench: A New Standard for Evaluating LLM Storytelling'))
[22.05.2025 11:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜWebNovelBenchÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÈïøÁØáÂ∞èËØ¥ÁîüÊàêËÉΩÂäõÁöÑÊñ∞Âü∫ÂáÜ„ÄÇËØ•Âü∫ÂáÜÂà©Áî®‰∫ÜË∂ÖËøá4000ÈÉ®‰∏≠ÊñáÁΩëÁªúÂ∞èËØ¥ÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂ∞ÜËØÑ‰º∞Ê°ÜÊû∂ËÆæÂÆö‰∏∫‰ªéÊëòË¶ÅÂà∞ÊïÖ‰∫ãÁîüÊàêÁöÑ‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Â§öÁª¥Â∫¶ÁöÑÊ°ÜÊû∂ÔºåÊ∂µÁõñÂÖ´‰∏™Âèô‰∫ãË¥®ÈáèÁª¥Â∫¶ÔºåÈÄöËøáLLM‰Ωú‰∏∫ËØÑÂà§ËÄÖÁöÑÊñπÂºèËøõË°åËá™Âä®ËØÑ‰º∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWebNovelBenchËÉΩÂ§üÊúâÊïàÂå∫ÂàÜ‰∫∫Á±ªÂàõ‰ΩúÁöÑÊù∞‰Ωú„ÄÅÂèóÊ¨¢ËøéÁöÑÁΩëÁªúÂ∞èËØ¥ÂíåLLMÁîüÊàêÁöÑÂÜÖÂÆπ„ÄÇ","title":"ËØÑ‰º∞ÈïøÁØáÂ∞èËØ¥ÁîüÊàêÁöÑÊñ∞Âü∫ÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜWebNovelBenchÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÈïøÁØáÂ∞èËØ¥ÁîüÊàêËÉΩÂäõÁöÑÊñ∞Âü∫ÂáÜ„ÄÇËØ•Âü∫ÂáÜÂà©Áî®‰∫ÜË∂ÖËøá4000ÈÉ®‰∏≠ÊñáÁΩëÁªúÂ∞èËØ¥ÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂ∞ÜËØÑ‰º∞Ê°ÜÊû∂ËÆæÂÆö‰∏∫‰ªéÊëòË¶ÅÂà∞ÊïÖ‰∫ãÁîüÊàêÁöÑ‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Â§öÁª¥Â∫¶ÁöÑÊ°ÜÊû∂ÔºåÊ∂µÁõñÂÖ´‰∏™Âèô‰∫ãË¥®ÈáèÁª¥Â∫¶ÔºåÈÄöËøáLLM‰Ωú‰∏∫ËØÑÂà§ËÄÖÁöÑÊñπÂºèËøõË°åËá™Âä®ËØÑ‰º∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWebNovelBenchËÉΩÂ§üÊúâÊïàÂå∫ÂàÜ‰∫∫Á±ªÂàõ‰ΩúÁöÑÊù∞‰Ωú„ÄÅÂèóÊ¨¢ËøéÁöÑÁΩëÁªúÂ∞èËØ¥ÂíåLLMÁîüÊàêÁöÑÂÜÖÂÆπ„ÄÇ', title='ËØÑ‰º∞ÈïøÁØáÂ∞èËØ¥ÁîüÊàêÁöÑÊñ∞Âü∫ÂáÜ'))
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rl", "#benchmark"], "emoji": "üß†", "ru": {"title": "–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω–∂–µ–Ω–µ—Ä–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ (pPE) –Ω–∞ –æ
[22.05.2025 11:12] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#alignment", "#benchmark", "#open_source", "#inference", "#low_resource", "#multilingual"], "emoji": "üåê", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏ –≤ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç —Ñ–µ–Ω–æ–º–µ–Ω —è–∑—ã–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π (LS
[22.05.2025 11:12] Querying the API.
[22.05.2025 11:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

HumaniBench evaluates state-of-the-art LMMs on seven human-centered AI principles using 32K real-world image-question pairs to ensure fairness, ethics, empathy, and inclusivity.  					AI-generated summary 				 Large multimodal models (LMMs) now excel on many vision language benchmarks, however, they still struggle with human centered criteria such as fairness, ethics, empathy, and inclusivity, key to aligning with human values. We introduce HumaniBench, a holistic benchmark of 32K real-world image question pairs, annotated via a scalable GPT4o assisted pipeline and exhaustively verified by domain experts. HumaniBench evaluates seven Human Centered AI (HCAI) principles: fairness, ethics, understanding, reasoning, language inclusivity, empathy, and robustness, across seven diverse tasks, including open and closed ended visual question answering (VQA), multilingual QA, visual grounding, empathetic captioning, and robustness tests. Benchmarking 15 state of the art LMMs (open and closed source) reveals that proprietary models generally lead, though robustness and visual grounding remain weak points. Some open-source models also struggle to balance accuracy with adherence to human-aligned principles. HumaniBench is the first benchmark purpose built around HCAI principles. It provides a rigorous testbed for diagnosing alignment gaps and guiding LMMs toward behavior that is both accurate and socially responsible. Dataset, annotation prompts, and evaluation code are available at: https://vectorinstitute.github.io/HumaniBench
[22.05.2025 11:13] Response: {
  "desc": "HumaniBench - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM) –ø–æ —Å–µ–º–∏ –ø—Ä–∏–Ω—Ü–∏–ø–∞–º —á–µ–ª–æ–≤–µ–∫–æ—Ü–µ–Ω—Ç—Ä–∏—á–Ω–æ–≥–æ –ò–ò. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 32 —Ç—ã—Å—è—á–∏ –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–≤–æ–ø—Ä–æ—Å –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö –∑–∞–¥–∞—á–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ QA, –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —ç–º–ø–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 15 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LMM –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Ü–µ–ª–æ–º –ª–∏–¥–∏—Ä—É—é—Ç, —Ö–æ—Ç—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∏ –≤–∏–∑—É–∞–ª—å–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å—Ç–∞—é—Ç—Å—è —Å–ª–∞–±—ã–º–∏ –º–µ—Å—Ç–∞–º–∏. HumaniBench –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç—Ä–æ–≥—É—é —Ç–µ—Å—Ç–æ–≤—É—é —Å—Ä–µ–¥—É –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏ –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è LMM –∫ –ø–æ–≤–µ–¥–µ–Ω–∏—é, –∫–æ—Ç–æ—Ä–æ–µ —è–≤–ª—è–µ—Ç—Å—è –∫–∞–∫ —Ç–æ—á–Ω—ã–º, —Ç–∞–∫ –∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–º.",
  "emoji": "üß†",
  "title": "–û—Ü–µ–Ω–∫–∞ –ò–ò –ø–æ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –º–µ—Ä–∫–∞–º"
}
[22.05.2025 11:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HumaniBench evaluates state-of-the-art LMMs on seven human-centered AI principles using 32K real-world image-question pairs to ensure fairness, ethics, empathy, and inclusivity.  					AI-generated summary 				 Large multimodal models (LMMs) now excel on many vision language benchmarks, however, they still struggle with human centered criteria such as fairness, ethics, empathy, and inclusivity, key to aligning with human values. We introduce HumaniBench, a holistic benchmark of 32K real-world image question pairs, annotated via a scalable GPT4o assisted pipeline and exhaustively verified by domain experts. HumaniBench evaluates seven Human Centered AI (HCAI) principles: fairness, ethics, understanding, reasoning, language inclusivity, empathy, and robustness, across seven diverse tasks, including open and closed ended visual question answering (VQA), multilingual QA, visual grounding, empathetic captioning, and robustness tests. Benchmarking 15 state of the art LMMs (open and closed source) reveals that proprietary models generally lead, though robustness and visual grounding remain weak points. Some open-source models also struggle to balance accuracy with adherence to human-aligned principles. HumaniBench is the first benchmark purpose built around HCAI principles. It provides a rigorous testbed for diagnosing alignment gaps and guiding LMMs toward behavior that is both accurate and socially responsible. Dataset, annotation prompts, and evaluation code are available at: https://vectorinstitute.github.io/HumaniBench"

[22.05.2025 11:13] Response: ```python
['BENCHMARK', 'MULTIMODAL', 'DATASET']
```
[22.05.2025 11:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HumaniBench evaluates state-of-the-art LMMs on seven human-centered AI principles using 32K real-world image-question pairs to ensure fairness, ethics, empathy, and inclusivity.  					AI-generated summary 				 Large multimodal models (LMMs) now excel on many vision language benchmarks, however, they still struggle with human centered criteria such as fairness, ethics, empathy, and inclusivity, key to aligning with human values. We introduce HumaniBench, a holistic benchmark of 32K real-world image question pairs, annotated via a scalable GPT4o assisted pipeline and exhaustively verified by domain experts. HumaniBench evaluates seven Human Centered AI (HCAI) principles: fairness, ethics, understanding, reasoning, language inclusivity, empathy, and robustness, across seven diverse tasks, including open and closed ended visual question answering (VQA), multilingual QA, visual grounding, empathetic captioning, and robustness tests. Benchmarking 15 state of the art LMMs (open and closed source) reveals that proprietary models generally lead, though robustness and visual grounding remain weak points. Some open-source models also struggle to balance accuracy with adherence to human-aligned principles. HumaniBench is the first benchmark purpose built around HCAI principles. It provides a rigorous testbed for diagnosing alignment gaps and guiding LMMs toward behavior that is both accurate and socially responsible. Dataset, annotation prompts, and evaluation code are available at: https://vectorinstitute.github.io/HumaniBench"

[22.05.2025 11:13] Response: ```python
['ETHICS', 'ALIGNMENT']
```
[22.05.2025 11:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HumaniBench is a new benchmark designed to evaluate large multimodal models (LMMs) based on seven human-centered AI principles, including fairness and empathy. It uses a dataset of 32,000 real-world image-question pairs, which are annotated with the help of a scalable GPT-4 assisted pipeline and verified by experts. The benchmark assesses LMMs across various tasks such as visual question answering and empathetic captioning, revealing that while proprietary models often perform better, they still have weaknesses in robustness and visual grounding. HumaniBench aims to identify alignment gaps in LMMs and promote their development towards more socially responsible and human-aligned behaviors.","title":"HumaniBench: Aligning AI with Human Values"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HumaniBench is a new benchmark designed to evaluate large multimodal models (LMMs) based on seven human-centered AI principles, including fairness and empathy. It uses a dataset of 32,000 real-world image-question pairs, which are annotated with the help of a scalable GPT-4 assisted pipeline and verified by experts. The benchmark assesses LMMs across various tasks such as visual question answering and empathetic captioning, revealing that while proprietary models often perform better, they still have weaknesses in robustness and visual grounding. HumaniBench aims to identify alignment gaps in LMMs and promote their development towards more socially responsible and human-aligned behaviors.', title='HumaniBench: Aligning AI with Human Values'))
[22.05.2025 11:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HumaniBench ÊòØ‰∏Ä‰∏™ËØÑ‰º∞Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÁöÑÂü∫ÂáÜÔºå‰∏ìÊ≥®‰∫é‰∏É‰∏™‰ª•‰∫∫‰∏∫Êú¨ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÂéüÂàôÔºåÂåÖÊã¨ÂÖ¨Âπ≥ÊÄß„ÄÅ‰º¶ÁêÜ„ÄÅÁêÜËß£„ÄÅÊé®ÁêÜ„ÄÅËØ≠Ë®ÄÂåÖÂÆπÊÄß„ÄÅÂêåÁêÜÂøÉÂíåÈ≤ÅÊ£íÊÄß„ÄÇËØ•Âü∫ÂáÜ‰ΩøÁî®32,000‰∏™ÁúüÂÆû‰∏ñÁïåÁöÑÂõæÂÉè-ÈóÆÈ¢òÂØπÔºåÈÄöËøáÂèØÊâ©Â±ïÁöÑGPT4oËæÖÂä©ÁÆ°ÈÅìËøõË°åÊ≥®ÈáäÔºåÂπ∂Áî±È¢ÜÂüü‰∏ìÂÆ∂ËøõË°åÂÖ®Èù¢È™åËØÅ„ÄÇÂ∞ΩÁÆ°‰∏Ä‰∫õ‰∏ìÊúâÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏äË°®Áé∞ËæÉÂ•ΩÔºå‰ΩÜÂú®È≤ÅÊ£íÊÄßÂíåËßÜËßâÂÆö‰ΩçÊñπÈù¢‰ªçÂ≠òÂú®‰∏çË∂≥ÔºåËÄå‰∏Ä‰∫õÂºÄÊ∫êÊ®°ÂûãÂú®ÂáÜÁ°ÆÊÄß‰∏éÈÅµÂæ™‰∫∫Á±ªÂØπÈΩêÂéüÂàô‰πãÈó¥‰πüÈöæ‰ª•Âπ≥Ë°°„ÄÇHumaniBench ÊòØÈ¶ñ‰∏™‰∏ìÈó®Âõ¥Áªï‰ª•‰∫∫‰∏∫Êú¨ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÂéüÂàôÊûÑÂª∫ÁöÑÂü∫ÂáÜÔºå‰∏∫ËØäÊñ≠ÂØπÈΩêÂ∑ÆË∑ùÂíåÂºïÂØºLMMsÊúùÂêëÂáÜÁ°Æ‰∏îÁ§æ‰ºöË¥£‰ªªÊÑüÂº∫ÁöÑË°å‰∏∫Êèê‰æõ‰∫Ü‰∏•Ê†ºÁöÑÊµãËØïÂπ≥Âè∞„ÄÇ","title":"‰ª•‰∫∫‰∏∫Êú¨ÁöÑ‰∫∫Â∑•Êô∫ËÉΩËØÑ‰º∞Âü∫ÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HumaniBench ÊòØ‰∏Ä‰∏™ËØÑ‰º∞Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÁöÑÂü∫ÂáÜÔºå‰∏ìÊ≥®‰∫é‰∏É‰∏™‰ª•‰∫∫‰∏∫Êú¨ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÂéüÂàôÔºåÂåÖÊã¨ÂÖ¨Âπ≥ÊÄß„ÄÅ‰º¶ÁêÜ„ÄÅÁêÜËß£„ÄÅÊé®ÁêÜ„ÄÅËØ≠Ë®ÄÂåÖÂÆπÊÄß„ÄÅÂêåÁêÜÂøÉÂíåÈ≤ÅÊ£íÊÄß„ÄÇËØ•Âü∫ÂáÜ‰ΩøÁî®32,000‰∏™ÁúüÂÆû‰∏ñÁïåÁöÑÂõæÂÉè-ÈóÆÈ¢òÂØπÔºåÈÄöËøáÂèØÊâ©Â±ïÁöÑGPT4oËæÖÂä©ÁÆ°ÈÅìËøõË°åÊ≥®ÈáäÔºåÂπ∂Áî±È¢ÜÂüü‰∏ìÂÆ∂ËøõË°åÂÖ®Èù¢È™åËØÅ„ÄÇÂ∞ΩÁÆ°‰∏Ä‰∫õ‰∏ìÊúâÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏äË°®Áé∞ËæÉÂ•ΩÔºå‰ΩÜÂú®È≤ÅÊ£íÊÄßÂíåËßÜËßâÂÆö‰ΩçÊñπÈù¢‰ªçÂ≠òÂú®‰∏çË∂≥ÔºåËÄå‰∏Ä‰∫õÂºÄÊ∫êÊ®°ÂûãÂú®ÂáÜÁ°ÆÊÄß‰∏éÈÅµÂæ™‰∫∫Á±ªÂØπÈΩêÂéüÂàô‰πãÈó¥‰πüÈöæ‰ª•Âπ≥Ë°°„ÄÇHumaniBench ÊòØÈ¶ñ‰∏™‰∏ìÈó®Âõ¥Áªï‰ª•‰∫∫‰∏∫Êú¨ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÂéüÂàôÊûÑÂª∫ÁöÑÂü∫ÂáÜÔºå‰∏∫ËØäÊñ≠ÂØπÈΩêÂ∑ÆË∑ùÂíåÂºïÂØºLMMsÊúùÂêëÂáÜÁ°Æ‰∏îÁ§æ‰ºöË¥£‰ªªÊÑüÂº∫ÁöÑË°å‰∏∫Êèê‰æõ‰∫Ü‰∏•Ê†ºÁöÑÊµãËØïÂπ≥Âè∞„ÄÇ', title='‰ª•‰∫∫‰∏∫Êú¨ÁöÑ‰∫∫Â∑•Êô∫ËÉΩËØÑ‰º∞Âü∫ÂáÜ'))
[22.05.2025 11:13] Loading Chinese text from previous data.
[22.05.2025 11:13] Renaming data file.
[22.05.2025 11:13] Renaming previous data. hf_papers.json to ./d/2025-05-22.json
[22.05.2025 11:13] Saving new data file.
[22.05.2025 11:13] Generating page.
[22.05.2025 11:13] Renaming previous page.
[22.05.2025 11:13] Renaming previous data. index.html to ./d/2025-05-22.html
[22.05.2025 11:13] [Experimental] Generating Chinese page for reading.
[22.05.2025 11:13] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'ÁΩëÈ°µÂØºËà™', 'pinyin': 'w«éng y√® d«éo h√°ng', 'trans': 'web navigation'}, {'word': 'Ëá™Âä®Âåñ', 'pinyin': 'z√¨ d√≤ng hu√†', 'trans': 'automation'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®n wu', 'trans': 'task'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': 'Â•ñÂä±Ê®°Âûã', 'pinyin': 'ji«éng l√¨ m√≥ x√≠ng', 'trans': 'reward model'}, {'word': 'ÈôêÂà∂', 'pinyin': 'xi√†n zh√¨', 'trans': 'limit'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'ËøáÁ®ãÂ•ñÂä±Ê®°Âûã', 'pinyin': 'gu√≤ ch√©ng ji«éng l√¨ m√≥ x√≠ng', 'trans': 'process reward model'}, {'word': 'ÈÄêÊ≠•', 'pinyin': 'zhu√≥ b√π', 'trans': 'step-by-step'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ng g≈´', 'trans': 'evaluate'}, {'word': 'Ë∑ØÂæÑ', 'pinyin': 'l√π j√¨ng', 'trans': 'path'}, {'word': 'ÂàõÂª∫', 'pinyin': 'chu√†ng ji√†n', 'trans': 'create'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√π j√π j√≠', 'trans': 'dataset'}, {'word': 'ËØÑ‰º∞Âü∫ÂáÜ', 'pinyin': 'p√≠ng g≈´ jƒ´ zh«în', 'trans': 'evaluation benchmark'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ËØÅÊòé', 'pinyin': 'zh√®ng m√≠ng', 'trans': 'prove'}, {'word': 'ÊúâÊïàÊÄß', 'pinyin': 'y«íu xi√†o x√¨ng', 'trans': 'effectiveness'}, {'word': 'ÊàêÊú¨ÊïàÁõä', 'pinyin': 'ch√©ng bƒõn xi√†o y√¨', 'trans': 'cost-effectiveness'}, {'word': 'ÂÖ¨ÂºÄÂèØÁî®', 'pinyin': 'g≈çng kƒÅi kƒõ y√≤ng', 'trans': 'publicly available'}]
[22.05.2025 11:13] Renaming previous Chinese page.
[22.05.2025 11:13] Renaming previous data. zh.html to ./d/2025-05-21_zh_reading_task.html
[22.05.2025 11:13] Writing Chinese reading task.
[22.05.2025 11:13] Writing result.
[22.05.2025 11:13] Renaming log file.
[22.05.2025 11:13] Renaming previous data. log.txt to ./logs/2025-05-22_last_log.txt
