[06.12.2024 18:14] Read previous papers.
[06.12.2024 18:14] Generating top page (month).
[06.12.2024 18:14] Writing top page (month).
[06.12.2024 19:08] Read previous papers.
[06.12.2024 19:08] Get feed.
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04467
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04455
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04454
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03895
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04424
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03679
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01339
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01506
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03632
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04146
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04431
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04315
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04280
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04378
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.02142
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04062
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01820
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03304
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01169
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04106
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04139
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2411.19574
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03704
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04003
[06.12.2024 19:08] Extract page data from URL. URL: https://huggingface.co/papers/2412.04462
[06.12.2024 19:08] Extract page data from URL. URL: https://huggingface.co/papers/2412.04449
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04262
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04448
[06.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04363
[06.12.2024 19:08] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.12.2024 19:08] No deleted papers detected.
[06.12.2024 19:08] Downloading and parsing papers (pdf, html). Total: 29.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04467.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.04467.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.04467.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04455.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.04455.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.04455.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04454.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.04454.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.04454.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.03895.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.03895.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.03895.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04424.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.04424.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.04424.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.03679.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.03679.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.03679.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.01339.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.01339.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.01339.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.01506.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.01506.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.01506.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.03632.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.03632.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.03632.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04146.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.04146.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.04146.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04431.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.04431.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.04431.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04315.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.04315.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.04315.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04280.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.04280.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.04280.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04378.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.04378.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.04378.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.02142.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.02142.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.02142.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04062.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.04062.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.04062.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.01820.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.01820.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.01820.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.03304.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.03304.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.03304.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.01169.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.01169.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.01169.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04106.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.04106.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.04106.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04139.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.04139.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.04139.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2411.19574.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2411.19574.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2411.19574.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.03704.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.03704.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.03704.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04003.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.04003.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.04003.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04462.
[06.12.2024 19:08] Downloading paper 2412.04462 from http://arxiv.org/pdf/2412.04462v1...
[06.12.2024 19:08] Extracting affiliations from text.
[06.12.2024 19:08] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. If there are no affiliations return empty list.

Text:"4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion Chaoyang Wang1,* Michael Vasilkovsky1 Peiye Zhuang1,* Tuan Duc Ngo1,2 Willi Menapace1 Ivan Skorokhodov1 Sergey Tulyakov1 Peter Wonka1, Aliaksandr Siarohin1 Hsin-Ying Lee1 4 2 0 2 5 ] . [ 1 2 6 4 4 0 . 2 1 4 2 : r 1Snap Inc 2Umass Amherst 3KAUST https://snap-research.github.io/4Real-Video/ Figure 1. 4Real-Video is 4D generation framework that (top-left) takes fixed-view video and freeze-time video as input and generates grid of consistent video frames. One axis of the grid varies in time, and the other axis varies the viewpoint. The input videos can be real videos or videos generated by video model. Note that our method can generate grids larger than 8 8 videos. Here, we present subsets of frames as an example. (top-right) 4D videos generated from generated videos. (bottom) We can also capture real-world scene, and generate 4D video given different prompts. "
[06.12.2024 19:08] Response: ```python
["Snap Inc", "Umass Amherst", "KAUST"]
```
[06.12.2024 19:08] Deleting PDF ./assets/pdf/2412.04462.pdf.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04449.
[06.12.2024 19:08] Downloading paper 2412.04449 from http://arxiv.org/pdf/2412.04449v1...
[06.12.2024 19:08] Extracting affiliations from text.
[06.12.2024 19:08] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. If there are no affiliations return empty list.

Text:"p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay Jun Zhang1,* Desen Meng1,* Ji Qi 2 Zhenpeng Huang 1 Tao Wu 1 Limin Wang 1,3, (cid:0) 1State Key Laboratory for Novel Software Technology, Nanjing University 2China Mobile (Suzhou) Software Technology Co., Ltd. 3Shanghai AI Lab https://github.com/MCG-NJU/p-MoD 4 2 0 2 5 ] . [ 1 9 4 4 4 0 . 2 1 4 2 : r a "
[06.12.2024 19:08] Response: ```python
[
    "State Key Laboratory for Novel Software Technology, Nanjing University",
    "China Mobile (Suzhou) Software Technology Co., Ltd.",
    "Shanghai AI Lab"
]
```
[06.12.2024 19:08] Deleting PDF ./assets/pdf/2412.04449.pdf.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04262.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.04262.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.04262.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04448.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.04448.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.04448.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.04363.
[06.12.2024 19:08] Extra JSON file exists (./assets/json/2412.04363.json), skip PDF parsing.
[06.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.04363.json), skip HTML parsing.
[06.12.2024 19:08] Success.
[06.12.2024 19:08] Enriching papers with extra data.
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 0. Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens generated by popular vision encoders, such as CLIP and...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 1. Automatic detection and prevention of open-set failures are crucial in closed-loop robotic systems. Recent studies often struggle to simultaneously identify unexpected failures reactively after they occur and prevent foreseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a nove...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 2. Graphical User Interfaces (GUIs) are critical to human-computer interaction, yet automating GUI tasks remains challenging due to the complexity and variability of visual environments. Existing approaches often rely on textual representations of GUIs, which introduce limitations in generalization, ef...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 3. Diffusion models excel in generating high-quality images. However, current diffusion models struggle to produce reliable images without guidance methods, such as classifier-free guidance (CFG). Are guidance methods truly necessary? Observing that noise obtained via diffusion inversion can reconstruc...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 4. We present Florence-VL, a new family of multimodal large language models (MLLMs) with enriched visual representations produced by Florence-2, a generative vision foundation model. Unlike the widely used CLIP-style vision transformer trained by contrastive learning, Florence-2 can capture different l...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 5. Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic ...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 6. Text-based adversarial guidance using a negative prompt has emerged as a widely adopted approach to push the output features away from undesired concepts. While useful, performing adversarial guidance using text alone can be insufficient to capture complex visual concepts and avoid undesired visual ...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 7. We introduce a novel 3D generation method for versatile and high-quality 3D asset creation. The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a spa...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 8. Existing multi-view image generation methods often make invasive modifications to pre-trained text-to-image (T2I) models and require full fine-tuning, leading to (1) high computational costs, especially with large base models and high-resolution images, and (2) degradation in image quality due to op...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 9. Recent advances in garment-centric image generation from text and image prompts based on diffusion models are impressive. However, existing methods lack support for various combinations of attire, and struggle to preserve the garment details while maintaining faithfulness to the text prompts, limiti...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 10. We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and ...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 11. Large Language Models (LLMs) have emerged as a milestone in artificial intelligence, and their performance can improve as the model size increases. However, this scaling brings great challenges to training and inference efficiency, particularly for deploying LLMs in resource-constrained environments...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 12. We present HumanEdit, a high-quality, human-rewarded dataset specifically designed for instruction-guided image editing, enabling precise and diverse image manipulations through open-form language instructions. Previous large-scale editing datasets often incorporate minimal human feedback, leading t...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 13. Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a "bag of words" behavior. At the same time, Large Vision-Language M...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 14. Multimodal Large Language Models (MLLMs) have become increasingly important due to their state-of-the-art performance and ability to integrate multiple data modalities, such as text, images, and audio, to perform complex tasks with high accuracy. This paper presents a comprehensive survey on persona...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 15. In this paper, we propose ZipAR, a training-free, plug-and-play parallel decoding framework for accelerating auto-regressive (AR) visual generation. The motivation stems from the observation that images exhibit local structures, and spatially distant regions tend to have minimal interdependence. Giv...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 16. As a globally celebrated sport, soccer has attracted widespread interest from fans all over the world. This paper aims to develop a comprehensive multi-modal framework for soccer video understanding. Specifically, we make the following contributions in this paper: (i) we introduce SoccerReplay-1988,...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 17. Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermo...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 18. We introduce OmniFlow, a novel generative model designed for any-to-any generation tasks such as text-to-image, text-to-audio, and audio-to-image synthesis. OmniFlow advances the rectified flow (RF) framework used in text-to-image models to handle the joint distribution of multiple modalities. It ou...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 19. Medical image segmentation has recently demonstrated impressive progress with deep neural networks, yet the heterogeneous modalities and scarcity of mask annotations limit the development of segmentation models on unannotated modalities. This paper investigates a new paradigm for leveraging generati...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 20. Understanding the internal computations of large language models (LLMs) is crucial for aligning them with human values and preventing undesirable behaviors like toxic content generation. However, mechanistic interpretability is hindered by polysemanticity -- where individual neurons respond to multi...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 21. The current large language models are mainly based on decode-only structure transformers, which have great in-context learning (ICL) capabilities. It is generally believed that the important foundation of its ICL capability is the induction heads mechanism, which requires at least two layers attenti...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 22. Despite significant advancements in vision-language models (VLMs), there lacks effective approaches to enhance response quality by scaling inference-time computation. This capability is known to be a core step towards the self-improving models in recent large language model studies. In this paper, w...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 23. Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 24. We propose 4Real-Video, a novel framework for generating 4D videos, organized as a grid of video frames with both time and viewpoint axes. In this grid, each row contains frames sharing the same timestep, while each column contains frames from the same viewpoint. We propose a novel two-stream archit...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 25. Despite the remarkable performance of multimodal large language models (MLLMs) across diverse tasks, the substantial training and inference costs impede their advancement. The majority of computation stems from the overwhelming volume of vision tokens processed by the transformer decoder. In this pa...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 26. Table extraction from document images is a challenging AI problem, and labelled data for many content domains is difficult to come by. Existing table extraction datasets often focus on scientific tables due to the vast amount of academic articles that are readily available, along with their source c...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 27. Recent advances in video diffusion models have unlocked new potential for realistic audio-driven talking video generation. However, achieving seamless audio-lip synchronization, maintaining long-term identity consistency, and producing natural, audio-aligned expressions in generated talking videos r...
[06.12.2024 19:08] ********************************************************************************
[06.12.2024 19:08] Abstract 28. Open community-driven platforms like Chatbot Arena that collect user preference data from site visitors have gained a reputation as one of the most trustworthy publicly available benchmarks for LLM performance. While now standard, it is tricky to implement effective guardrails to collect high-qualit...
[06.12.2024 19:08] Read previous papers.
[06.12.2024 19:08] Generating reviews via LLM API.
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#inference", "#interpretability", "#multimodal", "#optimization", "#cv"], "emoji": "üóúÔ∏è", "ru": {"title": "VisionZip: –°–∂–∏–º–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, —É—Å–∫–æ—Ä—è–µ–º –ò–ò", "desc": "VisionZip - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç–µ–º –≤—ã–±–æ—Ä–∞ –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#agents", "#robotics", "#optimization", "#cv", "#security"], "emoji": "ü§ñ", "ru": {"title": "–ö–æ–¥ –∫–∞–∫ –º–æ–Ω–∏—Ç–æ—Ä: —É–º–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Code-as-Monitor (CaM), –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –æ—à–∏–±
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#open_source", "#games", "#multimodal", "#agents", "#training", "#reasoning", "#dataset"], "emoji": "üñ•Ô∏è", "ru": {"title": "Aguvis: –ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π –ì–ò–ü-–∞–≥–µ–Ω—Ç –Ω–∞ —á–∏—Å—Ç–æ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Aguvis - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#cv", "#optimization", "#diffusion", "#inference"], "emoji": "üé®", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –Ω–∞–≤–µ–¥–µ–Ω–∏—è: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ—Ö–Ω–∏–∫ –Ω–∞–≤–µ–¥–µ–Ω–∏—è
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#alignment", "#training", "#benchmark", "#hallucinations", "#open_source", "#architecture", "#multimodal"], "emoji": "üñºÔ∏è", "ru": {"title": "Florence-VL: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "Florence-VL –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#data"], "emoji": "üß†", "ru": {"title": "AgoraBench: –ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω AgoraBench - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ. –ò—Å—Å–ª
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#training", "#cv", "#ethics", "#multimodal", "#security", "#diffusion"], "emoji": "üé®", "ru": {"title": "NegToMe: –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º NegToMe (negative token merging) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#3d", "#dataset", "#training", "#open_source"], "emoji": "üé®", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞. –û—Å–Ω–æ–≤–æ–π –º–µ—Ç–æ–¥–∞ —è–≤–ª—è–µ—Ç—Å—è —É–Ω–∏—Ñ–∏—Ü
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#training", "#synthetic", "#optimization", "#architecture", "#cv", "#3d"], "emoji": "üñºÔ∏è", "ru": {"title": "MV-Adapter: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MV-Adapter - –ø–µ—Ä–≤–æ–µ –∞–¥–∞–ø—Ç–µ—Ä–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#diffusion", "#games", "#cv", "#multimodal"], "emoji": "üëö", "ru": {"title": "AnyDressing: –í–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –ø—Ä–∏–º–µ—Ä–∫–∞ –ª—é–±–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –æ–¥–µ–∂–¥—ã —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –¥–µ—Ç–∞–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ AnyDressing –¥–ª—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–º–µ—Ä–∫–∏ –æ–¥–µ–∂–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ —Å
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#multimodal", "#cv", "#benchmark", "#architecture"], "emoji": "üîÑ", "ru": {"title": "Infinity: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–º —Å–ª–æ–≤–∞—Ä–µ–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Infinity - –±–∏—Ç–æ–≤–∞—è –≤–∏–∑—É–∞–ª—å–Ω–∞—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#training", "#benchmark", "#open_source", "#optimization", "#architecture", "#inference"], "emoji": "üìà", "ru": {"title": "–ü–ª–æ—Ç–Ω–æ—Å—Ç—å –º–æ—â–Ω–æ—Å—Ç–∏: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É '–ø–ª–æ—Ç–Ω–æ—Å—Ç—å –º–æ—â–Ω–æ—Å—Ç–∏' –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#data"], "emoji": "üé®", "ru": {"title": "HumanEdit: –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø–æ–¥—Ö–æ–¥–æ–º", "desc": "HumanEdit - —ç—Ç–æ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Å —É—á–∞—Å—Ç–∏–µ–º –ª—é–¥–µ–π. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 5751 –∏–∑–æ–±—Ä–∞–∂
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#architecture", "#cv", "#reasoning", "#benchmark", "#optimization", "#multimodal", "#training"], "emoji": "üîç", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–≥–æ –∏–∑ –¥–≤—É—Ö –º–∏—Ä–æ–≤: –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã–µ LVLM —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º —è–∑—ã–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö –≤–∏–∑
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#training", "#survey", "#multimodal", "#architecture", "#dataset", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò: –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –æ–±–∑–æ—Ä –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–úLLM).
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#optimization", "#training", "#cv"], "emoji": "üöÄ", "ru": {"title": "ZipAR: –£—Å–∫–æ—Ä—è–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º", "desc": "ZipAR - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –Ω–∞–±–ª—é–¥–µ
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#dataset", "#architecture", "#multimodal", "#video"], "emoji": "‚öΩ", "ru": {"title": "MatchVision: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏ –¥–ª—è —Ñ—É—Ç–±–æ–ª–∞", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å MatchVision –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ñ—É—Ç–±–æ–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –∫—Ä—É–ø–Ω–µ–π—à–∏–π –¥–∞—Ç–∞—Å–µ—Ç Socce
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#ethics", "#dataset", "#multilingual", "#low_resource", "#machine_translation"], "emoji": "üåç", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –±–∞—Ä—å–µ—Ä–æ–≤ –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –æ—Ü–µ–Ω–∫–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏–π –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è 
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#training", "#optimization", "#architecture"], "emoji": "üîÑ", "ru": {"title": "OmniFlow: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "OmniFlow - —ç—Ç–æ –Ω–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª—é–±–æ–≥–æ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –ª—é–±–æ–π –¥
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#diffusion", "#healthcare", "#synthetic", "#cv", "#data", "#dataset"], "emoji": "üß†", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ú–†–¢ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∏–Ω—Ç–µ–∑—É –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –Ω–µ–∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#training", "#interpretability", "#optimization", "#open_source", "#architecture", "#alignment"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ–∑—Ä–∞—á–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –º–æ–Ω–æ—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏–Ω–¥—É–∫—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –∏–Ω–¥—É–∫—Ü–∏–æ–Ω–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–∫ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#optimization", "#inference", "#cv", "#hallucinations", "#training", "#multimodal"], "emoji": "üîç", "ru": {"title": "VisVM: –£–ª—É—á—à–µ–Ω–∏–µ VLM —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≤—ã–≤–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Vision Value Model (VisVM) - –º–æ–¥–µ–ª—å, —É–ª—É—á—à–∞—é—â—É—é –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ
[06.12.2024 19:08] Using data from previous issue: {"categories": ["#training", "#machine_translation", "#dataset", "#low_resource", "#multilingual"], "emoji": "üåç", "ru": {"title": "Marco-LLM: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –±–∞—Ä—å–µ—Ä–∞ –≤ –º–∏—Ä–µ –ò–ò", "desc": "Marco-LLM - —ç—Ç–æ –Ω–æ–≤–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑
[06.12.2024 19:08] Querying the API.
[06.12.2024 19:08] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We propose 4Real-Video, a novel framework for generating 4D videos, organized as a grid of video frames with both time and viewpoint axes. In this grid, each row contains frames sharing the same timestep, while each column contains frames from the same viewpoint. We propose a novel two-stream architecture. One stream performs viewpoint updates on columns, and the other stream performs temporal updates on rows. After each diffusion transformer layer, a synchronization layer exchanges information between the two token streams. We propose two implementations of the synchronization layer, using either hard or soft synchronization. This feedforward architecture improves upon previous work in three ways: higher inference speed, enhanced visual quality (measured by FVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency (measured by VideoScore and Dust3R-Confidence).
[06.12.2024 19:08] Response: {
  "desc": "4Real-Video - —ç—Ç–æ –Ω–æ–≤–∞—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 4D-–≤–∏–¥–µ–æ, –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤ –≤–∏–¥–µ —Å–µ—Ç–∫–∏ –∫–∞–¥—Ä–æ–≤ —Å –æ—Å—è–º–∏ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ä–∞–∫—É—Ä—Å–∞. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –¥–≤—É—Ö–ø–æ—Ç–æ–∫–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: –æ–¥–∏–Ω –ø–æ—Ç–æ–∫ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Ä–∞–∫—É—Ä—Å—ã –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º, –¥—Ä—É–≥–æ–π - –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ —Å—Ç—Ä–æ–∫–∞–º. –ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –º–µ–∂–¥—É –ø–æ—Ç–æ–∫–∞–º–∏. –≠—Ç–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–∞–±–æ—Ç—ã –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–∞, –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –∫–∞—á–µ—Å—Ç–≤—É –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ä–∞–∫—É—Ä—Å–∞–º.",
  "emoji": "üé•",
  "title": "4Real-Video: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 4D-–≤–∏–¥–µ–æ"
}
[06.12.2024 19:08] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We propose 4Real-Video, a novel framework for generating 4D videos, organized as a grid of video frames with both time and viewpoint axes. In this grid, each row contains frames sharing the same timestep, while each column contains frames from the same viewpoint. We propose a novel two-stream architecture. One stream performs viewpoint updates on columns, and the other stream performs temporal updates on rows. After each diffusion transformer layer, a synchronization layer exchanges information between the two token streams. We propose two implementations of the synchronization layer, using either hard or soft synchronization. This feedforward architecture improves upon previous work in three ways: higher inference speed, enhanced visual quality (measured by FVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency (measured by VideoScore and Dust3R-Confidence)."

[06.12.2024 19:08] Response: ```python
['VIDEO', 'ARCHITECTURE']
```
[06.12.2024 19:08] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We propose 4Real-Video, a novel framework for generating 4D videos, organized as a grid of video frames with both time and viewpoint axes. In this grid, each row contains frames sharing the same timestep, while each column contains frames from the same viewpoint. We propose a novel two-stream architecture. One stream performs viewpoint updates on columns, and the other stream performs temporal updates on rows. After each diffusion transformer layer, a synchronization layer exchanges information between the two token streams. We propose two implementations of the synchronization layer, using either hard or soft synchronization. This feedforward architecture improves upon previous work in three ways: higher inference speed, enhanced visual quality (measured by FVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency (measured by VideoScore and Dust3R-Confidence)."

[06.12.2024 19:08] Response: []
[06.12.2024 19:08] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces 4Real-Video, a new framework designed to create 4D videos that are structured in a grid format, incorporating both time and viewpoint dimensions. It utilizes a two-stream architecture where one stream focuses on updating the viewpoint across columns, while the other stream handles temporal updates along the rows. A synchronization layer is implemented after each diffusion transformer layer to facilitate communication between the two streams, with options for hard or soft synchronization. This innovative approach results in faster inference, better visual quality, and improved consistency in both temporal and viewpoint aspects compared to previous methods.","title":"Revolutionizing 4D Video Generation with 4Real-Video"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='The paper introduces 4Real-Video, a new framework designed to create 4D videos that are structured in a grid format, incorporating both time and viewpoint dimensions. It utilizes a two-stream architecture where one stream focuses on updating the viewpoint across columns, while the other stream handles temporal updates along the rows. A synchronization layer is implemented after each diffusion transformer layer to facilitate communication between the two streams, with options for hard or soft synchronization. This innovative approach results in faster inference, better visual quality, and improved consistency in both temporal and viewpoint aspects compared to previous methods.', title='Revolutionizing 4D Video Generation with 4Real-Video'))
[06.12.2024 19:08] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êàë‰ª¨ÊèêÂá∫‰∫Ü4Real-VideoÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑ4DËßÜÈ¢ëÁîüÊàêÊ°ÜÊû∂ÔºåËßÜÈ¢ëÂ∏ß‰ª•Êó∂Èó¥ÂíåËßÜËßí‰∏∫ËΩ¥ÁªÑÁªáÊàêÁΩëÊ†º„ÄÇÂú®Ëøô‰∏™ÁΩëÊ†º‰∏≠ÔºåÊØè‰∏ÄË°åÂåÖÂê´Áõ∏ÂêåÊó∂Èó¥Ê≠•ÁöÑÂ∏ßÔºåËÄåÊØè‰∏ÄÂàóÂåÖÂê´Áõ∏ÂêåËßÜËßíÁöÑÂ∏ß„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂèåÊµÅÊû∂ÊûÑÔºå‰∏ÄÊù°ÊµÅÂú®Âàó‰∏äËøõË°åËßÜËßíÊõ¥Êñ∞ÔºåÂè¶‰∏ÄÊù°ÊµÅÂú®Ë°å‰∏äËøõË°åÊó∂Èó¥Êõ¥Êñ∞„ÄÇÈÄöËøáÂêåÊ≠•Â±ÇÂú®‰∏§‰∏™ÊµÅ‰πãÈó¥‰∫§Êç¢‰ø°ÊÅØÔºåÊàë‰ª¨ÁöÑÊû∂ÊûÑÂú®Êé®ÁêÜÈÄüÂ∫¶„ÄÅËßÜËßâË¥®ÈáèÂíåÊó∂Èó¥‰∏éËßÜËßí‰∏ÄËá¥ÊÄßÊñπÈù¢ÈÉΩÊúâÊòæËëóÊèêÂçá„ÄÇ","title":"4Real-VideoÔºöÈ´òÊïàÁîüÊàê4DËßÜÈ¢ëÁöÑÊñ∞Ê°ÜÊû∂"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='Êàë‰ª¨ÊèêÂá∫‰∫Ü4Real-VideoÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑ4DËßÜÈ¢ëÁîüÊàêÊ°ÜÊû∂ÔºåËßÜÈ¢ëÂ∏ß‰ª•Êó∂Èó¥ÂíåËßÜËßí‰∏∫ËΩ¥ÁªÑÁªáÊàêÁΩëÊ†º„ÄÇÂú®Ëøô‰∏™ÁΩëÊ†º‰∏≠ÔºåÊØè‰∏ÄË°åÂåÖÂê´Áõ∏ÂêåÊó∂Èó¥Ê≠•ÁöÑÂ∏ßÔºåËÄåÊØè‰∏ÄÂàóÂåÖÂê´Áõ∏ÂêåËßÜËßíÁöÑÂ∏ß„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂèåÊµÅÊû∂ÊûÑÔºå‰∏ÄÊù°ÊµÅÂú®Âàó‰∏äËøõË°åËßÜËßíÊõ¥Êñ∞ÔºåÂè¶‰∏ÄÊù°ÊµÅÂú®Ë°å‰∏äËøõË°åÊó∂Èó¥Êõ¥Êñ∞„ÄÇÈÄöËøáÂêåÊ≠•Â±ÇÂú®‰∏§‰∏™ÊµÅ‰πãÈó¥‰∫§Êç¢‰ø°ÊÅØÔºåÊàë‰ª¨ÁöÑÊû∂ÊûÑÂú®Êé®ÁêÜÈÄüÂ∫¶„ÄÅËßÜËßâË¥®ÈáèÂíåÊó∂Èó¥‰∏éËßÜËßí‰∏ÄËá¥ÊÄßÊñπÈù¢ÈÉΩÊúâÊòæËëóÊèêÂçá„ÄÇ', title='4Real-VideoÔºöÈ´òÊïàÁîüÊàê4DËßÜÈ¢ëÁöÑÊñ∞Ê°ÜÊû∂'))
[06.12.2024 19:08] Querying the API.
[06.12.2024 19:08] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Despite the remarkable performance of multimodal large language models (MLLMs) across diverse tasks, the substantial training and inference costs impede their advancement. The majority of computation stems from the overwhelming volume of vision tokens processed by the transformer decoder. In this paper, we propose to build efficient MLLMs by leveraging the Mixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects essential vision tokens to process while skipping redundant ones. However, integrating MoD into MLLMs is non-trivial. To address the challenges of training and inference stability as well as limited training data, we adapt the MoD module with two novel designs: tanh-gated weight normalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we observe that vision tokens exhibit higher redundancy in deeper layer and thus design a progressive ratio decay (PRD) strategy, which gradually reduces the token retention ratio layer by layer, employing a shifted cosine schedule. This crucial design fully unleashes the potential of MoD, significantly boosting the efficiency and performance of our models. To validate the effectiveness of our approach, we conduct extensive experiments with two baseline models across 14 benchmarks. Our model, p-MoD, matches or even surpasses the performance of the baseline models, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and 77.7% GPU hours during training.
[06.12.2024 19:09] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–≤—ã—à–µ–Ω–∏—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ö–∞–Ω–∏–∑–º–∞ Mixture-of-Depths (MoD). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥—É–ª—å MoD —Å –ø–æ–º–æ—â—å—é –¥–≤—É—Ö –Ω–æ–≤—ã—Ö —Ç–µ—Ö–Ω–∏–∫: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–æ–≥–æ —Ç–∞–Ω–≥–µ–Ω—Å–∞ (TanhNorm) –∏ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ (STRing). –¢–∞–∫–∂–µ –≤–≤–µ–¥–µ–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ —É–º–µ–Ω—å—à–µ–Ω–∏—è —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è (PRD), –∫–æ—Ç–æ—Ä–∞—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ç —Å–ª–æ—è –∫ —Å–ª–æ—é. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å p-MoD –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.",
  "emoji": "üöÄ",
  "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏: –º–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤, –±–æ–ª—å—à–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"
}
[06.12.2024 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Despite the remarkable performance of multimodal large language models (MLLMs) across diverse tasks, the substantial training and inference costs impede their advancement. The majority of computation stems from the overwhelming volume of vision tokens processed by the transformer decoder. In this paper, we propose to build efficient MLLMs by leveraging the Mixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects essential vision tokens to process while skipping redundant ones. However, integrating MoD into MLLMs is non-trivial. To address the challenges of training and inference stability as well as limited training data, we adapt the MoD module with two novel designs: tanh-gated weight normalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we observe that vision tokens exhibit higher redundancy in deeper layer and thus design a progressive ratio decay (PRD) strategy, which gradually reduces the token retention ratio layer by layer, employing a shifted cosine schedule. This crucial design fully unleashes the potential of MoD, significantly boosting the efficiency and performance of our models. To validate the effectiveness of our approach, we conduct extensive experiments with two baseline models across 14 benchmarks. Our model, p-MoD, matches or even surpasses the performance of the baseline models, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and 77.7% GPU hours during training."

[06.12.2024 19:09] Response: ```python
['MULTIMODAL', 'TRAINING', 'INFERENCE', 'BENCHMARK']
```
[06.12.2024 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Despite the remarkable performance of multimodal large language models (MLLMs) across diverse tasks, the substantial training and inference costs impede their advancement. The majority of computation stems from the overwhelming volume of vision tokens processed by the transformer decoder. In this paper, we propose to build efficient MLLMs by leveraging the Mixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects essential vision tokens to process while skipping redundant ones. However, integrating MoD into MLLMs is non-trivial. To address the challenges of training and inference stability as well as limited training data, we adapt the MoD module with two novel designs: tanh-gated weight normalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we observe that vision tokens exhibit higher redundancy in deeper layer and thus design a progressive ratio decay (PRD) strategy, which gradually reduces the token retention ratio layer by layer, employing a shifted cosine schedule. This crucial design fully unleashes the potential of MoD, significantly boosting the efficiency and performance of our models. To validate the effectiveness of our approach, we conduct extensive experiments with two baseline models across 14 benchmarks. Our model, p-MoD, matches or even surpasses the performance of the baseline models, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and 77.7% GPU hours during training."

[06.12.2024 19:09] Response: ```python
["OPTIMIZATION"]
```
[06.12.2024 19:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a method to enhance the efficiency of multimodal large language models (MLLMs) by using a Mixture-of-Depths (MoD) mechanism. The MoD mechanism allows the model to selectively process important vision tokens while ignoring redundant ones, reducing computational costs. To improve training and inference stability, the authors implement two innovative designs: tanh-gated weight normalization (TanhNorm) and symmetric token reweighting (STRing). Their experiments show that the proposed model, p-MoD, achieves comparable or superior performance to baseline models while significantly lowering resource usage during both training and inference.","title":"Efficient Multimodal Learning with Selective Token Processing"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper introduces a method to enhance the efficiency of multimodal large language models (MLLMs) by using a Mixture-of-Depths (MoD) mechanism. The MoD mechanism allows the model to selectively process important vision tokens while ignoring redundant ones, reducing computational costs. To improve training and inference stability, the authors implement two innovative designs: tanh-gated weight normalization (TanhNorm) and symmetric token reweighting (STRing). Their experiments show that the proposed model, p-MoD, achieves comparable or superior performance to baseline models while significantly lowering resource usage during both training and inference.', title='Efficient Multimodal Learning with Selective Token Processing'))
[06.12.2024 19:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈ´òÊïàÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÔºåÈÄöËøáÊ∑∑ÂêàÊ∑±Â∫¶ÔºàMoDÔºâÊú∫Âà∂Êù•‰ºòÂåñËÆ°ÁÆóÊïàÁéá„ÄÇËØ•Êú∫Âà∂ÂÖÅËÆ∏ÊØè‰∏™ÂèòÊç¢Âô®Ëß£Á†ÅÂô®Â±ÇÈÄâÊã©ÈáçË¶ÅÁöÑËßÜËßâÊ†áËÆ∞ËøõË°åÂ§ÑÁêÜÔºåÂêåÊó∂Ë∑≥ËøáÂÜó‰ΩôÁöÑÊ†áËÆ∞„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥ËÆ≠ÁªÉÂíåÊé®ÁêÜÁöÑÁ®≥ÂÆöÊÄßÈóÆÈ¢òÔºåÊú¨ÊñáÂºïÂÖ•‰∫Ü‰∏§ÁßçÊñ∞ËÆæËÆ°ÔºötanhÈó®ÊéßÊùÉÈáçÂΩí‰∏ÄÂåñÔºàTanhNormÔºâÂíåÂØπÁß∞Ê†áËÆ∞ÈáçÂä†ÊùÉÔºàSTRingÔºâ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑp-MoDÊ®°ÂûãÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóËµÑÊ∫êÁöÑÊ∂àËÄó„ÄÇ","title":"È´òÊïàÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂàõÊñ∞ËÆæËÆ°"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈ´òÊïàÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÔºåÈÄöËøáÊ∑∑ÂêàÊ∑±Â∫¶ÔºàMoDÔºâÊú∫Âà∂Êù•‰ºòÂåñËÆ°ÁÆóÊïàÁéá„ÄÇËØ•Êú∫Âà∂ÂÖÅËÆ∏ÊØè‰∏™ÂèòÊç¢Âô®Ëß£Á†ÅÂô®Â±ÇÈÄâÊã©ÈáçË¶ÅÁöÑËßÜËßâÊ†áËÆ∞ËøõË°åÂ§ÑÁêÜÔºåÂêåÊó∂Ë∑≥ËøáÂÜó‰ΩôÁöÑÊ†áËÆ∞„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥ËÆ≠ÁªÉÂíåÊé®ÁêÜÁöÑÁ®≥ÂÆöÊÄßÈóÆÈ¢òÔºåÊú¨ÊñáÂºïÂÖ•‰∫Ü‰∏§ÁßçÊñ∞ËÆæËÆ°ÔºötanhÈó®ÊéßÊùÉÈáçÂΩí‰∏ÄÂåñÔºàTanhNormÔºâÂíåÂØπÁß∞Ê†áËÆ∞ÈáçÂä†ÊùÉÔºàSTRingÔºâ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑp-MoDÊ®°ÂûãÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóËµÑÊ∫êÁöÑÊ∂àËÄó„ÄÇ', title='È´òÊïàÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂàõÊñ∞ËÆæËÆ°'))
[06.12.2024 19:09] Using data from previous issue: {"categories": ["#open_source", "#data", "#dataset", "#synthetic", "#transfer_learning", "#benchmark"], "emoji": "üìä", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ç–∞–±–ª–∏—Ü", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SynFinTabs - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏
[06.12.2024 19:09] Using data from previous issue: {"categories": ["#long_context", "#multimodal", "#video", "#diffusion"], "emoji": "üé≠", "ru": {"title": "MEMO: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –ø–æ—Ä—Ç—Ä–µ—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –ø–∞–º—è—Ç–∏ –∏ —ç–º–æ—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –≥–æ–≤–æ—Ä—è—â–∏–º–∏ –ø–æ—Ä—Ç—Ä–µ—Ç–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º MEMO (Memory-guid
[06.12.2024 19:09] Using data from previous issue: {"categories": ["#ethics", "#rlhf", "#hallucinations", "#alignment", "#benchmark"], "emoji": "üé≠", "ru": {"title": "–ê—Ö–∏–ª–ª–µ—Å–æ–≤–∞ –ø—è—Ç–∞ –∫—Ä–∞—É–¥—Å–æ—Ä—Å–∏–Ω–≥–æ–≤—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ Chatbot Arena. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç —Ç—Ä–∏ –∏—Å—Ç–æ—á
[06.12.2024 19:09] Loading Chinese text from previous data.
[06.12.2024 19:09] Renaming data file.
[06.12.2024 19:09] Renaming previous data. hf_papers.json to ./d/2024-12-06.json
[06.12.2024 19:09] Saving new data file.
[06.12.2024 19:09] Generating page.
[06.12.2024 19:09] Renaming previous page.
[06.12.2024 19:09] Renaming previous data. index.html to ./d/2024-12-06.html
[06.12.2024 19:09] [Experimental] Generating Chinese page for reading.
[06.12.2024 19:09] Chinese vocab [{'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ju√©', 'trans': 'vision'}, {'word': 'ËØ≠Ë®Ä', 'pinyin': 'y«îy√°n', 'trans': 'language'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'}, {'word': 'Â¢ûÂä†', 'pinyin': 'zƒìngjiƒÅ', 'trans': 'increase'}, {'word': 'ÈïøÂ∫¶', 'pinyin': 'ch√°ngd√π', 'trans': 'length'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ngn√©ng', 'trans': 'performance'}, {'word': 'ËÆ°ÁÆó', 'pinyin': 'j√¨su√†n', 'trans': 'computation'}, {'word': 'ÊàêÊú¨', 'pinyin': 'ch√©ngbƒõn', 'trans': 'cost'}, {'word': 'ÂèëÁé∞', 'pinyin': 'fƒÅxi√†n', 'trans': 'discover'}, {'word': 'ÊµÅË°å', 'pinyin': 'li√∫x√≠ng', 'trans': 'popular'}, {'word': 'ÁºñÁ†ÅÂô®', 'pinyin': 'biƒÅnm«éq√¨', 'trans': 'encoder'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generate'}, {'word': 'ÂÜó‰Ωô', 'pinyin': 'r√≥ngy√∫', 'trans': 'redundancy'}, {'word': 'ÂºïÂÖ•', 'pinyin': 'y«ênr√π', 'trans': 'introduce'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'}, {'word': 'ÈÄâÊã©', 'pinyin': 'xu«énz√©', 'trans': 'select'}, {'word': '‰∏∞ÂØå', 'pinyin': 'fƒìngf√π', 'trans': 'rich'}, {'word': 'ËæìÂÖ•', 'pinyin': 'sh≈´r√π', 'trans': 'input'}, {'word': 'ÊïàÁéá', 'pinyin': 'xi√†ol«ú', 'trans': 'efficiency'}, {'word': 'ÂõæÂÉè', 'pinyin': 't√∫xi√†ng', 'trans': 'image'}, {'word': 'ËßÜÈ¢ë', 'pinyin': 'sh√¨p√≠n', 'trans': 'video'}, {'word': 'ÁêÜËß£', 'pinyin': 'l«êjiƒõ', 'trans': 'understanding'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®nw√π', 'trans': 'task'}, {'word': 'ÁâπÂà´', 'pinyin': 't√®bi√©', 'trans': 'especially'}, {'word': 'ÈÄÇÂêà', 'pinyin': 'sh√¨h√©', 'trans': 'suitable'}, {'word': 'Â§öËΩÆ', 'pinyin': 'du≈çl√∫n', 'trans': 'multi-turn'}, {'word': 'ÂØπËØù', 'pinyin': 'du√¨hu√†', 'trans': 'dialogue'}, {'word': 'Âú∫ÊôØ', 'pinyin': 'ch«éngj«êng', 'trans': 'scenario'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√©gu«í', 'trans': 'result'}, {'word': 'ÊòæÁ§∫', 'pinyin': 'xi«énsh√¨', 'trans': 'show'}, {'word': 'Âá†‰πé', 'pinyin': 'jƒ´h≈´', 'trans': 'almost'}, {'word': 'ËÆæÁΩÆ', 'pinyin': 'sh√®zh√¨', 'trans': 'setting'}, {'word': 'ÊØî', 'pinyin': 'b«ê', 'trans': 'compare'}, {'word': '‰ª•Ââç', 'pinyin': 'y«êqi√°n', 'trans': 'before'}, {'word': 'ÊúÄ‰Ω≥', 'pinyin': 'zu√¨jiƒÅ', 'trans': 'best'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠gƒÅo', 'trans': 'improve'}]
[06.12.2024 19:09] Renaming previous Chinese page.
[06.12.2024 19:09] Renaming previous data. zh.html to ./d/2024-12-05_zh_reading_task.html
[06.12.2024 19:09] Writing Chinese reading task.
[06.12.2024 19:09] Writing result.
[06.12.2024 19:09] Renaming log file.
[06.12.2024 19:09] Renaming previous data. log.txt to ./logs/2024-12-06_last_log.txt
