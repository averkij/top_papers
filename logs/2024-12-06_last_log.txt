[06.12.2024 05:12] Read previous papers.
[06.12.2024 05:12] Generating top page (month).
[06.12.2024 05:12] Writing top page (month).
[06.12.2024 06:15] Read previous papers.
[06.12.2024 06:15] Get feed.
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04454
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04455
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04467
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03632
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01506
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01339
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03679
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04315
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04431
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04280
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04139
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01820
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04106
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.02142
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04424
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04062
[06.12.2024 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01169
[06.12.2024 06:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.12.2024 06:15] No deleted papers detected.
[06.12.2024 06:15] Downloading and parsing papers (pdf, html). Total: 17.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.04454.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.04454.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.04454.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.04455.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.04455.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.04455.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.04467.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.04467.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.04467.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.03632.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.03632.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.03632.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.01506.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.01506.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.01506.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.01339.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.01339.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.01339.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.03679.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.03679.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.03679.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.04315.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.04315.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.04315.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.04431.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.04431.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.04431.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.04280.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.04280.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.04280.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.04139.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.04139.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.04139.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.01820.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.01820.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.01820.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.04106.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.04106.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.04106.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.02142.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.02142.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.02142.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.04424.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.04424.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.04424.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.04062.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.04062.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.04062.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Downloading and parsing paper https://huggingface.co/papers/2412.01169.
[06.12.2024 06:15] Extra JSON file exists (./assets/json/2412.01169.json), skip PDF parsing.
[06.12.2024 06:15] Paper image links file exists (./assets/img_data/2412.01169.json), skip HTML parsing.
[06.12.2024 06:15] Success.
[06.12.2024 06:15] Enriching papers with extra data.
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 0. Graphical User Interfaces (GUIs) are critical to human-computer interaction, yet automating GUI tasks remains challenging due to the complexity and variability of visual environments. Existing approaches often rely on textual representations of GUIs, which introduce limitations in generalization, ef...
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 1. Automatic detection and prevention of open-set failures are crucial in closed-loop robotic systems. Recent studies often struggle to simultaneously identify unexpected failures reactively after they occur and prevent foreseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a nove...
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 2. Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens generated by popular vision encoders, such as CLIP and...
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 3. Existing multi-view image generation methods often make invasive modifications to pre-trained text-to-image (T2I) models and require full fine-tuning, leading to (1) high computational costs, especially with large base models and high-resolution images, and (2) degradation in image quality due to op...
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 4. We introduce a novel 3D generation method for versatile and high-quality 3D asset creation. The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a spa...
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 5. Text-based adversarial guidance using a negative prompt has emerged as a widely adopted approach to push the output features away from undesired concepts. While useful, performing adversarial guidance using text alone can be insufficient to capture complex visual concepts and avoid undesired visual ...
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 6. Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic ...
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 7. Large Language Models (LLMs) have emerged as a milestone in artificial intelligence, and their performance can improve as the model size increases. However, this scaling brings great challenges to training and inference efficiency, particularly for deploying LLMs in resource-constrained environments...
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 8. We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and ...
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 9. We present HumanEdit, a high-quality, human-rewarded dataset specifically designed for instruction-guided image editing, enabling precise and diverse image manipulations through open-form language instructions. Previous large-scale editing datasets often incorporate minimal human feedback, leading t...
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 10. Understanding the internal computations of large language models (LLMs) is crucial for aligning them with human values and preventing undesirable behaviors like toxic content generation. However, mechanistic interpretability is hindered by polysemanticity -- where individual neurons respond to multi...
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 11. As a globally celebrated sport, soccer has attracted widespread interest from fans all over the world. This paper aims to develop a comprehensive multi-modal framework for soccer video understanding. Specifically, we make the following contributions in this paper: (i) we introduce SoccerReplay-1988,...
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 12. Medical image segmentation has recently demonstrated impressive progress with deep neural networks, yet the heterogeneous modalities and scarcity of mask annotations limit the development of segmentation models on unannotated modalities. This paper investigates a new paradigm for leveraging generati...
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 13. Multimodal Large Language Models (MLLMs) have become increasingly important due to their state-of-the-art performance and ability to integrate multiple data modalities, such as text, images, and audio, to perform complex tasks with high accuracy. This paper presents a comprehensive survey on persona...
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 14. We present Florence-VL, a new family of multimodal large language models (MLLMs) with enriched visual representations produced by Florence-2, a generative vision foundation model. Unlike the widely used CLIP-style vision transformer trained by contrastive learning, Florence-2 can capture different l...
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 15. In this paper, we propose ZipAR, a training-free, plug-and-play parallel decoding framework for accelerating auto-regressive (AR) visual generation. The motivation stems from the observation that images exhibit local structures, and spatially distant regions tend to have minimal interdependence. Giv...
[06.12.2024 06:15] ********************************************************************************
[06.12.2024 06:15] Abstract 16. We introduce OmniFlow, a novel generative model designed for any-to-any generation tasks such as text-to-image, text-to-audio, and audio-to-image synthesis. OmniFlow advances the rectified flow (RF) framework used in text-to-image models to handle the joint distribution of multiple modalities. It ou...
[06.12.2024 06:15] Read previous papers.
[06.12.2024 06:15] Generating reviews via LLM API.
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#open_source", "#games", "#multimodal", "#agents", "#training", "#reasoning", "#dataset"], "emoji": "üñ•Ô∏è", "ru": {"title": "Aguvis: –ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π –ì–ò–ü-–∞–≥–µ–Ω—Ç –Ω–∞ —á–∏—Å—Ç–æ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Aguvis - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#agents", "#robotics", "#optimization", "#cv", "#security"], "emoji": "ü§ñ", "ru": {"title": "–ö–æ–¥ –∫–∞–∫ –º–æ–Ω–∏—Ç–æ—Ä: —É–º–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Code-as-Monitor (CaM), –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –æ—à–∏–±
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#inference", "#interpretability", "#multimodal", "#optimization", "#cv"], "emoji": "üóúÔ∏è", "ru": {"title": "VisionZip: –°–∂–∏–º–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, —É—Å–∫–æ—Ä—è–µ–º –ò–ò", "desc": "VisionZip - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç–µ–º –≤—ã–±–æ—Ä–∞ –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#training", "#synthetic", "#optimization", "#architecture", "#cv", "#3d"], "emoji": "üñºÔ∏è", "ru": {"title": "MV-Adapter: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MV-Adapter - –ø–µ—Ä–≤–æ–µ –∞–¥–∞–ø—Ç–µ—Ä–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#3d", "#dataset", "#training", "#open_source"], "emoji": "üé®", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞. –û—Å–Ω–æ–≤–æ–π –º–µ—Ç–æ–¥–∞ —è–≤–ª—è–µ—Ç—Å—è —É–Ω–∏—Ñ–∏—Ü
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#training", "#cv", "#ethics", "#multimodal", "#security", "#diffusion"], "emoji": "üé®", "ru": {"title": "NegToMe: –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º NegToMe (negative token merging) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#data"], "emoji": "üß†", "ru": {"title": "AgoraBench: –ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω AgoraBench - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ. –ò—Å—Å–ª
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#training", "#benchmark", "#open_source", "#optimization", "#architecture", "#inference"], "emoji": "üìà", "ru": {"title": "–ü–ª–æ—Ç–Ω–æ—Å—Ç—å –º–æ—â–Ω–æ—Å—Ç–∏: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É '–ø–ª–æ—Ç–Ω–æ—Å—Ç—å –º–æ—â–Ω–æ—Å—Ç–∏' –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#multimodal", "#cv", "#benchmark", "#architecture"], "emoji": "üîÑ", "ru": {"title": "Infinity: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–º —Å–ª–æ–≤–∞—Ä–µ–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Infinity - –±–∏—Ç–æ–≤–∞—è –≤–∏–∑—É–∞–ª—å–Ω–∞—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#data"], "emoji": "üé®", "ru": {"title": "HumanEdit: –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø–æ–¥—Ö–æ–¥–æ–º", "desc": "HumanEdit - —ç—Ç–æ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Å —É—á–∞—Å—Ç–∏–µ–º –ª—é–¥–µ–π. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 5751 –∏–∑–æ–±—Ä–∞–∂
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#training", "#interpretability", "#optimization", "#open_source", "#architecture", "#alignment"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ–∑—Ä–∞—á–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –º–æ–Ω–æ—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#dataset", "#architecture", "#multimodal", "#video"], "emoji": "‚öΩ", "ru": {"title": "MatchVision: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏ –¥–ª—è —Ñ—É—Ç–±–æ–ª–∞", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å MatchVision –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ñ—É—Ç–±–æ–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –∫—Ä—É–ø–Ω–µ–π—à–∏–π –¥–∞—Ç–∞—Å–µ—Ç Socce
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#diffusion", "#healthcare", "#synthetic", "#cv", "#data", "#dataset"], "emoji": "üß†", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ú–†–¢ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∏–Ω—Ç–µ–∑—É –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –Ω–µ–∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#training", "#survey", "#multimodal", "#architecture", "#dataset", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò: –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –æ–±–∑–æ—Ä –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–úLLM).
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#alignment", "#training", "#benchmark", "#hallucinations", "#open_source", "#architecture", "#multimodal"], "emoji": "üñºÔ∏è", "ru": {"title": "Florence-VL: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "Florence-VL –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#optimization", "#training", "#cv"], "emoji": "üöÄ", "ru": {"title": "ZipAR: –£—Å–∫–æ—Ä—è–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º", "desc": "ZipAR - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –Ω–∞–±–ª—é–¥–µ
[06.12.2024 06:15] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#training", "#optimization", "#architecture"], "emoji": "üîÑ", "ru": {"title": "OmniFlow: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "OmniFlow - —ç—Ç–æ –Ω–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª—é–±–æ–≥–æ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –ª—é–±–æ–π –¥
[06.12.2024 06:15] Loading Chinese text from previous data.
[06.12.2024 06:15] Renaming data file.
[06.12.2024 06:15] Renaming previous data. hf_papers.json to ./d/2024-12-06.json
[06.12.2024 06:15] Saving new data file.
[06.12.2024 06:15] Generating page.
[06.12.2024 06:15] Renaming previous page.
[06.12.2024 06:15] Renaming previous data. index.html to ./d/2024-12-06.html
[06.12.2024 06:15] [Experimental] Generating Chinese page for reading.
[06.12.2024 06:15] Chinese vocab [{'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'}, {'word': 'Êó®Âú®', 'pinyin': 'zh«êz√†i', 'trans': 'aim to'}, {'word': 'ÊîπËøõ', 'pinyin': 'g«éij√¨n', 'trans': 'improve'}, {'word': 'ÂçïÊ≠•', 'pinyin': 'dƒÅnb√π', 'trans': 'single-step'}, {'word': 'Êâ©Êï£', 'pinyin': 'ku√≤s√†n', 'trans': 'diffusion'}, {'word': 'ÊåáÂØº', 'pinyin': 'zh«êd«éo', 'trans': 'guidance'}, {'word': 'Êú∫Âà∂', 'pinyin': 'jƒ´zh√¨', 'trans': 'mechanism'}, {'word': 'Áé∞Êúâ', 'pinyin': 'xi√†ny«íu', 'trans': 'existing'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'}, {'word': 'Â§ÑÁêÜ', 'pinyin': 'ch«îl«ê', 'trans': 'handle'}, {'word': '‰∏çÂêå', 'pinyin': 'b√πt√≥ng', 'trans': 'different'}, {'word': 'È™®Êû∂', 'pinyin': 'g«îji√†', 'trans': 'skeleton'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éoxi√†n', 'trans': 'performance'}, {'word': '‰∏çÁ®≥ÂÆö', 'pinyin': 'b√πwƒõnd√¨ng', 'trans': 'unstable'}, {'word': '‰∏î', 'pinyin': 'qiƒõ', 'trans': 'and'}, {'word': '‰∏çÊîØÊåÅ', 'pinyin': 'b√π zhƒ´ch√≠', 'trans': 'not support'}, {'word': 'Ë¥üÈù¢', 'pinyin': 'f√πmi√†n', 'trans': 'negative'}, {'word': 'ÊèêÁ§∫', 'pinyin': 't√≠sh√¨', 'trans': 'prompt'}, {'word': 'ÈÄöËøá', 'pinyin': 't≈çnggu√≤', 'trans': 'through'}, {'word': 'PG-SB', 'pinyin': '', 'trans': 'PG-SB'}, {'word': 'NASA', 'pinyin': '', 'trans': 'NASA'}, {'word': 'Ëß£ÂÜ≥', 'pinyin': 'jiƒõju√©', 'trans': 'solve'}, {'word': 'Ëøô‰∫õ', 'pinyin': 'zh√®xiƒì', 'trans': 'these'}, {'word': 'ÈóÆÈ¢ò', 'pinyin': 'w√®nt√≠', 'trans': 'problems'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√©gu«í', 'trans': 'result'}, {'word': 'ÊòæÁ§∫', 'pinyin': 'xi«énsh√¨', 'trans': 'show'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'}, {'word': 'ÊèêÂçá', 'pinyin': 't√≠shƒìng', 'trans': 'enhance'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´zh«în', 'trans': 'benchmark'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√≠ngn√©ng', 'trans': 'performance'}, {'word': 'ËææÂà∞', 'pinyin': 'd√°d√†o', 'trans': 'reach'}, {'word': 'Êñ∞ÁöÑ', 'pinyin': 'xƒ´n de', 'trans': 'new'}, {'word': 'ÊúÄ‰Ω≥', 'pinyin': 'zu√¨jiƒÅ', 'trans': 'best'}, {'word': 'Ê∞¥Âπ≥', 'pinyin': 'shu«êp√≠ng', 'trans': 'level'}]
[06.12.2024 06:15] Renaming previous Chinese page.
[06.12.2024 06:15] Renaming previous data. zh.html to ./d/2024-12-05_zh_reading_task.html
[06.12.2024 06:15] Writing Chinese reading task.
[06.12.2024 06:15] Writing result.
[06.12.2024 06:15] Renaming log file.
[06.12.2024 06:15] Renaming previous data. log.txt to ./logs/2024-12-06_last_log.txt
