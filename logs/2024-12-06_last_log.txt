[06.12.2024 11:09] Read previous papers.
[06.12.2024 11:09] Generating top page (month).
[06.12.2024 11:09] Writing top page (month).
[06.12.2024 12:19] Read previous papers.
[06.12.2024 12:19] Get feed.
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04467
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04455
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04454
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03895
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03679
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03632
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01339
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01506
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04315
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04424
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04431
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01820
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04280
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.02142
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04062
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04106
[06.12.2024 12:19] Extract page data from URL. URL: https://huggingface.co/papers/2412.04378
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03304
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04139
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01169
[06.12.2024 12:19] Extract page data from URL. URL: https://huggingface.co/papers/2412.04146
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2411.19574
[06.12.2024 12:19] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04003
[06.12.2024 12:19] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.12.2024 12:19] No deleted papers detected.
[06.12.2024 12:19] Downloading and parsing papers (pdf, html). Total: 23.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.04467.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.04467.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.04467.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.04455.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.04455.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.04455.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.04454.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.04454.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.04454.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.03895.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.03895.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.03895.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.03679.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.03679.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.03679.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.03632.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.03632.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.03632.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.01339.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.01339.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.01339.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.01506.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.01506.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.01506.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.04315.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.04315.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.04315.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.04424.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.04424.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.04424.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.04431.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.04431.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.04431.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.01820.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.01820.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.01820.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.04280.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.04280.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.04280.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.02142.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.02142.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.02142.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.04062.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.04062.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.04062.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.04106.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.04106.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.04106.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.04378.
[06.12.2024 12:19] Downloading paper 2412.04378 from http://arxiv.org/pdf/2412.04378v1...
[06.12.2024 12:19] Extracting affiliations from text.
[06.12.2024 12:19] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. If there are no affiliations return empty list.

Text:"Discriminative Fine-tuning of LVLMs Yassine Ouali*1 Adrian Bulat*1,2 Alexandros Xenos1,3 Anestis Zaganidis1 Ioannis Maniadis Metaxas1 Brais Martinez1 Georgios Tzimiropoulos1,3 1Samsung AI Cambridge 2Technical University of Iasi 3Queen Mary University of London 4 2 0 2 ] . [ 1 8 7 3 4 0 . 2 1 4 2 : r a "
[06.12.2024 12:19] Response: ```python
["Samsung AI Cambridge", "Technical University of Iasi", "Queen Mary University of London"]
```
[06.12.2024 12:19] Deleting PDF ./assets/pdf/2412.04378.pdf.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.03304.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.03304.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.03304.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.04139.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.04139.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.04139.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.01169.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.01169.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.01169.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.04146.
[06.12.2024 12:19] Downloading paper 2412.04146 from http://arxiv.org/pdf/2412.04146v1...
[06.12.2024 12:19] Extracting affiliations from text.
[06.12.2024 12:19] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. If there are no affiliations return empty list.

Text:"AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models Xinghui Li1 Qichao Sun1 Pengze Zhang1 Fulong Ye1 Zhichao Liao2 Wanquan Feng1 1ByteDance Songtao Zhao1 Qian He1 2Tsinghua University https://crayon-shinchan.github.io/AnyDressing/ 4 2 0 2 5 ] . [ 1 6 4 1 4 0 . 2 1 4 2 : r Figure 1. Customizable virtual dressing results of our AnyDressing. Reliability: AnyDressing is well-suited for variety of scenes and complex garments. Compatibility: AnyDressing is compatible with LoRA [15] and plugins such as ControlNet [55] and FaceID [54]. "
[06.12.2024 12:19] Response: ```python
["ByteDance", "Tsinghua University"]
```
[06.12.2024 12:19] Deleting PDF ./assets/pdf/2412.04146.pdf.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2411.19574.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2411.19574.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2411.19574.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Downloading and parsing paper https://huggingface.co/papers/2412.04003.
[06.12.2024 12:19] Extra JSON file exists (./assets/json/2412.04003.json), skip PDF parsing.
[06.12.2024 12:19] Paper image links file exists (./assets/img_data/2412.04003.json), skip HTML parsing.
[06.12.2024 12:19] Success.
[06.12.2024 12:19] Enriching papers with extra data.
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 0. Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens generated by popular vision encoders, such as CLIP and...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 1. Automatic detection and prevention of open-set failures are crucial in closed-loop robotic systems. Recent studies often struggle to simultaneously identify unexpected failures reactively after they occur and prevent foreseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a nove...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 2. Graphical User Interfaces (GUIs) are critical to human-computer interaction, yet automating GUI tasks remains challenging due to the complexity and variability of visual environments. Existing approaches often rely on textual representations of GUIs, which introduce limitations in generalization, ef...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 3. Diffusion models excel in generating high-quality images. However, current diffusion models struggle to produce reliable images without guidance methods, such as classifier-free guidance (CFG). Are guidance methods truly necessary? Observing that noise obtained via diffusion inversion can reconstruc...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 4. Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic ...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 5. Existing multi-view image generation methods often make invasive modifications to pre-trained text-to-image (T2I) models and require full fine-tuning, leading to (1) high computational costs, especially with large base models and high-resolution images, and (2) degradation in image quality due to op...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 6. Text-based adversarial guidance using a negative prompt has emerged as a widely adopted approach to push the output features away from undesired concepts. While useful, performing adversarial guidance using text alone can be insufficient to capture complex visual concepts and avoid undesired visual ...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 7. We introduce a novel 3D generation method for versatile and high-quality 3D asset creation. The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a spa...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 8. Large Language Models (LLMs) have emerged as a milestone in artificial intelligence, and their performance can improve as the model size increases. However, this scaling brings great challenges to training and inference efficiency, particularly for deploying LLMs in resource-constrained environments...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 9. We present Florence-VL, a new family of multimodal large language models (MLLMs) with enriched visual representations produced by Florence-2, a generative vision foundation model. Unlike the widely used CLIP-style vision transformer trained by contrastive learning, Florence-2 can capture different l...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 10. We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and ...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 11. As a globally celebrated sport, soccer has attracted widespread interest from fans all over the world. This paper aims to develop a comprehensive multi-modal framework for soccer video understanding. Specifically, we make the following contributions in this paper: (i) we introduce SoccerReplay-1988,...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 12. We present HumanEdit, a high-quality, human-rewarded dataset specifically designed for instruction-guided image editing, enabling precise and diverse image manipulations through open-form language instructions. Previous large-scale editing datasets often incorporate minimal human feedback, leading t...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 13. Multimodal Large Language Models (MLLMs) have become increasingly important due to their state-of-the-art performance and ability to integrate multiple data modalities, such as text, images, and audio, to perform complex tasks with high accuracy. This paper presents a comprehensive survey on persona...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 14. In this paper, we propose ZipAR, a training-free, plug-and-play parallel decoding framework for accelerating auto-regressive (AR) visual generation. The motivation stems from the observation that images exhibit local structures, and spatially distant regions tend to have minimal interdependence. Giv...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 15. Medical image segmentation has recently demonstrated impressive progress with deep neural networks, yet the heterogeneous modalities and scarcity of mask annotations limit the development of segmentation models on unannotated modalities. This paper investigates a new paradigm for leveraging generati...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 16. Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a "bag of words" behavior. At the same time, Large Vision-Language M...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 17. Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermo...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 18. Understanding the internal computations of large language models (LLMs) is crucial for aligning them with human values and preventing undesirable behaviors like toxic content generation. However, mechanistic interpretability is hindered by polysemanticity -- where individual neurons respond to multi...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 19. We introduce OmniFlow, a novel generative model designed for any-to-any generation tasks such as text-to-image, text-to-audio, and audio-to-image synthesis. OmniFlow advances the rectified flow (RF) framework used in text-to-image models to handle the joint distribution of multiple modalities. It ou...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 20. Recent advances in garment-centric image generation from text and image prompts based on diffusion models are impressive. However, existing methods lack support for various combinations of attire, and struggle to preserve the garment details while maintaining faithfulness to the text prompts, limiti...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 21. The current large language models are mainly based on decode-only structure transformers, which have great in-context learning (ICL) capabilities. It is generally believed that the important foundation of its ICL capability is the induction heads mechanism, which requires at least two layers attenti...
[06.12.2024 12:19] ********************************************************************************
[06.12.2024 12:19] Abstract 22. Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages...
[06.12.2024 12:19] Read previous papers.
[06.12.2024 12:19] Generating reviews via LLM API.
[06.12.2024 12:19] Using data from previous issue: {"categories": ["#inference", "#interpretability", "#multimodal", "#optimization", "#cv"], "emoji": "🗜️", "ru": {"title": "VisionZip: Сжимаем визуальные токены, ускоряем ИИ", "desc": "VisionZip - это новый метод, который улучшает эффективность визуально-языковых моделей путем выбора наиболее информа
[06.12.2024 12:19] Using data from previous issue: {"categories": ["#agents", "#robotics", "#optimization", "#cv", "#security"], "emoji": "🤖", "ru": {"title": "Код как монитор: умное обнаружение ошибок для роботов", "desc": "В статье представлен метод Code-as-Monitor (CaM), использующий визуально-языковую модель для обнаружения и предотвращения ошиб
[06.12.2024 12:19] Using data from previous issue: {"categories": ["#open_source", "#games", "#multimodal", "#agents", "#training", "#reasoning", "#dataset"], "emoji": "🖥️", "ru": {"title": "Aguvis: Автономный ГИП-агент на чистом компьютерном зрении", "desc": "Авторы представляют Aguvis - унифицированную систему для автономных агентов графического и
[06.12.2024 12:19] Using data from previous issue: {"categories": ["#cv", "#optimization", "#diffusion", "#inference"], "emoji": "🎨", "ru": {"title": "Генерация изображений без наведения: революция в диффузионных моделях", "desc": "Статья представляет новый метод генерации изображений с помощью диффузионных моделей без использования техник наведения
[06.12.2024 12:19] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#data"], "emoji": "🧠", "ru": {"title": "AgoraBench: Новый взгляд на генерацию данных языковыми моделями", "desc": "В статье представлен AgoraBench - новый бенчмарк для оценки способности языковых моделей генерировать синтетические данные. Иссл
[06.12.2024 12:19] Using data from previous issue: {"categories": ["#training", "#synthetic", "#optimization", "#architecture", "#cv", "#3d"], "emoji": "🖼️", "ru": {"title": "MV-Adapter: Эффективная генерация многоракурсных изображений без переобучения", "desc": "Статья представляет MV-Adapter - первое адаптерное решение для генерации многоракурсных
[06.12.2024 12:19] Using data from previous issue: {"categories": ["#training", "#cv", "#ethics", "#multimodal", "#security", "#diffusion"], "emoji": "🎨", "ru": {"title": "NegToMe: визуальное руководство для диффузионных моделей", "desc": "Данная статья представляет новый метод под названием NegToMe (negative token merging) для улучшения генерации и
[06.12.2024 12:19] Using data from previous issue: {"categories": ["#3d", "#dataset", "#training", "#open_source"], "emoji": "🎨", "ru": {"title": "Универсальная генерация 3D-объектов с помощью структурированного латентного представления", "desc": "Статья представляет новый метод генерации 3D-объектов высокого качества. Основой метода является унифиц
[06.12.2024 12:19] Using data from previous issue: {"categories": ["#training", "#benchmark", "#open_source", "#optimization", "#architecture", "#inference"], "emoji": "📈", "ru": {"title": "Плотность мощности: новый путь к эффективным языковым моделям", "desc": "Статья представляет новую метрику 'плотность мощности' для оценки качества больших языко
[06.12.2024 12:19] Using data from previous issue: {"categories": ["#alignment", "#training", "#benchmark", "#hallucinations", "#open_source", "#architecture", "#multimodal"], "emoji": "🖼️", "ru": {"title": "Florence-VL: Новый уровень понимания изображений для языковых моделей", "desc": "Florence-VL представляет собой новое семейство мультимодальных
[06.12.2024 12:19] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#multimodal", "#cv", "#benchmark", "#architecture"], "emoji": "🔄", "ru": {"title": "Infinity: новый уровень генерации изображений с бесконечным словарем", "desc": "Представлена модель Infinity - битовая визуальная авторегрессионная модель для генерации 
[06.12.2024 12:19] Using data from previous issue: {"categories": ["#dataset", "#architecture", "#multimodal", "#video"], "emoji": "⚽", "ru": {"title": "MatchVision: революция в компьютерном зрении для футбола", "desc": "В этой статье представлена многомодальная модель MatchVision для анализа футбольных видео. Авторы создали крупнейший датасет Socce
[06.12.2024 12:19] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#data"], "emoji": "🎨", "ru": {"title": "HumanEdit: Редактирование изображений с человеческим подходом", "desc": "HumanEdit - это набор данных для редактирования изображений на основе текстовых инструкций, созданный с участием людей. Он включает 5751 изображ
[06.12.2024 12:19] Using data from previous issue: {"categories": ["#training", "#survey", "#multimodal", "#architecture", "#dataset", "#benchmark"], "emoji": "🤖", "ru": {"title": "Персонализация мультимодальных ИИ: от архитектуры до применения", "desc": "В статье представлен обзор персонализированных мультимодальных больших языковых моделей (МLLM).
[06.12.2024 12:19] Using data from previous issue: {"categories": ["#optimization", "#training", "#cv"], "emoji": "🚀", "ru": {"title": "ZipAR: Ускоряем генерацию изображений параллельным декодированием", "desc": "ZipAR - это новый подход к ускорению авторегрессионной генерации изображений без дополнительного обучения модели. Метод основан на наблюде
[06.12.2024 12:19] Using data from previous issue: {"categories": ["#diffusion", "#healthcare", "#synthetic", "#cv", "#data", "#dataset"], "emoji": "🧠", "ru": {"title": "Генерация синтетических МРТ для обучения сегментации без разметки", "desc": "В этой статье представлен новый подход к синтезу медицинских изображений для неаннотированных модальност
[06.12.2024 12:19] Querying the API.
[06.12.2024 12:19] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a "bag of words" behavior. At the same time, Large Vision-Language Models (LVLMs), which combine vision encoders with LLMs, have been shown capable of detailed vision-language reasoning, yet their autoregressive nature renders them less suitable for discriminative tasks.   In this work, we propose to combine "the best of both worlds": a new training approach for discriminative fine-tuning of LVLMs that results in strong discriminative and compositional capabilities. Essentially, our approach converts a generative LVLM into a discriminative one, unlocking its capability for powerful image-text discrimination combined with enhanced language understanding.   Our contributions include: (1) A carefully designed training/optimization framework that utilizes image-text pairs of variable length and granularity for training the model with both contrastive and next-token prediction losses. This is accompanied by ablation studies that justify the necessity of our framework's components. (2) A parameter-efficient adaptation method using a combination of soft prompting and LoRA adapters. (3) Significant improvements over state-of-the-art CLIP-like models of similar size, including standard image-text retrieval benchmarks and notable gains in compositionality.
[06.12.2024 12:20] Response: {
  "desc": "В статье предлагается новый подход к обучению больших визуально-языковых моделей (LVLM) для дискриминативных задач. Авторы комбинируют сильные стороны контрастивных моделей типа CLIP и генеративных LVLM, создавая модель с улучшенным пониманием языка и способностью к композиционному рассуждению. Предложенный метод включает специальную схему обучения с контрастивными и предиктивными потерями, а также эффективную адаптацию параметров. Результаты показывают значительное улучшение по сравнению с современными моделями типа CLIP в задачах поиска изображений по тексту и композиционности.",
  "emoji": "🔍",
  "title": "Объединение лучшего из двух миров: дискриминативные LVLM с улучшенным пониманием языка"
}
[06.12.2024 12:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a "bag of words" behavior. At the same time, Large Vision-Language Models (LVLMs), which combine vision encoders with LLMs, have been shown capable of detailed vision-language reasoning, yet their autoregressive nature renders them less suitable for discriminative tasks.   In this work, we propose to combine "the best of both worlds": a new training approach for discriminative fine-tuning of LVLMs that results in strong discriminative and compositional capabilities. Essentially, our approach converts a generative LVLM into a discriminative one, unlocking its capability for powerful image-text discrimination combined with enhanced language understanding.   Our contributions include: (1) A carefully designed training/optimization framework that utilizes image-text pairs of variable length and granularity for training the model with both contrastive and next-token prediction losses. This is accompanied by ablation studies that justify the necessity of our framework's components. (2) A parameter-efficient adaptation method using a combination of soft prompting and LoRA adapters. (3) Significant improvements over state-of-the-art CLIP-like models of similar size, including standard image-text retrieval benchmarks and notable gains in compositionality."

[06.12.2024 12:20] Response: ```python
["CV", "MULTIMODAL", "TRAINING", "BENCHMARK", "ARCHITECTURE"]
```
[06.12.2024 12:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a "bag of words" behavior. At the same time, Large Vision-Language Models (LVLMs), which combine vision encoders with LLMs, have been shown capable of detailed vision-language reasoning, yet their autoregressive nature renders them less suitable for discriminative tasks.   In this work, we propose to combine "the best of both worlds": a new training approach for discriminative fine-tuning of LVLMs that results in strong discriminative and compositional capabilities. Essentially, our approach converts a generative LVLM into a discriminative one, unlocking its capability for powerful image-text discrimination combined with enhanced language understanding.   Our contributions include: (1) A carefully designed training/optimization framework that utilizes image-text pairs of variable length and granularity for training the model with both contrastive and next-token prediction losses. This is accompanied by ablation studies that justify the necessity of our framework's components. (2) A parameter-efficient adaptation method using a combination of soft prompting and LoRA adapters. (3) Significant improvements over state-of-the-art CLIP-like models of similar size, including standard image-text retrieval benchmarks and notable gains in compositionality."

[06.12.2024 12:20] Response: ```python
["OPTIMIZATION", "REASONING"]
```
[06.12.2024 12:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new training method for Large Vision-Language Models (LVLMs) that enhances their ability to understand and discriminate between images and text. By combining contrastive learning with next-token prediction, the model achieves better performance in tasks requiring detailed vision-language reasoning. The authors also present a parameter-efficient adaptation technique that utilizes soft prompting and LoRA adapters, making the model more efficient. Overall, this work demonstrates significant improvements over existing models like CLIP, particularly in image-text retrieval and compositional understanding.","title":"Unlocking Discriminative Power in Vision-Language Models"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper introduces a new training method for Large Vision-Language Models (LVLMs) that enhances their ability to understand and discriminate between images and text. By combining contrastive learning with next-token prediction, the model achieves better performance in tasks requiring detailed vision-language reasoning. The authors also present a parameter-efficient adaptation technique that utilizes soft prompting and LoRA adapters, making the model more efficient. Overall, this work demonstrates significant improvements over existing models like CLIP, particularly in image-text retrieval and compositional understanding.', title='Unlocking Discriminative Power in Vision-Language Models'))
[06.12.2024 12:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新的训练方法，用于对大型视觉语言模型（LVLMs）进行区分性微调，从而提高其在图像-文本任务中的表现。我们的方法结合了对比学习和下一个标记预测损失，使得模型能够更好地理解语言并进行图像-文本的区分。通过使用可变长度和粒度的图像-文本对进行训练，我们的框架展示了其各个组成部分的必要性。最终，我们的方法在标准的图像-文本检索基准上显著超越了现有的CLIP类模型。","title":"结合优势，提升视觉语言模型的区分能力"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='本文提出了一种新的训练方法，用于对大型视觉语言模型（LVLMs）进行区分性微调，从而提高其在图像-文本任务中的表现。我们的方法结合了对比学习和下一个标记预测损失，使得模型能够更好地理解语言并进行图像-文本的区分。通过使用可变长度和粒度的图像-文本对进行训练，我们的框架展示了其各个组成部分的必要性。最终，我们的方法在标准的图像-文本检索基准上显著超越了现有的CLIP类模型。', title='结合优势，提升视觉语言模型的区分能力'))
[06.12.2024 12:20] Using data from previous issue: {"categories": ["#ethics", "#dataset", "#multilingual", "#low_resource", "#translation"], "emoji": "🌍", "ru": {"title": "Преодоление культурных барьеров в многоязычной оценке языковых моделей", "desc": "Статья рассматривает проблему культурных предубеждений в многоязычных наборах данных для оценки я
[06.12.2024 12:20] Using data from previous issue: {"categories": ["#training", "#interpretability", "#optimization", "#open_source", "#architecture", "#alignment"], "emoji": "🧠", "ru": {"title": "Прозрачные большие языковые модели: расшифровка внутренних вычислений с помощью моносемантических экспертов", "desc": "Статья представляет новую архитекту
[06.12.2024 12:20] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#training", "#optimization", "#architecture"], "emoji": "🔄", "ru": {"title": "OmniFlow: универсальная модель для мультимодальной генерации", "desc": "OmniFlow - это новая генеративная модель, разработанная для задач генерации любого типа данных в любой д
[06.12.2024 12:20] Querying the API.
[06.12.2024 12:20] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recent advances in garment-centric image generation from text and image prompts based on diffusion models are impressive. However, existing methods lack support for various combinations of attire, and struggle to preserve the garment details while maintaining faithfulness to the text prompts, limiting their performance across diverse scenarios. In this paper, we focus on a new task, i.e., Multi-Garment Virtual Dressing, and we propose a novel AnyDressing method for customizing characters conditioned on any combination of garments and any personalized text prompts. AnyDressing comprises two primary networks named GarmentsNet and DressingNet, which are respectively dedicated to extracting detailed clothing features and generating customized images. Specifically, we propose an efficient and scalable module called Garment-Specific Feature Extractor in GarmentsNet to individually encode garment textures in parallel. This design prevents garment confusion while ensuring network efficiency. Meanwhile, we design an adaptive Dressing-Attention mechanism and a novel Instance-Level Garment Localization Learning strategy in DressingNet to accurately inject multi-garment features into their corresponding regions. This approach efficiently integrates multi-garment texture cues into generated images and further enhances text-image consistency. Additionally, we introduce a Garment-Enhanced Texture Learning strategy to improve the fine-grained texture details of garments. Thanks to our well-craft design, AnyDressing can serve as a plug-in module to easily integrate with any community control extensions for diffusion models, improving the diversity and controllability of synthesized images. Extensive experiments show that AnyDressing achieves state-of-the-art results.
[06.12.2024 12:20] Response: {
  "desc": "Статья представляет новый метод AnyDressing для виртуальной примерки одежды на основе диффузионных моделей. Метод состоит из двух основных сетей: GarmentsNet для извлечения деталей одежды и DressingNet для генерации изображений. В GarmentsNet используется модуль Garment-Specific Feature Extractor для эффективного кодирования текстур одежды. DressingNet применяет механизм Dressing-Attention и стратегию Instance-Level Garment Localization Learning для точного наложения элементов одежды.",

  "emoji": "👚",

  "title": "AnyDressing: Виртуальная примерка любой комбинации одежды с сохранением деталей"
}
[06.12.2024 12:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advances in garment-centric image generation from text and image prompts based on diffusion models are impressive. However, existing methods lack support for various combinations of attire, and struggle to preserve the garment details while maintaining faithfulness to the text prompts, limiting their performance across diverse scenarios. In this paper, we focus on a new task, i.e., Multi-Garment Virtual Dressing, and we propose a novel AnyDressing method for customizing characters conditioned on any combination of garments and any personalized text prompts. AnyDressing comprises two primary networks named GarmentsNet and DressingNet, which are respectively dedicated to extracting detailed clothing features and generating customized images. Specifically, we propose an efficient and scalable module called Garment-Specific Feature Extractor in GarmentsNet to individually encode garment textures in parallel. This design prevents garment confusion while ensuring network efficiency. Meanwhile, we design an adaptive Dressing-Attention mechanism and a novel Instance-Level Garment Localization Learning strategy in DressingNet to accurately inject multi-garment features into their corresponding regions. This approach efficiently integrates multi-garment texture cues into generated images and further enhances text-image consistency. Additionally, we introduce a Garment-Enhanced Texture Learning strategy to improve the fine-grained texture details of garments. Thanks to our well-craft design, AnyDressing can serve as a plug-in module to easily integrate with any community control extensions for diffusion models, improving the diversity and controllability of synthesized images. Extensive experiments show that AnyDressing achieves state-of-the-art results."

[06.12.2024 12:20] Response: ```python
['CV', 'MULTIMODAL']
```
[06.12.2024 12:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advances in garment-centric image generation from text and image prompts based on diffusion models are impressive. However, existing methods lack support for various combinations of attire, and struggle to preserve the garment details while maintaining faithfulness to the text prompts, limiting their performance across diverse scenarios. In this paper, we focus on a new task, i.e., Multi-Garment Virtual Dressing, and we propose a novel AnyDressing method for customizing characters conditioned on any combination of garments and any personalized text prompts. AnyDressing comprises two primary networks named GarmentsNet and DressingNet, which are respectively dedicated to extracting detailed clothing features and generating customized images. Specifically, we propose an efficient and scalable module called Garment-Specific Feature Extractor in GarmentsNet to individually encode garment textures in parallel. This design prevents garment confusion while ensuring network efficiency. Meanwhile, we design an adaptive Dressing-Attention mechanism and a novel Instance-Level Garment Localization Learning strategy in DressingNet to accurately inject multi-garment features into their corresponding regions. This approach efficiently integrates multi-garment texture cues into generated images and further enhances text-image consistency. Additionally, we introduce a Garment-Enhanced Texture Learning strategy to improve the fine-grained texture details of garments. Thanks to our well-craft design, AnyDressing can serve as a plug-in module to easily integrate with any community control extensions for diffusion models, improving the diversity and controllability of synthesized images. Extensive experiments show that AnyDressing achieves state-of-the-art results."

[06.12.2024 12:20] Response: ```python
["DIFFUSION", "GAMES"]
```
[06.12.2024 12:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces AnyDressing, a novel method for Multi-Garment Virtual Dressing that enhances image generation from text and image prompts. It features two main networks: GarmentsNet, which extracts detailed clothing features, and DressingNet, which generates customized images while maintaining text-image consistency. The method includes a Garment-Specific Feature Extractor to efficiently encode garment textures and an adaptive Dressing-Attention mechanism to accurately place multi-garment features. Overall, AnyDressing improves the diversity and controllability of synthesized images, achieving state-of-the-art performance in garment-centric image generation.","title":"AnyDressing: Revolutionizing Multi-Garment Image Generation"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper introduces AnyDressing, a novel method for Multi-Garment Virtual Dressing that enhances image generation from text and image prompts. It features two main networks: GarmentsNet, which extracts detailed clothing features, and DressingNet, which generates customized images while maintaining text-image consistency. The method includes a Garment-Specific Feature Extractor to efficiently encode garment textures and an adaptive Dressing-Attention mechanism to accurately place multi-garment features. Overall, AnyDressing improves the diversity and controllability of synthesized images, achieving state-of-the-art performance in garment-centric image generation.', title='AnyDressing: Revolutionizing Multi-Garment Image Generation'))
[06.12.2024 12:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新的多服装虚拟穿衣任务，并提出了AnyDressing方法，以支持任意组合的服装和个性化文本提示。AnyDressing包含两个主要网络：GarmentsNet用于提取服装细节特征，DressingNet用于生成定制图像。我们设计了高效的服装特征提取模块，避免了服装混淆，同时提高了网络效率。此外，DressingNet中的自适应注意机制和实例级服装定位学习策略，确保了多服装特征的准确注入，从而提升了生成图像的多样性和一致性。","title":"AnyDressing：多服装虚拟穿衣的新方法"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='本文提出了一种新的多服装虚拟穿衣任务，并提出了AnyDressing方法，以支持任意组合的服装和个性化文本提示。AnyDressing包含两个主要网络：GarmentsNet用于提取服装细节特征，DressingNet用于生成定制图像。我们设计了高效的服装特征提取模块，避免了服装混淆，同时提高了网络效率。此外，DressingNet中的自适应注意机制和实例级服装定位学习策略，确保了多服装特征的准确注入，从而提升了生成图像的多样性和一致性。', title='AnyDressing：多服装虚拟穿衣的新方法'))
[06.12.2024 12:20] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "🧠", "ru": {"title": "Повышение эффективности индукционных механизмов в языковых моделях", "desc": "Эта статья исследует механизм индукционных головок в больших языковых моделях, основанных на трансформерах. Авторы предлагают н
[06.12.2024 12:20] Using data from previous issue: {"categories": ["#training", "#machine_translation", "#dataset", "#low_resource", "#multilingual"], "emoji": "🌍", "ru": {"title": "Marco-LLM: Преодоление языкового барьера в мире ИИ", "desc": "Marco-LLM - это новая многоязычная модель больших языковых моделей (LLM), разработанная для улучшения произ
[06.12.2024 12:20] Loading Chinese text from previous data.
[06.12.2024 12:20] Renaming data file.
[06.12.2024 12:20] Renaming previous data. hf_papers.json to ./d/2024-12-06.json
[06.12.2024 12:20] Saving new data file.
[06.12.2024 12:20] Generating page.
[06.12.2024 12:20] Renaming previous page.
[06.12.2024 12:20] Renaming previous data. index.html to ./d/2024-12-06.html
[06.12.2024 12:20] [Experimental] Generating Chinese page for reading.
[06.12.2024 12:20] Chinese vocab [{'word': '视觉', 'pinyin': 'shìjué', 'trans': 'vision'}, {'word': '语言', 'pinyin': 'yǔyán', 'trans': 'language'}, {'word': '模型', 'pinyin': 'móxíng', 'trans': 'model'}, {'word': '增加', 'pinyin': 'zēngjiā', 'trans': 'increase'}, {'word': '长度', 'pinyin': 'chángdù', 'trans': 'length'}, {'word': '性能', 'pinyin': 'xìngnéng', 'trans': 'performance'}, {'word': '计算', 'pinyin': 'jìsuàn', 'trans': 'computation'}, {'word': '成本', 'pinyin': 'chéngběn', 'trans': 'cost'}, {'word': '发现', 'pinyin': 'fāxiàn', 'trans': 'discover'}, {'word': '流行', 'pinyin': 'liúxíng', 'trans': 'popular'}, {'word': '编码器', 'pinyin': 'biānmǎqì', 'trans': 'encoder'}, {'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generate'}, {'word': '冗余', 'pinyin': 'róngyú', 'trans': 'redundancy'}, {'word': '引入', 'pinyin': 'yǐnrù', 'trans': 'introduce'}, {'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'}, {'word': '选择', 'pinyin': 'xuǎnzé', 'trans': 'select'}, {'word': '丰富', 'pinyin': 'fēngfù', 'trans': 'rich'}, {'word': '输入', 'pinyin': 'shūrù', 'trans': 'input'}, {'word': '效率', 'pinyin': 'xiàolǜ', 'trans': 'efficiency'}, {'word': '图像', 'pinyin': 'túxiàng', 'trans': 'image'}, {'word': '视频', 'pinyin': 'shìpín', 'trans': 'video'}, {'word': '理解', 'pinyin': 'lǐjiě', 'trans': 'understanding'}, {'word': '任务', 'pinyin': 'rènwù', 'trans': 'task'}, {'word': '特别', 'pinyin': 'tèbié', 'trans': 'especially'}, {'word': '适合', 'pinyin': 'shìhé', 'trans': 'suitable'}, {'word': '多轮', 'pinyin': 'duōlún', 'trans': 'multi-turn'}, {'word': '对话', 'pinyin': 'duìhuà', 'trans': 'dialogue'}, {'word': '场景', 'pinyin': 'chǎngjǐng', 'trans': 'scenario'}, {'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'}, {'word': '显示', 'pinyin': 'xiǎnshì', 'trans': 'show'}, {'word': '几乎', 'pinyin': 'jīhū', 'trans': 'almost'}, {'word': '设置', 'pinyin': 'shèzhì', 'trans': 'setting'}, {'word': '比', 'pinyin': 'bǐ', 'trans': 'compare'}, {'word': '以前', 'pinyin': 'yǐqián', 'trans': 'before'}, {'word': '最佳', 'pinyin': 'zuìjiā', 'trans': 'best'}, {'word': '提高', 'pinyin': 'tígāo', 'trans': 'improve'}]
[06.12.2024 12:20] Renaming previous Chinese page.
[06.12.2024 12:20] Renaming previous data. zh.html to ./d/2024-12-05_zh_reading_task.html
[06.12.2024 12:20] Writing Chinese reading task.
[06.12.2024 12:20] Writing result.
[06.12.2024 12:20] Renaming log file.
[06.12.2024 12:20] Renaming previous data. log.txt to ./logs/2024-12-06_last_log.txt
