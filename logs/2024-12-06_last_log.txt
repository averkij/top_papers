[06.12.2024 08:15] Read previous papers.
[06.12.2024 08:15] Generating top page (month).
[06.12.2024 08:15] Writing top page (month).
[06.12.2024 09:11] Read previous papers.
[06.12.2024 09:11] Get feed.
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04467
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04455
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04454
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03895
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03632
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03679
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01506
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01339
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04315
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04431
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04280
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.02142
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04139
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04062
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04424
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01820
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04106
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01169
[06.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04003
[06.12.2024 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.12.2024 09:11] No deleted papers detected.
[06.12.2024 09:11] Downloading and parsing papers (pdf, html). Total: 19.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.04467.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.04467.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.04467.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.04455.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.04455.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.04455.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.04454.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.04454.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.04454.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.03895.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.03895.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.03895.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.03632.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.03632.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.03632.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.03679.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.03679.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.03679.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.01506.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.01506.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.01506.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.01339.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.01339.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.01339.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.04315.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.04315.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.04315.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.04431.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.04431.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.04431.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.04280.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.04280.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.04280.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.02142.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.02142.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.02142.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.04139.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.04139.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.04139.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.04062.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.04062.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.04062.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.04424.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.04424.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.04424.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.01820.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.01820.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.01820.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.04106.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.04106.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.04106.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.01169.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.01169.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.01169.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.04003.
[06.12.2024 09:11] Extra JSON file exists (./assets/json/2412.04003.json), skip PDF parsing.
[06.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.04003.json), skip HTML parsing.
[06.12.2024 09:11] Success.
[06.12.2024 09:11] Enriching papers with extra data.
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 0. Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens generated by popular vision encoders, such as CLIP and...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 1. Automatic detection and prevention of open-set failures are crucial in closed-loop robotic systems. Recent studies often struggle to simultaneously identify unexpected failures reactively after they occur and prevent foreseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a nove...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 2. Graphical User Interfaces (GUIs) are critical to human-computer interaction, yet automating GUI tasks remains challenging due to the complexity and variability of visual environments. Existing approaches often rely on textual representations of GUIs, which introduce limitations in generalization, ef...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 3. Diffusion models excel in generating high-quality images. However, current diffusion models struggle to produce reliable images without guidance methods, such as classifier-free guidance (CFG). Are guidance methods truly necessary? Observing that noise obtained via diffusion inversion can reconstruc...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 4. Existing multi-view image generation methods often make invasive modifications to pre-trained text-to-image (T2I) models and require full fine-tuning, leading to (1) high computational costs, especially with large base models and high-resolution images, and (2) degradation in image quality due to op...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 5. Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic ...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 6. We introduce a novel 3D generation method for versatile and high-quality 3D asset creation. The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a spa...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 7. Text-based adversarial guidance using a negative prompt has emerged as a widely adopted approach to push the output features away from undesired concepts. While useful, performing adversarial guidance using text alone can be insufficient to capture complex visual concepts and avoid undesired visual ...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 8. Large Language Models (LLMs) have emerged as a milestone in artificial intelligence, and their performance can improve as the model size increases. However, this scaling brings great challenges to training and inference efficiency, particularly for deploying LLMs in resource-constrained environments...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 9. We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and ...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 10. We present HumanEdit, a high-quality, human-rewarded dataset specifically designed for instruction-guided image editing, enabling precise and diverse image manipulations through open-form language instructions. Previous large-scale editing datasets often incorporate minimal human feedback, leading t...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 11. Multimodal Large Language Models (MLLMs) have become increasingly important due to their state-of-the-art performance and ability to integrate multiple data modalities, such as text, images, and audio, to perform complex tasks with high accuracy. This paper presents a comprehensive survey on persona...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 12. Understanding the internal computations of large language models (LLMs) is crucial for aligning them with human values and preventing undesirable behaviors like toxic content generation. However, mechanistic interpretability is hindered by polysemanticity -- where individual neurons respond to multi...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 13. In this paper, we propose ZipAR, a training-free, plug-and-play parallel decoding framework for accelerating auto-regressive (AR) visual generation. The motivation stems from the observation that images exhibit local structures, and spatially distant regions tend to have minimal interdependence. Giv...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 14. We present Florence-VL, a new family of multimodal large language models (MLLMs) with enriched visual representations produced by Florence-2, a generative vision foundation model. Unlike the widely used CLIP-style vision transformer trained by contrastive learning, Florence-2 can capture different l...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 15. As a globally celebrated sport, soccer has attracted widespread interest from fans all over the world. This paper aims to develop a comprehensive multi-modal framework for soccer video understanding. Specifically, we make the following contributions in this paper: (i) we introduce SoccerReplay-1988,...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 16. Medical image segmentation has recently demonstrated impressive progress with deep neural networks, yet the heterogeneous modalities and scarcity of mask annotations limit the development of segmentation models on unannotated modalities. This paper investigates a new paradigm for leveraging generati...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 17. We introduce OmniFlow, a novel generative model designed for any-to-any generation tasks such as text-to-image, text-to-audio, and audio-to-image synthesis. OmniFlow advances the rectified flow (RF) framework used in text-to-image models to handle the joint distribution of multiple modalities. It ou...
[06.12.2024 09:11] ********************************************************************************
[06.12.2024 09:11] Abstract 18. Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages...
[06.12.2024 09:11] Read previous papers.
[06.12.2024 09:11] Generating reviews via LLM API.
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#inference", "#interpretability", "#multimodal", "#optimization", "#cv"], "emoji": "ğŸ—œï¸", "ru": {"title": "VisionZip: Ğ¡Ğ¶Ğ¸Ğ¼Ğ°ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, ÑƒÑĞºĞ¾Ñ€ÑĞµĞ¼ Ğ˜Ğ˜", "desc": "VisionZip - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#agents", "#robotics", "#optimization", "#cv", "#security"], "emoji": "ğŸ¤–", "ru": {"title": "ĞšĞ¾Ğ´ ĞºĞ°Ğº Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€: ÑƒĞ¼Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Code-as-Monitor (CaM), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#open_source", "#games", "#multimodal", "#agents", "#training", "#reasoning", "#dataset"], "emoji": "ğŸ–¥ï¸", "ru": {"title": "Aguvis: ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ“Ğ˜ĞŸ-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸", "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Aguvis - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#cv", "#optimization", "#diffusion", "#inference"], "emoji": "ğŸ¨", "ru": {"title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½Ğ°Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ½Ğ°Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#training", "#synthetic", "#optimization", "#architecture", "#cv", "#3d"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "MV-Adapter: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MV-Adapter - Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ…
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#data"], "emoji": "ğŸ§ ", "ru": {"title": "AgoraBench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ AgoraBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ˜ÑÑĞ»
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#3d", "#dataset", "#training", "#open_source"], "emoji": "ğŸ¨", "ru": {"title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#training", "#cv", "#ethics", "#multimodal", "#security", "#diffusion"], "emoji": "ğŸ¨", "ru": {"title": "NegToMe: Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ NegToMe (negative token merging) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#training", "#benchmark", "#open_source", "#optimization", "#architecture", "#inference"], "emoji": "ğŸ“ˆ", "ru": {"title": "ĞŸĞ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ 'Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸' Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#multimodal", "#cv", "#benchmark", "#architecture"], "emoji": "ğŸ”„", "ru": {"title": "Infinity: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼", "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Infinity - Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#data"], "emoji": "ğŸ¨", "ru": {"title": "HumanEdit: Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼", "desc": "HumanEdit - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ğ»ÑĞ´ĞµĞ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 5751 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#training", "#survey", "#multimodal", "#architecture", "#dataset", "#benchmark"], "emoji": "ğŸ¤–", "ru": {"title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜: Ğ¾Ñ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (ĞœLLM).
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#training", "#interpretability", "#optimization", "#open_source", "#architecture", "#alignment"], "emoji": "ğŸ§ ", "ru": {"title": "ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ñ€Ğ°ÑÑˆĞ¸Ñ„Ñ€Ğ¾Ğ²ĞºĞ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ½Ğ¾ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚Ñƒ
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#optimization", "#training", "#cv"], "emoji": "ğŸš€", "ru": {"title": "ZipAR: Ğ£ÑĞºĞ¾Ñ€ÑĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼", "desc": "ZipAR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğµ
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#alignment", "#training", "#benchmark", "#hallucinations", "#open_source", "#architecture", "#multimodal"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "Florence-VL: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Florence-VL Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#dataset", "#architecture", "#multimodal", "#video"], "emoji": "âš½", "ru": {"title": "MatchVision: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğ°", "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MatchVision Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Socce
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#diffusion", "#healthcare", "#synthetic", "#cv", "#data", "#dataset"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞœĞ Ğ¢ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸", "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½ĞµĞ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#training", "#optimization", "#architecture"], "emoji": "ğŸ”„", "ru": {"title": "OmniFlow: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸", "desc": "OmniFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ»ÑĞ±Ğ¾Ğ¹ Ğ´
[06.12.2024 09:11] Using data from previous issue: {"categories": ["#training", "#translation", "#dataset", "#low_resource", "#multilingual"], "emoji": "ğŸŒ", "ru": {"title": "Marco-LLM: ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ° Ğ² Ğ¼Ğ¸Ñ€Ğµ Ğ˜Ğ˜", "desc": "Marco-LLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒ
[06.12.2024 09:11] Trying to get texts in Chinese.
[06.12.2024 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens generated by popular vision encoders, such as CLIP and SigLIP, contain significant redundancy. To address this, we introduce VisionZip, a simple yet effective method that selects a set of informative tokens for input to the language model, reducing visual token redundancy and improving efficiency while maintaining model performance. The proposed VisionZip can be widely applied to image and video understanding tasks and is well-suited for multi-turn dialogues in real-world scenarios, where previous methods tend to underperform. Experimental results show that VisionZip outperforms the previous state-of-the-art method by at least 5% performance gains across nearly all settings. Moreover, our method significantly enhances model inference speed, improving the prefilling time by 8x and enabling the LLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model while achieving better results. Furthermore, we analyze the causes of this redundancy and encourage the community to focus on extracting better visual features rather than merely increasing token length. Our code is available at https://github.com/dvlab-research/VisionZip .
[06.12.2024 09:11] Mistral response. {"id": "2128387087fb40f8ac83a111cd3fb0a0", "object": "chat.completion", "created": 1733476295, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "\u6700\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u589e\u52a0\u89c6\u89c9\u6807\u8bb0\u7684\u957f\u5ea6\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u6d41\u884c\u7684\u89c6\u89c9\u7f16\u7801\u5668\u751f\u6210\u7684\u89c6\u89c9\u6807\u8bb0\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86VisionZip\uff0c\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u4e00\u7ec4\u4fe1\u606f\u4e30\u5bcc\u7684\u6807\u8bb0\u8f93\u5165\u8bed\u8a00\u6a21\u578b\uff0c\u51cf\u5c11\u5197\u4f59\u5e76\u63d0\u9ad8\u6548\u7387\u3002VisionZip\u9002\u7528\u4e8e\u56fe\u50cf\u548c\u89c6\u9891\u7406\u89e3\u4efb\u52a1\uff0c\u7279\u522b\u9002\u5408\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cVisionZip\u5728\u51e0\u4e4e\u6240\u6709\u8bbe\u7f6e\u4e2d\u90fd\u6bd4\u4ee5\u524d\u7684\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad8\u4e86\u81f3\u5c115%\u7684\u6027\u80fd\u3002", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 320, "total_tokens": 513, "completion_tokens": 193}}
[06.12.2024 09:11] Response: æœ€æ–°çš„è§†è§‰-è¯­è¨€æ¨¡å‹é€šè¿‡å¢åŠ è§†è§‰æ ‡è®°çš„é•¿åº¦æé«˜äº†æ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å‘ç°ï¼Œæµè¡Œçš„è§†è§‰ç¼–ç å™¨ç”Ÿæˆçš„è§†è§‰æ ‡è®°ä¸­å­˜åœ¨å¤§é‡å†—ä½™ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VisionZipï¼Œä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€‰æ‹©ä¸€ç»„ä¿¡æ¯ä¸°å¯Œçš„æ ‡è®°è¾“å…¥è¯­è¨€æ¨¡å‹ï¼Œå‡å°‘å†—ä½™å¹¶æé«˜æ•ˆç‡ã€‚VisionZipé€‚ç”¨äºå›¾åƒå’Œè§†é¢‘ç†è§£ä»»åŠ¡ï¼Œç‰¹åˆ«é€‚åˆå¤šè½®å¯¹è¯åœºæ™¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVisionZipåœ¨å‡ ä¹æ‰€æœ‰è®¾ç½®ä¸­éƒ½æ¯”ä»¥å‰çš„æœ€ä½³æ–¹æ³•æé«˜äº†è‡³å°‘5%çš„æ€§èƒ½ã€‚
[06.12.2024 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

æœ€æ–°çš„è§†è§‰-è¯­è¨€æ¨¡å‹é€šè¿‡å¢åŠ è§†è§‰æ ‡è®°çš„é•¿åº¦æé«˜äº†æ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å‘ç°ï¼Œæµè¡Œçš„è§†è§‰ç¼–ç å™¨ç”Ÿæˆçš„è§†è§‰æ ‡è®°ä¸­å­˜åœ¨å¤§é‡å†—ä½™ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VisionZipï¼Œä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€‰æ‹©ä¸€ç»„ä¿¡æ¯ä¸°å¯Œçš„æ ‡è®°è¾“å…¥è¯­è¨€æ¨¡å‹ï¼Œå‡å°‘å†—ä½™å¹¶æé«˜æ•ˆç‡ã€‚VisionZipé€‚ç”¨äºå›¾åƒå’Œè§†é¢‘ç†è§£ä»»åŠ¡ï¼Œç‰¹åˆ«é€‚åˆå¤šè½®å¯¹è¯åœºæ™¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVisionZipåœ¨å‡ ä¹æ‰€æœ‰è®¾ç½®ä¸­éƒ½æ¯”ä»¥å‰çš„æœ€ä½³æ–¹æ³•æé«˜äº†è‡³å°‘5%çš„æ€§èƒ½ã€‚
[06.12.2024 09:11] Mistral response. {"id": "baaa2cc60af74e359624e62a2ac752b0", "object": "chat.completion", "created": 1733476301, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "\u6700\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u589e\u52a0\u89c6\u89c9\u6807\u8bb0\u7684\u957f\u5ea6\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u6d41\u884c\u7684\u89c6\u89c9\u7f16\u7801\u5668\u751f\u6210\u7684\u89c6\u89c9\u6807\u8bb0\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86VisionZip\uff0c\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u4e00\u7ec4\u4fe1\u606f\u4e30\u5bcc\u7684\u6807\u8bb0\u8f93\u5165\u8bed\u8a00\u6a21\u578b\uff0c\u51cf\u5c11\u5197\u4f59\u5e76\u63d0\u9ad8\u6548\u7387\u3002VisionZip\u9002\u7528\u4e8e\u56fe\u50cf\u548c\u89c6\u9891\u7406\u89e3\u4efb\u52a1\uff0c\u7279\u522b\u9002\u5408\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cVisionZip\u5728\u51e0\u4e4e\u6240\u6709\u8bbe\u7f6e\u4e2d\u90fd\u6bd4\u4ee5\u524d\u7684\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad8\u4e86\u81f3\u5c115%\u7684\u6027\u80fd\u3002\n\nZu\u00ec x\u012bn de sh\u00ecju\u00e9-y\u01d4y\u00e1n m\u00f3x\u00edng t\u014dnggu\u00f2 z\u0113ngji\u0101 sh\u00ecju\u00e9 bi\u0101oj\u00ec de ch\u00e1ngd\u00f9 t\u00edg\u0101o le x\u00ecngn\u00e9ng, d\u00e0n y\u011b z\u0113ngji\u0101 le j\u00ecsu\u00e0n ch\u00e9ngb\u011bn. W\u01d2men f\u0101xi\u00e0n, li\u00fax\u00edng de sh\u00ecju\u00e9 bi\u0101nm\u01ceq\u00ec sh\u0113ngch\u00e9ng de sh\u00ecju\u00e9 bi\u0101oj\u00ec zh\u014dng c\u00fanz\u00e0i d\u00e0li\u00e0ng r\u01d2ngy\u00fa. W\u00e8i ji\u011bju\u00e9 zh\u00e8g\u00e8 w\u00e8nt\u00ed, w\u01d2men y\u01d0nr\u00f9 le VisionZip, y\u012bzh\u01d2ng ji\u01cend\u0101n y\u01d2uxi\u00e0o de f\u0101ngf\u01ce, xu\u01cenz\u00e9 y\u012b z\u01d4 x\u00ecnx\u012b f\u0113ngf\u00f9 de bi\u0101oj\u00ec sh\u016br\u00f9 y\u01d4y\u00e1n m\u00f3x\u00edng, ji\u01censh\u01ceo r\u01d2ngy\u00fa b\u00ecng t\u00edg\u0101o xi\u00e0ol\u01dc. VisionZip sh\u00ecy\u00f2ng y\u00fa t\u00faxi\u00e0ng h\u00e9 sh\u00ecp\u00edn l\u01d0ji\u011b r\u00e8nw\u00f9, t\u00e8bi\u00e9 sh\u00ech\u00e9 du\u014d l\u00fan du\u00echu\u00e0 ch\u01cengj\u01d0ng. Sh\u00edy\u00e0n ji\u00e9gu\u01d2 xi\u01censh\u00ec, VisionZip z\u00e0i j\u012bhu\u0101 su\u01d2y\u01d2u sh\u00e8zh\u00ec zh\u014dng d\u014du b\u01d0 y\u01d0qi\u00e1n de zu\u00ecji\u0101 f\u0101ngf\u01ce t\u00edg\u0101o le zh\u00ecsh\u01ceo 5% de x\u00ecngn\u00e9ng.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 208, "total_tokens": 793, "completion_tokens": 585}}
[06.12.2024 09:11] Response: æœ€æ–°çš„è§†è§‰-è¯­è¨€æ¨¡å‹é€šè¿‡å¢åŠ è§†è§‰æ ‡è®°çš„é•¿åº¦æé«˜äº†æ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å‘ç°ï¼Œæµè¡Œçš„è§†è§‰ç¼–ç å™¨ç”Ÿæˆçš„è§†è§‰æ ‡è®°ä¸­å­˜åœ¨å¤§é‡å†—ä½™ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VisionZipï¼Œä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€‰æ‹©ä¸€ç»„ä¿¡æ¯ä¸°å¯Œçš„æ ‡è®°è¾“å…¥è¯­è¨€æ¨¡å‹ï¼Œå‡å°‘å†—ä½™å¹¶æé«˜æ•ˆç‡ã€‚VisionZipé€‚ç”¨äºå›¾åƒå’Œè§†é¢‘ç†è§£ä»»åŠ¡ï¼Œç‰¹åˆ«é€‚åˆå¤šè½®å¯¹è¯åœºæ™¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVisionZipåœ¨å‡ ä¹æ‰€æœ‰è®¾ç½®ä¸­éƒ½æ¯”ä»¥å‰çš„æœ€ä½³æ–¹æ³•æé«˜äº†è‡³å°‘5%çš„æ€§èƒ½ã€‚

ZuÃ¬ xÄ«n de shÃ¬juÃ©-yÇ”yÃ¡n mÃ³xÃ­ng tÅngguÃ² zÄ“ngjiÄ shÃ¬juÃ© biÄojÃ¬ de chÃ¡ngdÃ¹ tÃ­gÄo le xÃ¬ngnÃ©ng, dÃ n yÄ› zÄ“ngjiÄ le jÃ¬suÃ n chÃ©ngbÄ›n. WÇ’men fÄxiÃ n, liÃºxÃ­ng de shÃ¬juÃ© biÄnmÇqÃ¬ shÄ“ngchÃ©ng de shÃ¬juÃ© biÄojÃ¬ zhÅng cÃºnzÃ i dÃ liÃ ng rÇ’ngyÃº. WÃ¨i jiÄ›juÃ© zhÃ¨gÃ¨ wÃ¨ntÃ­, wÇ’men yÇnrÃ¹ le VisionZip, yÄ«zhÇ’ng jiÇndÄn yÇ’uxiÃ o de fÄngfÇ, xuÇnzÃ© yÄ« zÇ” xÃ¬nxÄ« fÄ“ngfÃ¹ de biÄojÃ¬ shÅ«rÃ¹ yÇ”yÃ¡n mÃ³xÃ­ng, jiÇnshÇo rÇ’ngyÃº bÃ¬ng tÃ­gÄo xiÃ olÇœ. VisionZip shÃ¬yÃ²ng yÃº tÃºxiÃ ng hÃ© shÃ¬pÃ­n lÇjiÄ› rÃ¨nwÃ¹, tÃ¨biÃ© shÃ¬hÃ© duÅ lÃºn duÃ¬huÃ  chÇngjÇng. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, VisionZip zÃ i jÄ«huÄ suÇ’yÇ’u shÃ¨zhÃ¬ zhÅng dÅu bÇ yÇqiÃ¡n de zuÃ¬jiÄ fÄngfÇ tÃ­gÄo le zhÃ¬shÇo 5% de xÃ¬ngnÃ©ng.
[06.12.2024 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

æœ€æ–°çš„è§†è§‰-è¯­è¨€æ¨¡å‹é€šè¿‡å¢åŠ è§†è§‰æ ‡è®°çš„é•¿åº¦æé«˜äº†æ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å‘ç°ï¼Œæµè¡Œçš„è§†è§‰ç¼–ç å™¨ç”Ÿæˆçš„è§†è§‰æ ‡è®°ä¸­å­˜åœ¨å¤§é‡å†—ä½™ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VisionZipï¼Œä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€‰æ‹©ä¸€ç»„ä¿¡æ¯ä¸°å¯Œçš„æ ‡è®°è¾“å…¥è¯­è¨€æ¨¡å‹ï¼Œå‡å°‘å†—ä½™å¹¶æé«˜æ•ˆç‡ã€‚VisionZipé€‚ç”¨äºå›¾åƒå’Œè§†é¢‘ç†è§£ä»»åŠ¡ï¼Œç‰¹åˆ«é€‚åˆå¤šè½®å¯¹è¯åœºæ™¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVisionZipåœ¨å‡ ä¹æ‰€æœ‰è®¾ç½®ä¸­éƒ½æ¯”ä»¥å‰çš„æœ€ä½³æ–¹æ³•æé«˜äº†è‡³å°‘5%çš„æ€§èƒ½ã€‚
[06.12.2024 09:12] Mistral response. {"id": "401a5964312242959ce947894b276a8b", "object": "chat.completion", "created": 1733476316, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "[\n    {\"word\": \"\u89c6\u89c9\", \"pinyin\": \"sh\u00ecju\u00e9\", \"trans\": \"vision\"},\n    {\"word\": \"\u8bed\u8a00\", \"pinyin\": \"y\u01d4y\u00e1n\", \"trans\": \"language\"},\n    {\"word\": \"\u6a21\u578b\", \"pinyin\": \"m\u00f3x\u00edng\", \"trans\": \"model\"},\n    {\"word\": \"\u589e\u52a0\", \"pinyin\": \"z\u0113ngji\u0101\", \"trans\": \"increase\"},\n    {\"word\": \"\u957f\u5ea6\", \"pinyin\": \"ch\u00e1ngd\u00f9\", \"trans\": \"length\"},\n    {\"word\": \"\u6027\u80fd\", \"pinyin\": \"x\u00ecngn\u00e9ng\", \"trans\": \"performance\"},\n    {\"word\": \"\u8ba1\u7b97\", \"pinyin\": \"j\u00ecsu\u00e0n\", \"trans\": \"computation\"},\n    {\"word\": \"\u6210\u672c\", \"pinyin\": \"ch\u00e9ngb\u011bn\", \"trans\": \"cost\"},\n    {\"word\": \"\u53d1\u73b0\", \"pinyin\": \"f\u0101xi\u00e0n\", \"trans\": \"discover\"},\n    {\"word\": \"\u6d41\u884c\", \"pinyin\": \"li\u00fax\u00edng\", \"trans\": \"popular\"},\n    {\"word\": \"\u7f16\u7801\u5668\", \"pinyin\": \"bi\u0101nm\u01ceq\u00ec\", \"trans\": \"encoder\"},\n    {\"word\": \"\u751f\u6210\", \"pinyin\": \"sh\u0113ngch\u00e9ng\", \"trans\": \"generate\"},\n    {\"word\": \"\u5197\u4f59\", \"pinyin\": \"r\u00f3ngy\u00fa\", \"trans\": \"redundancy\"},\n    {\"word\": \"\u5f15\u5165\", \"pinyin\": \"y\u01d0nr\u00f9\", \"trans\": \"introduce\"},\n    {\"word\": \"\u65b9\u6cd5\", \"pinyin\": \"f\u0101ngf\u01ce\", \"trans\": \"method\"},\n    {\"word\": \"\u9009\u62e9\", \"pinyin\": \"xu\u01cenz\u00e9\", \"trans\": \"select\"},\n    {\"word\": \"\u4e30\u5bcc\", \"pinyin\": \"f\u0113ngf\u00f9\", \"trans\": \"rich\"},\n    {\"word\": \"\u8f93\u5165\", \"pinyin\": \"sh\u016br\u00f9\", \"trans\": \"input\"},\n    {\"word\": \"\u6548\u7387\", \"pinyin\": \"xi\u00e0ol\u01dc\", \"trans\": \"efficiency\"},\n    {\"word\": \"\u56fe\u50cf\", \"pinyin\": \"t\u00faxi\u00e0ng\", \"trans\": \"image\"},\n    {\"word\": \"\u89c6\u9891\", \"pinyin\": \"sh\u00ecp\u00edn\", \"trans\": \"video\"},\n    {\"word\": \"\u7406\u89e3\", \"pinyin\": \"l\u01d0ji\u011b\", \"trans\": \"understanding\"},\n    {\"word\": \"\u4efb\u52a1\", \"pinyin\": \"r\u00e8nw\u00f9\", \"trans\": \"task\"},\n    {\"word\": \"\u7279\u522b\", \"pinyin\": \"t\u00e8bi\u00e9\", \"trans\": \"especially\"},\n    {\"word\": \"\u9002\u5408\", \"pinyin\": \"sh\u00ech\u00e9\", \"trans\": \"suitable\"},\n    {\"word\": \"\u591a\u8f6e\", \"pinyin\": \"du\u014dl\u00fan\", \"trans\": \"multi-turn\"},\n    {\"word\": \"\u5bf9\u8bdd\", \"pinyin\": \"du\u00echu\u00e0\", \"trans\": \"dialogue\"},\n    {\"word\": \"\u573a\u666f\", \"pinyin\": \"ch\u01cengj\u01d0ng\", \"trans\": \"scenario\"},\n    {\"word\": \"\u5b9e\u9a8c\", \"pinyin\": \"sh\u00edy\u00e0n\", \"trans\": \"experiment\"},\n    {\"word\": \"\u7ed3\u679c\", \"pinyin\": \"ji\u00e9gu\u01d2\", \"trans\": \"result\"},\n    {\"word\": \"\u663e\u793a\", \"pinyin\": \"xi\u01censh\u00ec\", \"trans\": \"show\"},\n    {\"word\": \"\u51e0\u4e4e\", \"pinyin\": \"j\u012bh\u016b\", \"trans\": \"almost\"},\n    {\"word\": \"\u8bbe\u7f6e\", \"pinyin\": \"sh\u00e8zh\u00ec\", \"trans\": \"setting\"},\n    {\"word\": \"\u6bd4\", \"pinyin\": \"b\u01d0\", \"trans\": \"compare\"},\n    {\"word\": \"\u4ee5\u524d\", \"pinyin\": \"y\u01d0qi\u00e1n\", \"trans\": \"before\"},\n    {\"word\": \"\u6700\u4f73\", \"pinyin\": \"zu\u00ecji\u0101\", \"trans\": \"best\"},\n    {\"word\": \"\u63d0\u9ad8\", \"pinyin\": \"t\u00edg\u0101o\", \"trans\": \"improve\"}\n]", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 240, "total_tokens": 1250, "completion_tokens": 1010}}
[06.12.2024 09:12] Response: [
    {"word": "è§†è§‰", "pinyin": "shÃ¬juÃ©", "trans": "vision"},
    {"word": "è¯­è¨€", "pinyin": "yÇ”yÃ¡n", "trans": "language"},
    {"word": "æ¨¡å‹", "pinyin": "mÃ³xÃ­ng", "trans": "model"},
    {"word": "å¢åŠ ", "pinyin": "zÄ“ngjiÄ", "trans": "increase"},
    {"word": "é•¿åº¦", "pinyin": "chÃ¡ngdÃ¹", "trans": "length"},
    {"word": "æ€§èƒ½", "pinyin": "xÃ¬ngnÃ©ng", "trans": "performance"},
    {"word": "è®¡ç®—", "pinyin": "jÃ¬suÃ n", "trans": "computation"},
    {"word": "æˆæœ¬", "pinyin": "chÃ©ngbÄ›n", "trans": "cost"},
    {"word": "å‘ç°", "pinyin": "fÄxiÃ n", "trans": "discover"},
    {"word": "æµè¡Œ", "pinyin": "liÃºxÃ­ng", "trans": "popular"},
    {"word": "ç¼–ç å™¨", "pinyin": "biÄnmÇqÃ¬", "trans": "encoder"},
    {"word": "ç”Ÿæˆ", "pinyin": "shÄ“ngchÃ©ng", "trans": "generate"},
    {"word": "å†—ä½™", "pinyin": "rÃ³ngyÃº", "trans": "redundancy"},
    {"word": "å¼•å…¥", "pinyin": "yÇnrÃ¹", "trans": "introduce"},
    {"word": "æ–¹æ³•", "pinyin": "fÄngfÇ", "trans": "method"},
    {"word": "é€‰æ‹©", "pinyin": "xuÇnzÃ©", "trans": "select"},
    {"word": "ä¸°å¯Œ", "pinyin": "fÄ“ngfÃ¹", "trans": "rich"},
    {"word": "è¾“å…¥", "pinyin": "shÅ«rÃ¹", "trans": "input"},
    {"word": "æ•ˆç‡", "pinyin": "xiÃ olÇœ", "trans": "efficiency"},
    {"word": "å›¾åƒ", "pinyin": "tÃºxiÃ ng", "trans": "image"},
    {"word": "è§†é¢‘", "pinyin": "shÃ¬pÃ­n", "trans": "video"},
    {"word": "ç†è§£", "pinyin": "lÇjiÄ›", "trans": "understanding"},
    {"word": "ä»»åŠ¡", "pinyin": "rÃ¨nwÃ¹", "trans": "task"},
    {"word": "ç‰¹åˆ«", "pinyin": "tÃ¨biÃ©", "trans": "especially"},
    {"word": "é€‚åˆ", "pinyin": "shÃ¬hÃ©", "trans": "suitable"},
    {"word": "å¤šè½®", "pinyin": "duÅlÃºn", "trans": "multi-turn"},
    {"word": "å¯¹è¯", "pinyin": "duÃ¬huÃ ", "trans": "dialogue"},
    {"word": "åœºæ™¯", "pinyin": "chÇngjÇng", "trans": "scenario"},
    {"word": "å®éªŒ", "pinyin": "shÃ­yÃ n", "trans": "experiment"},
    {"word": "ç»“æœ", "pinyin": "jiÃ©guÇ’", "trans": "result"},
    {"word": "æ˜¾ç¤º", "pinyin": "xiÇnshÃ¬", "trans": "show"},
    {"word": "å‡ ä¹", "pinyin": "jÄ«hÅ«", "trans": "almost"},
    {"word": "è®¾ç½®", "pinyin": "shÃ¨zhÃ¬", "trans": "setting"},
    {"word": "æ¯”", "pinyin": "bÇ", "trans": "compare"},
    {"word": "ä»¥å‰", "pinyin": "yÇqiÃ¡n", "trans": "before"},
    {"word": "æœ€ä½³", "pinyin": "zuÃ¬jiÄ", "trans": "best"},
    {"word": "æé«˜", "pinyin": "tÃ­gÄo", "trans": "improve"}
]
[06.12.2024 09:12] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

æœ€æ–°çš„è§†è§‰-è¯­è¨€æ¨¡å‹é€šè¿‡å¢åŠ è§†è§‰æ ‡è®°çš„é•¿åº¦æé«˜äº†æ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å‘ç°ï¼Œæµè¡Œçš„è§†è§‰ç¼–ç å™¨ç”Ÿæˆçš„è§†è§‰æ ‡è®°ä¸­å­˜åœ¨å¤§é‡å†—ä½™ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VisionZipï¼Œä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€‰æ‹©ä¸€ç»„ä¿¡æ¯ä¸°å¯Œçš„æ ‡è®°è¾“å…¥è¯­è¨€æ¨¡å‹ï¼Œå‡å°‘å†—ä½™å¹¶æé«˜æ•ˆç‡ã€‚VisionZipé€‚ç”¨äºå›¾åƒå’Œè§†é¢‘ç†è§£ä»»åŠ¡ï¼Œç‰¹åˆ«é€‚åˆå¤šè½®å¯¹è¯åœºæ™¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVisionZipåœ¨å‡ ä¹æ‰€æœ‰è®¾ç½®ä¸­éƒ½æ¯”ä»¥å‰çš„æœ€ä½³æ–¹æ³•æé«˜äº†è‡³å°‘5%çš„æ€§èƒ½ã€‚
[06.12.2024 09:12] Mistral response. {"id": "5c929aa33c1b48549d9d5dad72e3edff", "object": "chat.completion", "created": 1733476339, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "The latest vision-language models have improved performance by increasing the length of visual tokens but have also increased computational costs. We found that popular visual encoders generate visual tokens with a significant amount of redundancy. To address this issue, we introduce VisionZip, a simple and effective method that selects a set of information-rich tokens to input into the language model, reducing redundancy and improving efficiency. VisionZip is applicable to image and video understanding tasks and is particularly suitable for multi-turn conversation scenarios. Experimental results show that VisionZip improves performance by at least 5% over previous best methods in almost all settings.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 206, "total_tokens": 339, "completion_tokens": 133}}
[06.12.2024 09:12] Response: The latest vision-language models have improved performance by increasing the length of visual tokens but have also increased computational costs. We found that popular visual encoders generate visual tokens with a significant amount of redundancy. To address this issue, we introduce VisionZip, a simple and effective method that selects a set of information-rich tokens to input into the language model, reducing redundancy and improving efficiency. VisionZip is applicable to image and video understanding tasks and is particularly suitable for multi-turn conversation scenarios. Experimental results show that VisionZip improves performance by at least 5% over previous best methods in almost all settings.
[06.12.2024 09:12] Renaming data file.
[06.12.2024 09:12] Renaming previous data. hf_papers.json to ./d/2024-12-06.json
[06.12.2024 09:12] Saving new data file.
[06.12.2024 09:12] Generating page.
[06.12.2024 09:12] Renaming previous page.
[06.12.2024 09:12] Renaming previous data. index.html to ./d/2024-12-06.html
[06.12.2024 09:12] [Experimental] Generating Chinese page for reading.
[06.12.2024 09:12] Chinese vocab [{'word': 'è§†è§‰', 'pinyin': 'shÃ¬juÃ©', 'trans': 'vision'}, {'word': 'è¯­è¨€', 'pinyin': 'yÇ”yÃ¡n', 'trans': 'language'}, {'word': 'æ¨¡å‹', 'pinyin': 'mÃ³xÃ­ng', 'trans': 'model'}, {'word': 'å¢åŠ ', 'pinyin': 'zÄ“ngjiÄ', 'trans': 'increase'}, {'word': 'é•¿åº¦', 'pinyin': 'chÃ¡ngdÃ¹', 'trans': 'length'}, {'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ngnÃ©ng', 'trans': 'performance'}, {'word': 'è®¡ç®—', 'pinyin': 'jÃ¬suÃ n', 'trans': 'computation'}, {'word': 'æˆæœ¬', 'pinyin': 'chÃ©ngbÄ›n', 'trans': 'cost'}, {'word': 'å‘ç°', 'pinyin': 'fÄxiÃ n', 'trans': 'discover'}, {'word': 'æµè¡Œ', 'pinyin': 'liÃºxÃ­ng', 'trans': 'popular'}, {'word': 'ç¼–ç å™¨', 'pinyin': 'biÄnmÇqÃ¬', 'trans': 'encoder'}, {'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ngchÃ©ng', 'trans': 'generate'}, {'word': 'å†—ä½™', 'pinyin': 'rÃ³ngyÃº', 'trans': 'redundancy'}, {'word': 'å¼•å…¥', 'pinyin': 'yÇnrÃ¹', 'trans': 'introduce'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄngfÇ', 'trans': 'method'}, {'word': 'é€‰æ‹©', 'pinyin': 'xuÇnzÃ©', 'trans': 'select'}, {'word': 'ä¸°å¯Œ', 'pinyin': 'fÄ“ngfÃ¹', 'trans': 'rich'}, {'word': 'è¾“å…¥', 'pinyin': 'shÅ«rÃ¹', 'trans': 'input'}, {'word': 'æ•ˆç‡', 'pinyin': 'xiÃ olÇœ', 'trans': 'efficiency'}, {'word': 'å›¾åƒ', 'pinyin': 'tÃºxiÃ ng', 'trans': 'image'}, {'word': 'è§†é¢‘', 'pinyin': 'shÃ¬pÃ­n', 'trans': 'video'}, {'word': 'ç†è§£', 'pinyin': 'lÇjiÄ›', 'trans': 'understanding'}, {'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨nwÃ¹', 'trans': 'task'}, {'word': 'ç‰¹åˆ«', 'pinyin': 'tÃ¨biÃ©', 'trans': 'especially'}, {'word': 'é€‚åˆ', 'pinyin': 'shÃ¬hÃ©', 'trans': 'suitable'}, {'word': 'å¤šè½®', 'pinyin': 'duÅlÃºn', 'trans': 'multi-turn'}, {'word': 'å¯¹è¯', 'pinyin': 'duÃ¬huÃ ', 'trans': 'dialogue'}, {'word': 'åœºæ™¯', 'pinyin': 'chÇngjÇng', 'trans': 'scenario'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­yÃ n', 'trans': 'experiment'}, {'word': 'ç»“æœ', 'pinyin': 'jiÃ©guÇ’', 'trans': 'result'}, {'word': 'æ˜¾ç¤º', 'pinyin': 'xiÇnshÃ¬', 'trans': 'show'}, {'word': 'å‡ ä¹', 'pinyin': 'jÄ«hÅ«', 'trans': 'almost'}, {'word': 'è®¾ç½®', 'pinyin': 'shÃ¨zhÃ¬', 'trans': 'setting'}, {'word': 'æ¯”', 'pinyin': 'bÇ', 'trans': 'compare'}, {'word': 'ä»¥å‰', 'pinyin': 'yÇqiÃ¡n', 'trans': 'before'}, {'word': 'æœ€ä½³', 'pinyin': 'zuÃ¬jiÄ', 'trans': 'best'}, {'word': 'æé«˜', 'pinyin': 'tÃ­gÄo', 'trans': 'improve'}]
[06.12.2024 09:12] Renaming previous Chinese page.
[06.12.2024 09:12] Renaming previous data. zh.html to ./d/2024-12-05_zh_reading_task.html
[06.12.2024 09:12] Writing Chinese reading task.
[06.12.2024 09:12] Writing result.
[06.12.2024 09:12] Renaming log file.
[06.12.2024 09:12] Renaming previous data. log.txt to ./logs/2024-12-06_last_log.txt
