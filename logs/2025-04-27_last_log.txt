[27.04.2025 06:31] Read previous papers.
[27.04.2025 06:31] Generating top page (month).
[27.04.2025 06:31] Writing top page (month).
[27.04.2025 12:42] Read previous papers.
[27.04.2025 12:42] Get feed.
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17192
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17761
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17502
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17432
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17207
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17670
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16511
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17789
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16828
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17040
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16064
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17414
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17343
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16921
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15921
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17069
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17788
[27.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17601
[27.04.2025 12:42] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.04.2025 12:42] No deleted papers detected.
[27.04.2025 12:42] Downloading and parsing papers (pdf, html). Total: 18.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17192.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17192.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17192.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17761.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17761.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17761.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17502.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17502.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17502.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17432.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17432.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17432.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17207.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17207.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17207.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17670.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17670.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17670.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.16511.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.16511.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.16511.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17789.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17789.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17789.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.16828.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.16828.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.16828.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17040.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17040.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17040.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.16064.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.16064.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.16064.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17414.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17414.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17414.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17343.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17343.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17343.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.16921.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.16921.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.16921.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.15921.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.15921.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.15921.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17069.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17069.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17069.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17788.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17788.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17788.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17601.
[27.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17601.json), skip PDF parsing.
[27.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17601.json), skip HTML parsing.
[27.04.2025 12:42] Success.
[27.04.2025 12:42] Enriching papers with extra data.
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 0. Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific d...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 1. In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a ...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 2. Subject-driven text-to-image (T2I) generation aims to produce images that align with a given textual description, while preserving the visual identity from a referenced subject image. Despite its broad downstream applicability -- ranging from enhanced personalization in image generation to consisten...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 3. The Contrastive Language-Image Pre-training (CLIP) framework has become a widely used approach for multimodal representation learning, particularly in image-text retrieval and clustering. However, its efficacy is constrained by three key limitations: (1) text token truncation, (2) isolated image-tex...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 4. We present a framework for perspective-aware reasoning in vision-language models (VLMs) through mental imagery simulation. Perspective-taking, the ability to perceive an environment or situation from an alternative viewpoint, is a key benchmark for human-level visual understanding, essential for env...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 5. With the advent of large-scale 3D datasets, feed-forward 3D generative models, such as the Large Reconstruction Model (LRM), have gained significant attention and achieved remarkable success. However, we observe that RGB images often lead to conflicting training objectives and lack the necessary cla...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 6. Quality and diversity are two critical metrics for the training data of large language models (LLMs), positively impacting performance. Existing studies often optimize these metrics separately, typically by first applying quality filtering and then adjusting data proportions. However, these approach...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 7. Autoregressive (AR) models, long dominant in language generation, are increasingly applied to image synthesis but are often considered less competitive than Diffusion-based models. A primary limitation is the substantial number of image tokens required for AR models, which constrains both training a...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 8. Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the sol...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 9. We present DyMU, an efficient, training-free framework that dynamically reduces the computational burden of vision-language models (VLMs) while maintaining high task performance. Our approach comprises two key components. First, Dynamic Token Merging (DToMe) reduces the number of visual token embedd...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 10. Latent diffusion models (LDMs) dominate high-quality image generation, yet integrating representation learning with generative modeling remains a challenge. We introduce a novel generative image modeling framework that seamlessly bridges this gap by leveraging a diffusion model to jointly model low-...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 11. Video try-on replaces clothing in videos with target garments. Existing methods struggle to generate high-quality and temporally consistent results when handling complex clothing patterns and diverse body poses. We present 3DV-TON, a novel diffusion-based framework for generating high-fidelity and t...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 12. The rapid growth of online video platforms, particularly live streaming services, has created an urgent need for real-time video understanding systems. These systems must process continuous video streams and respond to user queries instantaneously, presenting unique challenges for current Video Larg...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 13. Large Language Models (LLMs) remain difficult to evaluate comprehensively, particularly for languages other than English, where high-quality data is often limited. Existing benchmarks and leaderboards are predominantly English-centric, with only a few addressing other languages. These benchmarks fal...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 14. We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a system to summarise hour long videos with no-supervision. Most existing video understanding models work well on short videos of pre-segmented events, yet they struggle to summarise longer videos where relevant events are spar...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 15. Autoregressive patch-based image generation has recently shown competitive results in terms of image quality and scalability. It can also be easily integrated and scaled within Vision-Language models. Nevertheless, autoregressive models require a defined order for patch generation. While a natural o...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 16. Annotating camera poses on dynamic Internet videos at scale is critical for advancing fields like realistic video generation and simulation. However, collecting such a dataset is difficult, as most Internet videos are unsuitable for pose estimation. Furthermore, annotating dynamic Internet videos pr...
[27.04.2025 12:42] ********************************************************************************
[27.04.2025 12:42] Abstract 17. Dimensionality reduction techniques are fundamental for analyzing and visualizing high-dimensional data. With established methods like t-SNE and PCA presenting a trade-off between representational power and interpretability. This paper introduces a novel approach that bridges this gap by combining t...
[27.04.2025 12:42] Read previous papers.
[27.04.2025 12:42] Generating reviews via LLM API.
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#agents", "#architecture", "#benchmark", "#dataset", "#open_source", "#science"], "emoji": "ğŸ¤–", "ru": {"title": "ĞÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğº ĞºĞ¾Ğ´Ñƒ: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ", "desc": "PaperCoder - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#data", "#diffusion", "#training", "#multimodal", "#cv", "#dataset", "#open_source", "#benchmark"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "Step1X-Edit: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#dataset", "#multimodal"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "RefVNLI: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#benchmark", "#transfer_learning", "#optimization", "#multimodal"], "emoji": "ğŸ”", "ru": {"title": "UniME: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ", "desc": "UniME - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#cv", "#benchmark", "#agents"], "emoji": "ğŸ”„", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğµ
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#3d", "#optimization", "#dataset"], "emoji": "ğŸ§Š", "ru": {"title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DiMeR - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#benchmark", "#training", "#data", "#optimization"], "emoji": "ğŸ”„", "ru": {"title": "Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ QuaDMix - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). QuaDMi
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#architecture", "#training", "#diffusion", "#cv", "#benchmark", "#optimization", "#multimodal"], "emoji": "ğŸ”€", "ru": {"title": "Token-Shuffle: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Token-Shuffle Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğµ
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#optimization", "#long_context", "#data", "#reasoning", "#training", "#benchmark"], "emoji": "ğŸ§ ", "ru": {"title": "ThinkPRM: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ThinkPRM - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (P
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#cv", "#optimization", "#inference", "#multimodal"], "emoji": "ğŸ”¬", "ru": {"title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… VLM Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ", "desc": "DyMU - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğµ
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#cv", "#inference", "#optimization", "#training", "#diffusion", "#architecture"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#video", "#3d", "#leakage", "#diffusion", "#dataset", "#benchmark"], "emoji": "ğŸ‘š", "ru": {"title": "3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "3DV-TON - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞµ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#video", "#long_context", "#dataset", "#games", "#benchmark"], "emoji": "ğŸ¥", "ru": {"title": "TimeChat-Online: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "TimeChat-Online - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VideoLLM Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ - Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#open_source", "#multilingual", "#dataset", "#low_resource", "#benchmark"], "emoji": "ğŸŒ", "ru": {"title": "IberBench: Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° LLM Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾", "desc": "IberBench - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#video", "#multimodal", "#dataset", "#optimization", "#transfer_learning"], "emoji": "ğŸ¥", "ru": {"title": "ViSMap: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑÑƒĞ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ", "desc": "ViSMap - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ½ĞµÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ĞµÑ‚Ğ°Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#optimization", "#cv", "#training"], "emoji": "ğŸ§©", "ru": {"title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ¸Ğ¼ĞµĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ°Ñ‚Ñ‡Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸ Ğ² Ğ¿Ñ€
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#dataset", "#data", "#cv"], "emoji": "ğŸ¥", "ru": {"title": "DynPose-100K: ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "DynPose-100K - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ° Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸
[27.04.2025 12:42] Using data from previous issue: {"categories": ["#optimization", "#data", "#interpretability", "#architecture"], "emoji": "ğŸ”¬", "ru": {"title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ…
[27.04.2025 12:42] Trying to get texts in Chinese.
[27.04.2025 12:42] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, specifically from the original paper authors, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins.
[27.04.2025 12:42] Mistral response. {"id": "a89e72c25a0d471fad50dd1b9454ba30", "object": "chat.completion", "created": 1745757754, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u5c3d\u7ba1\u673a\u5668\u5b66\u4e60\u7814\u7a76\u8fc5\u901f\u53d1\u5c55\uff0c\u4f46\u76f8\u5e94\u7684\u4ee3\u7801\u5b9e\u73b0\u5e38\u5e38\u7f3a\u5931\uff0c\u4f7f\u5f97\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u590d\u73b0\u7ed3\u679c\u5e76\u5728\u524d\u4eba\u5de5\u4f5c\u57fa\u7840\u4e0a\u6784\u5efa\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u6700\u65b0\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u64c5\u957f\u7406\u89e3\u79d1\u5b66\u6587\u6863\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u4ee3\u7801\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u5f15\u5165\u4e86PaperCoder\uff0c\u4e00\u4e2a\u5c06\u673a\u5668\u5b66\u4e60\u8bba\u6587\u8f6c\u5316\u4e3a\u529f\u80fd\u4ee3\u7801\u4ed3\u5e93\u7684\u591a\u4ee3\u7406LLM\u6846\u67b6\u3002PaperCoder\u5206\u4e09\u4e2a\u9636\u6bb5\u8fd0\u884c\uff1a\u89c4\u5212\u3001\u5206\u6790\u548c\u751f\u6210\u3002\u6bcf\u4e2a\u9636\u6bb5\u7531\u4e00\u7ec4\u4e13\u95e8\u8bbe\u8ba1\u7684\u4ee3\u7406\u5b9e\u73b0\uff0c\u4ee5\u786e\u4fdd\u9ad8\u6548\u534f\u4f5c\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8bc1\u660ePaperCoder\u80fd\u591f\u521b\u5efa\u9ad8\u8d28\u91cf\u3001\u5fe0\u5b9e\u7684\u5b9e\u73b0\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 305, "total_tokens": 508, "completion_tokens": 203}}
[27.04.2025 12:42] Response: å°½ç®¡æœºå™¨å­¦ä¹ ç ”ç©¶è¿…é€Ÿå‘å±•ï¼Œä½†ç›¸åº”çš„ä»£ç å®ç°å¸¸å¸¸ç¼ºå¤±ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜éš¾ä»¥å¤ç°ç»“æœå¹¶åœ¨å‰äººå·¥ä½œåŸºç¡€ä¸Šæ„å»ºã€‚ä¸æ­¤åŒæ—¶ï¼Œæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ“…é•¿ç†è§£ç§‘å­¦æ–‡æ¡£å¹¶ç”Ÿæˆé«˜è´¨é‡ä»£ç ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†PaperCoderï¼Œä¸€ä¸ªå°†æœºå™¨å­¦ä¹ è®ºæ–‡è½¬åŒ–ä¸ºåŠŸèƒ½ä»£ç ä»“åº“çš„å¤šä»£ç†LLMæ¡†æ¶ã€‚PaperCoderåˆ†ä¸‰ä¸ªé˜¶æ®µè¿è¡Œï¼šè§„åˆ’ã€åˆ†æå’Œç”Ÿæˆã€‚æ¯ä¸ªé˜¶æ®µç”±ä¸€ç»„ä¸“é—¨è®¾è®¡çš„ä»£ç†å®ç°ï¼Œä»¥ç¡®ä¿é«˜æ•ˆåä½œã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜PaperCoderèƒ½å¤Ÿåˆ›å»ºé«˜è´¨é‡ã€å¿ å®çš„å®ç°ã€‚
[27.04.2025 12:42] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

å°½ç®¡æœºå™¨å­¦ä¹ ç ”ç©¶è¿…é€Ÿå‘å±•ï¼Œä½†ç›¸åº”çš„ä»£ç å®ç°å¸¸å¸¸ç¼ºå¤±ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜éš¾ä»¥å¤ç°ç»“æœå¹¶åœ¨å‰äººå·¥ä½œåŸºç¡€ä¸Šæ„å»ºã€‚ä¸æ­¤åŒæ—¶ï¼Œæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ“…é•¿ç†è§£ç§‘å­¦æ–‡æ¡£å¹¶ç”Ÿæˆé«˜è´¨é‡ä»£ç ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†PaperCoderï¼Œä¸€ä¸ªå°†æœºå™¨å­¦ä¹ è®ºæ–‡è½¬åŒ–ä¸ºåŠŸèƒ½ä»£ç ä»“åº“çš„å¤šä»£ç†LLMæ¡†æ¶ã€‚PaperCoderåˆ†ä¸‰ä¸ªé˜¶æ®µè¿è¡Œï¼šè§„åˆ’ã€åˆ†æå’Œç”Ÿæˆã€‚æ¯ä¸ªé˜¶æ®µç”±ä¸€ç»„ä¸“é—¨è®¾è®¡çš„ä»£ç†å®ç°ï¼Œä»¥ç¡®ä¿é«˜æ•ˆåä½œã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜PaperCoderèƒ½å¤Ÿåˆ›å»ºé«˜è´¨é‡ã€å¿ å®çš„å®ç°ã€‚
[27.04.2025 12:42] Mistral response. {"id": "969006e8dfa84eb0a92803bad817f7a6", "object": "chat.completion", "created": 1745757757, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "J\u01d0ngu\u01cen j\u012bqi xu\u00e9x\u00ed y\u00e1nji\u016b x\u00f9ns\u00f9 f\u0101zh\u01cen, d\u00e0n xi\u00e0ngy\u00ecng de d\u00e0im\u01ce sh\u00edxi\u00e0n ch\u00e1ngch\u00e1ng qu\u0113sh\u012b, sh\u01d0 de y\u00e1nji\u016b r\u00e9nyu\u00e1n n\u00e1n y\u01d0 f\u00f9xi\u00e0n ji\u00e9gu\u01d2 b\u00ecng z\u00e0i qi\u00e1nr\u00e9n g\u014dngzu\u00f2 j\u012bch\u01d4 sh\u00e0ng g\u00f2uji\u00e0n. Y\u01d4 c\u01d0 t\u00f3ngsh\u00ed, zu\u00ecx\u012bn de d\u00e0x\u00edng y\u01d4y\u00e1n m\u00f3x\u00edng (LLMs) sh\u00e0nch\u00e1ng l\u01d0ji\u011b k\u0113xu\u00e9 w\u00e9nji\u00e0n b\u00ecng sh\u0113ngch\u00e9ng g\u0101o zh\u00ecli\u00e0ng d\u00e0im\u01ce. Sh\u00f2u c\u01d0 q\u01d0f\u00e0, w\u01d2men y\u01d0n r\u00f9 le PaperCoder, y\u012bg\u00e8 ji\u0101ng j\u012bqi xu\u00e9x\u00ed l\u00f9nw\u00e9n zhu\u01cenhu\u00e0 w\u00e9i g\u014dngn\u00e9ng d\u00e0im\u01ce c\u00e1ngk\u00f9 de du\u014d d\u00e0il\u01d0 LLM ku\u00e0ngji\u00e0. PaperCoder f\u0113n s\u0101n g\u00e8 ji\u0113du\u00e0n y\u00f9nx\u00edng: gu\u012bhu\u00e0, f\u0113nxi h\u00e9 sh\u0113ngch\u00e9ng. M\u011bi g\u00e8 ji\u0113du\u00e0n y\u00f3u y\u012b z\u01d4 zhu\u0101nm\u00e9n sh\u00e8j\u00ec de d\u00e0il\u01d0 sh\u00edxi\u00e0n, y\u01d0 qu\u00e8b\u01ceo g\u0101oxi\u00e0o xi\u00e9zu\u00f2. W\u01d2men de ji\u00e9gu\u01d2 zh\u00e8ngm\u00edng PaperCoder n\u00e9ngg\u00f2u chu\u00e0ngji\u00e0n g\u0101o zh\u00ecli\u00e0ng, zh\u014dngsh\u00ed de sh\u00edxi\u00e0n."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 217, "total_tokens": 638, "completion_tokens": 421}}
[27.04.2025 12:42] Response: JÇnguÇn jÄ«qi xuÃ©xÃ­ yÃ¡njiÅ« xÃ¹nsÃ¹ fÄzhÇn, dÃ n xiÃ ngyÃ¬ng de dÃ imÇ shÃ­xiÃ n chÃ¡ngchÃ¡ng quÄ“shÄ«, shÇ de yÃ¡njiÅ« rÃ©nyuÃ¡n nÃ¡n yÇ fÃ¹xiÃ n jiÃ©guÇ’ bÃ¬ng zÃ i qiÃ¡nrÃ©n gÅngzuÃ² jÄ«chÇ” shÃ ng gÃ²ujiÃ n. YÇ” cÇ tÃ³ngshÃ­, zuÃ¬xÄ«n de dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) shÃ nchÃ¡ng lÇjiÄ› kÄ“xuÃ© wÃ©njiÃ n bÃ¬ng shÄ“ngchÃ©ng gÄo zhÃ¬liÃ ng dÃ imÇ. ShÃ²u cÇ qÇfÃ , wÇ’men yÇn rÃ¹ le PaperCoder, yÄ«gÃ¨ jiÄng jÄ«qi xuÃ©xÃ­ lÃ¹nwÃ©n zhuÇnhuÃ  wÃ©i gÅngnÃ©ng dÃ imÇ cÃ¡ngkÃ¹ de duÅ dÃ ilÇ LLM kuÃ ngjiÃ . PaperCoder fÄ“n sÄn gÃ¨ jiÄ“duÃ n yÃ¹nxÃ­ng: guÄ«huÃ , fÄ“nxi hÃ© shÄ“ngchÃ©ng. MÄ›i gÃ¨ jiÄ“duÃ n yÃ³u yÄ« zÇ” zhuÄnmÃ©n shÃ¨jÃ¬ de dÃ ilÇ shÃ­xiÃ n, yÇ quÃ¨bÇo gÄoxiÃ o xiÃ©zuÃ². WÇ’men de jiÃ©guÇ’ zhÃ¨ngmÃ­ng PaperCoder nÃ©nggÃ²u chuÃ ngjiÃ n gÄo zhÃ¬liÃ ng, zhÅngshÃ­ de shÃ­xiÃ n.
[27.04.2025 12:42] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

å°½ç®¡æœºå™¨å­¦ä¹ ç ”ç©¶è¿…é€Ÿå‘å±•ï¼Œä½†ç›¸åº”çš„ä»£ç å®ç°å¸¸å¸¸ç¼ºå¤±ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜éš¾ä»¥å¤ç°ç»“æœå¹¶åœ¨å‰äººå·¥ä½œåŸºç¡€ä¸Šæ„å»ºã€‚ä¸æ­¤åŒæ—¶ï¼Œæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ“…é•¿ç†è§£ç§‘å­¦æ–‡æ¡£å¹¶ç”Ÿæˆé«˜è´¨é‡ä»£ç ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†PaperCoderï¼Œä¸€ä¸ªå°†æœºå™¨å­¦ä¹ è®ºæ–‡è½¬åŒ–ä¸ºåŠŸèƒ½ä»£ç ä»“åº“çš„å¤šä»£ç†LLMæ¡†æ¶ã€‚PaperCoderåˆ†ä¸‰ä¸ªé˜¶æ®µè¿è¡Œï¼šè§„åˆ’ã€åˆ†æå’Œç”Ÿæˆã€‚æ¯ä¸ªé˜¶æ®µç”±ä¸€ç»„ä¸“é—¨è®¾è®¡çš„ä»£ç†å®ç°ï¼Œä»¥ç¡®ä¿é«˜æ•ˆåä½œã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜PaperCoderèƒ½å¤Ÿåˆ›å»ºé«˜è´¨é‡ã€å¿ å®çš„å®ç°ã€‚
[27.04.2025 12:42] Mistral response. {"id": "c4f936176d9a4da39b2b84300b5013f3", "object": "chat.completion", "created": 1745757766, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u5c3d\u7ba1\", \"pinyin\": \"j\u01d0n gu\u01cen\", \"trans\": \"although\"},\n    {\"word\": \"\u8fc5\u901f\", \"pinyin\": \"x\u00f9n s\u00f9\", \"trans\": \"rapidly\"},\n    {\"word\": \"\u7f3a\u5931\", \"pinyin\": \"qu\u0113 sh\u012b\", \"trans\": \"lack\"},\n    {\"word\": \"\u590d\u73b0\", \"pinyin\": \"f\u00f9 xi\u00e0n\", \"trans\": \"reproduce\"},\n    {\"word\": \"\u64c5\u957f\", \"pinyin\": \"sh\u00e0n ch\u00e1ng\", \"trans\": \"be good at\"},\n    {\"word\": \"\u542f\u53d1\", \"pinyin\": \"q\u01d0 f\u0101\", \"trans\": \"inspire\"},\n    {\"word\": \"\u5f15\u5165\", \"pinyin\": \"y\u01d0n r\u00f9\", \"trans\": \"introduce\"},\n    {\"word\": \"\u8f6c\u5316\", \"pinyin\": \"zhu\u01cen hu\u00e0\", \"trans\": \"transform\"},\n    {\"word\": \"\u529f\u80fd\", \"pinyin\": \"g\u014dng n\u00e9ng\", \"trans\": \"functional\"},\n    {\"word\": \"\u4ee3\u7406\", \"pinyin\": \"d\u00e0i l\u01d0\", \"trans\": \"agent\"},\n    {\"word\": \"\u786e\u4fdd\", \"pinyin\": \"qu\u00e8 b\u01ceo\", \"trans\": \"ensure\"},\n    {\"word\": \"\u534f\u4f5c\", \"pinyin\": \"xi\u00e9 zu\u00f2\", \"trans\": \"collaborate\"},\n    {\"word\": \"\u5fe0\u5b9e\", \"pinyin\": \"zh\u014dng sh\u00ed\", \"trans\": \"faithful\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 249, "total_tokens": 626, "completion_tokens": 377}}
[27.04.2025 12:42] Response: [
    {"word": "å°½ç®¡", "pinyin": "jÇn guÇn", "trans": "although"},
    {"word": "è¿…é€Ÿ", "pinyin": "xÃ¹n sÃ¹", "trans": "rapidly"},
    {"word": "ç¼ºå¤±", "pinyin": "quÄ“ shÄ«", "trans": "lack"},
    {"word": "å¤ç°", "pinyin": "fÃ¹ xiÃ n", "trans": "reproduce"},
    {"word": "æ“…é•¿", "pinyin": "shÃ n chÃ¡ng", "trans": "be good at"},
    {"word": "å¯å‘", "pinyin": "qÇ fÄ", "trans": "inspire"},
    {"word": "å¼•å…¥", "pinyin": "yÇn rÃ¹", "trans": "introduce"},
    {"word": "è½¬åŒ–", "pinyin": "zhuÇn huÃ ", "trans": "transform"},
    {"word": "åŠŸèƒ½", "pinyin": "gÅng nÃ©ng", "trans": "functional"},
    {"word": "ä»£ç†", "pinyin": "dÃ i lÇ", "trans": "agent"},
    {"word": "ç¡®ä¿", "pinyin": "quÃ¨ bÇo", "trans": "ensure"},
    {"word": "åä½œ", "pinyin": "xiÃ© zuÃ²", "trans": "collaborate"},
    {"word": "å¿ å®", "pinyin": "zhÅng shÃ­", "trans": "faithful"}
]
[27.04.2025 12:42] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

å°½ç®¡æœºå™¨å­¦ä¹ ç ”ç©¶è¿…é€Ÿå‘å±•ï¼Œä½†ç›¸åº”çš„ä»£ç å®ç°å¸¸å¸¸ç¼ºå¤±ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜éš¾ä»¥å¤ç°ç»“æœå¹¶åœ¨å‰äººå·¥ä½œåŸºç¡€ä¸Šæ„å»ºã€‚ä¸æ­¤åŒæ—¶ï¼Œæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ“…é•¿ç†è§£ç§‘å­¦æ–‡æ¡£å¹¶ç”Ÿæˆé«˜è´¨é‡ä»£ç ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†PaperCoderï¼Œä¸€ä¸ªå°†æœºå™¨å­¦ä¹ è®ºæ–‡è½¬åŒ–ä¸ºåŠŸèƒ½ä»£ç ä»“åº“çš„å¤šä»£ç†LLMæ¡†æ¶ã€‚PaperCoderåˆ†ä¸‰ä¸ªé˜¶æ®µè¿è¡Œï¼šè§„åˆ’ã€åˆ†æå’Œç”Ÿæˆã€‚æ¯ä¸ªé˜¶æ®µç”±ä¸€ç»„ä¸“é—¨è®¾è®¡çš„ä»£ç†å®ç°ï¼Œä»¥ç¡®ä¿é«˜æ•ˆåä½œã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜PaperCoderèƒ½å¤Ÿåˆ›å»ºé«˜è´¨é‡ã€å¿ å®çš„å®ç°ã€‚
[27.04.2025 12:43] Mistral response. {"id": "d8b967ef547344dc9c675e19876f9446", "object": "chat.completion", "created": 1745757770, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Despite the rapid advancement of machine learning research, corresponding code implementations are often lacking, making it difficult for researchers to reproduce results and build upon previous work. Meanwhile, the latest large language models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that converts machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, analysis, and generation. Each stage is implemented by a set of specifically designed agents to ensure efficient collaboration. Our results demonstrate that PaperCoder can create high-quality, faithful implementations."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 215, "total_tokens": 351, "completion_tokens": 136}}
[27.04.2025 12:43] Response: Despite the rapid advancement of machine learning research, corresponding code implementations are often lacking, making it difficult for researchers to reproduce results and build upon previous work. Meanwhile, the latest large language models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that converts machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, analysis, and generation. Each stage is implemented by a set of specifically designed agents to ensure efficient collaboration. Our results demonstrate that PaperCoder can create high-quality, faithful implementations.
[27.04.2025 12:43] Renaming data file.
[27.04.2025 12:43] Renaming previous data. hf_papers.json to ./d/2025-04-25.json
[27.04.2025 12:43] Saving new data file.
[27.04.2025 12:43] Generating page.
[27.04.2025 12:43] Renaming previous page.
[27.04.2025 12:43] Renaming previous data. index.html to ./d/2025-04-25.html
[27.04.2025 12:43] [Experimental] Generating Chinese page for reading.
[27.04.2025 12:43] Chinese vocab [{'word': 'å°½ç®¡', 'pinyin': 'jÇn guÇn', 'trans': 'although'}, {'word': 'è¿…é€Ÿ', 'pinyin': 'xÃ¹n sÃ¹', 'trans': 'rapidly'}, {'word': 'ç¼ºå¤±', 'pinyin': 'quÄ“ shÄ«', 'trans': 'lack'}, {'word': 'å¤ç°', 'pinyin': 'fÃ¹ xiÃ n', 'trans': 'reproduce'}, {'word': 'æ“…é•¿', 'pinyin': 'shÃ n chÃ¡ng', 'trans': 'be good at'}, {'word': 'å¯å‘', 'pinyin': 'qÇ fÄ', 'trans': 'inspire'}, {'word': 'å¼•å…¥', 'pinyin': 'yÇn rÃ¹', 'trans': 'introduce'}, {'word': 'è½¬åŒ–', 'pinyin': 'zhuÇn huÃ ', 'trans': 'transform'}, {'word': 'åŠŸèƒ½', 'pinyin': 'gÅng nÃ©ng', 'trans': 'functional'}, {'word': 'ä»£ç†', 'pinyin': 'dÃ i lÇ', 'trans': 'agent'}, {'word': 'ç¡®ä¿', 'pinyin': 'quÃ¨ bÇo', 'trans': 'ensure'}, {'word': 'åä½œ', 'pinyin': 'xiÃ© zuÃ²', 'trans': 'collaborate'}, {'word': 'å¿ å®', 'pinyin': 'zhÅng shÃ­', 'trans': 'faithful'}]
[27.04.2025 12:43] Renaming previous Chinese page.
[27.04.2025 12:43] Renaming previous data. zh.html to ./d/2025-04-26_zh_reading_task.html
[27.04.2025 12:43] Writing Chinese reading task.
[27.04.2025 12:43] Writing result.
[27.04.2025 12:43] Renaming log file.
[27.04.2025 12:43] Renaming previous data. log.txt to ./logs/2025-04-27_last_log.txt
