[07.04.2025 13:23] Read previous papers.
[07.04.2025 13:23] Generating top page (month).
[07.04.2025 13:23] Writing top page (month).
[07.04.2025 14:11] Read previous papers.
[07.04.2025 14:11] Get feed.
[07.04.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02605
[07.04.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03553
[07.04.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02807
[07.04.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03561
[07.04.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03641
[07.04.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02949
[07.04.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03011
[07.04.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24067
[07.04.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03601
[07.04.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03536
[07.04.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02402
[07.04.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03600
[07.04.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24310
[07.04.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.01328
[07.04.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00396
[07.04.2025 14:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[07.04.2025 14:11] No deleted papers detected.
[07.04.2025 14:11] Downloading and parsing papers (pdf, html). Total: 15.
[07.04.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2504.02605.
[07.04.2025 14:11] Extra JSON file exists (./assets/json/2504.02605.json), skip PDF parsing.
[07.04.2025 14:11] Paper image links file exists (./assets/img_data/2504.02605.json), skip HTML parsing.
[07.04.2025 14:11] Success.
[07.04.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2504.03553.
[07.04.2025 14:11] Extra JSON file exists (./assets/json/2504.03553.json), skip PDF parsing.
[07.04.2025 14:11] Paper image links file exists (./assets/img_data/2504.03553.json), skip HTML parsing.
[07.04.2025 14:11] Success.
[07.04.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2504.02807.
[07.04.2025 14:11] Extra JSON file exists (./assets/json/2504.02807.json), skip PDF parsing.
[07.04.2025 14:11] Paper image links file exists (./assets/img_data/2504.02807.json), skip HTML parsing.
[07.04.2025 14:11] Success.
[07.04.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2504.03561.
[07.04.2025 14:11] Extra JSON file exists (./assets/json/2504.03561.json), skip PDF parsing.
[07.04.2025 14:11] Paper image links file exists (./assets/img_data/2504.03561.json), skip HTML parsing.
[07.04.2025 14:11] Success.
[07.04.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2504.03641.
[07.04.2025 14:11] Extra JSON file exists (./assets/json/2504.03641.json), skip PDF parsing.
[07.04.2025 14:11] Paper image links file exists (./assets/img_data/2504.03641.json), skip HTML parsing.
[07.04.2025 14:11] Success.
[07.04.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2504.02949.
[07.04.2025 14:11] Extra JSON file exists (./assets/json/2504.02949.json), skip PDF parsing.
[07.04.2025 14:11] Paper image links file exists (./assets/img_data/2504.02949.json), skip HTML parsing.
[07.04.2025 14:11] Success.
[07.04.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2504.03011.
[07.04.2025 14:11] Extra JSON file exists (./assets/json/2504.03011.json), skip PDF parsing.
[07.04.2025 14:11] Paper image links file exists (./assets/img_data/2504.03011.json), skip HTML parsing.
[07.04.2025 14:11] Success.
[07.04.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2503.24067.
[07.04.2025 14:11] Extra JSON file exists (./assets/json/2503.24067.json), skip PDF parsing.
[07.04.2025 14:11] Paper image links file exists (./assets/img_data/2503.24067.json), skip HTML parsing.
[07.04.2025 14:11] Success.
[07.04.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2504.03601.
[07.04.2025 14:11] Extra JSON file exists (./assets/json/2504.03601.json), skip PDF parsing.
[07.04.2025 14:11] Paper image links file exists (./assets/img_data/2504.03601.json), skip HTML parsing.
[07.04.2025 14:11] Success.
[07.04.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2504.03536.
[07.04.2025 14:11] Extra JSON file exists (./assets/json/2504.03536.json), skip PDF parsing.
[07.04.2025 14:11] Paper image links file exists (./assets/img_data/2504.03536.json), skip HTML parsing.
[07.04.2025 14:11] Success.
[07.04.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2504.02402.
[07.04.2025 14:11] Extra JSON file exists (./assets/json/2504.02402.json), skip PDF parsing.
[07.04.2025 14:11] Paper image links file exists (./assets/img_data/2504.02402.json), skip HTML parsing.
[07.04.2025 14:11] Success.
[07.04.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2504.03600.
[07.04.2025 14:11] Extra JSON file exists (./assets/json/2504.03600.json), skip PDF parsing.
[07.04.2025 14:11] Paper image links file exists (./assets/img_data/2504.03600.json), skip HTML parsing.
[07.04.2025 14:11] Success.
[07.04.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2503.24310.
[07.04.2025 14:11] Extra JSON file exists (./assets/json/2503.24310.json), skip PDF parsing.
[07.04.2025 14:11] Paper image links file exists (./assets/img_data/2503.24310.json), skip HTML parsing.
[07.04.2025 14:11] Success.
[07.04.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2504.01328.
[07.04.2025 14:11] Extra JSON file exists (./assets/json/2504.01328.json), skip PDF parsing.
[07.04.2025 14:11] Paper image links file exists (./assets/img_data/2504.01328.json), skip HTML parsing.
[07.04.2025 14:11] Success.
[07.04.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2504.00396.
[07.04.2025 14:11] Extra JSON file exists (./assets/json/2504.00396.json), skip PDF parsing.
[07.04.2025 14:11] Paper image links file exists (./assets/img_data/2504.00396.json), skip HTML parsing.
[07.04.2025 14:11] Success.
[07.04.2025 14:11] Enriching papers with extra data.
[07.04.2025 14:11] ********************************************************************************
[07.04.2025 14:11] Abstract 0. The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To addre...
[07.04.2025 14:11] ********************************************************************************
[07.04.2025 14:11] Abstract 1. Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models....
[07.04.2025 14:11] ********************************************************************************
[07.04.2025 14:11] Abstract 2. Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present Mega...
[07.04.2025 14:11] ********************************************************************************
[07.04.2025 14:11] Abstract 3. In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomou...
[07.04.2025 14:11] ********************************************************************************
[07.04.2025 14:11] Abstract 4. Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabil...
[07.04.2025 14:11] ********************************************************************************
[07.04.2025 14:11] Abstract 5. In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integ...
[07.04.2025 14:11] ********************************************************************************
[07.04.2025 14:11] Abstract 6. This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such a generalizable model is extremely challenging due to the lack of dataset, restricti...
[07.04.2025 14:11] ********************************************************************************
[07.04.2025 14:11] Abstract 7. Transformers are the cornerstone of modern large language models, but their quadratic computational complexity limits efficiency in long-sequence processing. Recent advancements in Mamba, a state space model (SSM) with linear complexity, offer promising efficiency gains but suffer from unstable cont...
[07.04.2025 14:11] ********************************************************************************
[07.04.2025 14:11] Abstract 8. Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In t...
[07.04.2025 14:11] ********************************************************************************
[07.04.2025 14:11] Abstract 9. Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from ...
[07.04.2025 14:11] ********************************************************************************
[07.04.2025 14:11] Abstract 10. When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound. Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path. Recent advan...
[07.04.2025 14:11] ********************************************************************************
[07.04.2025 14:11] Abstract 11. Medical image and video segmentation is a critical task for precision medicine, which has witnessed considerable progress in developing task or modality-specific and generalist models for 2D images. However, there have been limited studies on building general-purpose models for 3D images and videos ...
[07.04.2025 14:11] ********************************************************************************
[07.04.2025 14:11] Abstract 12. In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range...
[07.04.2025 14:11] ********************************************************************************
[07.04.2025 14:11] Abstract 13. Balancing temporal resolution and spatial detail under limited compute budget remains a key challenge for video-based multi-modal large language models (MLLMs). Existing methods typically compress video representations using predefined rules before feeding them into the LLM, resulting in irreversibl...
[07.04.2025 14:11] ********************************************************************************
[07.04.2025 14:11] Abstract 14. Fine-tuning a pre-trained Text-to-Image (T2I) model on a tailored portrait dataset is the mainstream method for text-driven customization of portrait attributes. Due to Semantic Pollution during fine-tuning, existing methods struggle to maintain the original model's behavior and achieve incremental ...
[07.04.2025 14:11] Read previous papers.
[07.04.2025 14:11] Generating reviews via LLM API.
[07.04.2025 14:11] Using data from previous issue: {"categories": ["#agi", "#benchmark", "#rl", "#open_source", "#multilingual", "#dataset"], "emoji": "üåê", "ru": {"title": "–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ò–ò –≤ —Ä–µ—à–µ–Ω–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Multi-SWE-bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—à–∞—Ç—å 
[07.04.2025 14:11] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#agents", "#agi"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏–π —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, –Ω–∞–∑—ã–≤–∞–µ–º
[07.04.2025 14:11] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#reasoning", "#synthetic", "#data"], "emoji": "üßÆ", "ru": {"title": "MegaMath: –ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–º–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MegaMath - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏. –ê–≤—Ç–æ—Ä—ã –∏—Å
[07.04.2025 14:11] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#agents", "#rl"], "emoji": "üåê", "ru": {"title": "SynWorld: –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –≤ –Ω–æ–≤—ã—Ö —Å—Ä–µ–¥–∞—Ö", "desc": "SynWorld - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∞–≥–µ–Ω—Ç–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ —Å
[07.04.2025 14:11] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#survey"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (U-MLLM). –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ
[07.04.2025 14:11] Using data from previous issue: {"categories": ["#training", "#multimodal", "#rlhf", "#open_source", "#cv", "#optimization"], "emoji": "üñºÔ∏è", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "VARGPT-v1.1 - —ç—Ç–æ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª—å–Ω–∞—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, —Ä–∞–∑–≤
[07.04.2025 14:11] Using data from previous issue: {"categories": ["#inference", "#video", "#cv", "#diffusion"], "emoji": "üí°", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Å–≤–µ—â–µ–Ω–∏–µ–º –ª—é–¥–µ–π –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –∏ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Comprehensive Relighting - –ø–µ—Ä–≤—ã–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–æ–Ω—Ç—Ä–æ–ª—é –∏ –≥–∞—Ä–º–æ–Ω–∏–∑–∞—Ü–∏–∏ –æ—Å–≤–µ—â–µ–Ω–∏—è –ª—é–¥–µ–π –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏
[07.04.2025 14:11] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture", "#long_context"], "emoji": "üîÄ", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ Transformer –∏ Mamba –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π", "desc": "TransMamba - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è Transformer –∏ Mamba —á–µ—Ä–µ–∑ –æ–±—â–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –ø–∞
[07.04.2025 14:11] Using data from previous issue: {"categories": ["#dataset", "#synthetic", "#agents", "#open_source", "#data", "#training"], "emoji": "ü§ñ", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤: —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è", "desc": "APIGen-MT - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
[07.04.2025 14:11] Using data from previous issue: {"categories": ["#synthetic", "#cv", "#3d"], "emoji": "üßë‚Äçü¶∞", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ 3D-–º–æ–¥–µ–ª–∏ –ª—é–¥–µ–π –∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–æ—Ç–æ", "desc": "HumanDreamer-X - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ 3D-–º–æ–¥–µ–ª–µ–π —á–µ–ª–æ–≤–µ–∫–∞ –ø–æ –æ–¥–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é. –û–Ω–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –º—É–ª—å—Ç–∏—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é
[07.04.2025 14:11] Using data from previous issue: {"categories": ["#training", "#dataset", "#data", "#cv"], "emoji": "üîä", "ru": {"title": "–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –±–µ—Å–∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–º—É –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –∑–≤—É–∫–∞ —Å –ø–æ–º–æ—â—å—é —Å–æ–±—ã—Ç–∏–π–Ω—ã—Ö –∫–∞–º–µ—Ä", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –±–µ—Å–∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∑–≤—É–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–æ–±—ã—Ç–∏–π–Ω—ã—Ö –∫–∞–º–µ—Ä. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ 
[07.04.2025 14:11] Using data from previous issue: {"categories": ["#data", "#healthcare", "#3d", "#dataset", "#training", "#cv"], "emoji": "ü©ª", "ru": {"title": "MedSAM2: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö 3D-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ", "desc": "MedSAM2 - —ç—Ç–æ –º–æ–¥–µ–ª—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö 3D-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ fine-tuning Segment Anythin
[07.04.2025 14:11] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#ethics"], "emoji": "‚öñÔ∏è", "ru": {"title": "BEATS: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —ç—Ç–∏—á–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ BEATS - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏, —ç—Ç–∏–∫–∏, —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏ –∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ù–∞
[07.04.2025 14:11] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#multimodal", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é slow-fast –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–ª—è MLLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É slow-fast –¥–ª—è –≤–∏–¥–µ–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è
[07.04.2025 14:11] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#training", "#dataset"], "emoji": "üñºÔ∏è", "ru": {"title": "–¢–æ—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—Ä—Ç—Ä–µ—Ç–æ–≤ –±–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ SPF-Portrait –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Ä—Ç—Ä–µ—Ç–æ–≤. –ê–≤
[07.04.2025 14:11] Loading Chinese text from previous data.
[07.04.2025 14:11] Renaming data file.
[07.04.2025 14:11] Renaming previous data. hf_papers.json to ./d/2025-04-07.json
[07.04.2025 14:11] Saving new data file.
[07.04.2025 14:11] Generating page.
[07.04.2025 14:11] Renaming previous page.
[07.04.2025 14:11] Renaming previous data. index.html to ./d/2025-04-07.html
[07.04.2025 14:11] [Experimental] Generating Chinese page for reading.
[07.04.2025 14:11] Chinese vocab [{'word': 'Ê∂µÁõñ', 'pinyin': 'h√°ng√†i', 'trans': 'cover'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´zh«în', 'trans': 'benchmark'}, {'word': 'Ê≥®Èáä', 'pinyin': 'zh√πsh√¨', 'trans': 'annotate'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ngg≈´', 'trans': 'evaluate'}, {'word': 'ÂÖàËøõ', 'pinyin': 'xiƒÅnj√¨n', 'trans': 'advanced'}, {'word': 'ËØ¶ÁªÜ', 'pinyin': 'xi√°ngx√¨', 'trans': 'detailed'}, {'word': 'ÂàÜÊûê', 'pinyin': 'fƒìnxƒ´', 'trans': 'analysis'}, {'word': 'ÂÆ£Â∏É', 'pinyin': 'xuƒÅnb√π', 'trans': 'announce'}, {'word': 'ÊàêÁ´ã', 'pinyin': 'ch√©ngl√¨', 'trans': 'establish'}, {'word': 'ÂºÄÊ∫ê', 'pinyin': 'kƒÅiyu√°n', 'trans': 'open-source'}, {'word': 'Á§æÂå∫', 'pinyin': 'sh√®q≈´', 'trans': 'community'}, {'word': 'Êó®Âú®', 'pinyin': 'zh«êz√†i', 'trans': 'aim to'}, {'word': 'ÊûÑÂª∫', 'pinyin': 'g√≤uji√†n', 'trans': 'build'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®nw√π', 'trans': 'task'}, {'word': 'Âº∫ÂåñÂ≠¶‰π†', 'pinyin': 'qi√°ng hu√† xu√©x√≠', 'trans': 'reinforcement learning'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πnli√†n', 'trans': 'training'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√πj√πj√≠', 'trans': 'dataset'}]
[07.04.2025 14:11] Renaming previous Chinese page.
[07.04.2025 14:11] Renaming previous data. zh.html to ./d/2025-04-06_zh_reading_task.html
[07.04.2025 14:11] Writing Chinese reading task.
[07.04.2025 14:11] Writing result.
[07.04.2025 14:11] Renaming log file.
[07.04.2025 14:11] Renaming previous data. log.txt to ./logs/2025-04-07_last_log.txt
