[07.04.2025 06:20] Read previous papers.
[07.04.2025 06:20] Generating top page (month).
[07.04.2025 06:20] Writing top page (month).
[07.04.2025 07:12] Read previous papers.
[07.04.2025 07:12] Get feed.
[07.04.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02605
[07.04.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03553
[07.04.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03561
[07.04.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03641
[07.04.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02949
[07.04.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03601
[07.04.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03011
[07.04.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03536
[07.04.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24310
[07.04.2025 07:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[07.04.2025 07:12] No deleted papers detected.
[07.04.2025 07:12] Downloading and parsing papers (pdf, html). Total: 9.
[07.04.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2504.02605.
[07.04.2025 07:12] Extra JSON file exists (./assets/json/2504.02605.json), skip PDF parsing.
[07.04.2025 07:12] Paper image links file exists (./assets/img_data/2504.02605.json), skip HTML parsing.
[07.04.2025 07:12] Success.
[07.04.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2504.03553.
[07.04.2025 07:12] Extra JSON file exists (./assets/json/2504.03553.json), skip PDF parsing.
[07.04.2025 07:12] Paper image links file exists (./assets/img_data/2504.03553.json), skip HTML parsing.
[07.04.2025 07:12] Success.
[07.04.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2504.03561.
[07.04.2025 07:12] Extra JSON file exists (./assets/json/2504.03561.json), skip PDF parsing.
[07.04.2025 07:12] Paper image links file exists (./assets/img_data/2504.03561.json), skip HTML parsing.
[07.04.2025 07:12] Success.
[07.04.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2504.03641.
[07.04.2025 07:12] Extra JSON file exists (./assets/json/2504.03641.json), skip PDF parsing.
[07.04.2025 07:12] Paper image links file exists (./assets/img_data/2504.03641.json), skip HTML parsing.
[07.04.2025 07:12] Success.
[07.04.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2504.02949.
[07.04.2025 07:12] Extra JSON file exists (./assets/json/2504.02949.json), skip PDF parsing.
[07.04.2025 07:12] Paper image links file exists (./assets/img_data/2504.02949.json), skip HTML parsing.
[07.04.2025 07:12] Success.
[07.04.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2504.03601.
[07.04.2025 07:12] Extra JSON file exists (./assets/json/2504.03601.json), skip PDF parsing.
[07.04.2025 07:12] Paper image links file exists (./assets/img_data/2504.03601.json), skip HTML parsing.
[07.04.2025 07:12] Success.
[07.04.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2504.03011.
[07.04.2025 07:12] Extra JSON file exists (./assets/json/2504.03011.json), skip PDF parsing.
[07.04.2025 07:12] Paper image links file exists (./assets/img_data/2504.03011.json), skip HTML parsing.
[07.04.2025 07:12] Success.
[07.04.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2504.03536.
[07.04.2025 07:12] Extra JSON file exists (./assets/json/2504.03536.json), skip PDF parsing.
[07.04.2025 07:12] Paper image links file exists (./assets/img_data/2504.03536.json), skip HTML parsing.
[07.04.2025 07:12] Success.
[07.04.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2503.24310.
[07.04.2025 07:12] Extra JSON file exists (./assets/json/2503.24310.json), skip PDF parsing.
[07.04.2025 07:12] Paper image links file exists (./assets/img_data/2503.24310.json), skip HTML parsing.
[07.04.2025 07:12] Success.
[07.04.2025 07:12] Enriching papers with extra data.
[07.04.2025 07:12] ********************************************************************************
[07.04.2025 07:12] Abstract 0. The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To addre...
[07.04.2025 07:12] ********************************************************************************
[07.04.2025 07:12] Abstract 1. Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models....
[07.04.2025 07:12] ********************************************************************************
[07.04.2025 07:12] Abstract 2. In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomou...
[07.04.2025 07:12] ********************************************************************************
[07.04.2025 07:12] Abstract 3. Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabil...
[07.04.2025 07:12] ********************************************************************************
[07.04.2025 07:12] Abstract 4. In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integ...
[07.04.2025 07:12] ********************************************************************************
[07.04.2025 07:12] Abstract 5. Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In t...
[07.04.2025 07:12] ********************************************************************************
[07.04.2025 07:12] Abstract 6. This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such a generalizable model is extremely challenging due to the lack of dataset, restricti...
[07.04.2025 07:12] ********************************************************************************
[07.04.2025 07:12] Abstract 7. Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from ...
[07.04.2025 07:12] ********************************************************************************
[07.04.2025 07:12] Abstract 8. In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range...
[07.04.2025 07:12] Read previous papers.
[07.04.2025 07:12] Generating reviews via LLM API.
[07.04.2025 07:12] Using data from previous issue: {"categories": ["#agi", "#benchmark", "#rl", "#open_source", "#multilingual", "#dataset"], "emoji": "üåê", "ru": {"title": "–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ò–ò –≤ —Ä–µ—à–µ–Ω–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Multi-SWE-bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—à–∞—Ç—å 
[07.04.2025 07:12] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#agents", "#agi"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏–π —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, –Ω–∞–∑—ã–≤–∞–µ–º
[07.04.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#agents", "#rl"], "emoji": "üåê", "ru": {"title": "SynWorld: –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –≤ –Ω–æ–≤—ã—Ö —Å—Ä–µ–¥–∞—Ö", "desc": "SynWorld - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∞–≥–µ–Ω—Ç–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ —Å
[07.04.2025 07:12] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#survey"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (U-MLLM). –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ
[07.04.2025 07:12] Using data from previous issue: {"categories": ["#training", "#multimodal", "#rlhf", "#open_source", "#cv", "#optimization"], "emoji": "üñºÔ∏è", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "VARGPT-v1.1 - —ç—Ç–æ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª—å–Ω–∞—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, —Ä–∞–∑–≤
[07.04.2025 07:12] Using data from previous issue: {"categories": ["#dataset", "#synthetic", "#agents", "#open_source", "#data", "#training"], "emoji": "ü§ñ", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤: —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è", "desc": "APIGen-MT - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
[07.04.2025 07:12] Using data from previous issue: {"categories": ["#inference", "#video", "#cv", "#diffusion"], "emoji": "üí°", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Å–≤–µ—â–µ–Ω–∏–µ–º –ª—é–¥–µ–π –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –∏ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Comprehensive Relighting - –ø–µ—Ä–≤—ã–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–æ–Ω—Ç—Ä–æ–ª—é –∏ –≥–∞—Ä–º–æ–Ω–∏–∑–∞—Ü–∏–∏ –æ—Å–≤–µ—â–µ–Ω–∏—è –ª—é–¥–µ–π –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏
[07.04.2025 07:12] Using data from previous issue: {"categories": ["#synthetic", "#cv", "#3d"], "emoji": "üßë‚Äçü¶∞", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ 3D-–º–æ–¥–µ–ª–∏ –ª—é–¥–µ–π –∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–æ—Ç–æ", "desc": "HumanDreamer-X - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ 3D-–º–æ–¥–µ–ª–µ–π —á–µ–ª–æ–≤–µ–∫–∞ –ø–æ –æ–¥–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é. –û–Ω–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –º—É–ª—å—Ç–∏—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é
[07.04.2025 07:12] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#ethics"], "emoji": "‚öñÔ∏è", "ru": {"title": "BEATS: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —ç—Ç–∏—á–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ BEATS - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏, —ç—Ç–∏–∫–∏, —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏ –∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ù–∞
[07.04.2025 07:12] Loading Chinese text from previous data.
[07.04.2025 07:12] Renaming data file.
[07.04.2025 07:12] Renaming previous data. hf_papers.json to ./d/2025-04-07.json
[07.04.2025 07:12] Saving new data file.
[07.04.2025 07:12] Generating page.
[07.04.2025 07:12] Renaming previous page.
[07.04.2025 07:12] Renaming previous data. index.html to ./d/2025-04-07.html
[07.04.2025 07:12] [Experimental] Generating Chinese page for reading.
[07.04.2025 07:12] Chinese vocab [{'word': 'Â§ßÂûã', 'pinyin': 'd√†x√≠ng', 'trans': 'large-scale'}, {'word': 'ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'y«îy√°n m√≥x√≠ng', 'trans': 'language model'}, {'word': 'ÂèòÈù©', 'pinyin': 'bi√†ng√©', 'trans': 'transformation'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'}, {'word': 'ÊÑüÁü•', 'pinyin': 'g«énzhƒ´', 'trans': 'perception'}, {'word': 'Ë°åÂä®', 'pinyin': 'x√≠ngd√≤ng', 'trans': 'action'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ngl√¨', 'trans': 'ability'}, {'word': '‰ª£ÁêÜ', 'pinyin': 'd√†il«ê', 'trans': 'agent'}, {'word': 'Èù¢‰∏¥', 'pinyin': 'mi√†nl√≠n', 'trans': 'face'}, {'word': 'ÊåëÊàò', 'pinyin': 'ti«éozh√†n', 'trans': 'challenge'}, {'word': 'ÁªºÂêà', 'pinyin': 'z√≤ngh√©', 'trans': 'comprehensive'}, {'word': 'Ê¶ÇËø∞', 'pinyin': 'g√†ish√π', 'trans': 'overview'}, {'word': 'Ê®°ÂùóÂåñ', 'pinyin': 'm√≥ku√†ihu√†', 'trans': 'modularization'}, {'word': 'ËÑëÂêØÂèëÂºè', 'pinyin': 'n«éo q«êfƒÅsh√¨', 'trans': 'brain-inspired'}, {'word': 'Êû∂ÊûÑ', 'pinyin': 'ji√†g√≤u', 'trans': 'architecture'}, {'word': 'ËÆ§Áü•ÁßëÂ≠¶', 'pinyin': 'r√®nzhƒ´ kƒìxu√©', 'trans': 'cognitive science'}, {'word': 'Á•ûÁªèÁßëÂ≠¶', 'pinyin': 'sh√©njƒ´ng kƒìxu√©', 'trans': 'neuroscience'}, {'word': 'ËÆ°ÁÆó', 'pinyin': 'j√¨su√†n', 'trans': 'computation'}, {'word': 'ÂéüÂàô', 'pinyin': 'yu√°nz√©', 'trans': 'principle'}, {'word': 'ÈÉ®ÂàÜ', 'pinyin': 'b√πfen', 'trans': 'part'}, {'word': 'Êé¢ËÆ®', 'pinyin': 't√†nt«éo', 'trans': 'discuss'}, {'word': 'Âü∫Á°Ä', 'pinyin': 'jƒ´ch«î', 'trans': 'foundation'}, {'word': 'Ëá™ÊàëÂ¢ûÂº∫', 'pinyin': 'z√¨w«í zƒìngqi√°ng', 'trans': 'self-enhancement'}, {'word': 'Êú∫Âà∂', 'pinyin': 'jƒ´zh√¨', 'trans': 'mechanism'}, {'word': 'Â§ö‰ª£ÁêÜ', 'pinyin': 'du≈ç d√†il«ê', 'trans': 'multi-agent'}, {'word': 'Á≥ªÁªü', 'pinyin': 'x√¨t«íng', 'trans': 'system'}, {'word': 'ÂÆâÂÖ®', 'pinyin': 'ƒÅnqu√°n', 'trans': 'safe'}, {'word': 'ÂèØÈù†', 'pinyin': 'kƒõk√†o', 'trans': 'reliable'}]
[07.04.2025 07:12] Renaming previous Chinese page.
[07.04.2025 07:12] Renaming previous data. zh.html to ./d/2025-04-06_zh_reading_task.html
[07.04.2025 07:12] Writing Chinese reading task.
[07.04.2025 07:12] Writing result.
[07.04.2025 07:12] Renaming log file.
[07.04.2025 07:12] Renaming previous data. log.txt to ./logs/2025-04-07_last_log.txt
