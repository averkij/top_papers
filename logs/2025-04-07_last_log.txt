[07.04.2025 14:11] Read previous papers.
[07.04.2025 14:11] Generating top page (month).
[07.04.2025 14:11] Writing top page (month).
[07.04.2025 15:11] Read previous papers.
[07.04.2025 15:11] Get feed.
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02605
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03553
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02807
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03561
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03641
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02949
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03011
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24067
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03601
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03536
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02402
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03600
[07.04.2025 15:11] Extract page data from URL. URL: https://huggingface.co/papers/2504.03597
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24310
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.01328
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00396
[07.04.2025 15:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[07.04.2025 15:11] No deleted papers detected.
[07.04.2025 15:11] Downloading and parsing papers (pdf, html). Total: 16.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.02605.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.02605.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.02605.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.03553.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.03553.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.03553.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.02807.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.02807.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.02807.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.03561.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.03561.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.03561.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.03641.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.03641.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.03641.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.02949.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.02949.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.02949.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.03011.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.03011.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.03011.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.24067.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2503.24067.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2503.24067.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.03601.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.03601.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.03601.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.03536.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.03536.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.03536.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.02402.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.02402.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.02402.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.03600.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.03600.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.03600.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.03597.
[07.04.2025 15:11] Downloading paper 2504.03597 from http://arxiv.org/pdf/2504.03597v1...
[07.04.2025 15:12] Extracting affiliations from text.
[07.04.2025 15:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Real-is-Sim: Bridging the Sim-to-Real Gap with Dynamic Digital Twin for Real-World Robot Policy Evaluation Jad Abou-Chakra1 Lingfeng Sun1 Krishan Rana2 Brandon May1 Karl Schmeckpeper1 Maria Vittoria Minniti1 Laura Herlant1 5 2 0 2 4 ] . [ 1 7 9 5 3 0 . 4 0 5 2 : r Abstract Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, novel behavior cloning framework that incorporates dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robots joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com. I. INTRODUCTION Simulatio"
[07.04.2025 15:12] Response: ```python
[]
```
[07.04.2025 15:12] Extracting affiliations from text.
[07.04.2025 15:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Real-is-Sim: Bridging the Sim-to-Real Gap with Dynamic Digital Twin for Real-World Robot Policy Evaluation Jad Abou-Chakra1 Lingfeng Sun1 Krishan Rana2 Brandon May1 Karl Schmeckpeper1 Maria Vittoria Minniti1 Laura Herlant1 5 2 0 2 4 ] . [ 1 7 9 5 3 0 . 4 0 5 2 : r Abstract Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, novel behavior cloning framework that incorporates dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robots joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com. I. INTRODUCTION Simulation environments offer highly favorable setting for developing and evaluating robotic manipulation policies. They support continuous monitoring of task performance during training, enable large-scale parallel rollouts, and allow for precise control over resets and environment variation. Crucially, simulation also affords flexible and often privileged access to state information, such as exact object poses or custom visual observations from arbitrary viewpoints, that can simplify policy learning. These advantages make behavior cloning particularly effective in simulation. However, transferring these benefits to the real world remains challenging. Real-world policy evaluation is slow, laborintensive, and limited in scale, often relying on training loss as proxy for success despite its poor correlation with actual task performance [1], [2], [3]. This gap between simulation and real-world applicability motivates the core question driving our work: How can we retain the strengths of simulation for behavior cloning while operating in the real world? 1Robotics and AI Institute 2Queensland University of Technology Fig. 1. The real-is-sim framework, illustrating the information flow between its components. policy trained in correctable physics simulator (leveraging Embodied Gaussians) controls simulated robot. Real-world observations continuously update the simulator, maintaining its state close to ground truth. The real robot then mirrors the simulated robots joint positions. This approach shifts the sim-to-real gap challenge from the policy to the adaptable physics simulator. Despite the clear advantages of simulation, integrating these benefits into real-world policy development remains difficult. Broadly, two paradigms exist. The first, real-only training, relies exclusively on physical hardware [4]. While this offers immediate applicability, since policies are trained and deployed in the same environment, it comes with significant limitations: (i) it prevents offline evaluation, which is essential for diagnosing failure modes and tracking performance, and (ii) it restricts large-scale parallelization, which is useful for exploration, hyperparameter tuning, and proactive planning. The second paradigm, sim-to-real, aims to overcome these constraints by training in simulation and transferring policies to the real world. This allows for extensive offline evaluation and experimentation at scale [5]. However, it introduces the well-known sim-to-real gap: discrepancies in observations and dynamics between simulation and reality that can degrade policy performance. Bridging this gap often requires extensive, task-specific tuning of either the simulator or the policy, efforts that are difficult to generalize or scale. Current approaches treat simulation and reality as separate domains, creating mismatch between the observational inputs learned manipulation policy receives during training and deployment. This discrepancy also necessitates ensuring that the simulated robot behaves identically to its realworld counterpart, making policy transfer from simulation to reality challenging. We propose real-is-sim, paradigm shift where correctable simulator remains always-in-the-loop, even during real-world deployment. correctable simulator synchronizes with the physical world using sensor data (in our case, RGB images), creating live, dynamic digital twin of the robots environment. The first simulator to achieve this generally and in real-time is Embodied Gaussians [6]. Embodied Gaussians combines the use of Gaussian splatting and particle-based physics system to represent both the visual and physical aspects of the world. It allows, through its differentiable rendering process, the correction of the simulator state from observations received from cameras. In the real-is-sim framework, the always-in-the-loop simulator acts as mediator between the physical world and the policy. Policies are trained exclusively on representations derived from the simulators state, effectively transforming real-world interactions into simulated states. This approach collapses the traditional divide between training and deployment domains. Real-is-sim collects synchronized simulation states during real-world interactions, which serve as demonstration trajectories for policy training. Policies are trained on arbitrary representations extracted from these simulated states (which may include object poses, robot configuration, and/or rendered images from fixed or r"
[07.04.2025 15:12] Mistral response. {"id": "86c55cbee1294942aa9069755ba31190", "object": "chat.completion", "created": 1744038724, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Robotics and AI Institute\", \"Queensland University of Technology\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1396, "total_tokens": 1419, "completion_tokens": 23}}
[07.04.2025 15:12] Response: ```python
["Robotics and AI Institute", "Queensland University of Technology"]
```
[07.04.2025 15:12] Deleting PDF ./assets/pdf/2504.03597.pdf.
[07.04.2025 15:12] Success.
[07.04.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2503.24310.
[07.04.2025 15:12] Extra JSON file exists (./assets/json/2503.24310.json), skip PDF parsing.
[07.04.2025 15:12] Paper image links file exists (./assets/img_data/2503.24310.json), skip HTML parsing.
[07.04.2025 15:12] Success.
[07.04.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2504.01328.
[07.04.2025 15:12] Extra JSON file exists (./assets/json/2504.01328.json), skip PDF parsing.
[07.04.2025 15:12] Paper image links file exists (./assets/img_data/2504.01328.json), skip HTML parsing.
[07.04.2025 15:12] Success.
[07.04.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2504.00396.
[07.04.2025 15:12] Extra JSON file exists (./assets/json/2504.00396.json), skip PDF parsing.
[07.04.2025 15:12] Paper image links file exists (./assets/img_data/2504.00396.json), skip HTML parsing.
[07.04.2025 15:12] Success.
[07.04.2025 15:12] Enriching papers with extra data.
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 0. The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To addre...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 1. Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models....
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 2. Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present Mega...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 3. In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomou...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 4. Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabil...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 5. In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integ...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 6. This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such a generalizable model is extremely challenging due to the lack of dataset, restricti...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 7. Transformers are the cornerstone of modern large language models, but their quadratic computational complexity limits efficiency in long-sequence processing. Recent advancements in Mamba, a state space model (SSM) with linear complexity, offer promising efficiency gains but suffer from unstable cont...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 8. Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In t...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 9. Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from ...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 10. When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound. Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path. Recent advan...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 11. Medical image and video segmentation is a critical task for precision medicine, which has witnessed considerable progress in developing task or modality-specific and generalist models for 2D images. However, there have been limited studies on building general-purpose models for 3D images and videos ...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 12. Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequentl...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 13. In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 14. Balancing temporal resolution and spatial detail under limited compute budget remains a key challenge for video-based multi-modal large language models (MLLMs). Existing methods typically compress video representations using predefined rules before feeding them into the LLM, resulting in irreversibl...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 15. Fine-tuning a pre-trained Text-to-Image (T2I) model on a tailored portrait dataset is the mainstream method for text-driven customization of portrait attributes. Due to Semantic Pollution during fine-tuning, existing methods struggle to maintain the original model's behavior and achieve incremental ...
[07.04.2025 15:12] Read previous papers.
[07.04.2025 15:12] Generating reviews via LLM API.
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#agi", "#benchmark", "#rl", "#open_source", "#multilingual", "#dataset"], "emoji": "🌐", "ru": {"title": "Многоязычный бенчмарк для оценки ИИ в решении программных задач", "desc": "Представлен новый многоязычный бенчмарк Multi-SWE-bench для оценки способности языковых моделей решать 
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#agents", "#agi"], "emoji": "🧠", "ru": {"title": "Самоосознанность в планировании: эффективное использование знаний языковыми моделями", "desc": "Статья представляет новый подход к обучению языковых моделей для задач планирования, называем
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#reasoning", "#synthetic", "#data"], "emoji": "🧮", "ru": {"title": "MegaMath: Большие данные для умных вычислений", "desc": "Статья представляет MegaMath - крупномасштабный открытый датасет для предобучения языковых моделей в области математики. Авторы ис
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#agents", "#rl"], "emoji": "🌐", "ru": {"title": "SynWorld: Автономное исследование и обучение агентов в новых средах", "desc": "SynWorld - это фреймворк, позволяющий агентам на основе больших языковых моделей (LLM) автономно исследовать новые с
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#survey"], "emoji": "🧠", "ru": {"title": "Новый стандарт оценки мультимодальных языковых моделей", "desc": "Эта статья представляет новую систему оценки для унифицированных мультимодальных языковых моделей (U-MLLM). Авторы разработали комплексный фреймво
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#training", "#multimodal", "#rlhf", "#open_source", "#cv", "#optimization"], "emoji": "🖼️", "ru": {"title": "Единая модель для понимания, генерации и редактирования изображений", "desc": "VARGPT-v1.1 - это усовершенствованная унифицированная визуальная авторегрессионная модель, разв
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#inference", "#video", "#cv", "#diffusion"], "emoji": "💡", "ru": {"title": "Универсальное управление освещением людей на изображениях и видео", "desc": "Статья представляет Comprehensive Relighting - первый универсальный подход к контролю и гармонизации освещения людей на изображени
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture", "#long_context"], "emoji": "🔀", "ru": {"title": "Объединение Transformer и Mamba для эффективной обработки длинных последовательностей", "desc": "TransMamba - это новая архитектура, объединяющая Transformer и Mamba через общие матрицы па
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#dataset", "#synthetic", "#agents", "#open_source", "#data", "#training"], "emoji": "🤖", "ru": {"title": "Революция в обучении ИИ-агентов: синтетические данные для реалистичного многоходового взаимодействия", "desc": "APIGen-MT - это новый фреймворк для генерации качественных данных
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#synthetic", "#cv", "#3d"], "emoji": "🧑‍🦰", "ru": {"title": "Реалистичные 3D-модели людей из одного фото", "desc": "HumanDreamer-X - это новая система для реконструкции 3D-моделей человека по одному изображению. Она объединяет генерацию мультиракурсных изображений и 3D-реконструкцию
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#training", "#dataset", "#data", "#cv"], "emoji": "🔊", "ru": {"title": "Новый подход к бесконтактному восстановлению звука с помощью событийных камер", "desc": "Статья представляет новый метод бесконтактного восстановления звука с использованием событийных камер. Авторы разработали 
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#data", "#healthcare", "#3d", "#dataset", "#training", "#cv"], "emoji": "🩻", "ru": {"title": "MedSAM2: Революция в сегментации медицинских 3D-изображений и видео", "desc": "MedSAM2 - это модель сегментации медицинских 3D-изображений и видео, основанная на fine-tuning Segment Anythin
[07.04.2025 15:12] Querying the API.
[07.04.2025 15:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, a novel behavior cloning framework that incorporates a dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robot's joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com.
[07.04.2025 15:12] Response: {
  "desc": "Эта статья представляет новый подход к обучению роботов сложным манипуляционным задачам, называемый real-is-sim. Метод использует динамический цифровой двойник на основе Embodied Gaussians для сбора данных, обучения и развертывания политик. Real-is-sim позволяет оценивать политики в симуляторе, что значительно ускоряет процесс разработки. Авторы демонстрируют сильную корреляцию между успешностью в симуляторе и реальном мире на задаче PushT.",
  "emoji": "🤖",
  "title": "Цифровой двойник для эффективного обучения роботов"
}
[07.04.2025 15:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, a novel behavior cloning framework that incorporates a dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robot's joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com."

[07.04.2025 15:12] Response: ```python
['AGENTS', 'ROBOTICS', 'TRAINING']
```
[07.04.2025 15:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, a novel behavior cloning framework that incorporates a dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robot's joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com."

[07.04.2025 15:12] Response: ```python
["OPTIMIZATION"]
```
[07.04.2025 15:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces real-is-sim, a new behavior cloning framework designed to improve the training and evaluation of robots performing manipulation tasks. It utilizes a dynamic digital twin that aligns a simulated environment with the real world, allowing for better data collection and training processes. By enabling offline evaluation of policies in a simulator, it addresses the challenges of overfitting and underfitting while reducing reliance on costly real-world testing. The framework shows strong correlation between simulated success rates and actual performance, enhancing the efficiency of robot training.","title":"Bridging the Gap: Real-World Success through Simulated Training"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces real-is-sim, a new behavior cloning framework designed to improve the training and evaluation of robots performing manipulation tasks. It utilizes a dynamic digital twin that aligns a simulated environment with the real world, allowing for better data collection and training processes. By enabling offline evaluation of policies in a simulator, it addresses the challenges of overfitting and underfitting while reducing reliance on costly real-world testing. The framework shows strong correlation between simulated success rates and actual performance, enhancing the efficiency of robot training.', title='Bridging the Gap: Real-World Success through Simulated Training'))
[07.04.2025 15:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"最近，行为克隆技术的进步使得机器人能够执行复杂的操作任务。然而，准确评估训练性能仍然具有挑战性，尤其是在实际应用中，因为行为克隆损失与实际任务成功率的相关性较差。因此，研究人员不得不依赖于昂贵且耗时的实际评估来获取成功率指标，这使得识别最佳策略和检测过拟合或欠拟合变得不切实际。为了解决这些问题，我们提出了real-is-sim，一个新颖的行为克隆框架，通过动态数字双胞胎在整个策略开发流程中进行数据收集、训练和部署。","title":"动态数字双胞胎提升行为克隆性能"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='最近，行为克隆技术的进步使得机器人能够执行复杂的操作任务。然而，准确评估训练性能仍然具有挑战性，尤其是在实际应用中，因为行为克隆损失与实际任务成功率的相关性较差。因此，研究人员不得不依赖于昂贵且耗时的实际评估来获取成功率指标，这使得识别最佳策略和检测过拟合或欠拟合变得不切实际。为了解决这些问题，我们提出了real-is-sim，一个新颖的行为克隆框架，通过动态数字双胞胎在整个策略开发流程中进行数据收集、训练和部署。', title='动态数字双胞胎提升行为克隆性能'))
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#ethics"], "emoji": "⚖️", "ru": {"title": "BEATS: комплексная оценка этичности языковых моделей", "desc": "Исследователи представили BEATS - новую систему оценки предвзятости, этики, справедливости и фактической точности в больших языковых моделях (LLM). На
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#multimodal", "#video", "#architecture"], "emoji": "🎬", "ru": {"title": "Эффективное понимание видео с помощью slow-fast архитектуры для MLLM", "desc": "Статья представляет новую архитектуру slow-fast для видео-ориентированных мультимодальных больших я
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#training", "#dataset"], "emoji": "🖼️", "ru": {"title": "Точная настройка генерации портретов без семантического загрязнения", "desc": "Статья представляет новый метод SPF-Portrait для настройки моделей генерации изображений по тексту для создания портретов. Ав
[07.04.2025 15:12] Loading Chinese text from previous data.
[07.04.2025 15:12] Renaming data file.
[07.04.2025 15:12] Renaming previous data. hf_papers.json to ./d/2025-04-07.json
[07.04.2025 15:12] Saving new data file.
[07.04.2025 15:12] Generating page.
[07.04.2025 15:12] Renaming previous page.
[07.04.2025 15:12] Renaming previous data. index.html to ./d/2025-04-07.html
[07.04.2025 15:12] [Experimental] Generating Chinese page for reading.
[07.04.2025 15:12] Chinese vocab [{'word': '涵盖', 'pinyin': 'hángài', 'trans': 'cover'}, {'word': '基准', 'pinyin': 'jīzhǔn', 'trans': 'benchmark'}, {'word': '注释', 'pinyin': 'zhùshì', 'trans': 'annotate'}, {'word': '评估', 'pinyin': 'pínggū', 'trans': 'evaluate'}, {'word': '先进', 'pinyin': 'xiānjìn', 'trans': 'advanced'}, {'word': '详细', 'pinyin': 'xiángxì', 'trans': 'detailed'}, {'word': '分析', 'pinyin': 'fēnxī', 'trans': 'analysis'}, {'word': '宣布', 'pinyin': 'xuānbù', 'trans': 'announce'}, {'word': '成立', 'pinyin': 'chénglì', 'trans': 'establish'}, {'word': '开源', 'pinyin': 'kāiyuán', 'trans': 'open-source'}, {'word': '社区', 'pinyin': 'shèqū', 'trans': 'community'}, {'word': '旨在', 'pinyin': 'zhǐzài', 'trans': 'aim to'}, {'word': '构建', 'pinyin': 'gòujiàn', 'trans': 'build'}, {'word': '任务', 'pinyin': 'rènwù', 'trans': 'task'}, {'word': '强化学习', 'pinyin': 'qiáng huà xuéxí', 'trans': 'reinforcement learning'}, {'word': '训练', 'pinyin': 'xùnliàn', 'trans': 'training'}, {'word': '数据集', 'pinyin': 'shùjùjí', 'trans': 'dataset'}]
[07.04.2025 15:12] Renaming previous Chinese page.
[07.04.2025 15:12] Renaming previous data. zh.html to ./d/2025-04-06_zh_reading_task.html
[07.04.2025 15:12] Writing Chinese reading task.
[07.04.2025 15:12] Writing result.
[07.04.2025 15:12] Renaming log file.
[07.04.2025 15:12] Renaming previous data. log.txt to ./logs/2025-04-07_last_log.txt
