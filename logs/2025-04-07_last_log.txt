[07.04.2025 14:11] Read previous papers.
[07.04.2025 14:11] Generating top page (month).
[07.04.2025 14:11] Writing top page (month).
[07.04.2025 15:11] Read previous papers.
[07.04.2025 15:11] Get feed.
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02605
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03553
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02807
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03561
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03641
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02949
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03011
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24067
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03601
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03536
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02402
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03600
[07.04.2025 15:11] Extract page data from URL. URL: https://huggingface.co/papers/2504.03597
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24310
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.01328
[07.04.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00396
[07.04.2025 15:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[07.04.2025 15:11] No deleted papers detected.
[07.04.2025 15:11] Downloading and parsing papers (pdf, html). Total: 16.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.02605.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.02605.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.02605.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.03553.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.03553.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.03553.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.02807.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.02807.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.02807.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.03561.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.03561.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.03561.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.03641.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.03641.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.03641.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.02949.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.02949.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.02949.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.03011.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.03011.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.03011.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.24067.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2503.24067.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2503.24067.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.03601.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.03601.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.03601.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.03536.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.03536.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.03536.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.02402.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.02402.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.02402.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.03600.
[07.04.2025 15:11] Extra JSON file exists (./assets/json/2504.03600.json), skip PDF parsing.
[07.04.2025 15:11] Paper image links file exists (./assets/img_data/2504.03600.json), skip HTML parsing.
[07.04.2025 15:11] Success.
[07.04.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2504.03597.
[07.04.2025 15:11] Downloading paper 2504.03597 from http://arxiv.org/pdf/2504.03597v1...
[07.04.2025 15:12] Extracting affiliations from text.
[07.04.2025 15:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Real-is-Sim: Bridging the Sim-to-Real Gap with Dynamic Digital Twin for Real-World Robot Policy Evaluation Jad Abou-Chakra1 Lingfeng Sun1 Krishan Rana2 Brandon May1 Karl Schmeckpeper1 Maria Vittoria Minniti1 Laura Herlant1 5 2 0 2 4 ] . [ 1 7 9 5 3 0 . 4 0 5 2 : r Abstract Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, novel behavior cloning framework that incorporates dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robots joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com. I. INTRODUCTION Simulatio"
[07.04.2025 15:12] Response: ```python
[]
```
[07.04.2025 15:12] Extracting affiliations from text.
[07.04.2025 15:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Real-is-Sim: Bridging the Sim-to-Real Gap with Dynamic Digital Twin for Real-World Robot Policy Evaluation Jad Abou-Chakra1 Lingfeng Sun1 Krishan Rana2 Brandon May1 Karl Schmeckpeper1 Maria Vittoria Minniti1 Laura Herlant1 5 2 0 2 4 ] . [ 1 7 9 5 3 0 . 4 0 5 2 : r Abstract Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, novel behavior cloning framework that incorporates dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robots joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com. I. INTRODUCTION Simulation environments offer highly favorable setting for developing and evaluating robotic manipulation policies. They support continuous monitoring of task performance during training, enable large-scale parallel rollouts, and allow for precise control over resets and environment variation. Crucially, simulation also affords flexible and often privileged access to state information, such as exact object poses or custom visual observations from arbitrary viewpoints, that can simplify policy learning. These advantages make behavior cloning particularly effective in simulation. However, transferring these benefits to the real world remains challenging. Real-world policy evaluation is slow, laborintensive, and limited in scale, often relying on training loss as proxy for success despite its poor correlation with actual task performance [1], [2], [3]. This gap between simulation and real-world applicability motivates the core question driving our work: How can we retain the strengths of simulation for behavior cloning while operating in the real world? 1Robotics and AI Institute 2Queensland University of Technology Fig. 1. The real-is-sim framework, illustrating the information flow between its components. policy trained in correctable physics simulator (leveraging Embodied Gaussians) controls simulated robot. Real-world observations continuously update the simulator, maintaining its state close to ground truth. The real robot then mirrors the simulated robots joint positions. This approach shifts the sim-to-real gap challenge from the policy to the adaptable physics simulator. Despite the clear advantages of simulation, integrating these benefits into real-world policy development remains difficult. Broadly, two paradigms exist. The first, real-only training, relies exclusively on physical hardware [4]. While this offers immediate applicability, since policies are trained and deployed in the same environment, it comes with significant limitations: (i) it prevents offline evaluation, which is essential for diagnosing failure modes and tracking performance, and (ii) it restricts large-scale parallelization, which is useful for exploration, hyperparameter tuning, and proactive planning. The second paradigm, sim-to-real, aims to overcome these constraints by training in simulation and transferring policies to the real world. This allows for extensive offline evaluation and experimentation at scale [5]. However, it introduces the well-known sim-to-real gap: discrepancies in observations and dynamics between simulation and reality that can degrade policy performance. Bridging this gap often requires extensive, task-specific tuning of either the simulator or the policy, efforts that are difficult to generalize or scale. Current approaches treat simulation and reality as separate domains, creating mismatch between the observational inputs learned manipulation policy receives during training and deployment. This discrepancy also necessitates ensuring that the simulated robot behaves identically to its realworld counterpart, making policy transfer from simulation to reality challenging. We propose real-is-sim, paradigm shift where correctable simulator remains always-in-the-loop, even during real-world deployment. correctable simulator synchronizes with the physical world using sensor data (in our case, RGB images), creating live, dynamic digital twin of the robots environment. The first simulator to achieve this generally and in real-time is Embodied Gaussians [6]. Embodied Gaussians combines the use of Gaussian splatting and particle-based physics system to represent both the visual and physical aspects of the world. It allows, through its differentiable rendering process, the correction of the simulator state from observations received from cameras. In the real-is-sim framework, the always-in-the-loop simulator acts as mediator between the physical world and the policy. Policies are trained exclusively on representations derived from the simulators state, effectively transforming real-world interactions into simulated states. This approach collapses the traditional divide between training and deployment domains. Real-is-sim collects synchronized simulation states during real-world interactions, which serve as demonstration trajectories for policy training. Policies are trained on arbitrary representations extracted from these simulated states (which may include object poses, robot configuration, and/or rendered images from fixed or r"
[07.04.2025 15:12] Mistral response. {"id": "86c55cbee1294942aa9069755ba31190", "object": "chat.completion", "created": 1744038724, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Robotics and AI Institute\", \"Queensland University of Technology\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1396, "total_tokens": 1419, "completion_tokens": 23}}
[07.04.2025 15:12] Response: ```python
["Robotics and AI Institute", "Queensland University of Technology"]
```
[07.04.2025 15:12] Deleting PDF ./assets/pdf/2504.03597.pdf.
[07.04.2025 15:12] Success.
[07.04.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2503.24310.
[07.04.2025 15:12] Extra JSON file exists (./assets/json/2503.24310.json), skip PDF parsing.
[07.04.2025 15:12] Paper image links file exists (./assets/img_data/2503.24310.json), skip HTML parsing.
[07.04.2025 15:12] Success.
[07.04.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2504.01328.
[07.04.2025 15:12] Extra JSON file exists (./assets/json/2504.01328.json), skip PDF parsing.
[07.04.2025 15:12] Paper image links file exists (./assets/img_data/2504.01328.json), skip HTML parsing.
[07.04.2025 15:12] Success.
[07.04.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2504.00396.
[07.04.2025 15:12] Extra JSON file exists (./assets/json/2504.00396.json), skip PDF parsing.
[07.04.2025 15:12] Paper image links file exists (./assets/img_data/2504.00396.json), skip HTML parsing.
[07.04.2025 15:12] Success.
[07.04.2025 15:12] Enriching papers with extra data.
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 0. The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To addre...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 1. Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models....
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 2. Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present Mega...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 3. In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomou...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 4. Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabil...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 5. In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integ...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 6. This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such a generalizable model is extremely challenging due to the lack of dataset, restricti...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 7. Transformers are the cornerstone of modern large language models, but their quadratic computational complexity limits efficiency in long-sequence processing. Recent advancements in Mamba, a state space model (SSM) with linear complexity, offer promising efficiency gains but suffer from unstable cont...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 8. Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In t...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 9. Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from ...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 10. When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound. Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path. Recent advan...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 11. Medical image and video segmentation is a critical task for precision medicine, which has witnessed considerable progress in developing task or modality-specific and generalist models for 2D images. However, there have been limited studies on building general-purpose models for 3D images and videos ...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 12. Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequentl...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 13. In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 14. Balancing temporal resolution and spatial detail under limited compute budget remains a key challenge for video-based multi-modal large language models (MLLMs). Existing methods typically compress video representations using predefined rules before feeding them into the LLM, resulting in irreversibl...
[07.04.2025 15:12] ********************************************************************************
[07.04.2025 15:12] Abstract 15. Fine-tuning a pre-trained Text-to-Image (T2I) model on a tailored portrait dataset is the mainstream method for text-driven customization of portrait attributes. Due to Semantic Pollution during fine-tuning, existing methods struggle to maintain the original model's behavior and achieve incremental ...
[07.04.2025 15:12] Read previous papers.
[07.04.2025 15:12] Generating reviews via LLM API.
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#agi", "#benchmark", "#rl", "#open_source", "#multilingual", "#dataset"], "emoji": "ğŸŒ", "ru": {"title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡", "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Multi-SWE-bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ 
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#agents", "#agi"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#reasoning", "#synthetic", "#data"], "emoji": "ğŸ§®", "ru": {"title": "MegaMath: Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MegaMath - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ñ
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#agents", "#rl"], "emoji": "ğŸŒ", "ru": {"title": "SynWorld: ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…", "desc": "SynWorld - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#survey"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (U-MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#training", "#multimodal", "#rlhf", "#open_source", "#cv", "#optimization"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹", "desc": "VARGPT-v1.1 - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ğ²
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#inference", "#video", "#cv", "#diffusion"], "emoji": "ğŸ’¡", "ru": {"title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ»ÑĞ´ĞµĞ¹ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Comprehensive Relighting - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¸ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ»ÑĞ´ĞµĞ¹ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture", "#long_context"], "emoji": "ğŸ”€", "ru": {"title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Transformer Ğ¸ Mamba Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹", "desc": "TransMamba - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Transformer Ğ¸ Mamba Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ¿Ğ°
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#dataset", "#synthetic", "#agents", "#open_source", "#data", "#training"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ", "desc": "APIGen-MT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#synthetic", "#cv", "#3d"], "emoji": "ğŸ§‘â€ğŸ¦°", "ru": {"title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ‚Ğ¾", "desc": "HumanDreamer-X - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#training", "#dataset", "#data", "#cv"], "emoji": "ğŸ”Š", "ru": {"title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ±ĞµÑĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµÑĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ 
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#data", "#healthcare", "#3d", "#dataset", "#training", "#cv"], "emoji": "ğŸ©»", "ru": {"title": "MedSAM2: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… 3D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "MedSAM2 - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… 3D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° fine-tuning Segment Anythin
[07.04.2025 15:12] Querying the API.
[07.04.2025 15:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, a novel behavior cloning framework that incorporates a dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robot's joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com.
[07.04.2025 15:12] Response: {
  "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ real-is-sim. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Embodied Gaussians Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº. Real-is-sim Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğµ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğµ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ PushT.",
  "emoji": "ğŸ¤–",
  "title": "Ğ¦Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸Ğº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²"
}
[07.04.2025 15:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, a novel behavior cloning framework that incorporates a dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robot's joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com."

[07.04.2025 15:12] Response: ```python
['AGENTS', 'ROBOTICS', 'TRAINING']
```
[07.04.2025 15:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, a novel behavior cloning framework that incorporates a dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robot's joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com."

[07.04.2025 15:12] Response: ```python
["OPTIMIZATION"]
```
[07.04.2025 15:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces real-is-sim, a new behavior cloning framework designed to improve the training and evaluation of robots performing manipulation tasks. It utilizes a dynamic digital twin that aligns a simulated environment with the real world, allowing for better data collection and training processes. By enabling offline evaluation of policies in a simulator, it addresses the challenges of overfitting and underfitting while reducing reliance on costly real-world testing. The framework shows strong correlation between simulated success rates and actual performance, enhancing the efficiency of robot training.","title":"Bridging the Gap: Real-World Success through Simulated Training"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces real-is-sim, a new behavior cloning framework designed to improve the training and evaluation of robots performing manipulation tasks. It utilizes a dynamic digital twin that aligns a simulated environment with the real world, allowing for better data collection and training processes. By enabling offline evaluation of policies in a simulator, it addresses the challenges of overfitting and underfitting while reducing reliance on costly real-world testing. The framework shows strong correlation between simulated success rates and actual performance, enhancing the efficiency of robot training.', title='Bridging the Gap: Real-World Success through Simulated Training'))
[07.04.2025 15:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ€è¿‘ï¼Œè¡Œä¸ºå…‹éš†æŠ€æœ¯çš„è¿›æ­¥ä½¿å¾—æœºå™¨äººèƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„æ“ä½œä»»åŠ¡ã€‚ç„¶è€Œï¼Œå‡†ç¡®è¯„ä¼°è®­ç»ƒæ€§èƒ½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå› ä¸ºè¡Œä¸ºå…‹éš†æŸå¤±ä¸å®é™…ä»»åŠ¡æˆåŠŸç‡çš„ç›¸å…³æ€§è¾ƒå·®ã€‚å› æ­¤ï¼Œç ”ç©¶äººå‘˜ä¸å¾—ä¸ä¾èµ–äºæ˜‚è´µä¸”è€—æ—¶çš„å®é™…è¯„ä¼°æ¥è·å–æˆåŠŸç‡æŒ‡æ ‡ï¼Œè¿™ä½¿å¾—è¯†åˆ«æœ€ä½³ç­–ç•¥å’Œæ£€æµ‹è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆå˜å¾—ä¸åˆ‡å®é™…ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†real-is-simï¼Œä¸€ä¸ªæ–°é¢–çš„è¡Œä¸ºå…‹éš†æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€æ•°å­—åŒèƒèƒåœ¨æ•´ä¸ªç­–ç•¥å¼€å‘æµç¨‹ä¸­è¿›è¡Œæ•°æ®æ”¶é›†ã€è®­ç»ƒå’Œéƒ¨ç½²ã€‚","title":"åŠ¨æ€æ•°å­—åŒèƒèƒæå‡è¡Œä¸ºå…‹éš†æ€§èƒ½"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ€è¿‘ï¼Œè¡Œä¸ºå…‹éš†æŠ€æœ¯çš„è¿›æ­¥ä½¿å¾—æœºå™¨äººèƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„æ“ä½œä»»åŠ¡ã€‚ç„¶è€Œï¼Œå‡†ç¡®è¯„ä¼°è®­ç»ƒæ€§èƒ½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå› ä¸ºè¡Œä¸ºå…‹éš†æŸå¤±ä¸å®é™…ä»»åŠ¡æˆåŠŸç‡çš„ç›¸å…³æ€§è¾ƒå·®ã€‚å› æ­¤ï¼Œç ”ç©¶äººå‘˜ä¸å¾—ä¸ä¾èµ–äºæ˜‚è´µä¸”è€—æ—¶çš„å®é™…è¯„ä¼°æ¥è·å–æˆåŠŸç‡æŒ‡æ ‡ï¼Œè¿™ä½¿å¾—è¯†åˆ«æœ€ä½³ç­–ç•¥å’Œæ£€æµ‹è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆå˜å¾—ä¸åˆ‡å®é™…ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†real-is-simï¼Œä¸€ä¸ªæ–°é¢–çš„è¡Œä¸ºå…‹éš†æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€æ•°å­—åŒèƒèƒåœ¨æ•´ä¸ªç­–ç•¥å¼€å‘æµç¨‹ä¸­è¿›è¡Œæ•°æ®æ”¶é›†ã€è®­ç»ƒå’Œéƒ¨ç½²ã€‚', title='åŠ¨æ€æ•°å­—åŒèƒèƒæå‡è¡Œä¸ºå…‹éš†æ€§èƒ½'))
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#ethics"], "emoji": "âš–ï¸", "ru": {"title": "BEATS: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ BEATS - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸, ÑÑ‚Ğ¸ĞºĞ¸, ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ°
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#multimodal", "#video", "#architecture"], "emoji": "ğŸ¬", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ slow-fast Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ MLLM", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ slow-fast Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ
[07.04.2025 15:12] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#training", "#dataset"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ğ±ĞµĞ· ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SPF-Portrait Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ². ĞĞ²
[07.04.2025 15:12] Loading Chinese text from previous data.
[07.04.2025 15:12] Renaming data file.
[07.04.2025 15:12] Renaming previous data. hf_papers.json to ./d/2025-04-07.json
[07.04.2025 15:12] Saving new data file.
[07.04.2025 15:12] Generating page.
[07.04.2025 15:12] Renaming previous page.
[07.04.2025 15:12] Renaming previous data. index.html to ./d/2025-04-07.html
[07.04.2025 15:12] [Experimental] Generating Chinese page for reading.
[07.04.2025 15:12] Chinese vocab [{'word': 'æ¶µç›–', 'pinyin': 'hÃ¡ngÃ i', 'trans': 'cover'}, {'word': 'åŸºå‡†', 'pinyin': 'jÄ«zhÇ”n', 'trans': 'benchmark'}, {'word': 'æ³¨é‡Š', 'pinyin': 'zhÃ¹shÃ¬', 'trans': 'annotate'}, {'word': 'è¯„ä¼°', 'pinyin': 'pÃ­nggÅ«', 'trans': 'evaluate'}, {'word': 'å…ˆè¿›', 'pinyin': 'xiÄnjÃ¬n', 'trans': 'advanced'}, {'word': 'è¯¦ç»†', 'pinyin': 'xiÃ¡ngxÃ¬', 'trans': 'detailed'}, {'word': 'åˆ†æ', 'pinyin': 'fÄ“nxÄ«', 'trans': 'analysis'}, {'word': 'å®£å¸ƒ', 'pinyin': 'xuÄnbÃ¹', 'trans': 'announce'}, {'word': 'æˆç«‹', 'pinyin': 'chÃ©nglÃ¬', 'trans': 'establish'}, {'word': 'å¼€æº', 'pinyin': 'kÄiyuÃ¡n', 'trans': 'open-source'}, {'word': 'ç¤¾åŒº', 'pinyin': 'shÃ¨qÅ«', 'trans': 'community'}, {'word': 'æ—¨åœ¨', 'pinyin': 'zhÇzÃ i', 'trans': 'aim to'}, {'word': 'æ„å»º', 'pinyin': 'gÃ²ujiÃ n', 'trans': 'build'}, {'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨nwÃ¹', 'trans': 'task'}, {'word': 'å¼ºåŒ–å­¦ä¹ ', 'pinyin': 'qiÃ¡ng huÃ  xuÃ©xÃ­', 'trans': 'reinforcement learning'}, {'word': 'è®­ç»ƒ', 'pinyin': 'xÃ¹nliÃ n', 'trans': 'training'}, {'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹jÃ¹jÃ­', 'trans': 'dataset'}]
[07.04.2025 15:12] Renaming previous Chinese page.
[07.04.2025 15:12] Renaming previous data. zh.html to ./d/2025-04-06_zh_reading_task.html
[07.04.2025 15:12] Writing Chinese reading task.
[07.04.2025 15:12] Writing result.
[07.04.2025 15:12] Renaming log file.
[07.04.2025 15:12] Renaming previous data. log.txt to ./logs/2025-04-07_last_log.txt
