[28.12.2024 06:28] Read previous papers.
[28.12.2024 06:28] Generating top page (month).
[28.12.2024 06:28] Writing top page (month).
[28.12.2024 12:38] Read previous papers.
[28.12.2024 12:38] Get feed.
[28.12.2024 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2412.17743
[28.12.2024 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2412.17483
[28.12.2024 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2412.18176
[28.12.2024 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2412.18072
[28.12.2024 12:38] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.12.2024 12:38] No deleted papers detected.
[28.12.2024 12:38] Downloading and parsing papers (pdf, html). Total: 4.
[28.12.2024 12:38] Downloading and parsing paper https://huggingface.co/papers/2412.17743.
[28.12.2024 12:38] Extra JSON file exists (./assets/json/2412.17743.json), skip PDF parsing.
[28.12.2024 12:38] Paper image links file exists (./assets/img_data/2412.17743.json), skip HTML parsing.
[28.12.2024 12:38] Success.
[28.12.2024 12:38] Downloading and parsing paper https://huggingface.co/papers/2412.17483.
[28.12.2024 12:38] Extra JSON file exists (./assets/json/2412.17483.json), skip PDF parsing.
[28.12.2024 12:38] Paper image links file exists (./assets/img_data/2412.17483.json), skip HTML parsing.
[28.12.2024 12:38] Success.
[28.12.2024 12:38] Downloading and parsing paper https://huggingface.co/papers/2412.18176.
[28.12.2024 12:38] Extra JSON file exists (./assets/json/2412.18176.json), skip PDF parsing.
[28.12.2024 12:38] Paper image links file exists (./assets/img_data/2412.18176.json), skip HTML parsing.
[28.12.2024 12:38] Success.
[28.12.2024 12:38] Downloading and parsing paper https://huggingface.co/papers/2412.18072.
[28.12.2024 12:38] Extra JSON file exists (./assets/json/2412.18072.json), skip PDF parsing.
[28.12.2024 12:38] Paper image links file exists (./assets/img_data/2412.18072.json), skip HTML parsing.
[28.12.2024 12:38] Success.
[28.12.2024 12:38] Enriching papers with extra data.
[28.12.2024 12:38] ********************************************************************************
[28.12.2024 12:38] Abstract 0. Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-t...
[28.12.2024 12:38] ********************************************************************************
[28.12.2024 12:38] Abstract 1. In this work, we provide a thorough investigation of gist-based context compression methods to improve long-context processing in large language models. We focus on two key questions: (1) How well can these methods replace full attention models? and (2) What potential failure patterns arise due to c...
[28.12.2024 12:38] ********************************************************************************
[28.12.2024 12:38] Abstract 2. Sequential recommendation (SR) systems have evolved significantly over the past decade, transitioning from traditional collaborative filtering to deep learning approaches and, more recently, to large language models (LLMs). While the adoption of LLMs has driven substantial advancements, these models...
[28.12.2024 12:38] ********************************************************************************
[28.12.2024 12:38] Abstract 3. With advances in foundational and vision-language models, and effective fine-tuning techniques, a large number of both general and special-purpose models have been developed for a variety of visual tasks. Despite the flexibility and accessibility of these models, no single model is able to handle al...
[28.12.2024 12:38] Read previous papers.
[28.12.2024 12:38] Generating reviews via LLM API.
[28.12.2024 12:38] Using data from previous issue: {"categories": ["#long_context", "#open_source", "#optimization", "#small_models", "#data", "#training"], "emoji": "🚀", "ru": {"title": "Эффективное предобучение больших языковых моделей с меньшими ресурсами", "desc": "Статья представляет YuLan-Mini - языковую модель с 2.42 миллиардами параметров, д
[28.12.2024 12:38] Using data from previous issue: {"categories": ["#training", "#rag", "#optimization", "#long_context", "#data"], "emoji": "🗜️", "ru": {"title": "Сжатие без потерь: оптимизация обработки длинного контекста в больших языковых моделях", "desc": "Это исследование посвящено методам сжатия контекста на основе гист-токенов для улучшения 
[28.12.2024 12:38] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#games", "#optimization"], "emoji": "🧠", "ru": {"title": "Molar: Мультимодальные рекомендации с учетом коллаборативной фильтрации", "desc": "Статья представляет Molar - новую систему последовательных рекомендаций, основанную на мультимодальных больших язык
[28.12.2024 12:38] Using data from previous issue: {"categories": ["#benchmark", "#agi", "#optimization", "#agents", "#multimodal"], "emoji": "🏭", "ru": {"title": "MMFactory: универсальная фабрика решений для визуальных задач", "desc": "MMFactory - это универсальная платформа для решения сложных визуальных задач с использованием различных моделей ма
[28.12.2024 12:38] Trying to get texts in Chinese.
[28.12.2024 12:38] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-tier performance among models of similar parameter scale. Our pre-training approach focuses on enhancing training efficacy through three key technical contributions: an elaborate data pipeline combines data cleaning with data schedule strategies, a robust optimization method to mitigate training instability, and an effective annealing approach that incorporates targeted data selection and long context training. Remarkably, YuLan-Mini, trained on 1.08T tokens, achieves performance comparable to industry-leading models that require significantly more data. To facilitate reproduction, we release the full details of the data composition for each training phase. Project details can be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.
[28.12.2024 12:39] Mistral response. {"id": "8a516381d3b1441c8c754e971033c594", "object": "chat.completion", "created": 1735389538, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aYuLan-Mini\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u8be5\u6a21\u578b\u67092.42B\u53c2\u6570\uff0c\u6027\u80fd\u5353\u8d8a\u3002\u7814\u7a76\u56e2\u961f\u901a\u8fc7\u4e09\u79cd\u6280\u672f\u8d21\u732e\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff1a\u6570\u636e\u7ba1\u9053\u3001\u4f18\u5316\u65b9\u6cd5\u548c\u9000\u706b\u65b9\u6cd5\u3002YuLan-Mini\u57281.08T tokens\u4e0a\u8bad\u7ec3\uff0c\u6027\u80fd\u4e0e\u9700\u8981\u66f4\u591a\u6570\u636e\u7684\u884c\u4e1a\u9886\u5148\u6a21\u578b\u76f8\u5f53\u3002\u9879\u76ee\u7ec6\u8282\u53ef\u4ee5\u5728GitHub\u4e0a\u627e\u5230\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 249, "total_tokens": 384, "completion_tokens": 135}}
[28.12.2024 12:39] Response: 这篇文章介绍了一个名为YuLan-Mini的大型语言模型。该模型有2.42B参数，性能卓越。研究团队通过三种技术贡献提高了训练效率：数据管道、优化方法和退火方法。YuLan-Mini在1.08T tokens上训练，性能与需要更多数据的行业领先模型相当。项目细节可以在GitHub上找到。
[28.12.2024 12:39] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

这篇文章介绍了一个名为YuLan-Mini的大型语言模型。该模型有2.42B参数，性能卓越。研究团队通过三种技术贡献提高了训练效率：数据管道、优化方法和退火方法。YuLan-Mini在1.08T tokens上训练，性能与需要更多数据的行业领先模型相当。项目细节可以在GitHub上找到。
[28.12.2024 12:39] Mistral response. {"id": "51ac4855616d423e89ddded0797255d0", "object": "chat.completion", "created": 1735389541, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aYuLan-Mini\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng ji\u00e8sh\u00e0o le y\u012bg\u00e8 m\u00edngw\u00e8i YuLan-Mini de d\u00e0x\u00edng y\u01d4y\u00e1n m\u00f3x\u00edng.\n\n\u8be5\u6a21\u578b\u67092.42B\u53c2\u6570\uff0c\u6027\u80fd\u5353\u8d8a\u3002\nG\u01cei m\u00f3x\u00edng y\u01d2u 2.42B c\u0101nsh\u00f9, x\u00ecngn\u00e9ng zhu\u00f3yu\u00e8.\n\n\u7814\u7a76\u56e2\u961f\u901a\u8fc7\u4e09\u79cd\u6280\u672f\u8d21\u732e\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff1a\u6570\u636e\u7ba1\u9053\u3001\u4f18\u5316\u65b9\u6cd5\u548c\u9000\u706b\u65b9\u6cd5\u3002\nY\u00e1nji\u016b tu\u00e1ndu\u00ec t\u014dnggu\u00f2 s\u0101n zh\u01d2ng j\u00ecsh\u00f9 g\u00f2ngxi\u00e0n t\u00edg\u0101o le x\u00f9nli\u00e0n xi\u00e0ol\u01dc: sh\u00f9j\u00f9 gu\u01cend\u00e0o, y\u014duhu\u00e0 f\u0101ngf\u01ce h\u00e9 tu\u00echu\u01d2 f\u0101ngf\u01ce.\n\nYuLan-Mini\u57281.08T tokens\u4e0a\u8bad\u7ec3\uff0c\u6027\u80fd\u4e0e\u9700\u8981\u66f4\u591a\u6570\u636e\u7684\u884c\u4e1a\u9886\u5148\u6a21\u578b\u76f8\u5f53\u3002\nYuLan-Mini z\u00e0i 1.08T tokens sh\u00e0ng x\u00f9nli\u00e0n, x\u00ecngn\u00e9ng y\u01d4 x\u016by\u00e0o g\u00e8ng du\u014d sh\u00f9j\u00f9 de h\u00e1ngy\u00e8 l\u01d0ngxi\u0101n m\u00f3x\u00edng xi\u0101ngd\u0101ng.\n\n\u9879\u76ee\u7ec6\u8282\u53ef\u4ee5\u5728GitHub\u4e0a\u627e\u5230\u3002\nXi\u00e0ngm\u00f9 x\u00ecji\u00e9 k\u011by\u01d0 z\u00e0i GitHub sh\u00e0ng zh\u01ceo d\u00e0o."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 150, "total_tokens": 565, "completion_tokens": 415}}
[28.12.2024 12:39] Response: 这篇文章介绍了一个名为YuLan-Mini的大型语言模型。
Zhè piān wénzhāng jièshào le yīgè míngwèi YuLan-Mini de dàxíng yǔyán móxíng.

该模型有2.42B参数，性能卓越。
Gǎi móxíng yǒu 2.42B cānshù, xìngnéng zhuóyuè.

研究团队通过三种技术贡献提高了训练效率：数据管道、优化方法和退火方法。
Yánjiū tuánduì tōngguò sān zhǒng jìshù gòngxiàn tígāo le xùnliàn xiàolǜ: shùjù guǎndào, yōuhuà fāngfǎ hé tuìhuǒ fāngfǎ.

YuLan-Mini在1.08T tokens上训练，性能与需要更多数据的行业领先模型相当。
YuLan-Mini zài 1.08T tokens shàng xùnliàn, xìngnéng yǔ xūyào gèng duō shùjù de hángyè lǐngxiān móxíng xiāngdāng.

项目细节可以在GitHub上找到。
Xiàngmù xìjié kěyǐ zài GitHub shàng zhǎo dào.
[28.12.2024 12:39] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

这篇文章介绍了一个名为YuLan-Mini的大型语言模型。该模型有2.42B参数，性能卓越。研究团队通过三种技术贡献提高了训练效率：数据管道、优化方法和退火方法。YuLan-Mini在1.08T tokens上训练，性能与需要更多数据的行业领先模型相当。项目细节可以在GitHub上找到。
[28.12.2024 12:39] Mistral response. {"id": "dcf885dc20f640418e5b14890d48f1d3", "object": "chat.completion", "created": 1735389549, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u8bed\u8a00\u6a21\u578b\", \"pinyin\": \"y\u01d4y\u00e1n m\u00f3x\u00edng\", \"trans\": \"language model\"},\n    {\"word\": \"\u53c2\u6570\", \"pinyin\": \"c\u0101nsh\u00f9\", \"trans\": \"parameters\"},\n    {\"word\": \"\u6027\u80fd\", \"pinyin\": \"x\u00ecngn\u00e9ng\", \"trans\": \"performance\"},\n    {\"word\": \"\u5353\u8d8a\", \"pinyin\": \"zhu\u00f3yu\u00e8\", \"trans\": \"outstanding\"},\n    {\"word\": \"\u7814\u7a76\u56e2\u961f\", \"pinyin\": \"y\u00e1nji\u016b tu\u00e1ndu\u00ec\", \"trans\": \"research team\"},\n    {\"word\": \"\u6280\u672f\u8d21\u732e\", \"pinyin\": \"j\u00ecsh\u00f9 g\u00f2ngxi\u00e0n\", \"trans\": \"technical contributions\"},\n    {\"word\": \"\u63d0\u9ad8\", \"pinyin\": \"t\u00edg\u0101o\", \"trans\": \"improve\"},\n    {\"word\": \"\u6548\u7387\", \"pinyin\": \"xi\u00e0ol\u01dc\", \"trans\": \"efficiency\"},\n    {\"word\": \"\u6570\u636e\u7ba1\u9053\", \"pinyin\": \"sh\u00f9j\u00f9 gu\u01cend\u00e0o\", \"trans\": \"data pipeline\"},\n    {\"word\": \"\u4f18\u5316\u65b9\u6cd5\", \"pinyin\": \"y\u014duhu\u00e0 f\u0101ngf\u01ce\", \"trans\": \"optimization methods\"},\n    {\"word\": \"\u9000\u706b\u65b9\u6cd5\", \"pinyin\": \"tu\u00echu\u01d2 f\u0101ngf\u01ce\", \"trans\": \"annealing methods\"},\n    {\"word\": \"tokens\", \"pinyin\": \"tokens\", \"trans\": \"tokens\"},\n    {\"word\": \"\u884c\u4e1a\u9886\u5148\", \"pinyin\": \"h\u00e1ngy\u00e8 l\u01d0ngxi\u0101n\", \"trans\": \"industry-leading\"},\n    {\"word\": \"\u9879\u76ee\u7ec6\u8282\", \"pinyin\": \"xi\u00e0ngm\u00f9 x\u00ecji\u00e9\", \"trans\": \"project details\"},\n    {\"word\": \"GitHub\", \"pinyin\": \"GitHub\", \"trans\": \"GitHub\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 182, "total_tokens": 662, "completion_tokens": 480}}
[28.12.2024 12:39] Response: [
    {"word": "语言模型", "pinyin": "yǔyán móxíng", "trans": "language model"},
    {"word": "参数", "pinyin": "cānshù", "trans": "parameters"},
    {"word": "性能", "pinyin": "xìngnéng", "trans": "performance"},
    {"word": "卓越", "pinyin": "zhuóyuè", "trans": "outstanding"},
    {"word": "研究团队", "pinyin": "yánjiū tuánduì", "trans": "research team"},
    {"word": "技术贡献", "pinyin": "jìshù gòngxiàn", "trans": "technical contributions"},
    {"word": "提高", "pinyin": "tígāo", "trans": "improve"},
    {"word": "效率", "pinyin": "xiàolǜ", "trans": "efficiency"},
    {"word": "数据管道", "pinyin": "shùjù guǎndào", "trans": "data pipeline"},
    {"word": "优化方法", "pinyin": "yōuhuà fāngfǎ", "trans": "optimization methods"},
    {"word": "退火方法", "pinyin": "tuìhuǒ fāngfǎ", "trans": "annealing methods"},
    {"word": "tokens", "pinyin": "tokens", "trans": "tokens"},
    {"word": "行业领先", "pinyin": "hángyè lǐngxiān", "trans": "industry-leading"},
    {"word": "项目细节", "pinyin": "xiàngmù xìjié", "trans": "project details"},
    {"word": "GitHub", "pinyin": "GitHub", "trans": "GitHub"}
]
[28.12.2024 12:39] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

这篇文章介绍了一个名为YuLan-Mini的大型语言模型。该模型有2.42B参数，性能卓越。研究团队通过三种技术贡献提高了训练效率：数据管道、优化方法和退火方法。YuLan-Mini在1.08T tokens上训练，性能与需要更多数据的行业领先模型相当。项目细节可以在GitHub上找到。
[28.12.2024 12:39] Mistral response. {"id": "27e95af155334c568ceb09545ed919bb", "object": "chat.completion", "created": 1735389559, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article introduces a large language model named YuLan-Mini. The model has 2.42B parameters and delivers outstanding performance. The research team enhanced training efficiency through three technical contributions: data pipelines, optimization methods, and annealing techniques. YuLan-Mini was trained on 1.08T tokens and performs comparably to industry-leading models that require more data. Project details can be found on GitHub."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 148, "total_tokens": 243, "completion_tokens": 95}}
[28.12.2024 12:39] Response: This article introduces a large language model named YuLan-Mini. The model has 2.42B parameters and delivers outstanding performance. The research team enhanced training efficiency through three technical contributions: data pipelines, optimization methods, and annealing techniques. YuLan-Mini was trained on 1.08T tokens and performs comparably to industry-leading models that require more data. Project details can be found on GitHub.
[28.12.2024 12:39] Renaming data file.
[28.12.2024 12:39] Renaming previous data. hf_papers.json to ./d/2024-12-27.json
[28.12.2024 12:39] Saving new data file.
[28.12.2024 12:39] Generating page.
[28.12.2024 12:39] Renaming previous page.
[28.12.2024 12:39] Renaming previous data. index.html to ./d/2024-12-27.html
[28.12.2024 12:39] [Experimental] Generating Chinese page for reading.
[28.12.2024 12:39] Chinese vocab [{'word': '语言模型', 'pinyin': 'yǔyán móxíng', 'trans': 'language model'}, {'word': '参数', 'pinyin': 'cānshù', 'trans': 'parameters'}, {'word': '性能', 'pinyin': 'xìngnéng', 'trans': 'performance'}, {'word': '卓越', 'pinyin': 'zhuóyuè', 'trans': 'outstanding'}, {'word': '研究团队', 'pinyin': 'yánjiū tuánduì', 'trans': 'research team'}, {'word': '技术贡献', 'pinyin': 'jìshù gòngxiàn', 'trans': 'technical contributions'}, {'word': '提高', 'pinyin': 'tígāo', 'trans': 'improve'}, {'word': '效率', 'pinyin': 'xiàolǜ', 'trans': 'efficiency'}, {'word': '数据管道', 'pinyin': 'shùjù guǎndào', 'trans': 'data pipeline'}, {'word': '优化方法', 'pinyin': 'yōuhuà fāngfǎ', 'trans': 'optimization methods'}, {'word': '退火方法', 'pinyin': 'tuìhuǒ fāngfǎ', 'trans': 'annealing methods'}, {'word': 'tokens', 'pinyin': 'tokens', 'trans': 'tokens'}, {'word': '行业领先', 'pinyin': 'hángyè lǐngxiān', 'trans': 'industry-leading'}, {'word': '项目细节', 'pinyin': 'xiàngmù xìjié', 'trans': 'project details'}, {'word': 'GitHub', 'pinyin': 'GitHub', 'trans': 'GitHub'}]
[28.12.2024 12:39] Renaming previous Chinese page.
[28.12.2024 12:39] Renaming previous data. zh.html to ./d/2024-12-27_zh_reading_task.html
[28.12.2024 12:39] Writing Chinese reading task.
[28.12.2024 12:39] Writing result.
[28.12.2024 12:39] Renaming log file.
[28.12.2024 12:39] Renaming previous data. log.txt to ./logs/2024-12-28_last_log.txt
