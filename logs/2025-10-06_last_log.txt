[06.10.2025 13:24] Read previous papers.
[06.10.2025 13:24] Generating top page (month).
[06.10.2025 13:24] Writing top page (month).
[06.10.2025 14:12] Read previous papers.
[06.10.2025 14:12] Get feed.
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01141
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00515
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00938
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01068
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02665
[06.10.2025 14:12] Extract page data from URL. URL: https://huggingface.co/papers/2509.23202
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26354
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03194
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22033
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03120
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01879
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03204
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01354
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03230
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01459
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26388
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25771
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03232
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03160
[06.10.2025 14:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.02880
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02571
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01698
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01132
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24975
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00658
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25944
[06.10.2025 14:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.02375
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25122
[06.10.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23291
[06.10.2025 14:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.10.2025 14:12] No deleted papers detected.
[06.10.2025 14:12] Downloading and parsing papers (pdf, html). Total: 29.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.01141.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.01141.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.01141.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.00515.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.00515.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.00515.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.00938.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.00938.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.00938.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.01068.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.01068.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.01068.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.02665.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.02665.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.02665.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2509.23202.
[06.10.2025 14:12] Downloading paper 2509.23202 from http://arxiv.org/pdf/2509.23202v1...
[06.10.2025 14:12] Extracting affiliations from text.
[06.10.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 2 0 2 3 2 . 9 0 5 2 : r BRIDGING THE GAP BETWEEN PROMISE AND PERFORMANCE FOR MICROSCALING FP4 QUANTIZATION Roberto L. Castro ISTA & Red Hat AI Vage Egiazarian ISTA Denis Kuznedelev Yandex Research Andrei Panferov ISTA Eldar Kurtic ISTA & Red Hat AI Saleh Ashkboos ETH Zürich Torsten Hoefler ETH Zürich Dan Alistarh ISTA & Red Hat AI "
[06.10.2025 14:12] Response: ```python
["ISTA & Red Hat AI", "Yandex Research", "ETH Zürich"]
```
[06.10.2025 14:12] Deleting PDF ./assets/pdf/2509.23202.pdf.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2509.26354.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2509.26354.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2509.26354.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.03194.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.03194.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.03194.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2509.22033.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2509.22033.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2509.22033.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.03120.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.03120.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.03120.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.01879.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.01879.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.01879.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.03204.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.03204.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.03204.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.01354.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.01354.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.01354.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.03230.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.03230.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.03230.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.01459.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.01459.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.01459.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2509.26388.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2509.26388.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2509.26388.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2509.25771.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2509.25771.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2509.25771.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.03232.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.03232.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.03232.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.03160.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.03160.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.03160.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.02880.
[06.10.2025 14:12] Downloading paper 2510.02880 from http://arxiv.org/pdf/2510.02880v1...
[06.10.2025 14:12] Extracting affiliations from text.
[06.10.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 ] . [ 1 0 8 8 2 0 . 0 1 5 2 : r Preprint. Tianren Ma, Mu Zhang, Yibing Wang, Qixiang Ye University of Chinese Academy of Sciences matianren18@mails.ucas.ac.cn;qxye@ucas.ac.cn "
[06.10.2025 14:12] Response: ```python
["University of Chinese Academy of Sciences"]
```
[06.10.2025 14:12] Deleting PDF ./assets/pdf/2510.02880.pdf.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.02571.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.02571.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.02571.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.01698.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.01698.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.01698.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.01132.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.01132.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.01132.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2509.24975.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2509.24975.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2509.24975.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.00658.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2510.00658.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2510.00658.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2509.25944.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2509.25944.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2509.25944.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2510.02375.
[06.10.2025 14:12] Downloading paper 2510.02375 from http://arxiv.org/pdf/2510.02375v1...
[06.10.2025 14:12] Extracting affiliations from text.
[06.10.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Pretraining with hierarchical memories: separating long-tail and common knowledge Hadi Pouransari, David Grangier, Thomas, Michael Kirchhof, Oncel Tuzel Apple The impressive performance gains of modern language models currently rely on scaling parameters: larger models store more world knowledge and reason better. Yet compressing all world knowledge into parameters is unnecessary, as only fraction is used per prompt, and impractical for edge devices with limited inference-time memory and compute. We address this shortcoming by memory-augmented architecture and pretraining strategy aligned with existing hardware paradigms. We introduce small language models that access large hierarchical parametric memory banks encoding world knowledge. During pretraining and inference, we fetch small, context-dependent memory block and add it to the model. Our pretraining learns to store long-tail world knowledge in the memory parameters, while the small language model acts as an anchor capturing common knowledge and general reasoning abilities. Through trillion-token-scale experiments, we show significant gains: 160M-parameters model augmented with an 18M-parameters memory fetched from 4.6B memory bank obtains comparable performance to regular model with more than 2 the parameters. Through extensive experiments, we study the optimal type and size of parametric memories in transformers, scaling them to over 21B parameters. We find that our proposed hierarchical feed-forward memories work robustly across transformer architectures, whether added during pretraining or post-hoc. Correspondence: Hadi Pouransari: mpouransari@apple.com Date: October 6, 5 2 0 2 9 2 ] . [ 1 5 7 3 2 0 . 0 1 5 2 : r Figure 1 Left: Schematic of pretraining-with-memories: some parameters are always used (anchor parameters), others are fetched per input document (memory parameters). Middle: Accuracy improvement over baseline when 10% of parameters are allocated as memories for knowledge-intensive task (predicting"
[06.10.2025 14:12] Response: ```python
["Apple"]
```
[06.10.2025 14:12] Deleting PDF ./assets/pdf/2510.02375.pdf.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2509.25122.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2509.25122.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2509.25122.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2509.23291.
[06.10.2025 14:12] Extra JSON file exists (./assets/json/2509.23291.json), skip PDF parsing.
[06.10.2025 14:12] Paper image links file exists (./assets/img_data/2509.23291.json), skip HTML parsing.
[06.10.2025 14:12] Success.
[06.10.2025 14:12] Enriching papers with extra data.
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 0. A 15-billion parameter multimodal reasoning model achieves competitive performance through a progressive training methodology without reinforcement learning, demonstrating efficient use of computational resources.  					AI-generated summary 				 We present Apriel-1.5-15B-Thinker, a 15-billion parame...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 1. EPIC, a progressive learning framework, improves the efficiency of multi-modal large models by reducing training difficulty through token and layer consistency distillation during visual token compression.  					AI-generated summary 				 Visual tokens consume substantial computational resources in m...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 2. RECAP, a reinforcement learning method, enhances the safety and robustness of large reasoning models by teaching them to override flawed reasoning and maintain safety without additional training costs.  					AI-generated summary 				 Large reasoning models (LRMs) "think" by generating structured cha...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 3. General Policy Composition (GPC) enhances robotic control performance by combining pre-trained diffusion-based policies without additional training, leading to superior results across various benchmarks.  					AI-generated summary 				 Diffusion-based models for robotic control, including vision-lan...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 4. A survey of self-improvement methods in Multimodal Large Language Models (MLLMs) from data collection, organization, and model optimization perspectives.  					AI-generated summary 				 Recent advancements in self-improvement for Large Language Models (LLMs) have efficiently enhanced model capabilit...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 5. A new quantization method, Micro-Rotated-GPTQ, addresses the challenges of 4-bit floating-point formats MXFP4 and NVFP4, achieving high performance and accuracy in large language model inference.  					AI-generated summary 				 The recent hardware-accelerated microscaling 4-bit floating-point format...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 6. Self-evolving agents based on Large Language Models can deviate in unintended ways, leading to various risks such as safety misalignment and vulnerability introduction, necessitating new safety paradigms.  					AI-generated summary 				 Advances in Large Language Models (LLMs) have enabled a new cla...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 7. CoDA, a multi-agent system using specialized LLM agents, enhances visualization automation by managing data complexity and ensuring high-quality visualizations through collaborative workflows.  					AI-generated summary 				 Deep research has revolutionized data analysis, yet data scientists still d...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 8. Orthogonal Sparse Autoencoders (OrtSAE) mitigate feature absorption and composition by enforcing orthogonality, leading to better feature discovery and improved performance on spurious correlation removal.  					AI-generated summary 				 Sparse autoencoders (SAEs) are a technique for sparse decompos...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 9. A new evaluation framework, SurveyBench, assesses the quality of automatically generated academic surveys using a quiz-driven approach, revealing deficiencies in current LLM4Survey methods.  					AI-generated summary 				 Academic survey writing, which distills vast literature into a coherent and in...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 10. REPAIR is a lifelong editing framework for large language models that enhances editing accuracy and reduces knowledge forgetting through progressive adaptive intervention and reintegration.  					AI-generated summary 				 Post-training for large language models (LLMs) is constrained by the high cost...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 11. FocusAgent uses a lightweight LLM retriever to extract relevant content from web page observations, improving efficiency and security in web agents.  					AI-generated summary 				 Web agents powered by large language models (LLMs) must process lengthy web page observations to complete user goals; t...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 12. A comprehensive benchmark study evaluates the detection of prompt injection attacks against web agents, revealing that current detectors perform well against explicit attacks but struggle with subtle ones.  					AI-generated summary 				 Multiple prompt injection attacks have been proposed against w...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 13. Explicit coordinate markers and improved spatial encoding enhance GUI grounding accuracy across diverse resolutions and platforms.  					AI-generated summary 				 GUI grounding, the task of mapping natural-language instructions to pixel coordinates, is crucial for autonomous agents, yet remains diff...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 14. Length-aware Sampling for Policy Optimization (LSPO) is a meta-RLVR algorithm that dynamically selects training data based on response length, improving learning effectiveness in large language models.  					AI-generated summary 				 Since the release of Deepseek-R1, reinforcement learning with veri...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 15. The Game-Time Benchmark evaluates the temporal dynamics and real-time interaction capabilities of conversational spoken language models, highlighting performance gaps in instruction-following and synchronized responses.  					AI-generated summary 				 Conversational Spoken Language Models (SLMs) are...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 16. A new framework, Text Preference Optimization (TPO), aligns text-to-image models with human preferences without requiring paired image preference data, improving text-to-image alignment and human preference scores.  					AI-generated summary 				 Recent advances in diffusion-based text-to-image (T2I...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 17. LEAML, a label-efficient adaptation framework, enhances MLLMs for specialized domains by generating pseudo question-answer pairs and selectively updating relevant neurons, outperforming standard fine-tuning with minimal supervision.  					AI-generated summary 				 Multimodal Large Language Models (M...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 18. SpineMed, an ecosystem with SpineMed-450k and SpineBench, addresses the lack of level-aware, multimodal datasets and benchmarks for AI-assisted diagnosis of spine disorders, improving model performance through fine-grained, level-specific reasoning.  					AI-generated summary 				 Spine disorders af...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 19. MaskGRPO addresses challenges in optimizing discrete diffusion models with rewards through effective importance sampling and modality-specific adaptations, improving reasoning and generation quality.  					AI-generated summary 				 Optimizing discrete diffusion model (DDM) with rewards remains a cha...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 20. A framework for uncertainty quantification in generative video models is introduced, including a metric for calibration, a black-box method called S-QUBED, and a benchmark dataset, demonstrating improved uncertainty estimates and task accuracy.  					AI-generated summary 				 Generative video models...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 21. A unified LLM-based music recommendation system with tool calling integrates various retrieval methods to enhance user intent interpretation and recommendation performance.  					AI-generated summary 				 While the recent developments in large language models (LLMs) have successfully enabled generat...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 22. Research identifies key design choices for training large language models as agents via multi-turn reinforcement learning, focusing on environment complexity, reward sparsity, and policy methods.  					AI-generated summary 				 We study what actually works and what doesn't for training large languag...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 23. DiffTester is an acceleration framework for diffusion LLMs in unit test generation, improving efficiency without sacrificing test quality by identifying and leveraging common structural patterns.  					AI-generated summary 				 Software development relies heavily on extensive unit testing, which mak...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 24. Align Your Tangent (AYT) improves Consistency Model training by reducing oscillatory tangents and enabling faster convergence with small batch sizes.  					AI-generated summary 				 With diffusion and flow matching models achieving state-of-the-art generating performance, the interest of the communi...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 25. NuRisk, a comprehensive VQA dataset, addresses the lack of spatio-temporal reasoning in current VLMs for autonomous driving by providing agent-level risk annotations in sequential images, improving accuracy and reducing latency.  					AI-generated summary 				 Understanding risk in autonomous drivin...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 26. A memory-augmented architecture with hierarchical parametric memory banks improves language model performance while reducing parameter size and computational requirements.  					AI-generated summary 				 The impressive performance gains of modern language models currently rely on scaling parameters:...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 27. Triangle Splatting+ optimizes triangles within a differentiable framework for real-time, high-fidelity 3D scene reconstruction and novel view synthesis, compatible with standard graphics engines.  					AI-generated summary 				 Reconstructing 3D scenes and synthesizing novel views has seen rapid pro...
[06.10.2025 14:12] ********************************************************************************
[06.10.2025 14:12] Abstract 28. Policy Reasoning Traces (PRT) enhance LLMs' policy compliance assessment by providing detailed reasoning chains, improving accuracy and policy clause citation.  					AI-generated summary 				 Policy compliance assessment is a fundamental task of evaluating whether an input case strictly complies wit...
[06.10.2025 14:12] Read previous papers.
[06.10.2025 14:12] Generating reviews via LLM API.
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#reasoning", "#architecture", "#agi", "#dataset", "#training", "#multimodal", "#inference", "#open_source"], "emoji": "🧠", "ru": {"title": "Эффективное мультимодальное мышление без избыточных ресурсов", "desc": "Представлена модель Apriel-1.5-15B-Thinker с 15 миллиардами параметров,
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture", "#multimodal"], "emoji": "🎯", "ru": {"title": "Прогрессивное сжатие визуальных токенов через дистилляцию", "desc": "EPIC — это фреймворк для эффективного обучения мультимодальных LLM, который решает проблему больших вычислительных затрат
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#training", "#rl", "#security", "#rlhf", "#reasoning", "#alignment"], "emoji": "🛡️", "ru": {"title": "Обучение AI моделей переосмысливать ошибочные рассуждения для безопасности", "desc": "Исследователи представили RECAP — метод обучения с подкреплением для больших reasoning моделей,
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#training", "#robotics", "#optimization", "#benchmark", "#diffusion", "#agents"], "emoji": "🤝", "ru": {"title": "Композиция policy без обучения превосходит отдельные модели", "desc": "Статья представляет метод General Policy Composition (GPC), который позволяет улучшить производител
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#training", "#survey", "#optimization", "#multimodal", "#data", "#dataset"], "emoji": "🔄", "ru": {"title": "Мультимодальные LLM учатся сами: обзор методов самосовершенствования", "desc": "Статья представляет первый комплексный обзор методов самосовершенствования мультимодальных LLM.
[06.10.2025 14:12] Querying the API.
[06.10.2025 14:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A new quantization method, Micro-Rotated-GPTQ, addresses the challenges of 4-bit floating-point formats MXFP4 and NVFP4, achieving high performance and accuracy in large language model inference.  					AI-generated summary 				 The recent hardware-accelerated microscaling 4-bit floating-point formats such as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to revolutionize large language model (LLM) inference. Yet, their practical benefits remain unproven. We present the first comprehensive study of MXFP4 and NVFP4 for post-training quantization, revealing gaps between their promise and real-world performance. Our analysis shows that state-of-the-art methods struggle with FP4, due to two key issues: (1) NVFP4's small group size provably neutralizes traditional outlier mitigation techniques; (2) MXFP4's power-of-two scale quantization severely degrades accuracy due to high induced error. To bridge this gap, we introduce Micro-Rotated-GPTQ (MR-GPTQ), a variant of the classic GPTQ quantization algorithm that tailors the quantization process to FP4's unique properties, by using block-wise Hadamard transforms and format-specific optimizations. We support our proposal with a set of high-performance GPU kernels that enable the MR-GPTQ format with negligible overhead, by rotation fusion into the weights, and fast online computation of the activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and 2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on RTX5090. Our extensive empirical evaluation demonstrates that MR-GPTQ matches or outperforms state-of-the-art accuracy, significantly boosting MXFP4, to the point where it nears that of NVFP4. We conclude that, while FP4 is not an automatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock a new frontier of accuracy-performance trade-offs.
[06.10.2025 14:12] Response: ```json
{
  "desc": "Исследователи представили Micro-Rotated-GPTQ — новый метод квантизации для 4-битных форматов с плавающей точкой MXFP4 и NVFP4, которые поддерживаются на GPU NVIDIA и AMD. Традиционные методы плохо работают с FP4 из-за малого размера групп в NVFP4 и проблем с квантизацией масштаба в MXFP4. MR-GPTQ использует блочные преобразования Адамара и специальные оптимизации для каждого формата, достигая ускорения до 3.6x на уровне слоёв и до 2.2x end-to-end на NVIDIA B200. Метод показывает, что FP4 форматы могут конкурировать с INT4 при правильной специализации алгоритма квантизации.",
  "emoji": "🔄",
  "title": "Микро-вращения раскрывают потенциал 4-битных форматов для LLM"
}
```
[06.10.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new quantization method, Micro-Rotated-GPTQ, addresses the challenges of 4-bit floating-point formats MXFP4 and NVFP4, achieving high performance and accuracy in large language model inference.  					AI-generated summary 				 The recent hardware-accelerated microscaling 4-bit floating-point formats such as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to revolutionize large language model (LLM) inference. Yet, their practical benefits remain unproven. We present the first comprehensive study of MXFP4 and NVFP4 for post-training quantization, revealing gaps between their promise and real-world performance. Our analysis shows that state-of-the-art methods struggle with FP4, due to two key issues: (1) NVFP4's small group size provably neutralizes traditional outlier mitigation techniques; (2) MXFP4's power-of-two scale quantization severely degrades accuracy due to high induced error. To bridge this gap, we introduce Micro-Rotated-GPTQ (MR-GPTQ), a variant of the classic GPTQ quantization algorithm that tailors the quantization process to FP4's unique properties, by using block-wise Hadamard transforms and format-specific optimizations. We support our proposal with a set of high-performance GPU kernels that enable the MR-GPTQ format with negligible overhead, by rotation fusion into the weights, and fast online computation of the activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and 2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on RTX5090. Our extensive empirical evaluation demonstrates that MR-GPTQ matches or outperforms state-of-the-art accuracy, significantly boosting MXFP4, to the point where it nears that of NVFP4. We conclude that, while FP4 is not an automatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock a new frontier of accuracy-performance trade-offs."

[06.10.2025 14:12] Response: ```python
["INFERENCE", "TRAINING"]
```
[06.10.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new quantization method, Micro-Rotated-GPTQ, addresses the challenges of 4-bit floating-point formats MXFP4 and NVFP4, achieving high performance and accuracy in large language model inference.  					AI-generated summary 				 The recent hardware-accelerated microscaling 4-bit floating-point formats such as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to revolutionize large language model (LLM) inference. Yet, their practical benefits remain unproven. We present the first comprehensive study of MXFP4 and NVFP4 for post-training quantization, revealing gaps between their promise and real-world performance. Our analysis shows that state-of-the-art methods struggle with FP4, due to two key issues: (1) NVFP4's small group size provably neutralizes traditional outlier mitigation techniques; (2) MXFP4's power-of-two scale quantization severely degrades accuracy due to high induced error. To bridge this gap, we introduce Micro-Rotated-GPTQ (MR-GPTQ), a variant of the classic GPTQ quantization algorithm that tailors the quantization process to FP4's unique properties, by using block-wise Hadamard transforms and format-specific optimizations. We support our proposal with a set of high-performance GPU kernels that enable the MR-GPTQ format with negligible overhead, by rotation fusion into the weights, and fast online computation of the activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and 2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on RTX5090. Our extensive empirical evaluation demonstrates that MR-GPTQ matches or outperforms state-of-the-art accuracy, significantly boosting MXFP4, to the point where it nears that of NVFP4. We conclude that, while FP4 is not an automatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock a new frontier of accuracy-performance trade-offs."

[06.10.2025 14:12] Response: ```python
["OPTIMIZATION"]
```
[06.10.2025 14:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces a new quantization method called Micro-Rotated-GPTQ (MR-GPTQ) that improves the performance and accuracy of large language model inference using 4-bit floating-point formats, specifically MXFP4 and NVFP4. It identifies challenges with existing methods that struggle with these formats, such as ineffective outlier mitigation and accuracy degradation due to quantization errors. MR-GPTQ leverages block-wise Hadamard transforms and optimizations tailored to the unique properties of FP4, resulting in significant speed improvements on NVIDIA GPUs. The empirical results show that MR-GPTQ not only enhances the performance of MXFP4 but also achieves accuracy levels comparable to NVFP4, demonstrating its potential in optimizing inference for large language models.","title":"Unlocking Performance with Micro-Rotated-GPTQ for 4-bit Inference"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces a new quantization method called Micro-Rotated-GPTQ (MR-GPTQ) that improves the performance and accuracy of large language model inference using 4-bit floating-point formats, specifically MXFP4 and NVFP4. It identifies challenges with existing methods that struggle with these formats, such as ineffective outlier mitigation and accuracy degradation due to quantization errors. MR-GPTQ leverages block-wise Hadamard transforms and optimizations tailored to the unique properties of FP4, resulting in significant speed improvements on NVIDIA GPUs. The empirical results show that MR-GPTQ not only enhances the performance of MXFP4 but also achieves accuracy levels comparable to NVFP4, demonstrating its potential in optimizing inference for large language models.', title='Unlocking Performance with Micro-Rotated-GPTQ for 4-bit Inference'))
[06.10.2025 14:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新的量化方法，称为Micro-Rotated-GPTQ，旨在解决4位浮点格式MXFP4和NVFP4在大语言模型推理中的挑战。研究表明，现有的量化方法在FP4格式下表现不佳，主要由于NVFP4的小组大小和MXFP4的二次幂量化导致的高误差。Micro-Rotated-GPTQ通过使用分块Hadamard变换和特定格式的优化，针对FP4的独特特性调整量化过程，从而提高了性能和准确性。实验结果显示，MR-GPTQ在NVIDIA B200和RTX5090上实现了显著的速度提升，并且在准确性上与最先进的方法相当或更优。","title":"量化新方法：Micro-Rotated-GPTQ提升大语言模型性能"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新的量化方法，称为Micro-Rotated-GPTQ，旨在解决4位浮点格式MXFP4和NVFP4在大语言模型推理中的挑战。研究表明，现有的量化方法在FP4格式下表现不佳，主要由于NVFP4的小组大小和MXFP4的二次幂量化导致的高误差。Micro-Rotated-GPTQ通过使用分块Hadamard变换和特定格式的优化，针对FP4的独特特性调整量化过程，从而提高了性能和准确性。实验结果显示，MR-GPTQ在NVIDIA B200和RTX5090上实现了显著的速度提升，并且在准确性上与最先进的方法相当或更优。', title='量化新方法：Micro-Rotated-GPTQ提升大语言模型性能'))
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#alignment", "#ethics", "#agents", "#security", "#safety", "#rl"], "emoji": "🧬", "ru": {"title": "Когда AI-агенты эволюционируют в неправильную сторону", "desc": "Исследование изучает новый тип рисков в самообучающихся агентах на основе LLM, которые автономно улучшаются через взаимо
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#agents", "#optimization", "#data", "#interpretability", "#multimodal"], "emoji": "🤝", "ru": {"title": "Команда AI-агентов для автоматической визуализации данных", "desc": "Статья представляет CoDA — мультиагентную систему на основе LLM, которая автоматизирует создание визуализаций 
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture"], "emoji": "⊥", "ru": {"title": "Ортогональность против запутанности: как разделить признаки нейросети", "desc": "Исследователи представили Orthogonal SAE (OrtSAE) — улучшенную версию sparse autoencoders для разложения активаций нейронных
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#survey", "#benchmark"], "emoji": "📊", "ru": {"title": "SurveyBench: бенчмарк для проверки AI-генерации научных обзоров через викторины", "desc": "Исследователи представили SurveyBench — новый фреймворк для оценки качества автоматически сгенерированных научных обзоров с помощью LLM.
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#inference"], "emoji": "🔧", "ru": {"title": "Непрерывное обучение языковых моделей без забывания знаний", "desc": "В статье представлен фреймворк REPAIR для редактирования больших языковых моделей (LLM), который позволяет исправлять ошибки и добавлять н
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#long_context", "#inference", "#benchmark", "#agents", "#security", "#reasoning"], "emoji": "🎯", "ru": {"title": "Фокусировка внимания веб-агентов для эффективности и безопасности", "desc": "FocusAgent — это подход для создания веб-агентов на основе LLM, который использует лёгкий re
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#agents", "#dataset", "#benchmark", "#security"], "emoji": "🕵️", "ru": {"title": "Детекторы инъекций промптов не справляются с тонкими атаками на веб-агентов", "desc": "Исследователи провели первое комплексное тестирование методов обнаружения атак типа prompt injection на веб-агенто
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#interpretability", "#agents", "#cv", "#optimization"], "emoji": "🎯", "ru": {"title": "Явные координаты вместо угадывания: как научить модели точно находить элементы интерфейса", "desc": "Статья посвящена проблеме GUI grounding — задаче сопоставления текстовых инструкций с координат
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#optimization", "#rlhf", "#training", "#rl", "#reasoning"], "emoji": "📏", "ru": {"title": "Учёт длины ответов для эффективного обучения LLM", "desc": "В статье представлен LSPO — мета-алгоритм обучения с подкреплением, который динамически выбирает обучающие данные на основе длины от
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#audio", "#alignment", "#benchmark", "#games"], "emoji": "⏱️", "ru": {"title": "Когда AI не попадает в такт: тестируем разговорные модели на чувство времени", "desc": "Исследователи представили Game-Time Benchmark — новый бенчмарк для оценки способности разговорных речевых language 
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#open_source", "#alignment", "#benchmark", "#rlhf", "#multimodal", "#diffusion"], "emoji": "🎯", "ru": {"title": "Бесплатное выравнивание: обучение без парных предпочтений", "desc": "Представлен фреймворк Text Preference Optimization (TPO) для выравнивания text-to-image моделей с чел
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#optimization", "#data", "#dataset", "#multimodal", "#training", "#transfer_learning", "#healthcare"], "emoji": "🎯", "ru": {"title": "Эффективная адаптация мультимодальных LLM с минимальной разметкой", "desc": "LEAML — это фреймворк для адаптации мультимодальных больших языковых мод
[06.10.2025 14:12] Using data from previous issue: {"categories": ["#reasoning", "#training", "#healthcare", "#benchmark", "#dataset", "#multimodal", "#science"], "emoji": "🦴", "ru": {"title": "SpineMed: AI-система для точной диагностики позвоночника на уровне отдельных позвонков", "desc": "Статья представляет SpineMed — экосистему для AI-диагностик
[06.10.2025 14:12] Querying the API.
[06.10.2025 14:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MaskGRPO addresses challenges in optimizing discrete diffusion models with rewards through effective importance sampling and modality-specific adaptations, improving reasoning and generation quality.  					AI-generated summary 				 Optimizing discrete diffusion model (DDM) with rewards remains a challenge: the non-autoregressive paradigm makes importance sampling intractable and rollout complex, puzzling reinforcement learning methods such as Group Relative Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first viable approach to enable scalable multimodal reinforcement learning in discrete diffusion with effective importance sampling and modality-specific adaptations. To this end, we first clarify the theoretical foundation for DDMs, which facilitates building an importance estimator that captures valuable token fluctuation for gradient updates. We then delicately tailored the rollout method for visual sequences, which yields diverse completions and reliable optimization gradients. Upon math reasoning, coding, and visual generation benchmarks, MaskGRPO brings more stable and efficient updates, leading to stronger reasoning performance and better generation quality. This study establishes MaskGRPO as a systematic policy optimization approach and the first practical way for discretized visual diffusion.
[06.10.2025 14:12] Response: ```json
{
  "title": "Эффективная оптимизация дискретных диффузионных моделей через обучение с подкреплением",
  "desc": "Статья представляет MaskGRPO — первый практический метод применения reinforcement learning к дискретным диффузионным моделям (DDM). Ключевая проблема заключалась в том, что неавторегрессивная природа DDM делала importance sampling неосуществимым, что мешало использованию стандартных методов оптимизации политик. Авторы разработали теоретическую основу для корректной оценки importance weights и адаптировали метод rollout специально для визуальных последовательностей. Эксперименты показали улучшение качества в задачах математического рассуждения, программирования и генерации изображений.",
  "emoji": "🎭"
}
```
[06.10.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MaskGRPO addresses challenges in optimizing discrete diffusion models with rewards through effective importance sampling and modality-specific adaptations, improving reasoning and generation quality.  					AI-generated summary 				 Optimizing discrete diffusion model (DDM) with rewards remains a challenge: the non-autoregressive paradigm makes importance sampling intractable and rollout complex, puzzling reinforcement learning methods such as Group Relative Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first viable approach to enable scalable multimodal reinforcement learning in discrete diffusion with effective importance sampling and modality-specific adaptations. To this end, we first clarify the theoretical foundation for DDMs, which facilitates building an importance estimator that captures valuable token fluctuation for gradient updates. We then delicately tailored the rollout method for visual sequences, which yields diverse completions and reliable optimization gradients. Upon math reasoning, coding, and visual generation benchmarks, MaskGRPO brings more stable and efficient updates, leading to stronger reasoning performance and better generation quality. This study establishes MaskGRPO as a systematic policy optimization approach and the first practical way for discretized visual diffusion."

[06.10.2025 14:12] Response: ```python
['RL', 'RLHF', 'MULTIMODAL', 'BENCHMARK', 'MATH', 'ARCHITECTURE']
```
[06.10.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MaskGRPO addresses challenges in optimizing discrete diffusion models with rewards through effective importance sampling and modality-specific adaptations, improving reasoning and generation quality.  					AI-generated summary 				 Optimizing discrete diffusion model (DDM) with rewards remains a challenge: the non-autoregressive paradigm makes importance sampling intractable and rollout complex, puzzling reinforcement learning methods such as Group Relative Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first viable approach to enable scalable multimodal reinforcement learning in discrete diffusion with effective importance sampling and modality-specific adaptations. To this end, we first clarify the theoretical foundation for DDMs, which facilitates building an importance estimator that captures valuable token fluctuation for gradient updates. We then delicately tailored the rollout method for visual sequences, which yields diverse completions and reliable optimization gradients. Upon math reasoning, coding, and visual generation benchmarks, MaskGRPO brings more stable and efficient updates, leading to stronger reasoning performance and better generation quality. This study establishes MaskGRPO as a systematic policy optimization approach and the first practical way for discretized visual diffusion."

[06.10.2025 14:12] Response: ```python
['OPTIMIZATION', 'REASONING', 'DIFFUSION']
```
[06.10.2025 14:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MaskGRPO is a novel method designed to enhance the optimization of discrete diffusion models (DDMs) using rewards. It tackles the difficulties of importance sampling and rollout in non-autoregressive settings, which are common in reinforcement learning. By developing a robust importance estimator and a tailored rollout strategy for visual sequences, MaskGRPO improves the quality of reasoning and generation in multimodal tasks. This approach not only stabilizes updates but also leads to superior performance in various benchmarks, marking a significant advancement in policy optimization for discrete visual diffusion.","title":"MaskGRPO: Revolutionizing Discrete Diffusion Model Optimization"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MaskGRPO is a novel method designed to enhance the optimization of discrete diffusion models (DDMs) using rewards. It tackles the difficulties of importance sampling and rollout in non-autoregressive settings, which are common in reinforcement learning. By developing a robust importance estimator and a tailored rollout strategy for visual sequences, MaskGRPO improves the quality of reasoning and generation in multimodal tasks. This approach not only stabilizes updates but also leads to superior performance in various benchmarks, marking a significant advancement in policy optimization for discrete visual diffusion.', title='MaskGRPO: Revolutionizing Discrete Diffusion Model Optimization'))
[06.10.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MaskGRPO 解决了在优化离散扩散模型时面临的挑战，特别是在使用奖励进行优化时。通过有效的重要性采样和特定模态的适应，MaskGRPO 提高了推理和生成的质量。我们首先阐明了离散扩散模型的理论基础，以便构建一个能够捕捉有价值的标记波动的重要性估计器。最终，MaskGRPO 在数学推理、编码和视觉生成基准测试中表现出更稳定和高效的更新，提升了推理性能和生成质量。","title":"MaskGRPO：优化离散扩散模型的新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MaskGRPO 解决了在优化离散扩散模型时面临的挑战，特别是在使用奖励进行优化时。通过有效的重要性采样和特定模态的适应，MaskGRPO 提高了推理和生成的质量。我们首先阐明了离散扩散模型的理论基础，以便构建一个能够捕捉有价值的标记波动的重要性估计器。最终，MaskGRPO 在数学推理、编码和视觉生成基准测试中表现出更稳定和高效的更新，提升了推理性能和生成质量。', title='MaskGRPO：优化离散扩散模型的新方法'))
[06.10.2025 14:13] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#hallucinations", "#dataset", "#video"], "emoji": "🎬", "ru": {"title": "Когда AI не уверен в своём видео", "desc": "Исследователи представили первый фреймворк для количественной оценки неопределённости в генеративных видеомоделях, которые, как и LLM, с
[06.10.2025 14:13] Using data from previous issue: {"categories": ["#games", "#multimodal", "#rag", "#interpretability"], "emoji": "🎵", "ru": {"title": "LLM как дирижёр музыкальных рекомендаций", "desc": "Исследователи создали систему музыкальных рекомендаций на основе LLM, которая использует механизм вызова инструментов (tool calling) для интеграци
[06.10.2025 14:13] Using data from previous issue: {"categories": ["#agents", "#games", "#reasoning", "#rlhf", "#training", "#rl", "#optimization"], "emoji": "🤖", "ru": {"title": "Рецепт обучения языковых моделей как агентов через reinforcement learning", "desc": "Исследование систематизирует ключевые факторы для обучения LLM в роли агентов через mu
[06.10.2025 14:13] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#dataset", "#plp", "#benchmark", "#open_source", "#training"], "emoji": "⚡", "ru": {"title": "Ускорение генерации unit-тестов через структурные паттерны", "desc": "DiffTester - это фреймворк для ускорения diffusion LLM при генерации unit-тестов. Ключев
[06.10.2025 14:13] Using data from previous issue: {"categories": ["#training", "#diffusion", "#optimization"], "emoji": "🎯", "ru": {"title": "Выравнивание градиентов для быстрого обучения Consistency Models", "desc": "Consistency Models (CMs) позволяют генерировать изображения за один-два шага, но требуют долгого обучения с большими батчами. Авторы
[06.10.2025 14:13] Using data from previous issue: {"categories": ["#reasoning", "#games", "#cv", "#dataset", "#training", "#benchmark"], "emoji": "🚗", "ru": {"title": "Обучение понимать риски на дороге во времени и пространстве", "desc": "Исследователи представили NuRisk — датасет для оценки пространственно-временного рассуждения Vision Language Mo
[06.10.2025 14:13] Querying the API.
[06.10.2025 14:13] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A memory-augmented architecture with hierarchical parametric memory banks improves language model performance while reducing parameter size and computational requirements.  					AI-generated summary 				 The impressive performance gains of modern language models currently rely on scaling parameters: larger models store more world knowledge and reason better. Yet compressing all world knowledge into parameters is unnecessary, as only a fraction is used per prompt, and impractical for edge devices with limited inference-time memory and compute. We address this shortcoming by a memory-augmented architecture and a pretraining strategy aligned with existing hardware paradigms. We introduce small language models that access large hierarchical parametric memory banks encoding world knowledge. During pretraining and inference, we fetch a small, context-dependent memory block and add it to the model. Our pretraining learns to store long-tail world knowledge in the memory parameters, while the small language model acts as an anchor capturing common knowledge and general reasoning abilities. Through trillion-token-scale experiments, we show significant gains: a 160M-parameters model augmented with an 18M-parameters memory fetched from a 4.6B memory bank obtains comparable performance to a regular model with more than 2x the parameters. Through extensive experiments, we study the optimal type and size of parametric memories in transformers, scaling them to over 21B parameters. We find that our proposed hierarchical feed-forward memories work robustly across transformer architectures, whether added during pretraining or post-hoc.
[06.10.2025 14:13] Response: ```json
{
  "title": "Иерархическая память вместо раздутых параметров",
  "desc": "Исследователи предлагают архитектуру языковых моделей с внешними банками параметрической памяти, которая позволяет хранить знания отдельно от основной модели. Маленькая LLM на 160M параметров с доступом к памяти размером 4.6B показывает результаты, сравнимые с обычной моделью в два раза большего размера. Иерархическая структура памяти хранит редкие факты о мире, в то время как компактная модель фокусируется на общих знаниях и логике. Подход особенно полезен для edge-устройств с ограниченными ресурсами, так как загружается только небольшой блок памяти в зависимости от контекста.",
  "emoji": "🗄️"
}
```
[06.10.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A memory-augmented architecture with hierarchical parametric memory banks improves language model performance while reducing parameter size and computational requirements.  					AI-generated summary 				 The impressive performance gains of modern language models currently rely on scaling parameters: larger models store more world knowledge and reason better. Yet compressing all world knowledge into parameters is unnecessary, as only a fraction is used per prompt, and impractical for edge devices with limited inference-time memory and compute. We address this shortcoming by a memory-augmented architecture and a pretraining strategy aligned with existing hardware paradigms. We introduce small language models that access large hierarchical parametric memory banks encoding world knowledge. During pretraining and inference, we fetch a small, context-dependent memory block and add it to the model. Our pretraining learns to store long-tail world knowledge in the memory parameters, while the small language model acts as an anchor capturing common knowledge and general reasoning abilities. Through trillion-token-scale experiments, we show significant gains: a 160M-parameters model augmented with an 18M-parameters memory fetched from a 4.6B memory bank obtains comparable performance to a regular model with more than 2x the parameters. Through extensive experiments, we study the optimal type and size of parametric memories in transformers, scaling them to over 21B parameters. We find that our proposed hierarchical feed-forward memories work robustly across transformer architectures, whether added during pretraining or post-hoc."

[06.10.2025 14:13] Response: ```python
['ARCHITECTURE', 'SMALL_MODELS', 'TRAINING']
```
[06.10.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A memory-augmented architecture with hierarchical parametric memory banks improves language model performance while reducing parameter size and computational requirements.  					AI-generated summary 				 The impressive performance gains of modern language models currently rely on scaling parameters: larger models store more world knowledge and reason better. Yet compressing all world knowledge into parameters is unnecessary, as only a fraction is used per prompt, and impractical for edge devices with limited inference-time memory and compute. We address this shortcoming by a memory-augmented architecture and a pretraining strategy aligned with existing hardware paradigms. We introduce small language models that access large hierarchical parametric memory banks encoding world knowledge. During pretraining and inference, we fetch a small, context-dependent memory block and add it to the model. Our pretraining learns to store long-tail world knowledge in the memory parameters, while the small language model acts as an anchor capturing common knowledge and general reasoning abilities. Through trillion-token-scale experiments, we show significant gains: a 160M-parameters model augmented with an 18M-parameters memory fetched from a 4.6B memory bank obtains comparable performance to a regular model with more than 2x the parameters. Through extensive experiments, we study the optimal type and size of parametric memories in transformers, scaling them to over 21B parameters. We find that our proposed hierarchical feed-forward memories work robustly across transformer architectures, whether added during pretraining or post-hoc."

[06.10.2025 14:13] Response: ```python
["OPTIMIZATION", "AGI"]
```
[06.10.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel memory-augmented architecture that enhances the performance of language models while minimizing their size and computational demands. Instead of relying solely on large parameters, the model utilizes hierarchical parametric memory banks to store and retrieve world knowledge efficiently. During both pretraining and inference, the model accesses small, context-specific memory blocks, allowing it to leverage extensive knowledge without the need for a massive parameter count. The results demonstrate that a smaller model with memory augmentation can achieve performance comparable to much larger models, showcasing the effectiveness of this approach in optimizing language model capabilities.","title":"Memory Augmentation: Boosting Language Models with Less!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a novel memory-augmented architecture that enhances the performance of language models while minimizing their size and computational demands. Instead of relying solely on large parameters, the model utilizes hierarchical parametric memory banks to store and retrieve world knowledge efficiently. During both pretraining and inference, the model accesses small, context-specific memory blocks, allowing it to leverage extensive knowledge without the need for a massive parameter count. The results demonstrate that a smaller model with memory augmentation can achieve performance comparable to much larger models, showcasing the effectiveness of this approach in optimizing language model capabilities.', title='Memory Augmentation: Boosting Language Models with Less!'))
[06.10.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"这篇论文提出了一种增强记忆的架构，利用分层参数记忆库来提高语言模型的性能，同时减少参数规模和计算需求。现代语言模型的优异表现通常依赖于参数的扩展，但将所有世界知识压缩到参数中是不必要的，因为每次提示只使用其中的一小部分。我们通过一种记忆增强架构和与现有硬件相适应的预训练策略来解决这一问题。实验表明，使用小型语言模型结合大型记忆库，可以在保持较少参数的情况下，获得与更大模型相当的性能。","title":"记忆增强架构：小模型，大智慧"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='这篇论文提出了一种增强记忆的架构，利用分层参数记忆库来提高语言模型的性能，同时减少参数规模和计算需求。现代语言模型的优异表现通常依赖于参数的扩展，但将所有世界知识压缩到参数中是不必要的，因为每次提示只使用其中的一小部分。我们通过一种记忆增强架构和与现有硬件相适应的预训练策略来解决这一问题。实验表明，使用小型语言模型结合大型记忆库，可以在保持较少参数的情况下，获得与更大模型相当的性能。', title='记忆增强架构：小模型，大智慧'))
[06.10.2025 14:13] Using data from previous issue: {"categories": ["#3d", "#games", "#optimization"], "emoji": "🔺", "ru": {"title": "Треугольники вместо гауссиан: прямая оптимизация mesh'ей для real-time 3D рендеринга", "desc": "Triangle Splatting+ представляет новый подход к 3D реконструкции сцен, который напрямую оптимизирует треугольники в диффер
[06.10.2025 14:13] Using data from previous issue: {"categories": ["#alignment", "#training", "#rlhf", "#reasoning"], "emoji": "⚖️", "ru": {"title": "Цепочки рассуждений для проверки соответствия политикам", "desc": "Статья представляет метод Policy Reasoning Traces (PRT) для улучшения способности LLM оценивать соответствие входных данных заданным п
[06.10.2025 14:13] Renaming data file.
[06.10.2025 14:13] Renaming previous data. hf_papers.json to ./d/2025-10-06.json
[06.10.2025 14:13] Saving new data file.
[06.10.2025 14:13] Generating page.
[06.10.2025 14:13] Renaming previous page.
[06.10.2025 14:13] Renaming previous data. index.html to ./d/2025-10-06.html
[06.10.2025 14:13] Writing result.
[06.10.2025 14:13] Renaming log file.
[06.10.2025 14:13] Renaming previous data. log.txt to ./logs/2025-10-06_last_log.txt
