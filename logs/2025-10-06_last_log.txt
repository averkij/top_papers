[06.10.2025 04:13] Read previous papers.
[06.10.2025 04:13] Generating top page (month).
[06.10.2025 04:13] Writing top page (month).
[06.10.2025 05:12] Read previous papers.
[06.10.2025 05:12] Get feed.
[06.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01068
[06.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02665
[06.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01879
[06.10.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2509.26354
[06.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03204
[06.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03120
[06.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03230
[06.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03160
[06.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02571
[06.10.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.01459
[06.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25771
[06.10.2025 05:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.10.2025 05:12] No deleted papers detected.
[06.10.2025 05:12] Downloading and parsing papers (pdf, html). Total: 11.
[06.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.01068.
[06.10.2025 05:12] Extra JSON file exists (./assets/json/2510.01068.json), skip PDF parsing.
[06.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.01068.json), skip HTML parsing.
[06.10.2025 05:12] Success.
[06.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.02665.
[06.10.2025 05:12] Extra JSON file exists (./assets/json/2510.02665.json), skip PDF parsing.
[06.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.02665.json), skip HTML parsing.
[06.10.2025 05:12] Success.
[06.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.01879.
[06.10.2025 05:12] Extra JSON file exists (./assets/json/2510.01879.json), skip PDF parsing.
[06.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.01879.json), skip HTML parsing.
[06.10.2025 05:12] Success.
[06.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.26354.
[06.10.2025 05:12] Downloading paper 2509.26354 from http://arxiv.org/pdf/2509.26354v1...
[06.10.2025 05:12] Extracting affiliations from text.
[06.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 3 ] . [ 1 4 5 3 6 2 . 9 0 5 2 : r Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents Shuai Shao1,2, Qihan Ren1,2 , Chen Qian1,3, Boyi Wei4, Dadi Guo5, Jingyi Yang6, Xinhao Song2, Linfeng Zhang2, Weinan Zhang2, Dongrui Liu1, Jing Shao1 1Shanghai Artificial Intelligence Laboratory 2Shanghai Jiao Tong University 3Renmin University of China 4Princeton University 5Hong Kong University of Science and Technology 6Fudan University {shaoshuai.ederson,renqihan}@sjtu.edu.cn {liudongrui,shaojing}@pjlab.org.cn "
[06.10.2025 05:12] Response: ```python
[
    "Shanghai Artificial Intelligence Laboratory",
    "Shanghai Jiao Tong University",
    "Renmin University of China",
    "Princeton University",
    "Hong Kong University of Science and Technology",
    "Fudan University"
]
```
[06.10.2025 05:12] Deleting PDF ./assets/pdf/2509.26354.pdf.
[06.10.2025 05:12] Success.
[06.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.03204.
[06.10.2025 05:12] Extra JSON file exists (./assets/json/2510.03204.json), skip PDF parsing.
[06.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.03204.json), skip HTML parsing.
[06.10.2025 05:12] Success.
[06.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.03120.
[06.10.2025 05:12] Extra JSON file exists (./assets/json/2510.03120.json), skip PDF parsing.
[06.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.03120.json), skip HTML parsing.
[06.10.2025 05:12] Success.
[06.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.03230.
[06.10.2025 05:12] Extra JSON file exists (./assets/json/2510.03230.json), skip PDF parsing.
[06.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.03230.json), skip HTML parsing.
[06.10.2025 05:12] Success.
[06.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.03160.
[06.10.2025 05:12] Extra JSON file exists (./assets/json/2510.03160.json), skip PDF parsing.
[06.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.03160.json), skip HTML parsing.
[06.10.2025 05:12] Success.
[06.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.02571.
[06.10.2025 05:12] Extra JSON file exists (./assets/json/2510.02571.json), skip PDF parsing.
[06.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.02571.json), skip HTML parsing.
[06.10.2025 05:12] Success.
[06.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.01459.
[06.10.2025 05:12] Downloading paper 2510.01459 from http://arxiv.org/pdf/2510.01459v1...
[06.10.2025 05:12] Extracting affiliations from text.
[06.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 ] . [ 1 9 5 4 1 0 . 0 1 5 2 : r Preprint LSPO: LENGTH-AWARE DYNAMIC SAMPLING FOR POLICY OPTIMIZATION IN LLM REASONING Weizhe Chen1, Sven Koenig2, Bistra Dilkina1 1University of Southern California, 2 University of California, Irvine weizhech@usc.edu, sven.koenig@uci.edu, dilkina@usc.edu "
[06.10.2025 05:12] Response: ```python
["University of Southern California", "University of California, Irvine"]
```
[06.10.2025 05:12] Deleting PDF ./assets/pdf/2510.01459.pdf.
[06.10.2025 05:12] Success.
[06.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.25771.
[06.10.2025 05:12] Extra JSON file exists (./assets/json/2509.25771.json), skip PDF parsing.
[06.10.2025 05:12] Paper image links file exists (./assets/img_data/2509.25771.json), skip HTML parsing.
[06.10.2025 05:12] Success.
[06.10.2025 05:12] Enriching papers with extra data.
[06.10.2025 05:12] ********************************************************************************
[06.10.2025 05:12] Abstract 0. General Policy Composition (GPC) enhances robotic control performance by combining pre-trained diffusion-based policies without additional training, leading to superior results across various benchmarks.  					AI-generated summary 				 Diffusion-based models for robotic control, including vision-lan...
[06.10.2025 05:12] ********************************************************************************
[06.10.2025 05:12] Abstract 1. A survey of self-improvement methods in Multimodal Large Language Models (MLLMs) from data collection, organization, and model optimization perspectives.  					AI-generated summary 				 Recent advancements in self-improvement for Large Language Models (LLMs) have efficiently enhanced model capabilit...
[06.10.2025 05:12] ********************************************************************************
[06.10.2025 05:12] Abstract 2. REPAIR is a lifelong editing framework for large language models that enhances editing accuracy and reduces knowledge forgetting through progressive adaptive intervention and reintegration.  					AI-generated summary 				 Post-training for large language models (LLMs) is constrained by the high cost...
[06.10.2025 05:12] ********************************************************************************
[06.10.2025 05:12] Abstract 3. Self-evolving agents based on Large Language Models can deviate in unintended ways, leading to various risks such as safety misalignment and vulnerability introduction, necessitating new safety paradigms.  					AI-generated summary 				 Advances in Large Language Models (LLMs) have enabled a new cla...
[06.10.2025 05:12] ********************************************************************************
[06.10.2025 05:12] Abstract 4. FocusAgent uses a lightweight LLM retriever to extract relevant content from web page observations, improving efficiency and security in web agents.  					AI-generated summary 				 Web agents powered by large language models (LLMs) must process lengthy web page observations to complete user goals; t...
[06.10.2025 05:12] ********************************************************************************
[06.10.2025 05:12] Abstract 5. A new evaluation framework, SurveyBench, assesses the quality of automatically generated academic surveys using a quiz-driven approach, revealing deficiencies in current LLM4Survey methods.  					AI-generated summary 				 Academic survey writing, which distills vast literature into a coherent and in...
[06.10.2025 05:12] ********************************************************************************
[06.10.2025 05:12] Abstract 6. Explicit coordinate markers and improved spatial encoding enhance GUI grounding accuracy across diverse resolutions and platforms.  					AI-generated summary 				 GUI grounding, the task of mapping natural-language instructions to pixel coordinates, is crucial for autonomous agents, yet remains diff...
[06.10.2025 05:12] ********************************************************************************
[06.10.2025 05:12] Abstract 7. SpineMed, an ecosystem with SpineMed-450k and SpineBench, addresses the lack of level-aware, multimodal datasets and benchmarks for AI-assisted diagnosis of spine disorders, improving model performance through fine-grained, level-specific reasoning.  					AI-generated summary 				 Spine disorders af...
[06.10.2025 05:12] ********************************************************************************
[06.10.2025 05:12] Abstract 8. A framework for uncertainty quantification in generative video models is introduced, including a metric for calibration, a black-box method called S-QUBED, and a benchmark dataset, demonstrating improved uncertainty estimates and task accuracy.  					AI-generated summary 				 Generative video models...
[06.10.2025 05:12] ********************************************************************************
[06.10.2025 05:12] Abstract 9. Length-aware Sampling for Policy Optimization (LSPO) is a meta-RLVR algorithm that dynamically selects training data based on response length, improving learning effectiveness in large language models.  					AI-generated summary 				 Since the release of Deepseek-R1, reinforcement learning with veri...
[06.10.2025 05:12] ********************************************************************************
[06.10.2025 05:12] Abstract 10. A new framework, Text Preference Optimization (TPO), aligns text-to-image models with human preferences without requiring paired image preference data, improving text-to-image alignment and human preference scores.  					AI-generated summary 				 Recent advances in diffusion-based text-to-image (T2I...
[06.10.2025 05:12] Read previous papers.
[06.10.2025 05:12] Generating reviews via LLM API.
[06.10.2025 05:12] Using data from previous issue: {"categories": ["#training", "#robotics", "#optimization", "#benchmark", "#diffusion", "#agents"], "emoji": "🤝", "ru": {"title": "Композиция policy без обучения превосходит отдельные модели", "desc": "Статья представляет метод General Policy Composition (GPC), который позволяет улучшить производител
[06.10.2025 05:12] Using data from previous issue: {"categories": ["#training", "#survey", "#optimization", "#multimodal", "#data", "#dataset"], "emoji": "🔄", "ru": {"title": "Мультимодальные LLM учатся сами: обзор методов самосовершенствования", "desc": "Статья представляет первый комплексный обзор методов самосовершенствования мультимодальных LLM.
[06.10.2025 05:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#inference"], "emoji": "🔧", "ru": {"title": "Непрерывное обучение языковых моделей без забывания знаний", "desc": "В статье представлен фреймворк REPAIR для редактирования больших языковых моделей (LLM), который позволяет исправлять ошибки и добавлять н
[06.10.2025 05:12] Querying the API.
[06.10.2025 05:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Self-evolving agents based on Large Language Models can deviate in unintended ways, leading to various risks such as safety misalignment and vulnerability introduction, necessitating new safety paradigms.  					AI-generated summary 				 Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature.
[06.10.2025 05:12] Response: ```json
{
  "desc": "Исследование изучает новый тип рисков в самообучающихся агентах на основе LLM, которые автономно улучшаются через взаимодействие со средой. Авторы вводят концепцию «мисэволюции» — когда самостоятельная эволюция агента отклоняется в нежелательном направлении, приводя к вредным последствиям. Эксперименты показали, что эта проблема затрагивает даже агентов на базе передовых LLM (таких как Gemini-2.5-Pro) по четырём эволюционным направлениям: модель, память, инструменты и рабочий процесс. Работа подчёркивает необходимость разработки новых парадигм безопасности для самоэволюционирующих AI-агентов и предлагает потенциальные стратегии смягчения рисков.",
  "emoji": "🧬",
  "title": "Когда AI-агенты эволюционируют в неправильную сторону"
}
```
[06.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Self-evolving agents based on Large Language Models can deviate in unintended ways, leading to various risks such as safety misalignment and vulnerability introduction, necessitating new safety paradigms.  					AI-generated summary 				 Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature."

[06.10.2025 05:12] Response: ```python
['AGENTS', 'RL', 'SAFETY']
```
[06.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Self-evolving agents based on Large Language Models can deviate in unintended ways, leading to various risks such as safety misalignment and vulnerability introduction, necessitating new safety paradigms.  					AI-generated summary 				 Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature."

[06.10.2025 05:12] Response: ```python
['ALIGNMENT', 'ETHICS', 'SECURITY']
```
[06.10.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates the concept of \'misevolution\' in self-evolving agents powered by Large Language Models (LLMs). Misevolution refers to unintended deviations during the self-improvement process, which can lead to safety misalignment and the introduction of vulnerabilities. The authors evaluate misevolution across four pathways: model, memory, tool, and workflow, revealing that even advanced LLMs can experience significant risks. The study emphasizes the urgent need for new safety frameworks to address these challenges and proposes potential strategies for creating safer self-evolving agents.","title":"Understanding Misevolution: The Hidden Risks of Self-Evolving AI"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper investigates the concept of 'misevolution' in self-evolving agents powered by Large Language Models (LLMs). Misevolution refers to unintended deviations during the self-improvement process, which can lead to safety misalignment and the introduction of vulnerabilities. The authors evaluate misevolution across four pathways: model, memory, tool, and workflow, revealing that even advanced LLMs can experience significant risks. The study emphasizes the urgent need for new safety frameworks to address these challenges and proposes potential strategies for creating safer self-evolving agents.", title='Understanding Misevolution: The Hidden Risks of Self-Evolving AI'))
[06.10.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本研究探讨了基于大型语言模型（LLM）的自我进化代理可能出现的意外偏差，称之为“误进化”。这种误进化可能导致安全不对齐和引入脆弱性等风险，亟需新的安全范式。我们系统地评估了误进化的四个关键路径：模型、记忆、工具和工作流程。研究结果表明，误进化是一个普遍存在的风险，影响到即使是顶级LLM构建的代理，强调了构建更安全和可信的自我进化代理的必要性。","title":"自我进化代理的误进化风险与安全挑战"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本研究探讨了基于大型语言模型（LLM）的自我进化代理可能出现的意外偏差，称之为“误进化”。这种误进化可能导致安全不对齐和引入脆弱性等风险，亟需新的安全范式。我们系统地评估了误进化的四个关键路径：模型、记忆、工具和工作流程。研究结果表明，误进化是一个普遍存在的风险，影响到即使是顶级LLM构建的代理，强调了构建更安全和可信的自我进化代理的必要性。', title='自我进化代理的误进化风险与安全挑战'))
[06.10.2025 05:12] Using data from previous issue: {"categories": ["#long_context", "#inference", "#benchmark", "#agents", "#security", "#reasoning"], "emoji": "🎯", "ru": {"title": "Фокусировка внимания веб-агентов для эффективности и безопасности", "desc": "FocusAgent — это подход для создания веб-агентов на основе LLM, который использует лёгкий re
[06.10.2025 05:12] Using data from previous issue: {"categories": ["#survey", "#benchmark"], "emoji": "📊", "ru": {"title": "SurveyBench: бенчмарк для проверки AI-генерации научных обзоров через викторины", "desc": "Исследователи представили SurveyBench — новый фреймворк для оценки качества автоматически сгенерированных научных обзоров с помощью LLM.
[06.10.2025 05:12] Using data from previous issue: {"categories": ["#interpretability", "#agents", "#cv", "#optimization"], "emoji": "🎯", "ru": {"title": "Явные координаты вместо угадывания: как научить модели точно находить элементы интерфейса", "desc": "Статья посвящена проблеме GUI grounding — задаче сопоставления текстовых инструкций с координат
[06.10.2025 05:12] Using data from previous issue: {"categories": ["#reasoning", "#training", "#healthcare", "#benchmark", "#dataset", "#multimodal", "#science"], "emoji": "🦴", "ru": {"title": "SpineMed: AI-система для точной диагностики позвоночника на уровне отдельных позвонков", "desc": "Статья представляет SpineMed — экосистему для AI-диагностик
[06.10.2025 05:12] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#hallucinations", "#dataset", "#video"], "emoji": "🎬", "ru": {"title": "Когда AI не уверен в своём видео", "desc": "Исследователи представили первый фреймворк для количественной оценки неопределённости в генеративных видеомоделях, которые, как и LLM, с
[06.10.2025 05:12] Querying the API.
[06.10.2025 05:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Length-aware Sampling for Policy Optimization (LSPO) is a meta-RLVR algorithm that dynamically selects training data based on response length, improving learning effectiveness in large language models.  					AI-generated summary 				 Since the release of Deepseek-R1, reinforcement learning with verifiable rewards (RLVR) has become a central approach for training large language models (LLMs) on reasoning tasks. Recent work has largely focused on modifying loss functions to make RLVR more efficient and effective. In this paper, motivated by studies of overthinking in LLMs, we propose Length-aware Sampling for Policy Optimization (LSPO), a novel meta-RLVR algorithm that dynamically selects training data at each step based on the average response length. We evaluate LSPO across multiple base models and datasets, demonstrating that it consistently improves learning effectiveness. In addition, we conduct a detailed ablation study to examine alternative ways of incorporating length signals into dynamic sampling, offering further insights and highlighting promising directions for future research.
[06.10.2025 05:12] Response: ```json
{
  "desc": "В статье представлен LSPO — мета-алгоритм обучения с подкреплением, который динамически выбирает обучающие данные на основе длины ответов модели. Это помогает бороться с проблемой «overthinking» (избыточных рассуждений) в больших языковых моделях при решении задач, требующих reasoning. Алгоритм показывает стабильное улучшение эффективности обучения на разных базовых моделях и датасетах по сравнению со стандартным RLVR. Авторы также проводят детальный ablation study различных способов использования информации о длине ответов для динамического сэмплирования данных.",
  "emoji": "📏",
  "title": "Учёт длины ответов для эффективного обучения LLM"
}
```
[06.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Length-aware Sampling for Policy Optimization (LSPO) is a meta-RLVR algorithm that dynamically selects training data based on response length, improving learning effectiveness in large language models.  					AI-generated summary 				 Since the release of Deepseek-R1, reinforcement learning with verifiable rewards (RLVR) has become a central approach for training large language models (LLMs) on reasoning tasks. Recent work has largely focused on modifying loss functions to make RLVR more efficient and effective. In this paper, motivated by studies of overthinking in LLMs, we propose Length-aware Sampling for Policy Optimization (LSPO), a novel meta-RLVR algorithm that dynamically selects training data at each step based on the average response length. We evaluate LSPO across multiple base models and datasets, demonstrating that it consistently improves learning effectiveness. In addition, we conduct a detailed ablation study to examine alternative ways of incorporating length signals into dynamic sampling, offering further insights and highlighting promising directions for future research."

[06.10.2025 05:12] Response: ```python
['RL', 'RLHF', 'TRAINING']
```
[06.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Length-aware Sampling for Policy Optimization (LSPO) is a meta-RLVR algorithm that dynamically selects training data based on response length, improving learning effectiveness in large language models.  					AI-generated summary 				 Since the release of Deepseek-R1, reinforcement learning with verifiable rewards (RLVR) has become a central approach for training large language models (LLMs) on reasoning tasks. Recent work has largely focused on modifying loss functions to make RLVR more efficient and effective. In this paper, motivated by studies of overthinking in LLMs, we propose Length-aware Sampling for Policy Optimization (LSPO), a novel meta-RLVR algorithm that dynamically selects training data at each step based on the average response length. We evaluate LSPO across multiple base models and datasets, demonstrating that it consistently improves learning effectiveness. In addition, we conduct a detailed ablation study to examine alternative ways of incorporating length signals into dynamic sampling, offering further insights and highlighting promising directions for future research."

[06.10.2025 05:12] Response: ```python
["OPTIMIZATION", "REASONING"]
```
[06.10.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Length-aware Sampling for Policy Optimization (LSPO) is a new algorithm designed to enhance the training of large language models (LLMs) using reinforcement learning with verifiable rewards (RLVR). It focuses on selecting training data based on the average length of responses, which helps the model learn more effectively. The paper shows that LSPO improves learning outcomes across various models and datasets. Additionally, it includes an ablation study that explores different methods of integrating length information into the sampling process, providing valuable insights for future research.","title":"Optimizing Learning with Length-Aware Sampling"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Length-aware Sampling for Policy Optimization (LSPO) is a new algorithm designed to enhance the training of large language models (LLMs) using reinforcement learning with verifiable rewards (RLVR). It focuses on selecting training data based on the average length of responses, which helps the model learn more effectively. The paper shows that LSPO improves learning outcomes across various models and datasets. Additionally, it includes an ablation study that explores different methods of integrating length information into the sampling process, providing valuable insights for future research.', title='Optimizing Learning with Length-Aware Sampling'))
[06.10.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新的元强化学习算法，称为长度感知采样（LSPO），旨在提高大语言模型的学习效果。LSPO通过动态选择训练数据，依据响应的平均长度来优化策略。我们在多个基础模型和数据集上评估了LSPO，结果表明其在学习效果上具有一致的提升。通过详细的消融研究，我们探讨了将长度信号融入动态采样的其他方法，为未来的研究提供了有价值的见解。","title":"长度感知采样：提升大语言模型学习效果的关键"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新的元强化学习算法，称为长度感知采样（LSPO），旨在提高大语言模型的学习效果。LSPO通过动态选择训练数据，依据响应的平均长度来优化策略。我们在多个基础模型和数据集上评估了LSPO，结果表明其在学习效果上具有一致的提升。通过详细的消融研究，我们探讨了将长度信号融入动态采样的其他方法，为未来的研究提供了有价值的见解。', title='长度感知采样：提升大语言模型学习效果的关键'))
[06.10.2025 05:12] Using data from previous issue: {"categories": ["#open_source", "#alignment", "#benchmark", "#rlhf", "#multimodal", "#diffusion"], "emoji": "🎯", "ru": {"title": "Бесплатное выравнивание: обучение без парных предпочтений", "desc": "Представлен фреймворк Text Preference Optimization (TPO) для выравнивания text-to-image моделей с чел
[06.10.2025 05:12] Renaming data file.
[06.10.2025 05:12] Renaming previous data. hf_papers.json to ./d/2025-10-06.json
[06.10.2025 05:12] Saving new data file.
[06.10.2025 05:12] Generating page.
[06.10.2025 05:12] Renaming previous page.
[06.10.2025 05:12] Renaming previous data. index.html to ./d/2025-10-06.html
[06.10.2025 05:12] Writing result.
[06.10.2025 05:12] Renaming log file.
[06.10.2025 05:12] Renaming previous data. log.txt to ./logs/2025-10-06_last_log.txt
