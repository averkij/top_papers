[06.10.2025 08:17] Read previous papers.
[06.10.2025 08:17] Generating top page (month).
[06.10.2025 08:17] Writing top page (month).
[06.10.2025 09:15] Read previous papers.
[06.10.2025 09:15] Get feed.
[06.10.2025 09:15] Extract page data from URL. URL: https://huggingface.co/papers/2510.01141
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00515
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01068
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02665
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26354
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03120
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03194
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01879
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03204
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03230
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01459
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25771
[06.10.2025 09:15] Extract page data from URL. URL: https://huggingface.co/papers/2510.03232
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03160
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02571
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01354
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01132
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00658
[06.10.2025 09:15] Extract page data from URL. URL: https://huggingface.co/papers/2509.25944
[06.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25122
[06.10.2025 09:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.10.2025 09:15] No deleted papers detected.
[06.10.2025 09:15] Downloading and parsing papers (pdf, html). Total: 20.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.01141.
[06.10.2025 09:15] Downloading paper 2510.01141 from http://arxiv.org/pdf/2510.01141v1...
[06.10.2025 09:15] Extracting affiliations from text.
[06.10.2025 09:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 ] . [ 1 1 4 1 1 0 . 0 1 5 2 : r Apriel-1.5-15B-Thinker: Mid-training is all you need SLAM Lab ServiceNow "
[06.10.2025 09:15] Response: []
[06.10.2025 09:15] Extracting affiliations from text.
[06.10.2025 09:15] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 ] . [ 1 1 4 1 1 0 . 0 1 5 2 : r Apriel-1.5-15B-Thinker: Mid-training is all you need SLAM Lab ServiceNowWe present Apriel-1.5-15B-Thinker, 15-billion parameter open-weights multimodal reasoning model that achieves frontier-level performance through training design rather than sheer scale. Starting from Pixtral-12B, we apply progressive three-stage methodology: (1) depth upscaling to expand reasoning capacity without pretraining from scratch, (2) staged continual pre-training that first develops foundational text and vision understanding, then enhances visual reasoning through targeted synthetic data generation addressing spatial structure, compositional understanding, and fine-grained perception, and (3) high-quality text-only supervised fine-tuning on curated instruction-response pairs with explicit reasoning traces spanning mathematics, coding, science, and tool use. Notably, our model achieves competitive results without reinforcement learning or preference optimization, isolating the contribution of our data-centric continual pre-training approach. On the Artificial Analysis Intelligence Index, Apriel-1.5-15B-Thinker attains score of 52, matching DeepSeek-R1-0528 despite requiring significantly fewer computational resources. Across ten image benchmarks, its performance is on average within five points of Gemini-2.5-Flash and Claude Sonnet-3.7, key achievement for model operating within single-GPU deployment constraints. Our results demonstrate that thoughtful mid-training 2 design can close substantial capability gaps without massive scale, making frontier-level multimodal reasoning accessible to organizations with limited infrastructure. We release the model checkpoint, all training recipes, and evaluation protocols under the MIT license to to advance open-source research.Large language models (LLMs) continue to advance rapidly across general capability, long-context reasoning, and multimodal understanding. Open-weight families such as Qwen [1, 2] and Llama [3, 4] have demonstrated strong, scalable baselines, while proprietary systems like Gemini [5, 6] and Claude [7] have pushed frontier performance across complex reasoning and multimodal tasks. Recent reasoning-first training approaches, exemplified by DeepSeek-R1 [8], reveal that careful data curation and training strategy can unlock sophisticated chain-of-thought competence without relying solely on extreme scale. Yet despite these advances, fundamental tension persists between Contributors are listed in Section 8 2We define mid-training as combination of the continual pretraining and SFT stages Figure 1: Apriel-1.5-15B-Thinker compared to the best open source LLMs on the Artificial Analysis Intelligence Index. capability and accessibility, as these insights have not fully addressed the challenges facing real-world applications. Two critical barriers remain for widespread adoption. First, organizations requiring on-premises or air-gapped deployments for privacy and compliance need compact models with predictable resource footprints that can operate within strict infrastructure constraints. Second, the cost profile spanning both training and inference often becomes the decisive factor in whether frontier-level capability can be deployed at production scale. These practical considerations raise fundamental question: Can compact, open, multimodal model achieve frontier-level reasoning while remaining economical to train and deploy? This work introduces Apriel-1.5-15B-Thinker, 15B-parameter open-weights multimodal reasoning model designed with that guiding question in mind. Our approach centers on the midtraining/continual pretraining phase, where both data selection and staged presentation exert strong influence on downstream reasoning. Concretely, the training corpus spans curated pretraining-style corpora, diverse web-style text and images, reasoning-rich samples, and mix of verified and unverified synthetic data, all introduced through staged curriculum. Our core innovation lies in progressive, cost-effective multimodal training pipeline that effectively scales reasoning capabilities across text and vision through three carefully orchestrated stages: (1) Integrated Multimodal Architecture: Beginning with Pixtral-12B [9] as our foundation, we expand it to model size capable of advanced reasoning across modalities, without requiring pretraining from scratch. (2) Staged Multimodal Continual Pretraining (CPT): We adopt two-phase CPT strategy. The first phase develops foundational text reasoning and broad multimodal capabilities, while the second enhances visual reasoning through synthetic data targeting spatial structure, compositional understanding, and fine-grained perception. This staged progression enables balanced strengthening of both modalities and provides stable foundation for subsequent training stages, even when later stages emphasize narrower set of modalities. (3) High-Quality Supervised Fine-Tuning (SFT): We curate diverse, high-quality, and high-signal set of samples for supervised fine-tuning. Each response includes explicit reasoning traces, enabling the model to learn transparent thought processes. Coupled with the strong base model, this yields frontier-level performance across broad range of reasoning benchmarks without requiring additional post-training. Given computational constraints, the current release focuses on maximizing the potential of the base model through mid-training, without employing reinforcement learning or preference optimization. 2 This design choice also allows for clearer assessment of the contribution of the mid-training recipe itself to the overall performance of the model. The result is compact model tailored to enterprise-friendly deployment constraints (memory, latency, throughput) while still achieving frontier-level reasoning. Apriel-1.5-15B-Thinker attains score of 52 on the Artificial Analysis Intelligence Index, matching DeepSeek-R1-0528 [10] despite requiring significantly fewer computational resources. Across ten multimodal benchmarks, the model demonstrates competitive performance, averaging only 5 points behind Gemini-2.5-Flash and Claude Sonnet-3.7 [6, 7], remarkable achievement for 15B parameter model operating within singleGPU deployment constraints. These empirical results provide compelling evidence that thoughtful continual pre-training with heterogeneous synthetic signals, applied to compact architecture, can close substantial capability gaps without "
[06.10.2025 09:15] Mistral response. {"id": "093f6a99251c4b7588f4f1ff0b29e98f", "created": 1759742116, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1301, "total_tokens": 1319, "completion_tokens": 18}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"SLAM Lab\",\n    \"ServiceNow\"\n]\n```"}}]}
[06.10.2025 09:15] Response: ```python
[
    "SLAM Lab",
    "ServiceNow"
]
```
[06.10.2025 09:15] Deleting PDF ./assets/pdf/2510.01141.pdf.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.00515.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2510.00515.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.00515.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.01068.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2510.01068.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.01068.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.02665.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2510.02665.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.02665.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2509.26354.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2509.26354.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2509.26354.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.03120.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2510.03120.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.03120.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.03194.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2510.03194.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.03194.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.01879.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2510.01879.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.01879.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.03204.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2510.03204.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.03204.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.03230.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2510.03230.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.03230.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.01459.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2510.01459.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.01459.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2509.25771.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2509.25771.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2509.25771.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.03232.
[06.10.2025 09:15] Downloading paper 2510.03232 from http://arxiv.org/pdf/2510.03232v1...
[06.10.2025 09:15] Extracting affiliations from text.
[06.10.2025 09:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models Ci-Siang Lin1 Yu-Chiang Frank Wang1,2 1Graduate Institute of Communication Engineering, National Taiwan University, Taiwan 2NVIDIA Min-Hung Chen2 Yu-Yang Sheng1 5 2 0 2 ] . [ 1 2 3 2 3 0 . 0 1 5 2 : r Abstract Multimodal Large Language Models (MLLMs) have achieved strong performance on general visual benchmarks but struggle with out-of-distribution (OOD) tasks in specialized domains such as medical imaging, where labeled data is limited and expensive. We introduce LEAML, labelefficient adaptation framework that leverages both scarce labeled VQA samples and abundant unlabeled images. Our approach generates domain-relevant pseudo question-answer pairs for unlabeled data using QA generator regularized by caption distillation. Importantly, we selectively update only those neurons most relevant to question-answering, enabling the QA Generator to efficiently acquire domainspecific knowledge during distillation. Experiments on gastrointestinal endoscopy and sports VQA demonstrate that LEAML consistently outperforms standard fine-tuning under minimal supervision, highlighting the effectiveness of our proposed LEAML framework. Introduction Large Language Models (LLMs) (Touvron et al. 2023; Achiam et al. 2023; Bai et al. 2023) have demonstrated impressive capabilities across diverse language tasks. By incorporating visual understanding, Multimodal Large Language Models (MLLMs) (Alayrac et al. 2022; Liu et al. 2023a; Wang et al. 2024a) extend these capabilities to visual question answering (VQA), image captioning, and multimodal reasoning tasks. (Wang et al. 2024b) Recent MLLMs have achieved remarkable performance on general visual benchmarks, showing strong generalization within their training distributions. Although scaling laws (Kaplan et al. 2020) show promise for enhancing model performance, they do not eliminate dependence on pre-training data, which limits generali"
[06.10.2025 09:15] Response: ```python
[
    "Graduate Institute of Communication Engineering, National Taiwan University, Taiwan",
    "NVIDIA"
]
```
[06.10.2025 09:15] Deleting PDF ./assets/pdf/2510.03232.pdf.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.03160.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2510.03160.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.03160.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.02571.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2510.02571.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.02571.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.01354.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2510.01354.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.01354.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.01132.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2510.01132.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.01132.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.00658.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2510.00658.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.00658.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2509.25944.
[06.10.2025 09:15] Downloading paper 2509.25944 from http://arxiv.org/pdf/2509.25944v1...
[06.10.2025 09:15] Extracting affiliations from text.
[06.10.2025 09:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"NuRisk: Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving Yuan Gao, Mattia Piccinini, Roberto Brusnicki, Yuchen Zhang, Johannes Betz https://NuRisk.github.io/ 5 2 0 2 0 3 ] . [ 1 4 4 9 5 2 . 9 0 5 2 : r Abstract Understanding risk in autonomous driving requires not only perception and prediction, but also high-level reasoning about agent behavior and context. Current Vision Language Models (VLMs)-based methods primarily ground agents in static images and provide qualitative judgments, lacking the spatiotemporal reasoning needed to capture how risks evolve over time. To address this gap, we propose NuRisk, comprehensive Visual Question Answering (VQA) dataset comprising 2.9K scenarios and 1.1M agent-level samples, built on real-world data from nuScenes and Waymo, completed with safety-critical scenarios from the CommonRoad simulator. The dataset provides Bird-Eye-View (BEV) based sequential images with quantitative, agent-level risk annotations, enabling spatiotemporal reasoning. We benchmark well-known VLMs across different prompting techniques and find that they fail to perform explicit spatio-temporal reasoning, resulting in peak accuracy of 33% at high latency. To address these shortcomings, our fine-tuned 7B VLM agent improves accuracy to 41% and reduces latency by 75%, demonstrating explicit spatio-temporal reasoning capabilities that proprietary models lacked. While this represents significant step forward, the modest accuracy underscores the profound challenge of the task, establishing NuRisk as critical benchmark for advancing spatio-temporal reasoning in autonomous driving. I. INTRODUCTION Autonomous driving has progressed rapidly, with milestones like Waymos fully autonomous robotaxi services demonstrating SAE Level 4 capabilities in defined urban environments [1], [2]. These achievements are built on either modular software stacks covering perception, prediction, planning, and control [3], or more recently, end-to-end"
[06.10.2025 09:15] Response: ```python
[]
```
[06.10.2025 09:15] Extracting affiliations from text.
[06.10.2025 09:15] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"NuRisk: Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving Yuan Gao, Mattia Piccinini, Roberto Brusnicki, Yuchen Zhang, Johannes Betz https://NuRisk.github.io/ 5 2 0 2 0 3 ] . [ 1 4 4 9 5 2 . 9 0 5 2 : r Abstract Understanding risk in autonomous driving requires not only perception and prediction, but also high-level reasoning about agent behavior and context. Current Vision Language Models (VLMs)-based methods primarily ground agents in static images and provide qualitative judgments, lacking the spatiotemporal reasoning needed to capture how risks evolve over time. To address this gap, we propose NuRisk, comprehensive Visual Question Answering (VQA) dataset comprising 2.9K scenarios and 1.1M agent-level samples, built on real-world data from nuScenes and Waymo, completed with safety-critical scenarios from the CommonRoad simulator. The dataset provides Bird-Eye-View (BEV) based sequential images with quantitative, agent-level risk annotations, enabling spatiotemporal reasoning. We benchmark well-known VLMs across different prompting techniques and find that they fail to perform explicit spatio-temporal reasoning, resulting in peak accuracy of 33% at high latency. To address these shortcomings, our fine-tuned 7B VLM agent improves accuracy to 41% and reduces latency by 75%, demonstrating explicit spatio-temporal reasoning capabilities that proprietary models lacked. While this represents significant step forward, the modest accuracy underscores the profound challenge of the task, establishing NuRisk as critical benchmark for advancing spatio-temporal reasoning in autonomous driving. I. INTRODUCTION Autonomous driving has progressed rapidly, with milestones like Waymos fully autonomous robotaxi services demonstrating SAE Level 4 capabilities in defined urban environments [1], [2]. These achievements are built on either modular software stacks covering perception, prediction, planning, and control [3], or more recently, end-to-end learning-based approaches [4]. However, both paradigms face fundamental limitation: they struggle to handle the variability of real-world driving, particularly the rare, safetycritical corner cases that fall outside their operational design domains (ODDs) [5]. Ensuring trustworthiness requires methods that can reason beyond hand-crafted rules or training data distributions. Recent advances in VLMs offer such potential: by combining scalable, cross-modal knowledge with flexible reasoning, they can improve the coverage of autonomous driving tasks, complementing traditional AD software stacks [6]. Despite this potential, the application of VLMs to autonomous driving has been focused on scenario risk assessment, such as hazard detection [7] or qualitative risk assessment [8]. While these models can identify potential Y. Gao, M. Piccinini, R. Brusnicki, Y. Zhang, and J. Betz are with the Professorship of Autonomous Vehicle Systems, TUM School of Engineering and Design, Technical University of Munich, 85748 Garching, Germany; Munich Institute of Robotics and Machine Intelligence (MIRMI) Fig. 1. Overview of NuRisk: Existing VLM-based risk assessment is typically limited to (i) image-plane grounding and (ii) qualitative assessment. NuRisk introduces an agent-level quantitative dataset that enables (iii) spatialtemporal quantitative assessment for risk reasoning. hazard, they typically lack the quantitative reasoning required for rigorous safety evaluation. For example, VLM can identify vehicle cutting into the ego-cars lane yet offer qualitative assessment like, This is dangerous situation, please drive slowly, which encourages conservative behavior. This raises question: can pre-trained VLMs perform agent-level quantitative risk assessment by leveraging spatiotemporal reasoning to interpret safety metrics, such as the temporal metric Time-to-Collision (TTC) and the spatial metric Distance to Collision (DTC)? Our work, illustrated in Figure 1, confronts this question by proposing Visual Question Answering (VQA) dataset to evaluate this capability. Such quantitative insights are crucial for downstream motion planners to make informed, precise decisions rather than resorting to overly conservative maneuvers. A. Related Work 1) VLMs in Autonomous Driving: Recent studies have explored the integration of VLMs into autonomous driving systems. Surveys such as [6], [9] provide comprehensive overviews of the role of VLMs in tasks including perception, motion planning, scene understanding, and visual reasoning. In the context of scenario analysis, VLMs have shown promising progress. Recently, comprehensive survey [10] summarizes four main application areas of VLMs-based scenario analysis: VQA dataset, scene understanding, benchmark, and risk assessment. 2) VLMs-based Risk Assessment in Autonomous Driving: Current approaches to VLM-based risk assessment can be categorized into two paradigms: prompting-based and finetuning-based methods. Prompting-based approaches adapt the reasoning capabilities of large pre-trained models via instructions. GPT-4V has been applied to structured scene representations for risk scoring with natural language justifications [11], demonstrating the potential for interpretable risk assessment. Similarly, frameworks like LATTE [7] and Ronecker et al. [12] integrate visual foundation models with contextual prompting for hazard detection and anomaly recognition. Fine-tuning strategies like LoRA [13] have emerged to address domain-specific requirements and improve robustness by training model parameters specifically. Think-Driver [14] fine-tunes VLM with chain-of-thought style VQA data with multi-view images for hazard reasoning and maneuver risk evaluation. By decomposing risk assessment into sequential reasoning steps, this approach provides more transparent and auditable decision-making processes compared to direct predictions. Lee et al. [15] adapt VLMs to occlusion-aware BEV representations for uncertainty prediction, while works like INSIGHT [16] enhance hazard localization and interpretability. And LKAlert [17] focuses on failure anticipation by predicting lane-keeping assist failures using multimodal cues. These methods are focused on qualitative agent risk assessment. However, these methods all lack the quantitative labels required for explicit spatio-temporal reasoning, which is gap our NuRisk dataset is precisely designed to fill. 3) VQA datasets in Autonomous Driving: VQA datasets for autonomous driv"
[06.10.2025 09:15] Mistral response. {"id": "8d584a39dc68425395d184f952f468b6", "created": 1759742125, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1355, "total_tokens": 1410, "completion_tokens": 55}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Professorship of Autonomous Vehicle Systems, TUM School of Engineering and Design, Technical University of Munich, 85748 Garching, Germany; Munich Institute of Robotics and Machine Intelligence (MIRMI)\"\n]\n```"}}]}
[06.10.2025 09:15] Response: ```python
[
    "Professorship of Autonomous Vehicle Systems, TUM School of Engineering and Design, Technical University of Munich, 85748 Garching, Germany; Munich Institute of Robotics and Machine Intelligence (MIRMI)"
]
```
[06.10.2025 09:15] Deleting PDF ./assets/pdf/2509.25944.pdf.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2509.25122.
[06.10.2025 09:15] Extra JSON file exists (./assets/json/2509.25122.json), skip PDF parsing.
[06.10.2025 09:15] Paper image links file exists (./assets/img_data/2509.25122.json), skip HTML parsing.
[06.10.2025 09:15] Success.
[06.10.2025 09:15] Enriching papers with extra data.
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 0. A 15-billion parameter multimodal reasoning model achieves competitive performance through a progressive training methodology without reinforcement learning, demonstrating efficient use of computational resources.  					AI-generated summary 				 We present Apriel-1.5-15B-Thinker, a 15-billion parame...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 1. EPIC, a progressive learning framework, improves the efficiency of multi-modal large models by reducing training difficulty through token and layer consistency distillation during visual token compression.  					AI-generated summary 				 Visual tokens consume substantial computational resources in m...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 2. General Policy Composition (GPC) enhances robotic control performance by combining pre-trained diffusion-based policies without additional training, leading to superior results across various benchmarks.  					AI-generated summary 				 Diffusion-based models for robotic control, including vision-lan...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 3. A survey of self-improvement methods in Multimodal Large Language Models (MLLMs) from data collection, organization, and model optimization perspectives.  					AI-generated summary 				 Recent advancements in self-improvement for Large Language Models (LLMs) have efficiently enhanced model capabilit...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 4. Self-evolving agents based on Large Language Models can deviate in unintended ways, leading to various risks such as safety misalignment and vulnerability introduction, necessitating new safety paradigms.  					AI-generated summary 				 Advances in Large Language Models (LLMs) have enabled a new cla...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 5. A new evaluation framework, SurveyBench, assesses the quality of automatically generated academic surveys using a quiz-driven approach, revealing deficiencies in current LLM4Survey methods.  					AI-generated summary 				 Academic survey writing, which distills vast literature into a coherent and in...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 6. CoDA, a multi-agent system using specialized LLM agents, enhances visualization automation by managing data complexity and ensuring high-quality visualizations through collaborative workflows.  					AI-generated summary 				 Deep research has revolutionized data analysis, yet data scientists still d...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 7. REPAIR is a lifelong editing framework for large language models that enhances editing accuracy and reduces knowledge forgetting through progressive adaptive intervention and reintegration.  					AI-generated summary 				 Post-training for large language models (LLMs) is constrained by the high cost...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 8. FocusAgent uses a lightweight LLM retriever to extract relevant content from web page observations, improving efficiency and security in web agents.  					AI-generated summary 				 Web agents powered by large language models (LLMs) must process lengthy web page observations to complete user goals; t...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 9. Explicit coordinate markers and improved spatial encoding enhance GUI grounding accuracy across diverse resolutions and platforms.  					AI-generated summary 				 GUI grounding, the task of mapping natural-language instructions to pixel coordinates, is crucial for autonomous agents, yet remains diff...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 10. Length-aware Sampling for Policy Optimization (LSPO) is a meta-RLVR algorithm that dynamically selects training data based on response length, improving learning effectiveness in large language models.  					AI-generated summary 				 Since the release of Deepseek-R1, reinforcement learning with veri...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 11. A new framework, Text Preference Optimization (TPO), aligns text-to-image models with human preferences without requiring paired image preference data, improving text-to-image alignment and human preference scores.  					AI-generated summary 				 Recent advances in diffusion-based text-to-image (T2I...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 12. LEAML, a label-efficient adaptation framework, enhances MLLMs for specialized domains by generating pseudo question-answer pairs and selectively updating relevant neurons, outperforming standard fine-tuning with minimal supervision.  					AI-generated summary 				 Multimodal Large Language Models (M...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 13. SpineMed, an ecosystem with SpineMed-450k and SpineBench, addresses the lack of level-aware, multimodal datasets and benchmarks for AI-assisted diagnosis of spine disorders, improving model performance through fine-grained, level-specific reasoning.  					AI-generated summary 				 Spine disorders af...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 14. A framework for uncertainty quantification in generative video models is introduced, including a metric for calibration, a black-box method called S-QUBED, and a benchmark dataset, demonstrating improved uncertainty estimates and task accuracy.  					AI-generated summary 				 Generative video models...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 15. A comprehensive benchmark study evaluates the detection of prompt injection attacks against web agents, revealing that current detectors perform well against explicit attacks but struggle with subtle ones.  					AI-generated summary 				 Multiple prompt injection attacks have been proposed against w...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 16. Research identifies key design choices for training large language models as agents via multi-turn reinforcement learning, focusing on environment complexity, reward sparsity, and policy methods.  					AI-generated summary 				 We study what actually works and what doesn't for training large languag...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 17. Align Your Tangent (AYT) improves Consistency Model training by reducing oscillatory tangents and enabling faster convergence with small batch sizes.  					AI-generated summary 				 With diffusion and flow matching models achieving state-of-the-art generating performance, the interest of the communi...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 18. NuRisk, a comprehensive VQA dataset, addresses the lack of spatio-temporal reasoning in current VLMs for autonomous driving by providing agent-level risk annotations in sequential images, improving accuracy and reducing latency.  					AI-generated summary 				 Understanding risk in autonomous drivin...
[06.10.2025 09:15] ********************************************************************************
[06.10.2025 09:15] Abstract 19. Triangle Splatting+ optimizes triangles within a differentiable framework for real-time, high-fidelity 3D scene reconstruction and novel view synthesis, compatible with standard graphics engines.  					AI-generated summary 				 Reconstructing 3D scenes and synthesizing novel views has seen rapid pro...
[06.10.2025 09:15] Read previous papers.
[06.10.2025 09:15] Generating reviews via LLM API.
[06.10.2025 09:15] Querying the API.
[06.10.2025 09:15] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A 15-billion parameter multimodal reasoning model achieves competitive performance through a progressive training methodology without reinforcement learning, demonstrating efficient use of computational resources.  					AI-generated summary 				 We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights multimodal reasoning model that achieves frontier-level performance through training design rather than sheer scale. Starting from Pixtral-12B, we apply a progressive three-stage methodology: (1) depth upscaling to expand reasoning capacity without pretraining from scratch, (2) staged continual pre-training that first develops foundational text and vision understanding, then enhances visual reasoning through targeted synthetic data generation addressing spatial structure, compositional understanding, and fine-grained perception, and (3) high-quality text-only supervised fine-tuning on curated instruction-response pairs with explicit reasoning traces spanning mathematics, coding, science, and tool use. Notably, our model achieves competitive results without reinforcement learning or preference optimization, isolating the contribution of our data-centric continual pre-training approach. On the Artificial Analysis Intelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching DeepSeek-R1-0528 despite requiring significantly fewer computational resources. Across ten image benchmarks, its performance is on average within five points of Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model operating within single-GPU deployment constraints. Our results demonstrate that thoughtful mid-training 2 design can close substantial capability gaps without massive scale, making frontier-level multimodal reasoning accessible to organizations with limited infrastructure. We release the model checkpoint, all training recipes, and evaluation protocols under the MIT license to to advance open-source research.
[06.10.2025 09:15] Response: ```json
{
  "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Apriel-1.5-15B-Thinker —Å 15 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —á–µ—Ä–µ–∑ –ø—Ä–æ–¥—É–º–∞–Ω–Ω—É—é –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è, –∞ –Ω–µ –∑–∞ —Å—á—ë—Ç –º–∞—Å—à—Ç–∞–±–∞. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä—ë—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥: —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã —Å–µ—Ç–∏, –ø–æ—ç—Ç–∞–ø–Ω—ã–π continual pre-training –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏ supervised fine-tuning –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —è–≤–Ω—ã–º–∏ —Ü–µ–ø–æ—á–∫–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ü—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã—Ö —Å –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤—Ä–æ–¥–µ Gemini-2.5-Flash –∏ Claude Sonnet-3.7, –ø—Ä–∏ —ç—Ç–æ–º —Ä–∞–±–æ—Ç–∞—è –Ω–∞ –æ–¥–Ω–æ–º GPU –∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—è reinforcement learning. –í—Å–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏, —Ä–µ—Ü–µ–ø—Ç—ã –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –æ—Ü–µ–Ω–∫–∏ –≤—ã–ø—É—â–µ–Ω—ã –ø–æ–¥ –æ—Ç–∫—Ä—ã—Ç–æ–π –ª–∏—Ü–µ–Ω–∑–∏–µ–π MIT –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è open-source –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.",
  "emoji": "üß†",
  "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –±–µ–∑ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤"
}
```
[06.10.2025 09:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A 15-billion parameter multimodal reasoning model achieves competitive performance through a progressive training methodology without reinforcement learning, demonstrating efficient use of computational resources.  					AI-generated summary 				 We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights multimodal reasoning model that achieves frontier-level performance through training design rather than sheer scale. Starting from Pixtral-12B, we apply a progressive three-stage methodology: (1) depth upscaling to expand reasoning capacity without pretraining from scratch, (2) staged continual pre-training that first develops foundational text and vision understanding, then enhances visual reasoning through targeted synthetic data generation addressing spatial structure, compositional understanding, and fine-grained perception, and (3) high-quality text-only supervised fine-tuning on curated instruction-response pairs with explicit reasoning traces spanning mathematics, coding, science, and tool use. Notably, our model achieves competitive results without reinforcement learning or preference optimization, isolating the contribution of our data-centric continual pre-training approach. On the Artificial Analysis Intelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching DeepSeek-R1-0528 despite requiring significantly fewer computational resources. Across ten image benchmarks, its performance is on average within five points of Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model operating within single-GPU deployment constraints. Our results demonstrate that thoughtful mid-training 2 design can close substantial capability gaps without massive scale, making frontier-level multimodal reasoning accessible to organizations with limited infrastructure. We release the model checkpoint, all training recipes, and evaluation protocols under the MIT license to to advance open-source research."

[06.10.2025 09:15] Response: ```python
['MULTIMODAL', 'TRAINING', 'DATASET', 'INFERENCE', 'ARCHITECTURE']
```
[06.10.2025 09:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A 15-billion parameter multimodal reasoning model achieves competitive performance through a progressive training methodology without reinforcement learning, demonstrating efficient use of computational resources.  					AI-generated summary 				 We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights multimodal reasoning model that achieves frontier-level performance through training design rather than sheer scale. Starting from Pixtral-12B, we apply a progressive three-stage methodology: (1) depth upscaling to expand reasoning capacity without pretraining from scratch, (2) staged continual pre-training that first develops foundational text and vision understanding, then enhances visual reasoning through targeted synthetic data generation addressing spatial structure, compositional understanding, and fine-grained perception, and (3) high-quality text-only supervised fine-tuning on curated instruction-response pairs with explicit reasoning traces spanning mathematics, coding, science, and tool use. Notably, our model achieves competitive results without reinforcement learning or preference optimization, isolating the contribution of our data-centric continual pre-training approach. On the Artificial Analysis Intelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching DeepSeek-R1-0528 despite requiring significantly fewer computational resources. Across ten image benchmarks, its performance is on average within five points of Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model operating within single-GPU deployment constraints. Our results demonstrate that thoughtful mid-training 2 design can close substantial capability gaps without massive scale, making frontier-level multimodal reasoning accessible to organizations with limited infrastructure. We release the model checkpoint, all training recipes, and evaluation protocols under the MIT license to to advance open-source research."

[06.10.2025 09:15] Response: ```python
['AGI', 'REASONING', 'OPEN_SOURCE']
```
[06.10.2025 09:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces Apriel-1.5-15B-Thinker, a multimodal reasoning model with 15 billion parameters that achieves high performance through a unique training approach rather than relying on large scale. It employs a three-stage progressive training methodology that enhances reasoning capabilities by first upscaling depth, then using continual pre-training to improve text and vision understanding, and finally fine-tuning with high-quality text data. This model stands out by achieving competitive results without the use of reinforcement learning, focusing instead on a data-centric approach. The findings suggest that effective training strategies can significantly enhance model performance while minimizing computational resource requirements, making advanced multimodal reasoning more accessible.","title":"Efficient Multimodal Reasoning Without Reinforcement Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces Apriel-1.5-15B-Thinker, a multimodal reasoning model with 15 billion parameters that achieves high performance through a unique training approach rather than relying on large scale. It employs a three-stage progressive training methodology that enhances reasoning capabilities by first upscaling depth, then using continual pre-training to improve text and vision understanding, and finally fine-tuning with high-quality text data. This model stands out by achieving competitive results without the use of reinforcement learning, focusing instead on a data-centric approach. The findings suggest that effective training strategies can significantly enhance model performance while minimizing computational resource requirements, making advanced multimodal reasoning more accessible.', title='Efficient Multimodal Reasoning Without Reinforcement Learning'))
[06.10.2025 09:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫Apriel-1.5-15B-ThinkerÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÊ®°ÂûãÔºåÂÖ∑Êúâ150‰∫ø‰∏™ÂèÇÊï∞„ÄÇËØ•Ê®°ÂûãÈÄöËøáÊ∏êËøõÂºèËÆ≠ÁªÉÊñπÊ≥ïÂÆûÁé∞‰∫ÜÁ´û‰∫âÂäõÁöÑÊÄßËÉΩÔºåËÄåÊó†ÈúÄÂº∫ÂåñÂ≠¶‰π†„ÄÇËÆ≠ÁªÉËøáÁ®ãÂåÖÊã¨‰∏â‰∏™Èò∂ÊÆµÔºöÊ∑±Â∫¶Êâ©Â±ï„ÄÅÂàÜÈò∂ÊÆµÊåÅÁª≠È¢ÑËÆ≠ÁªÉÂíåÈ´òË¥®ÈáèÁöÑÊñáÊú¨ÁõëÁù£ÂæÆË∞É„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂêàÁêÜÁöÑËÆ≠ÁªÉËÆæËÆ°ÂèØ‰ª•Âú®‰∏ç‰æùËµñÂ§ßËßÑÊ®°ËÆ°ÁÆóËµÑÊ∫êÁöÑÊÉÖÂÜµ‰∏ãÔºåÁº©Â∞èËÉΩÂäõÂ∑ÆË∑ùÔºå‰ΩøÂâçÊ≤øÁ∫ßÂà´ÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÂØπËµÑÊ∫êÊúâÈôêÁöÑÁªÑÁªáÂèòÂæóÂèØÂèä„ÄÇ","title":"È´òÊïàËÆ≠ÁªÉÔºåÂâçÊ≤øÊé®ÁêÜÔºÅ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫Apriel-1.5-15B-ThinkerÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÊ®°ÂûãÔºåÂÖ∑Êúâ150‰∫ø‰∏™ÂèÇÊï∞„ÄÇËØ•Ê®°ÂûãÈÄöËøáÊ∏êËøõÂºèËÆ≠ÁªÉÊñπÊ≥ïÂÆûÁé∞‰∫ÜÁ´û‰∫âÂäõÁöÑÊÄßËÉΩÔºåËÄåÊó†ÈúÄÂº∫ÂåñÂ≠¶‰π†„ÄÇËÆ≠ÁªÉËøáÁ®ãÂåÖÊã¨‰∏â‰∏™Èò∂ÊÆµÔºöÊ∑±Â∫¶Êâ©Â±ï„ÄÅÂàÜÈò∂ÊÆµÊåÅÁª≠È¢ÑËÆ≠ÁªÉÂíåÈ´òË¥®ÈáèÁöÑÊñáÊú¨ÁõëÁù£ÂæÆË∞É„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂêàÁêÜÁöÑËÆ≠ÁªÉËÆæËÆ°ÂèØ‰ª•Âú®‰∏ç‰æùËµñÂ§ßËßÑÊ®°ËÆ°ÁÆóËµÑÊ∫êÁöÑÊÉÖÂÜµ‰∏ãÔºåÁº©Â∞èËÉΩÂäõÂ∑ÆË∑ùÔºå‰ΩøÂâçÊ≤øÁ∫ßÂà´ÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÂØπËµÑÊ∫êÊúâÈôêÁöÑÁªÑÁªáÂèòÂæóÂèØÂèä„ÄÇ', title='È´òÊïàËÆ≠ÁªÉÔºåÂâçÊ≤øÊé®ÁêÜÔºÅ'))
[06.10.2025 09:15] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture", "#multimodal"], "emoji": "üéØ", "ru": {"title": "–ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é", "desc": "EPIC ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –±–æ–ª—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç
[06.10.2025 09:15] Using data from previous issue: {"categories": ["#training", "#robotics", "#optimization", "#benchmark", "#diffusion", "#agents"], "emoji": "ü§ù", "ru": {"title": "–ö–æ–º–ø–æ–∑–∏—Ü–∏—è policy –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ General Policy Composition (GPC), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª
[06.10.2025 09:15] Using data from previous issue: {"categories": ["#training", "#survey", "#optimization", "#multimodal", "#data", "#dataset"], "emoji": "üîÑ", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ LLM —É—á–∞—Ç—Å—è —Å–∞–º–∏: –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM.
[06.10.2025 09:15] Using data from previous issue: {"categories": ["#alignment", "#ethics", "#agents", "#security", "#safety", "#rl"], "emoji": "üß¨", "ru": {"title": "–ö–æ–≥–¥–∞ AI-–∞–≥–µ–Ω—Ç—ã —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—Ç –≤ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Ç–æ—Ä–æ–Ω—É", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç –Ω–æ–≤—ã–π —Ç–∏–ø —Ä–∏—Å–∫–æ–≤ –≤ —Å–∞–º–æ–æ–±—É—á–∞—é—â–∏—Ö—Å—è –∞–≥–µ–Ω—Ç–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –∫–æ—Ç–æ—Ä—ã–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ —É–ª—É—á—à–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ
[06.10.2025 09:15] Using data from previous issue: {"categories": ["#survey", "#benchmark"], "emoji": "üìä", "ru": {"title": "SurveyBench: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞—É—á–Ω—ã—Ö –æ–±–∑–æ—Ä–æ–≤ —á–µ—Ä–µ–∑ –≤–∏–∫—Ç–æ—Ä–∏–Ω—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ SurveyBench ‚Äî –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –æ–±–∑–æ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM.
[06.10.2025 09:15] Using data from previous issue: {"categories": ["#agents", "#optimization", "#data", "#interpretability", "#multimodal"], "emoji": "ü§ù", "ru": {"title": "–ö–æ–º–∞–Ω–¥–∞ AI-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CoDA ‚Äî –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π 
[06.10.2025 09:15] Using data from previous issue: {"categories": ["#optimization", "#training", "#inference"], "emoji": "üîß", "ru": {"title": "–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –∑–∞–±—ã–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ REPAIR –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å –æ—à–∏–±–∫–∏ –∏ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω
[06.10.2025 09:15] Using data from previous issue: {"categories": ["#long_context", "#inference", "#benchmark", "#agents", "#security", "#reasoning"], "emoji": "üéØ", "ru": {"title": "–§–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "desc": "FocusAgent ‚Äî —ç—Ç–æ –ø–æ–¥—Ö–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª—ë–≥–∫–∏–π re
[06.10.2025 09:15] Using data from previous issue: {"categories": ["#interpretability", "#agents", "#cv", "#optimization"], "emoji": "üéØ", "ru": {"title": "–Ø–≤–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤–º–µ—Å—Ç–æ —É–≥–∞–¥—ã–≤–∞–Ω–∏—è: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ —Ç–æ—á–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—å —ç–ª–µ–º–µ–Ω—Ç—ã –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ GUI grounding ‚Äî –∑–∞–¥–∞—á–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
[06.10.2025 09:15] Using data from previous issue: {"categories": ["#optimization", "#rlhf", "#training", "#rl", "#reasoning"], "emoji": "üìè", "ru": {"title": "–£—á—ë—Ç –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω LSPO ‚Äî –º–µ—Ç–∞-–∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –æ—Ç
[06.10.2025 09:15] Using data from previous issue: {"categories": ["#open_source", "#alignment", "#benchmark", "#rlhf", "#multimodal", "#diffusion"], "emoji": "üéØ", "ru": {"title": "–ë–µ—Å–ø–ª–∞—Ç–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ: –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –ø–∞—Ä–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Text Preference Optimization (TPO) –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è text-to-image –º–æ–¥–µ–ª–µ–π —Å —á–µ–ª
[06.10.2025 09:15] Querying the API.
[06.10.2025 09:15] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LEAML, a label-efficient adaptation framework, enhances MLLMs for specialized domains by generating pseudo question-answer pairs and selectively updating relevant neurons, outperforming standard fine-tuning with minimal supervision.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have achieved strong performance on general visual benchmarks but struggle with out-of-distribution (OOD) tasks in specialized domains such as medical imaging, where labeled data is limited and expensive. We introduce LEAML, a label-efficient adaptation framework that leverages both scarce labeled VQA samples and abundant unlabeled images. Our approach generates domain-relevant pseudo question-answer pairs for unlabeled data using a QA generator regularized by caption distillation. Importantly, we selectively update only those neurons most relevant to question-answering, enabling the QA Generator to efficiently acquire domain-specific knowledge during distillation. Experiments on gastrointestinal endoscopy and sports VQA demonstrate that LEAML consistently outperforms standard fine-tuning under minimal supervision, highlighting the effectiveness of our proposed LEAML framework.
[06.10.2025 09:15] Response: ```json
{
  "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π",
  "desc": "LEAML ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–æ–º–µ–Ω–∞–º –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Å–µ–≤–¥–æ-–ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –¥–ª—è –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é QA-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞, —Ä–µ–≥—É–ª—è—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–µ–π –æ–ø–∏—Å–∞–Ω–∏–π. –ö–ª—é—á–µ–≤–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å ‚Äî —Å–µ–ª–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ —Ç–µ—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã –¥–ª—è –∑–∞–¥–∞—á–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —ç–Ω–¥–æ—Å–∫–æ–ø–∏–∏ –∏ —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥–æ–º –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–µ.",
  "emoji": "üéØ"
}
```
[06.10.2025 09:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LEAML, a label-efficient adaptation framework, enhances MLLMs for specialized domains by generating pseudo question-answer pairs and selectively updating relevant neurons, outperforming standard fine-tuning with minimal supervision.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have achieved strong performance on general visual benchmarks but struggle with out-of-distribution (OOD) tasks in specialized domains such as medical imaging, where labeled data is limited and expensive. We introduce LEAML, a label-efficient adaptation framework that leverages both scarce labeled VQA samples and abundant unlabeled images. Our approach generates domain-relevant pseudo question-answer pairs for unlabeled data using a QA generator regularized by caption distillation. Importantly, we selectively update only those neurons most relevant to question-answering, enabling the QA Generator to efficiently acquire domain-specific knowledge during distillation. Experiments on gastrointestinal endoscopy and sports VQA demonstrate that LEAML consistently outperforms standard fine-tuning under minimal supervision, highlighting the effectiveness of our proposed LEAML framework."

[06.10.2025 09:15] Response: ```python
['DATASET', 'DATA', 'TRAINING', 'MULTIMODAL', 'HEALTHCARE']
```
[06.10.2025 09:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LEAML, a label-efficient adaptation framework, enhances MLLMs for specialized domains by generating pseudo question-answer pairs and selectively updating relevant neurons, outperforming standard fine-tuning with minimal supervision.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have achieved strong performance on general visual benchmarks but struggle with out-of-distribution (OOD) tasks in specialized domains such as medical imaging, where labeled data is limited and expensive. We introduce LEAML, a label-efficient adaptation framework that leverages both scarce labeled VQA samples and abundant unlabeled images. Our approach generates domain-relevant pseudo question-answer pairs for unlabeled data using a QA generator regularized by caption distillation. Importantly, we selectively update only those neurons most relevant to question-answering, enabling the QA Generator to efficiently acquire domain-specific knowledge during distillation. Experiments on gastrointestinal endoscopy and sports VQA demonstrate that LEAML consistently outperforms standard fine-tuning under minimal supervision, highlighting the effectiveness of our proposed LEAML framework."

[06.10.2025 09:15] Response: ```python
['TRANSFER_LEARNING', 'OPTIMIZATION']
```
[06.10.2025 09:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LEAML is a framework designed to improve Multimodal Large Language Models (MLLMs) for specialized fields with limited labeled data. It creates pseudo question-answer pairs from unlabeled images, which helps the model learn relevant information without needing extensive supervision. By focusing on updating only the neurons that are crucial for question-answering, LEAML efficiently incorporates domain-specific knowledge. Experiments show that this method surpasses traditional fine-tuning techniques, making it a powerful tool for tasks like medical imaging and sports analysis.","title":"Efficient Learning with LEAML: Mastering Specialized Domains with Minimal Labels"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LEAML is a framework designed to improve Multimodal Large Language Models (MLLMs) for specialized fields with limited labeled data. It creates pseudo question-answer pairs from unlabeled images, which helps the model learn relevant information without needing extensive supervision. By focusing on updating only the neurons that are crucial for question-answering, LEAML efficiently incorporates domain-specific knowledge. Experiments show that this method surpasses traditional fine-tuning techniques, making it a powerful tool for tasks like medical imaging and sports analysis.', title='Efficient Learning with LEAML: Mastering Specialized Domains with Minimal Labels'))
[06.10.2025 09:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LEAMLÊòØ‰∏ÄÁßçÊ†áÁ≠æÈ´òÊïàÁöÑÈÄÇÂ∫îÊ°ÜÊû∂ÔºåÊó®Âú®Â¢ûÂº∫Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®‰∏ì‰∏öÈ¢ÜÂüüÁöÑË°®Áé∞„ÄÇÂÆÉÈÄöËøáÁîüÊàê‰º™ÈóÆÈ¢ò-Á≠îÊ°àÂØπÔºåÂπ∂ÈÄâÊã©ÊÄßÂú∞Êõ¥Êñ∞‰∏éÈóÆÈ¢òÂõûÁ≠îÁõ∏ÂÖ≥ÁöÑÁ•ûÁªèÂÖÉÔºåÊù•ÊèêÈ´òÊ®°ÂûãÁöÑÈÄÇÂ∫îËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®Á®ÄÁº∫ÁöÑÊ†áËÆ∞Ê†∑Êú¨Âíå‰∏∞ÂØåÁöÑÊú™Ê†áËÆ∞ÂõæÂÉèÔºåËÉΩÂ§üÂú®Êï∞ÊçÆÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ãÊúâÊïàÂ≠¶‰π†„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLEAMLÂú®ÂÜÖÁ™•ÈïúÂíå‰ΩìËÇ≤ËßÜËßâÈóÆÁ≠î‰ªªÂä°‰∏≠ÔºåË°®Áé∞‰ºò‰∫éÊ†áÂáÜÂæÆË∞ÉÊñπÊ≥ïÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ","title":"LEAMLÔºöÈ´òÊïàÈÄÇÂ∫î‰∏ì‰∏öÈ¢ÜÂüüÁöÑÊ†áÁ≠æÊ°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LEAMLÊòØ‰∏ÄÁßçÊ†áÁ≠æÈ´òÊïàÁöÑÈÄÇÂ∫îÊ°ÜÊû∂ÔºåÊó®Âú®Â¢ûÂº∫Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®‰∏ì‰∏öÈ¢ÜÂüüÁöÑË°®Áé∞„ÄÇÂÆÉÈÄöËøáÁîüÊàê‰º™ÈóÆÈ¢ò-Á≠îÊ°àÂØπÔºåÂπ∂ÈÄâÊã©ÊÄßÂú∞Êõ¥Êñ∞‰∏éÈóÆÈ¢òÂõûÁ≠îÁõ∏ÂÖ≥ÁöÑÁ•ûÁªèÂÖÉÔºåÊù•ÊèêÈ´òÊ®°ÂûãÁöÑÈÄÇÂ∫îËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®Á®ÄÁº∫ÁöÑÊ†áËÆ∞Ê†∑Êú¨Âíå‰∏∞ÂØåÁöÑÊú™Ê†áËÆ∞ÂõæÂÉèÔºåËÉΩÂ§üÂú®Êï∞ÊçÆÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ãÊúâÊïàÂ≠¶‰π†„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLEAMLÂú®ÂÜÖÁ™•ÈïúÂíå‰ΩìËÇ≤ËßÜËßâÈóÆÁ≠î‰ªªÂä°‰∏≠ÔºåË°®Áé∞‰ºò‰∫éÊ†áÂáÜÂæÆË∞ÉÊñπÊ≥ïÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ', title='LEAMLÔºöÈ´òÊïàÈÄÇÂ∫î‰∏ì‰∏öÈ¢ÜÂüüÁöÑÊ†áÁ≠æÊ°ÜÊû∂'))
[06.10.2025 09:15] Using data from previous issue: {"categories": ["#reasoning", "#training", "#healthcare", "#benchmark", "#dataset", "#multimodal", "#science"], "emoji": "ü¶¥", "ru": {"title": "SpineMed: AI-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ç–æ—á–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø–æ–∑–≤–æ–Ω–æ—á–Ω–∏–∫–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–æ–∑–≤–æ–Ω–∫–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SpineMed ‚Äî —ç–∫–æ—Å–∏—Å—Ç–µ–º—É –¥–ª—è AI-–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫
[06.10.2025 09:15] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#hallucinations", "#dataset", "#video"], "emoji": "üé¨", "ru": {"title": "–ö–æ–≥–¥–∞ AI –Ω–µ —É–≤–µ—Ä–µ–Ω –≤ —Å–≤–æ—ë–º –≤–∏–¥–µ–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –ø–µ—Ä–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ, –∫–∞–∫ –∏ LLM, —Å
[06.10.2025 09:15] Using data from previous issue: {"categories": ["#agents", "#dataset", "#benchmark", "#security"], "emoji": "üïµÔ∏è", "ru": {"title": "–î–µ—Ç–µ–∫—Ç–æ—Ä—ã –∏–Ω—ä–µ–∫—Ü–∏–π –ø—Ä–æ–º–ø—Ç–æ–≤ –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å —Ç–æ–Ω–∫–∏–º–∏ –∞—Ç–∞–∫–∞–º–∏ –Ω–∞ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–≤–µ–ª–∏ –ø–µ—Ä–≤–æ–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞—Ç–∞–∫ —Ç–∏–ø–∞ prompt injection –Ω–∞ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ
[06.10.2025 09:15] Using data from previous issue: {"categories": ["#agents", "#games", "#reasoning", "#rlhf", "#training", "#rl", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–†–µ—Ü–µ–ø—Ç –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–∞–∫ –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ reinforcement learning", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LLM –≤ —Ä–æ–ª–∏ –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ mu
[06.10.2025 09:15] Using data from previous issue: {"categories": ["#training", "#diffusion", "#optimization"], "emoji": "üéØ", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è Consistency Models", "desc": "Consistency Models (CMs) –ø–æ–∑–≤–æ–ª—è—é—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞ –æ–¥–∏–Ω-–¥–≤–∞ —à–∞–≥–∞, –Ω–æ —Ç—Ä–µ–±—É—é—Ç –¥–æ–ª–≥–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –±–æ–ª—å—à–∏–º–∏ –±–∞—Ç—á–∞–º–∏. –ê–≤—Ç–æ—Ä—ã
[06.10.2025 09:15] Querying the API.
[06.10.2025 09:15] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

NuRisk, a comprehensive VQA dataset, addresses the lack of spatio-temporal reasoning in current VLMs for autonomous driving by providing agent-level risk annotations in sequential images, improving accuracy and reducing latency.  					AI-generated summary 				 Understanding risk in autonomous driving requires not only perception and prediction, but also high-level reasoning about agent behavior and context. Current Vision Language Models (VLMs)-based methods primarily ground agents in static images and provide qualitative judgments, lacking the spatio-temporal reasoning needed to capture how risks evolve over time. To address this gap, we propose NuRisk, a comprehensive Visual Question Answering (VQA) dataset comprising 2,900 scenarios and 1.1 million agent-level samples, built on real-world data from nuScenes and Waymo, supplemented with safety-critical scenarios from the CommonRoad simulator. The dataset provides Bird-Eye-View (BEV) based sequential images with quantitative, agent-level risk annotations, enabling spatio-temporal reasoning. We benchmark well-known VLMs across different prompting techniques and find that they fail to perform explicit spatio-temporal reasoning, resulting in a peak accuracy of 33% at high latency. To address these shortcomings, our fine-tuned 7B VLM agent improves accuracy to 41% and reduces latency by 75%, demonstrating explicit spatio-temporal reasoning capabilities that proprietary models lacked. While this represents a significant step forward, the modest accuracy underscores the profound challenge of the task, establishing NuRisk as a critical benchmark for advancing spatio-temporal reasoning in autonomous driving.
[06.10.2025 09:16] Response: ```json
{
  "title": "–û–±—É—á–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞—Ç—å —Ä–∏—Å–∫–∏ –Ω–∞ –¥–æ—Ä–æ–≥–µ –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ",
  "emoji": "üöó",
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ NuRisk ‚Äî –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è Vision Language Models –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º –≤–æ–∂–¥–µ–Ω–∏–∏. –î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç 2900 —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∏ 1.1 –º–∏–ª–ª–∏–æ–Ω–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏ —Ä–∏—Å–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å –≤–∏–¥–æ–º —Å–≤–µ—Ä—Ö—É. –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ VLM –ø–æ–∫–∞–∑–∞–ª–∏ —Ç–æ—á–Ω–æ—Å—Ç—å –≤—Å–µ–≥–æ 33%, —Ç–∞–∫ –∫–∞–∫ –Ω–µ —É–º–µ—é—Ç —è–≤–Ω–æ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –æ —Ä–∞–∑–≤–∏—Ç–∏–∏ —Ä–∏—Å–∫–æ–≤ –≤–æ –≤—Ä–µ–º–µ–Ω–∏. –î–æ–æ–±—É—á–µ–Ω–Ω–∞—è 7B –º–æ–¥–µ–ª—å —É–ª—É—á—à–∏–ª–∞ —Ç–æ—á–Ω–æ—Å—Ç—å –¥–æ 41% –∏ —Å–Ω–∏–∑–∏–ª–∞ –∑–∞–¥–µ—Ä–∂–∫—É –Ω–∞ 75%, –Ω–æ —Å–∫—Ä–æ–º–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∏—Å–∫–æ–≤ –¥–ª—è –∞–≤—Ç–æ–ø–∏–ª–æ—Ç–æ–≤."
}
```
[06.10.2025 09:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"NuRisk, a comprehensive VQA dataset, addresses the lack of spatio-temporal reasoning in current VLMs for autonomous driving by providing agent-level risk annotations in sequential images, improving accuracy and reducing latency.  					AI-generated summary 				 Understanding risk in autonomous driving requires not only perception and prediction, but also high-level reasoning about agent behavior and context. Current Vision Language Models (VLMs)-based methods primarily ground agents in static images and provide qualitative judgments, lacking the spatio-temporal reasoning needed to capture how risks evolve over time. To address this gap, we propose NuRisk, a comprehensive Visual Question Answering (VQA) dataset comprising 2,900 scenarios and 1.1 million agent-level samples, built on real-world data from nuScenes and Waymo, supplemented with safety-critical scenarios from the CommonRoad simulator. The dataset provides Bird-Eye-View (BEV) based sequential images with quantitative, agent-level risk annotations, enabling spatio-temporal reasoning. We benchmark well-known VLMs across different prompting techniques and find that they fail to perform explicit spatio-temporal reasoning, resulting in a peak accuracy of 33% at high latency. To address these shortcomings, our fine-tuned 7B VLM agent improves accuracy to 41% and reduces latency by 75%, demonstrating explicit spatio-temporal reasoning capabilities that proprietary models lacked. While this represents a significant step forward, the modest accuracy underscores the profound challenge of the task, establishing NuRisk as a critical benchmark for advancing spatio-temporal reasoning in autonomous driving."

[06.10.2025 09:16] Response: ```python
['DATASET', 'BENCHMARK', 'CV', 'TRAINING']
```
[06.10.2025 09:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"NuRisk, a comprehensive VQA dataset, addresses the lack of spatio-temporal reasoning in current VLMs for autonomous driving by providing agent-level risk annotations in sequential images, improving accuracy and reducing latency.  					AI-generated summary 				 Understanding risk in autonomous driving requires not only perception and prediction, but also high-level reasoning about agent behavior and context. Current Vision Language Models (VLMs)-based methods primarily ground agents in static images and provide qualitative judgments, lacking the spatio-temporal reasoning needed to capture how risks evolve over time. To address this gap, we propose NuRisk, a comprehensive Visual Question Answering (VQA) dataset comprising 2,900 scenarios and 1.1 million agent-level samples, built on real-world data from nuScenes and Waymo, supplemented with safety-critical scenarios from the CommonRoad simulator. The dataset provides Bird-Eye-View (BEV) based sequential images with quantitative, agent-level risk annotations, enabling spatio-temporal reasoning. We benchmark well-known VLMs across different prompting techniques and find that they fail to perform explicit spatio-temporal reasoning, resulting in a peak accuracy of 33% at high latency. To address these shortcomings, our fine-tuned 7B VLM agent improves accuracy to 41% and reduces latency by 75%, demonstrating explicit spatio-temporal reasoning capabilities that proprietary models lacked. While this represents a significant step forward, the modest accuracy underscores the profound challenge of the task, establishing NuRisk as a critical benchmark for advancing spatio-temporal reasoning in autonomous driving."

[06.10.2025 09:16] Response: ```python
["REASONING", "GAMES"]
```
[06.10.2025 09:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"NuRisk is a new Visual Question Answering (VQA) dataset designed to enhance spatio-temporal reasoning in Vision Language Models (VLMs) for autonomous driving. It includes 2,900 scenarios and 1.1 million samples with detailed agent-level risk annotations, allowing models to understand how risks change over time. The dataset is built from real-world data and includes safety-critical scenarios, providing a comprehensive resource for training and evaluating VLMs. Our experiments show that while existing models struggle with spatio-temporal reasoning, our fine-tuned 7B VLM agent significantly improves accuracy and reduces latency, highlighting the importance of this dataset for future research.","title":"NuRisk: Advancing Spatio-Temporal Reasoning in Autonomous Driving"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='NuRisk is a new Visual Question Answering (VQA) dataset designed to enhance spatio-temporal reasoning in Vision Language Models (VLMs) for autonomous driving. It includes 2,900 scenarios and 1.1 million samples with detailed agent-level risk annotations, allowing models to understand how risks change over time. The dataset is built from real-world data and includes safety-critical scenarios, providing a comprehensive resource for training and evaluating VLMs. Our experiments show that while existing models struggle with spatio-temporal reasoning, our fine-tuned 7B VLM agent significantly improves accuracy and reduces latency, highlighting the importance of this dataset for future research.', title='NuRisk: Advancing Spatio-Temporal Reasoning in Autonomous Driving'))
[06.10.2025 09:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"NuRiskÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑËßÜËßâÈóÆÁ≠îÔºàVQAÔºâÊï∞ÊçÆÈõÜÔºåÊó®Âú®Ëß£ÂÜ≥ÂΩìÂâçËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Ëá™Âä®È©æÈ©∂‰∏≠Áº∫‰πèÊó∂Á©∫Êé®ÁêÜÁöÑÈóÆÈ¢ò„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´2900‰∏™Âú∫ÊôØÂíå110‰∏á‰∏™Âü∫‰∫éÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÁöÑ‰ª£ÁêÜÁ∫ßÈ£éÈô©Ê≥®ÈáäÊ†∑Êú¨ÔºåÊîØÊåÅÊó∂Á©∫Êé®ÁêÜ„ÄÇÈÄöËøáÂØπ‰∏çÂêåÊèêÁ§∫ÊäÄÊúØÁöÑÂü∫ÂáÜÊµãËØïÔºåÊàë‰ª¨ÂèëÁé∞Áé∞ÊúâÁöÑVLMsÂú®Êó∂Á©∫Êé®ÁêÜÊñπÈù¢Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂáÜÁ°ÆÁéá‰ªÖ‰∏∫33%„ÄÇÁªèËøáÂæÆË∞ÉÁöÑ7B VLM‰ª£ÁêÜÂ∞ÜÂáÜÁ°ÆÁéáÊèêÈ´òÂà∞41%ÔºåÂπ∂Â∞ÜÂª∂ËøüÂáèÂ∞ë‰∫Ü75%ÔºåÂ±ïÁ§∫‰∫ÜÊòæÂºèÁöÑÊó∂Á©∫Êé®ÁêÜËÉΩÂäõÔºåÊ†áÂøóÁùÄÂú®Ëá™Âä®È©æÈ©∂È¢ÜÂüüÁöÑÈáçË¶ÅËøõÂ±ï„ÄÇ","title":"NuRiskÔºöÊèêÂçáËá™Âä®È©æÈ©∂ÁöÑÊó∂Á©∫Êé®ÁêÜËÉΩÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='NuRiskÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑËßÜËßâÈóÆÁ≠îÔºàVQAÔºâÊï∞ÊçÆÈõÜÔºåÊó®Âú®Ëß£ÂÜ≥ÂΩìÂâçËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Ëá™Âä®È©æÈ©∂‰∏≠Áº∫‰πèÊó∂Á©∫Êé®ÁêÜÁöÑÈóÆÈ¢ò„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´2900‰∏™Âú∫ÊôØÂíå110‰∏á‰∏™Âü∫‰∫éÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÁöÑ‰ª£ÁêÜÁ∫ßÈ£éÈô©Ê≥®ÈáäÊ†∑Êú¨ÔºåÊîØÊåÅÊó∂Á©∫Êé®ÁêÜ„ÄÇÈÄöËøáÂØπ‰∏çÂêåÊèêÁ§∫ÊäÄÊúØÁöÑÂü∫ÂáÜÊµãËØïÔºåÊàë‰ª¨ÂèëÁé∞Áé∞ÊúâÁöÑVLMsÂú®Êó∂Á©∫Êé®ÁêÜÊñπÈù¢Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂáÜÁ°ÆÁéá‰ªÖ‰∏∫33%„ÄÇÁªèËøáÂæÆË∞ÉÁöÑ7B VLM‰ª£ÁêÜÂ∞ÜÂáÜÁ°ÆÁéáÊèêÈ´òÂà∞41%ÔºåÂπ∂Â∞ÜÂª∂ËøüÂáèÂ∞ë‰∫Ü75%ÔºåÂ±ïÁ§∫‰∫ÜÊòæÂºèÁöÑÊó∂Á©∫Êé®ÁêÜËÉΩÂäõÔºåÊ†áÂøóÁùÄÂú®Ëá™Âä®È©æÈ©∂È¢ÜÂüüÁöÑÈáçË¶ÅËøõÂ±ï„ÄÇ', title='NuRiskÔºöÊèêÂçáËá™Âä®È©æÈ©∂ÁöÑÊó∂Á©∫Êé®ÁêÜËÉΩÂäõ'))
[06.10.2025 09:16] Using data from previous issue: {"categories": ["#3d", "#games", "#optimization"], "emoji": "üî∫", "ru": {"title": "–¢—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∏ –≤–º–µ—Å—Ç–æ –≥–∞—É—Å—Å–∏–∞–Ω: –ø—Ä—è–º–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è mesh'–µ–π –¥–ª—è real-time 3D —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞", "desc": "Triangle Splatting+ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å—Ü–µ–Ω, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–ø—Ä—è–º—É—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∏ –≤ –¥–∏—Ñ—Ñ–µ—Ä
[06.10.2025 09:16] Renaming data file.
[06.10.2025 09:16] Renaming previous data. hf_papers.json to ./d/2025-10-06.json
[06.10.2025 09:16] Saving new data file.
[06.10.2025 09:16] Generating page.
[06.10.2025 09:16] Renaming previous page.
[06.10.2025 09:16] Renaming previous data. index.html to ./d/2025-10-06.html
[06.10.2025 09:16] Writing result.
[06.10.2025 09:16] Renaming log file.
[06.10.2025 09:16] Renaming previous data. log.txt to ./logs/2025-10-06_last_log.txt
