[16.01.2026 10:27] Read previous papers.
[16.01.2026 10:27] Generating top page (month).
[16.01.2026 10:27] Writing top page (month).
[16.01.2026 11:21] Read previous papers.
[16.01.2026 11:21] Get feed.
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09668
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10477
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08763
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09667
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07641
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10305
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10061
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10402
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10332
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10712
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10527
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10156
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10103
[16.01.2026 11:21] Extract page data from URL. URL: https://huggingface.co/papers/2601.02242
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09881
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10611
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09142
[16.01.2026 11:21] Extract page data from URL. URL: https://huggingface.co/papers/2601.10714
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10592
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06431
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10553
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10129
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08881
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10201
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10080
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09923
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10547
[16.01.2026 11:21] Extract page data from URL. URL: https://huggingface.co/papers/2601.10124
[16.01.2026 11:21] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10716
[16.01.2026 11:21] Extract page data from URL. URL: https://huggingface.co/papers/2601.00756
[16.01.2026 11:21] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.01.2026 11:21] No deleted papers detected.
[16.01.2026 11:21] Downloading and parsing papers (pdf, html). Total: 30.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.09668.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.09668.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.09668.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10477.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10477.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10477.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.08763.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.08763.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.08763.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.09667.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.09667.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.09667.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.07641.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.07641.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.07641.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10305.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10305.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10305.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10061.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10061.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10061.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10402.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10402.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10402.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10332.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10332.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10332.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10712.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10712.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10712.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10527.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10527.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10527.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10156.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10156.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10156.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10103.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10103.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10103.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.02242.
[16.01.2026 11:21] Downloading paper 2601.02242 from https://arxiv.org/pdf/2601.02242v1...
[16.01.2026 11:21] Extracting affiliations from text.
[16.01.2026 11:21] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"VIBE: Visual Instruction Based Editor Grigorii Alekseenko* Aleksandr Gordeev* Irina Tolstykh* Bulat Suleimanov Vladimir Dokholyan Georgii Fedorov Sergey Yakubson Aleksandra Tsybina Mikhail Chernyshov Maksim Kuprashevich R&D Department, SALUTEDEV Abstract *Equal contribution Corresponding author Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached new level, with dozens of open-source models released alongside highly capable commercial systems. However, only limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents compact, high-throughput instruction-based image editing pipeline that uses modern 2Bparameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation. Project page: https://riko0.github.io/VIBE/ 6 2 0 2 5 ] . [ 1 2 4 2 2 0 . 1 0 6 2"
[16.01.2026 11:21] Response: ```python
["R&D Department, SALUTEDEV"]
```
[16.01.2026 11:21] Deleting PDF ./assets/pdf/2601.02242.pdf.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.09881.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.09881.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.09881.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10611.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10611.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10611.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.09142.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.09142.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.09142.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10714.
[16.01.2026 11:21] Downloading paper 2601.10714 from https://arxiv.org/pdf/2601.10714v1...
[16.01.2026 11:21] Extracting affiliations from text.
[16.01.2026 11:21] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Alterbute: Editing Intrinsic Attributes of Objects in Images Tal Reiss1,2 Daniel Winter1 Matan Cohen1 Alex Rav-Acha1 Yael Pritch Ariel Shamir*1,3 Yedid Hoshen1,2 1Google 2The Hebrew University of Jerusalem 3Reichman University https://talreiss.github.io/alterbute 6 2 0 2 5 1 ] . [ 1 4 1 7 0 1 . 1 0 6 2 : r Figure 1. Given an input image (center) and text prompt describing the desired intrinsic attribute, Alterbute performs object intrinsic attribute edits, specifically changes to color, texture, material or shape, while faithfully preserving the objects identity. "
[16.01.2026 11:21] Response: ```python
['Google', 'The Hebrew University of Jerusalem', 'Reichman University']
```
[16.01.2026 11:21] Deleting PDF ./assets/pdf/2601.10714.pdf.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10592.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10592.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10592.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.06431.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.06431.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.06431.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10553.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10553.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10553.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10129.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10129.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10129.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.08881.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.08881.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.08881.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10201.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10201.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10201.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10080.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10080.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10080.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.09923.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.09923.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.09923.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10547.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10547.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10547.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10124.
[16.01.2026 11:21] Downloading paper 2601.10124 from https://arxiv.org/pdf/2601.10124v1...
[16.01.2026 11:21] Extracting affiliations from text.
[16.01.2026 11:21] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 5 1 ] . [ 1 4 2 1 0 1 . 1 0 6 2 : r VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation Sicheng Yang1 Zhaohu Xing1 Lei Zhu1,2 1The Hong Kong University of Science and Technology (Guangzhou) 2The Hong Kong University of Science and Technology "
[16.01.2026 11:21] Response: ```python
[
    "The Hong Kong University of Science and Technology (Guangzhou)",
    "The Hong Kong University of Science and Technology"
]
```
[16.01.2026 11:21] Deleting PDF ./assets/pdf/2601.10124.pdf.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.10716.
[16.01.2026 11:21] Extra JSON file exists (./assets/json/2601.10716.json), skip PDF parsing.
[16.01.2026 11:21] Paper image links file exists (./assets/img_data/2601.10716.json), skip HTML parsing.
[16.01.2026 11:21] Success.
[16.01.2026 11:21] Downloading and parsing paper https://huggingface.co/papers/2601.00756.
[16.01.2026 11:21] Downloading paper 2601.00756 from https://arxiv.org/pdf/2601.00756v1...
[16.01.2026 11:22] Extracting affiliations from text.
[16.01.2026 11:22] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Thomas Katraouras tkatraouras@uth.gr University of Thessaly Volos, Greece Dimitrios Rafailidis draf@uth.gr University of Thessaly Volos, Greece 6 2 0 2 2 ] . [ 1 6 5 7 0 0 . 1 0 6 2 : r Abstract Large Language Models (LLMs) have become mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memoryaugmented approaches address this by equipping LLMs with memory bank, that is an external memory module which stores information for future use. However, these methods face critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, model that compresses the memory bank through codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark questionanswering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC. CCS Concepts Computing methodologies Natural language generation; Machine learning; Artificial intelligence; Natural language processing; Online learning settings. Keywords Large Language Models, Continual Learning, Memory Bank, Memory Compression, Question Answering, Memory-Augmented LLMs ACM Reference Format: Thomas Katraouras and Di"
[16.01.2026 11:22] Response: ```python
["University of Thessaly"]
```
[16.01.2026 11:22] Deleting PDF ./assets/pdf/2601.00756.pdf.
[16.01.2026 11:22] Success.
[16.01.2026 11:22] Enriching papers with extra data.
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 0. STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.  					AI-generated summary 				 We pre...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 1. Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.  					AI-generated summary 				 As hubs of human activity, urban surfaces consist of a wealth of semantic ...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 2. Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.  					AI-generated summary 				 Reinforcement learning (RL) has become a central ...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 3. Multi-Agent Test-Time Reinforcement Learning (MATTRL) enhances multi-agent reasoning through structured textual experience injection and consensus-based decision making at inference time.  					AI-generated summary 				 Multi-agent systems have evolved into practical LLM-driven collaborators for man...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 4. Test-Time Tool Evolution enables AI agents to dynamically create and refine computational tools during inference, overcoming limitations of static tool libraries in scientific applications.  					AI-generated summary 				 The central challenge of AI for Science is not reasoning alone, but the abilit...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 5. A large-scale Chinese image-text dataset called DanQing is introduced to advance vision-language pretraining, demonstrating superior performance in various downstream tasks through continual pretraining of the SigLIP2 model.  					AI-generated summary 				 Vision-Language Pre-training (VLP) models d...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 6. Chain-of-Frame reasoning is integrated into text-to-image generation through progressive visual refinement with explicit intermediate steps, achieving superior performance on benchmark datasets.  					AI-generated summary 				 Recent video generation models have revealed the emergence of Chain-of-Fr...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 7. ML-Master 2.0 enables long-term autonomous machine learning engineering through hierarchical cognitive caching that manages extended context and learns from execution traces.  					AI-generated summary 				 The advancement of artificial intelligence toward agentic science is currently bottlenecked b...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 8. Text-to-image diffusion models enhanced with language model reasoning capabilities achieve improved factual consistency and semantic alignment through a think-then-generate paradigm with dual-gradient reinforcement optimization.  					AI-generated summary 				 Recent progress in text-to-image (T2I) ...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 9. MatchTIR enhances LLM reasoning by introducing fine-grained credit assignment through bipartite matching and dual-level advantage estimation for tool-integrated tasks.  					AI-generated summary 				 Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by int...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 10. Frontier language and vision models show varied safety performance across different evaluation criteria, highlighting the need for comprehensive, standardized safety assessments.  					AI-generated summary 				 The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models ...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 11. A guardrail model and reasoning framework are developed to detect and prevent unsafe tool invocations in LLM agents, improving both safety and task performance under adversarial conditions.  					AI-generated summary 				 While LLM-based agents can interact with environments via invoking external to...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 12. FlowAct-R1 enables real-time interactive humanoid video generation with high-fidelity synthesis and low-latency responsiveness through MMDiT architecture and chunkwise diffusion forcing strategies.  					AI-generated summary 				 Interactive humanoid video generation aims to synthesize lifelike visu...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 13. A compact image editing system uses a 2B-parameter model for guidance and a 1.6B-parameter diffusion model to achieve high-quality edits with low computational requirements and strict source consistency.  					AI-generated summary 				 Instruction-based image editing is among the fastest developing ...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 14. Transition Matching Distillation enables efficient video generation by distilling diffusion models into few-step predictors using conditional flows and semantic representation decomposition.  					AI-generated summary 				 Large video diffusion and flow models have achieved remarkable success in hig...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 15. Molmo2 is a new open-source video-language model family that achieves state-of-the-art performance through novel datasets and training methods, particularly excelling in video grounding tasks without relying on proprietary models.  					AI-generated summary 				 Today's strongest video-language mode...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 16. EvasionBench introduces a large-scale benchmark for detecting evasive responses in earnings calls using a multi-model annotation framework that leverages disagreement between advanced language models to identify challenging examples, resulting in a highly accurate model with significantly reduced in...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 17. Alterbute presents a diffusion-based approach for editing object intrinsic attributes while preserving identity and context through relaxed training objectives and visual named entities for scalable supervision.  					AI-generated summary 				 We introduce Alterbute, a diffusion-based method for edi...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 18. Action100M is a large-scale video action dataset constructed from internet instructional videos using automated pipelines with V-JEPA embeddings and GPT-based reasoning for structured annotations.  					AI-generated summary 				 Inferring physical actions from visual observations is a fundamental ca...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 19. A logic-structured training framework explicitly models instruction logic through constraint-aware reward mechanisms, improving instruction-following and reasoning capabilities in large language models.  					AI-generated summary 				 Instruction-following is critical for large language models, but ...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 20. Latent world models enhance video generation physics plausibility through inference-time alignment and trajectory steering, achieving superior performance in challenging benchmarks.  					AI-generated summary 				 State-of-the-art video generative models produce promising visual content yet often vi...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 21. LaViT addresses the perception gap in multimodal reasoning by aligning latent visual thoughts through autoregressive reconstruction of visual semantics and attention trajectories, improving visual grounding and model performance.  					AI-generated summary 				 Current multimodal latent reasoning of...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 22. A novel framework injects semantic intent into Mixture-of-Experts routing for image generation and editing, resolving task interference through hierarchical task annotation and predictive alignment regularization.  					AI-generated summary 				 Unified image generation and editing models suffer fro...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 23. Process Reward Learning decomposes reinforcement learning objectives into intermediate steps to provide fine-grained supervision for improving large language model reasoning abilities.  					AI-generated summary 				 Improving the reasoning abilities of Large Language Models (LLMs) has been a contin...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 24. Executable and interpretable decision trees are induced from narrative data to create robust behavioral profiles for role-playing agents, outperforming traditional methods in consistency and reliability.  					AI-generated summary 				 Role-playing (RP) agents rely on behavioral profiles to act cons...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 25. Computer Use Agents face security challenges from prompt injection attacks, but a single-shot planning approach with architectural isolation enables secure autonomous task execution while maintaining performance.  					AI-generated summary 				 AI agents are vulnerable to prompt injection attacks, w...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 26. A suite of open-source music foundation models is introduced, featuring components for audio-text alignment, lyric recognition, music coding, and large language model-based song generation with controllable attributes and scalable parameterization.  					AI-generated summary 				 We present a family...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 27. VQ-Seg introduces a vector quantization-based perturbation method for medical image segmentation that replaces dropout with a controllable quantized perturbation module while maintaining performance through a dual-branch architecture and foundation model guidance.  					AI-generated summary 				 Con...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 28. WildRayZer is a self-supervised framework for novel view synthesis in dynamic environments that uses analysis-by-synthesis to handle moving cameras and objects through motion masking and gradient gating.  					AI-generated summary 				 We present WildRayZer, a self-supervised framework for novel vie...
[16.01.2026 11:22] ********************************************************************************
[16.01.2026 11:22] Abstract 29. A memory-augmented continual learning approach for large language models that compresses memory banks through codebook optimization while maintaining retention accuracy.  					AI-generated summary 				 Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as da...
[16.01.2026 11:22] Read previous papers.
[16.01.2026 11:22] Generating reviews via LLM API.
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#architecture", "#open_source", "#reasoning", "#training"], "emoji": "üî¨", "ru": {"title": "–ú–æ—â–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π AI –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–º –ø–∞–∫–µ—Ç–µ", "desc": "STEP3-VL-10B ‚Äî —ç—Ç–æ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–∞–∑–º–µ—Ä–æ–º 10 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –¥–æ—Å—Ç–∏–≥–∞
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#reasoning", "#rl", "#dataset", "#open_source"], "emoji": "üõ∞Ô∏è", "ru": {"title": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–º—ã—Å–ª–∞ –≥–æ—Ä–æ–¥–∞ —á–µ—Ä–µ–∑ –¥–∏–∞–ª–æ–≥ –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≥–æ—Ä–æ–¥—Å–∫–∏—Ö —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ vision-language –º–æ–¥–µ
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#alignment", "#reasoning", "#optimization", "#rl", "#training"], "emoji": "üéØ", "ru": {"title": "–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —á–µ—Ä–µ–∑ –ø–æ–æ—â—Ä–µ–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π –≤ RL –¥–ª—è LLM", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø—É—Ç—ë–º –≤–≤–µ–¥–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#alignment", "#reasoning", "#inference", "#rl", "#agents", "#training"], "emoji": "ü§ù", "ru": {"title": "–ú–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ —á–µ—Ä–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–ø—ã—Ç –∏ –∫–æ–Ω—Å–µ–Ω—Å—É—Å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MATTRL ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#science", "#open_source", "#dataset", "#agents", "#benchmark", "#reasoning"], "emoji": "üîß", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ AI –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ Test-Time Tool Evolution (TTE), –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM-–∞–≥–µ–Ω—Ç–∞–º –¥–∏–Ω–∞–º
[16.01.2026 11:22] Using data from previous issue: {"categories": [], "emoji": "üá®üá≥", "ru": {"title": "DanQing: –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–∏—Ç–∞–π—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è vision-language pretraining", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç DanQing, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 100 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞–Ω –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è vision-la
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#video", "#dataset"], "emoji": "üé¨", "ru": {"title": "–ü–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –∫–∞–¥—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç Chain-of-Frame —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ–≥
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#long_context", "#science", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∑–∞ –¥–æ–ª–≥–∏–µ —Å—Ä–æ–∫–∏", "desc": "ML-Master 2.0 ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∞–≥–µ–Ω—Ç, —Å–ø–æ—Å–æ–±–Ω—ã–π –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤ –≤—Ä–µ–º–µ
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#diffusion", "#rlhf", "#reasoning", "#benchmark", "#optimization", "#multimodal", "#training"], "emoji": "üß†", "ru": {"title": "–ú—ã—à–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º: —É—Å–∏–ª–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#rl", "#agents", "#training", "#small_models"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞—Å–ª—É–≥ —á–µ—Ä–µ–∑ –±–∏partite matching –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ MatchTIR –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –≤–Ω–µ—à–Ω–∏
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#security", "#benchmark", "#ethics", "#multimodal", "#multilingual", "#survey"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –Ω–∞ –º–Ω–æ–≥–∏—Ö —Ñ—Ä–æ–Ω—Ç–∞—Ö: –ø–æ—á–µ–º—É –Ω—É–∂–Ω—ã –µ–¥–∏–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –æ—Ü–µ–Ω–∫–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å–µ–º–∏ –ø–µ—Ä–µ–¥–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ 
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#benchmark", "#alignment", "#rl", "#agents", "#security", "#reasoning", "#training"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –æ—Ç –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å –∑–∞—â–∏
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#multimodal", "#inference", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –≤–∏–¥–µ–æ —Å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–º–∏ –≥—É–º–∞–Ω–æ–∏–¥–∞–º–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "FlowAct-R1 ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å —á–µ–ª–æ–≤–µ–∫–æ–ø–æ–¥–æ–±–Ω—ã–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ
[16.01.2026 11:22] Querying the API.
[16.01.2026 11:22] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A compact image editing system uses a 2B-parameter model for guidance and a 1.6B-parameter diffusion model to achieve high-quality edits with low computational requirements and strict source consistency.  					AI-generated summary 				 Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.
[16.01.2026 11:22] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 2B-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤—É—é –º–æ–¥–µ–ª—å Qwen3-VL –¥–ª—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ 1.6B-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ Sana1.5 –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –°–∏—Å—Ç–µ–º–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏ –Ω–∏–∑–∫–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö, —Ç—Ä–µ–±—É—è –≤—Å–µ–≥–æ 24 –ì–ë GPU –ø–∞–º—è—Ç–∏ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º 2K –∑–∞ 4 —Å–µ–∫—É–Ω–¥—ã. –ù–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö ImgEdit –∏ GEdit –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –Ω–∞–º–Ω–æ–≥–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏, —Ç—Ä–µ–±—É—é—â–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –∞—Ç—Ä–∏–±—É—Ç–æ–≤, —É–¥–∞–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –∑–∞–º–µ–Ω–∞ —Ñ–æ–Ω–æ–≤. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å—Ç—Ä–æ–≥–æ–π –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –∏ —ç–∫–æ–Ω–æ–º–∏—á–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.",
  "emoji": "‚úèÔ∏è",
  "title": "–ö–æ–º–ø–∞–∫—Ç–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö"
}
```
[16.01.2026 11:22] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A compact image editing system uses a 2B-parameter model for guidance and a 1.6B-parameter diffusion model to achieve high-quality edits with low computational requirements and strict source consistency.  					AI-generated summary 				 Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation."

[16.01.2026 11:22] Response: ```python
['CV', 'SMALL_MODELS', 'MULTIMODAL', 'BENCHMARK', 'TRAINING']
```
[16.01.2026 11:22] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A compact image editing system uses a 2B-parameter model for guidance and a 1.6B-parameter diffusion model to achieve high-quality edits with low computational requirements and strict source consistency.  					AI-generated summary 				 Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation."

[16.01.2026 11:22] Response: ```python
["DIFFUSION", "OPEN_SOURCE", "OPTIMIZATION"]
```
[16.01.2026 11:22] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a compact image editing system that utilizes a 2B-parameter guidance model and a 1.6B-parameter diffusion model to perform high-quality image edits efficiently. The system is designed to maintain strict source consistency while minimizing computational costs, making it suitable for real-world applications. It outperforms larger models with significantly more parameters in various editing tasks, particularly those that require preserving the original image. The proposed method achieves fast inference times and high-resolution outputs, demonstrating the potential of smaller models in generative AI for image editing.","title":"Efficient Image Editing with Compact Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a compact image editing system that utilizes a 2B-parameter guidance model and a 1.6B-parameter diffusion model to perform high-quality image edits efficiently. The system is designed to maintain strict source consistency while minimizing computational costs, making it suitable for real-world applications. It outperforms larger models with significantly more parameters in various editing tasks, particularly those that require preserving the original image. The proposed method achieves fast inference times and high-resolution outputs, demonstrating the potential of smaller models in generative AI for image editing.', title='Efficient Image Editing with Compact Models'))
[16.01.2026 11:22] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÁ¥ßÂáëÁöÑÂõæÂÉèÁºñËæëÁ≥ªÁªüÔºå‰ΩøÁî®‰∫Ü‰∏Ä‰∏™2BÂèÇÊï∞ÁöÑÊ®°ÂûãÊù•ÊåáÂØºÁºñËæëËøáÁ®ãÔºå‰ª•Âèä‰∏Ä‰∏™1.6BÂèÇÊï∞ÁöÑÊâ©Êï£Ê®°ÂûãÊù•ÁîüÊàêÈ´òË¥®ÈáèÁöÑÂõæÂÉè„ÄÇËØ•Á≥ªÁªüÂú®ËÆ°ÁÆóÈúÄÊ±Ç‰ΩéÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§ü‰øùÊåÅ‰∏•Ê†ºÁöÑÊ∫ê‰∏ÄËá¥ÊÄßÔºåÂπ∂Âú®‰∏ªË¶ÅÁºñËæëÁ±ªÂà´‰∏≠‰øùÊåÅÈ´òË¥®Èáè„ÄÇ‰∏é‰º†ÁªüÁöÑ6BÂà∞20BÂèÇÊï∞ÁöÑÊâ©Êï£Ê®°ÂûãÁõ∏ÊØîÔºåËøôÁßçÊñπÊ≥ïÂú®ÊÄßËÉΩ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶Å‰øùÁïôËæìÂÖ•ÂõæÂÉèÁöÑÁºñËæë‰ªªÂä°‰∏≠ÔºåÂ¶ÇÂ±ûÊÄßË∞ÉÊï¥ÂíåÂØπË±°ÁßªÈô§„ÄÇËØ•Ê®°ÂûãÂú®NVIDIA H100‰∏äËøêË°åÊó∂ÔºåËÉΩÂ§üÂú®Á∫¶4ÁßíÂÜÖÁîüÊàêÈ´òËææ2KÂàÜËæ®ÁéáÁöÑÁºñËæëÂõæÂÉèÔºå‰∏î‰ªÖÈúÄ24GBÁöÑGPUÂÜÖÂ≠ò„ÄÇ","title":"È´òÊïàÁ¥ßÂáëÁöÑÂõæÂÉèÁºñËæëÁ≥ªÁªü"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÁ¥ßÂáëÁöÑÂõæÂÉèÁºñËæëÁ≥ªÁªüÔºå‰ΩøÁî®‰∫Ü‰∏Ä‰∏™2BÂèÇÊï∞ÁöÑÊ®°ÂûãÊù•ÊåáÂØºÁºñËæëËøáÁ®ãÔºå‰ª•Âèä‰∏Ä‰∏™1.6BÂèÇÊï∞ÁöÑÊâ©Êï£Ê®°ÂûãÊù•ÁîüÊàêÈ´òË¥®ÈáèÁöÑÂõæÂÉè„ÄÇËØ•Á≥ªÁªüÂú®ËÆ°ÁÆóÈúÄÊ±Ç‰ΩéÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§ü‰øùÊåÅ‰∏•Ê†ºÁöÑÊ∫ê‰∏ÄËá¥ÊÄßÔºåÂπ∂Âú®‰∏ªË¶ÅÁºñËæëÁ±ªÂà´‰∏≠‰øùÊåÅÈ´òË¥®Èáè„ÄÇ‰∏é‰º†ÁªüÁöÑ6BÂà∞20BÂèÇÊï∞ÁöÑÊâ©Êï£Ê®°ÂûãÁõ∏ÊØîÔºåËøôÁßçÊñπÊ≥ïÂú®ÊÄßËÉΩ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶Å‰øùÁïôËæìÂÖ•ÂõæÂÉèÁöÑÁºñËæë‰ªªÂä°‰∏≠ÔºåÂ¶ÇÂ±ûÊÄßË∞ÉÊï¥ÂíåÂØπË±°ÁßªÈô§„ÄÇËØ•Ê®°ÂûãÂú®NVIDIA H100‰∏äËøêË°åÊó∂ÔºåËÉΩÂ§üÂú®Á∫¶4ÁßíÂÜÖÁîüÊàêÈ´òËææ2KÂàÜËæ®ÁéáÁöÑÁºñËæëÂõæÂÉèÔºå‰∏î‰ªÖÈúÄ24GBÁöÑGPUÂÜÖÂ≠ò„ÄÇ', title='È´òÊïàÁ¥ßÂáëÁöÑÂõæÂÉèÁºñËæëÁ≥ªÁªü'))
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#video", "#architecture", "#diffusion", "#inference", "#optimization", "#open_source", "#training"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –ø–æ—Ç–æ–∫–æ–≤ —Å –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–µ–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Transition Matching Distil
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#video", "#benchmark", "#synthetic", "#multimodal", "#open_source", "#training", "#dataset"], "emoji": "üé•", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç–∞—è –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å —Ç–æ—á–Ω—ã–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –ø–æ–Ω–∏–º–∞–µ–Ω–∏–µ–º –≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "Molmo2 ‚Äî —ç—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ 
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#benchmark", "#small_models", "#dataset"], "emoji": "ü§ù", "ru": {"title": "–ö–æ–≥–¥–∞ –º–æ–¥–µ–ª–∏ –Ω–µ —Å–æ–≥–ª–∞—Å–Ω—ã ‚Äî —É—á–∏–º—Å—è –Ω–∞ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EvasionBench ‚Äî –º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —É–∫–ª–æ–Ω—á–∏–≤—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –≤ –æ—Ç—á—ë—Ç–∞—Ö –∫–æ–º–ø–∞–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ–π —Å—Ö–µ–º—ã –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏, –∫–æ
[16.01.2026 11:22] Querying the API.
[16.01.2026 11:22] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Alterbute presents a diffusion-based approach for editing object intrinsic attributes while preserving identity and context through relaxed training objectives and visual named entities for scalable supervision.  					AI-generated summary 				 We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.
[16.01.2026 11:22] Response: ```json
{
  "desc": "Alterbute –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ü–≤–µ—Ç, —Ç–µ–∫—Å—Ç—É—Ä–∞ –∏ —Ñ–æ—Ä–º–∞, –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å—Ü–µ–Ω—ã. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Å–ª–∞–±–ª–µ–Ω–Ω—É—é —Ü–µ–ª–µ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –∏–∑–º–µ–Ω—è—Ç—å –∞—Ç—Ä–∏–±—É—Ç—ã –ø–æ–¥ –≤–ª–∏—è–Ω–∏–µ–º —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏, —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –∏ –º–∞—Å–∫–∏ —Ñ–æ–Ω–∞. –ö–ª—é—á–µ–≤–æ–π –∏–Ω–Ω–æ–≤–∞—Ü–∏–µ–π —è–≤–ª—è—é—Ç—Å—è Visual Named Entities (VNE) - –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –≥—Ä—É–ø–ø–∏—Ä—É—é—Ç –æ–±—ä–µ–∫—Ç—ã —Å –æ–±—â–∏–º–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏, –ø–æ–∑–≤–æ–ª—è—è –≤–∞—Ä–∏–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç vision-language –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–æ–∫ VNE –∏–∑ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏.",
  "emoji": "üé®",
  "title": "–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏"
}
```
[16.01.2026 11:22] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Alterbute presents a diffusion-based approach for editing object intrinsic attributes while preserving identity and context through relaxed training objectives and visual named entities for scalable supervision.  					AI-generated summary 				 We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing."

[16.01.2026 11:22] Response: ```python
["CV", "TRAINING", "MULTIMODAL"]
```
[16.01.2026 11:22] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Alterbute presents a diffusion-based approach for editing object intrinsic attributes while preserving identity and context through relaxed training objectives and visual named entities for scalable supervision.  					AI-generated summary 				 We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing."

[16.01.2026 11:22] Response: ```python
["DIFFUSION", "OPEN_SOURCE"]
```
[16.01.2026 11:22] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Alterbute is a novel diffusion-based method designed for editing intrinsic attributes of objects in images, such as color and texture, while maintaining their identity and contextual integrity. It introduces a relaxed training objective that allows for flexible changes to both intrinsic and extrinsic attributes, guided by an identity reference image and a textual description of the desired changes. The method employs Visual Named Entities (VNEs) to categorize objects based on identity-defining features, facilitating scalable supervision without compromising on identity preservation. By reusing the original background and object mask during inference, Alterbute ensures that only the intended intrinsic attributes are modified, outperforming existing techniques in this domain.","title":"Alterbute: Preserve Identity While Editing Object Attributes!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Alterbute is a novel diffusion-based method designed for editing intrinsic attributes of objects in images, such as color and texture, while maintaining their identity and contextual integrity. It introduces a relaxed training objective that allows for flexible changes to both intrinsic and extrinsic attributes, guided by an identity reference image and a textual description of the desired changes. The method employs Visual Named Entities (VNEs) to categorize objects based on identity-defining features, facilitating scalable supervision without compromising on identity preservation. By reusing the original background and object mask during inference, Alterbute ensures that only the intended intrinsic attributes are modified, outperforming existing techniques in this domain.', title='Alterbute: Preserve Identity While Editing Object Attributes!'))
[16.01.2026 11:22] Response: ParsedChatCompletionMessage[Article](content='{"desc":"AlterbuteÊòØ‰∏ÄÁßçÂü∫‰∫éÊâ©Êï£ÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÁºñËæëÂõæÂÉè‰∏≠Áâ©‰ΩìÁöÑÂÜÖÂú®Â±ûÊÄßÔºåÂêåÊó∂‰øùÊåÅÁâ©‰ΩìÁöÑË∫´‰ªΩÂíåÂú∫ÊôØ‰∏ä‰∏ãÊñá„ÄÇËØ•ÊñπÊ≥ïÂÖÅËÆ∏ÊîπÂèòÁâ©‰ΩìÁöÑÈ¢úËâ≤„ÄÅÁ∫πÁêÜ„ÄÅÊùêÊñôÁîöËá≥ÂΩ¢Áä∂ÔºåËÄå‰∏ç‰ºöÂΩ±ÂìçÂÖ∂ÊÑüÁü•Ë∫´‰ªΩ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåAlterbuteÈááÁî®‰∫ÜÊîæÂÆΩÁöÑËÆ≠ÁªÉÁõÆÊ†áÔºåÁªìÂêàË∫´‰ªΩÂèÇËÄÉÂõæÂÉè„ÄÅÊèèËø∞ÁõÆÊ†áÂÜÖÂú®Â±ûÊÄßÁöÑÊñáÊú¨ÊèêÁ§∫‰ª•ÂèäÂÆö‰πâÂ§ñÈÉ®‰∏ä‰∏ãÊñáÁöÑËÉåÊôØÂõæÂÉèÂíåÁâ©‰ΩìÊé©Á†ÅÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÁÅµÊ¥ªÁöÑÂ±ûÊÄßÂèòÂåñ„ÄÇÈÄöËøá‰ΩøÁî®ËßÜËßâÂëΩÂêçÂÆû‰ΩìÔºàVNEsÔºâÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üËá™Âä®ÊèêÂèñÁªÜÁ≤íÂ∫¶ÁöÑËßÜËßâË∫´‰ªΩÁ±ªÂà´ÔºåÁ°Æ‰øùÂú®ÁºñËæëËøáÁ®ã‰∏≠‰øùÊåÅË∫´‰ªΩÁöÑÂêåÊó∂ÂÆûÁé∞ÂÜÖÂú®Â±ûÊÄßÁöÑÂèòÂåñ„ÄÇ","title":"AlterbuteÔºö‰øùÁïôË∫´‰ªΩÁöÑÁâ©‰ΩìÂ±ûÊÄßÁºñËæëÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='AlterbuteÊòØ‰∏ÄÁßçÂü∫‰∫éÊâ©Êï£ÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÁºñËæëÂõæÂÉè‰∏≠Áâ©‰ΩìÁöÑÂÜÖÂú®Â±ûÊÄßÔºåÂêåÊó∂‰øùÊåÅÁâ©‰ΩìÁöÑË∫´‰ªΩÂíåÂú∫ÊôØ‰∏ä‰∏ãÊñá„ÄÇËØ•ÊñπÊ≥ïÂÖÅËÆ∏ÊîπÂèòÁâ©‰ΩìÁöÑÈ¢úËâ≤„ÄÅÁ∫πÁêÜ„ÄÅÊùêÊñôÁîöËá≥ÂΩ¢Áä∂ÔºåËÄå‰∏ç‰ºöÂΩ±ÂìçÂÖ∂ÊÑüÁü•Ë∫´‰ªΩ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåAlterbuteÈááÁî®‰∫ÜÊîæÂÆΩÁöÑËÆ≠ÁªÉÁõÆÊ†áÔºåÁªìÂêàË∫´‰ªΩÂèÇËÄÉÂõæÂÉè„ÄÅÊèèËø∞ÁõÆÊ†áÂÜÖÂú®Â±ûÊÄßÁöÑÊñáÊú¨ÊèêÁ§∫‰ª•ÂèäÂÆö‰πâÂ§ñÈÉ®‰∏ä‰∏ãÊñáÁöÑËÉåÊôØÂõæÂÉèÂíåÁâ©‰ΩìÊé©Á†ÅÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÁÅµÊ¥ªÁöÑÂ±ûÊÄßÂèòÂåñ„ÄÇÈÄöËøá‰ΩøÁî®ËßÜËßâÂëΩÂêçÂÆû‰ΩìÔºàVNEsÔºâÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üËá™Âä®ÊèêÂèñÁªÜÁ≤íÂ∫¶ÁöÑËßÜËßâË∫´‰ªΩÁ±ªÂà´ÔºåÁ°Æ‰øùÂú®ÁºñËæëËøáÁ®ã‰∏≠‰øùÊåÅË∫´‰ªΩÁöÑÂêåÊó∂ÂÆûÁé∞ÂÜÖÂú®Â±ûÊÄßÁöÑÂèòÂåñ„ÄÇ', title='AlterbuteÔºö‰øùÁïôË∫´‰ªΩÁöÑÁâ©‰ΩìÂ±ûÊÄßÁºñËæëÊñ∞ÊñπÊ≥ï'))
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#multimodal", "#video", "#dataset"], "emoji": "üé¨", "ru": {"title": "–°—Ç–æ–º–∏–ª–ª–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —Ä–µ–≤–æ–ª—é—Ü–∏–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Action100M ‚Äî –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π –≤ –≤–∏–¥–µ–æ, —Å–æ–∑–¥–∞–Ω–Ω—ã–π –∏–∑ 1.2 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –æ–±—É—á–∞—é—â–∏—Ö –≤–∏–¥–µ–æ—Ä–æ–ª–∏–∫–æ–≤ —Å –∏–Ω
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#reasoning", "#alignment"], "emoji": "üß©", "ru": {"title": "–õ–æ–≥–∏—á–µ—Å–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ LLM", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —è–≤–Ω–æ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–Ω—Å
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#video", "#inference"], "emoji": "üé¨", "ru": {"title": "–§–∏–∑–∏–∫–∞ –≤ —Ñ–æ–∫—É—Å–µ: –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ —Å–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞—Ä—É—à–µ–Ω–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∑–∞–∫–æ–Ω–æ–≤ –≤ –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∏—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#multimodal", "#small_models", "#training"], "emoji": "üëÅÔ∏è", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º—ã—Å–ª–µ–π –≤–º–µ—Å—Ç–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ LaViT, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç—ë–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#diffusion", "#architecture", "#optimization", "#cv", "#training"], "emoji": "üé®", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Mixture-
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#optimization", "#rl", "#reasoning", "#training"], "emoji": "üéØ", "ru": {"title": "–ü–æ—à–∞–≥–æ–≤–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ: –æ—Ç –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∫ –ø—Ä–æ—Ü–µ—Å—Å—É –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Process Reward Learning (PRL), –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–±–∏–≤–∞–µ—Ç –∑–∞–¥–∞—á—É –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –ø—Ä–æ
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#dataset"], "emoji": "üé≠", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –Ω–∞–¥–µ–∂–Ω—ã—Ö –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã—Ö —Ä–æ–ª–µ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Codified Decision Trees (CDT) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π —Ä–æ–ª–µ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—Ä—Ä–∞
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#security", "#open_source"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –æ–¥–Ω—É —Ñ–∞–∑—É –¥–ª—è –∑–∞—â–∏—Ç—ã –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –æ—Ç –∞—Ç–∞–∫", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∑–∞—â–∏—Ç–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –æ—Ç –∞—Ç–∞–∫ prompt injection, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –ø–µ—Ä–µ—Ö–≤–∞—Ç–∏—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –∏ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –∫—Ä–∞–∂–µ –¥–∞
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#audio", "#architecture", "#dataset"], "emoji": "üéµ", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç—ã–µ –º—É–∑—ã–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º—É–∑—ã–∫–∏ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ–º—å—è –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º—É–∑—ã–∫–∞–ª—å–Ω—ã—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—é—â–∞—è —á–µ—Ç—ã—Ä–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
[16.01.2026 11:22] Querying the API.
[16.01.2026 11:22] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

VQ-Seg introduces a vector quantization-based perturbation method for medical image segmentation that replaces dropout with a controllable quantized perturbation module while maintaining performance through a dual-branch architecture and foundation model guidance.  					AI-generated summary 				 Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg.
[16.01.2026 11:22] Response: ```json
{
  "desc": "VQ-Seg –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤–æ–∑–º—É—â–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –¥–ª—è –ø–æ–ª—É–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∑–∞–º–µ–Ω—è—è —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π dropout –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–º –º–æ–¥—É–ª–µ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–æ–∑–º—É—â–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥–≤—É—Ö–≤–µ—Ç–≤–µ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –≥–¥–µ –æ–¥–Ω–∞ –≤–µ—Ç–≤—å –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∞ –¥—Ä—É–≥–∞—è –∑–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é, —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø—Ä–∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–∏. –î–ª—è –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏ –ø–æ—Ç–µ—Ä–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤–æ –≤—Ä–µ–º—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∞–¥–∞–ø—Ç–µ—Ä Post-VQ Feature Adapter, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–Ω–∞–Ω–∏—è –æ—Ç foundation model. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –Ω–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ —Ä–∞–∫–∞ –ª–µ–≥–∫–∏—Ö –∏ –¥—Ä—É–≥–∏—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö.",
  "emoji": "ü´Å",
  "title": "–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–µ –≤–æ–∑–º—É—â–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ dropout –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"
}
```
[16.01.2026 11:22] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VQ-Seg introduces a vector quantization-based perturbation method for medical image segmentation that replaces dropout with a controllable quantized perturbation module while maintaining performance through a dual-branch architecture and foundation model guidance.  					AI-generated summary 				 Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg."

[16.01.2026 11:22] Response: ```python
['HEALTHCARE', 'DATASET', 'ARCHITECTURE', 'TRAINING']
```
[16.01.2026 11:22] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VQ-Seg introduces a vector quantization-based perturbation method for medical image segmentation that replaces dropout with a controllable quantized perturbation module while maintaining performance through a dual-branch architecture and foundation model guidance.  					AI-generated summary 				 Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg."

[16.01.2026 11:22] Response: ```python
['OPTIMIZATION']
```
[16.01.2026 11:22] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VQ-Seg is a novel method for medical image segmentation that utilizes vector quantization to enhance performance while replacing traditional dropout techniques. It introduces a Quantized Perturbation Module (QPM) that shuffles codebook indices to create controlled perturbations, improving regularization without the need for sensitive hyperparameter tuning. The method employs a dual-branch architecture to share post-quantization features between image reconstruction and segmentation tasks, minimizing information loss. Additionally, a Post-VQ Feature Adapter (PFA) integrates guidance from a foundation model to retain high-level semantic information, demonstrating superior performance on a large-scale Lung Cancer dataset and other benchmarks.","title":"Revolutionizing Medical Image Segmentation with Vector Quantization"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VQ-Seg is a novel method for medical image segmentation that utilizes vector quantization to enhance performance while replacing traditional dropout techniques. It introduces a Quantized Perturbation Module (QPM) that shuffles codebook indices to create controlled perturbations, improving regularization without the need for sensitive hyperparameter tuning. The method employs a dual-branch architecture to share post-quantization features between image reconstruction and segmentation tasks, minimizing information loss. Additionally, a Post-VQ Feature Adapter (PFA) integrates guidance from a foundation model to retain high-level semantic information, demonstrating superior performance on a large-scale Lung Cancer dataset and other benchmarks.', title='Revolutionizing Medical Image Segmentation with Vector Quantization'))
[16.01.2026 11:22] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VQ-SegÊòØ‰∏ÄÁßçÂü∫‰∫éÂêëÈáèÈáèÂåñÁöÑÂåªÂ≠¶ÂõæÂÉèÂàÜÂâ≤ÊñπÊ≥ïÔºåÂÆÉÁî®ÂèØÊéßÁöÑÈáèÂåñÊâ∞Âä®Ê®°ÂùóÊõø‰ª£‰∫Ü‰º†ÁªüÁöÑdropoutÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂàÜÂâ≤ÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÈááÁî®ÂèåÂàÜÊîØÊû∂ÊûÑÔºåÁ°Æ‰øùÂú®ÈáèÂåñÂêéÁâπÂæÅÁ©∫Èó¥‰∏≠ÔºåÂõæÂÉèÈáçÂª∫ÂíåÂàÜÂâ≤‰ªªÂä°ÂÖ±‰∫´‰ø°ÊÅØÔºåÂáèÂ∞ë‰ø°ÊÅØÊçüÂ§±„ÄÇÈÄöËøáÂºïÂÖ•ÂêéÈáèÂåñÁâπÂæÅÈÄÇÈÖçÂô®ÔºåVQ-SegËÉΩÂ§üÁªìÂêàÂü∫Á°ÄÊ®°ÂûãÁöÑÊåáÂØºÔºåË°•ÂÖÖÈáèÂåñËøáÁ®ã‰∏≠‰∏¢Â§±ÁöÑÈ´òÂ±ÇËØ≠‰πâ‰ø°ÊÅØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVQ-SegÂú®Â§ßËßÑÊ®°ËÇ∫ÁôåÊï∞ÊçÆÈõÜÂíåÂÖ∂‰ªñÂÖ¨ÂÖ±Âü∫ÂáÜ‰∏äË°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ","title":"VQ-SegÔºöÂèØÊéßÈáèÂåñÊâ∞Âä®ÁöÑÂåªÂ≠¶ÂõæÂÉèÂàÜÂâ≤Êñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VQ-SegÊòØ‰∏ÄÁßçÂü∫‰∫éÂêëÈáèÈáèÂåñÁöÑÂåªÂ≠¶ÂõæÂÉèÂàÜÂâ≤ÊñπÊ≥ïÔºåÂÆÉÁî®ÂèØÊéßÁöÑÈáèÂåñÊâ∞Âä®Ê®°ÂùóÊõø‰ª£‰∫Ü‰º†ÁªüÁöÑdropoutÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂàÜÂâ≤ÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÈááÁî®ÂèåÂàÜÊîØÊû∂ÊûÑÔºåÁ°Æ‰øùÂú®ÈáèÂåñÂêéÁâπÂæÅÁ©∫Èó¥‰∏≠ÔºåÂõæÂÉèÈáçÂª∫ÂíåÂàÜÂâ≤‰ªªÂä°ÂÖ±‰∫´‰ø°ÊÅØÔºåÂáèÂ∞ë‰ø°ÊÅØÊçüÂ§±„ÄÇÈÄöËøáÂºïÂÖ•ÂêéÈáèÂåñÁâπÂæÅÈÄÇÈÖçÂô®ÔºåVQ-SegËÉΩÂ§üÁªìÂêàÂü∫Á°ÄÊ®°ÂûãÁöÑÊåáÂØºÔºåË°•ÂÖÖÈáèÂåñËøáÁ®ã‰∏≠‰∏¢Â§±ÁöÑÈ´òÂ±ÇËØ≠‰πâ‰ø°ÊÅØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVQ-SegÂú®Â§ßËßÑÊ®°ËÇ∫ÁôåÊï∞ÊçÆÈõÜÂíåÂÖ∂‰ªñÂÖ¨ÂÖ±Âü∫ÂáÜ‰∏äË°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ', title='VQ-SegÔºöÂèØÊéßÈáèÂåñÊâ∞Âä®ÁöÑÂåªÂ≠¶ÂõæÂÉèÂàÜÂâ≤Êñ∞ÊñπÊ≥ï'))
[16.01.2026 11:22] Using data from previous issue: {"categories": ["#optimization"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –Ω–æ–≤—ã—Ö –≤–∏–¥–æ–≤ –≤ –¥–∏–Ω–∞–º–∏—á–Ω–æ–º –º–∏—Ä–µ —á–µ—Ä–µ–∑ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è", "desc": "WildRayZer ‚Äî —ç—Ç–æ self-supervised —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö –≤–∏–¥–æ–≤ (novel view synthesis) –≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ö, –≥–¥–µ –¥–≤–∏–∂—É—Ç—Å—è –∫–∞–∫ –∫–∞–º–µ—Ä–∞, —Ç–∞–∫ –∏ –æ–±—ä–µ–∫—Ç—ã. –ú–µ—Ç–æ–¥ 
[16.01.2026 11:22] Querying the API.
[16.01.2026 11:22] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A memory-augmented continual learning approach for large language models that compresses memory banks through codebook optimization while maintaining retention accuracy.  					AI-generated summary 				 Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.
[16.01.2026 11:22] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç MBC ‚Äî –º–µ—Ç–æ–¥ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞ –≤–Ω–µ—à–Ω–µ–π –ø–∞–º—è—Ç–∏. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∫–æ–¥–æ–≤–æ–π –∫–Ω–∏–≥–∏ –¥–ª—è —Å–∂–∞—Ç–∏—è –ø–∞–º—è—Ç–∏ –∏ –º–µ—Ö–∞–Ω–∏–∑–º –æ–Ω–ª–∞–π–Ω-–ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∫–æ–ª–ª–∞–ø—Å–∞ –∫–æ–¥–æ–≤–æ–π –∫–Ω–∏–≥–∏. –î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–∂–∞—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –ø–∞–º—è—Ç–∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–Ω–≥–∞ –≤ —Å–ª–æ—è—Ö –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ —Å–Ω–∏–∂–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –ø–∞–º—è—Ç–∏ –¥–æ 0,3% –æ—Ç –±–∞–∑–æ–≤–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å —É–¥–µ—Ä–∂–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –ø–æ—Ç–æ–∫–∞—Ö –¥–∞–Ω–Ω—ã—Ö.",
  "emoji": "üíæ",
  "title": "–°–∂–∏–º–∞–µ–º –ø–∞–º—è—Ç—å –º–æ–¥–µ–ª–µ–π, —Å–æ—Ö—Ä–∞–Ω—è—è –∑–Ω–∞–Ω–∏—è"
}
```
[16.01.2026 11:22] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A memory-augmented continual learning approach for large language models that compresses memory banks through codebook optimization while maintaining retention accuracy.  					AI-generated summary 				 Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC."

[16.01.2026 11:22] Response: ```python
["TRAINING", "ARCHITECTURE"]
```

**Justification:**

- **TRAINING**: The paper focuses on improving model training and fine-tuning methods, specifically addressing continual learning, catastrophic forgetting prevention, and online adaptation learning for LLMs.

- **ARCHITECTURE**: The paper proposes novel neural architecture components, including memory-augmented approaches with codebook optimization and Key-Value Low-Rank Adaptation in attention layers.
[16.01.2026 11:22] Error. Failed to parse JSON from LLM. ["TRAINING", "ARCHITECTURE"]


**Justification:**

- **TRAINING**: The paper focuses on improving model training and fine-tuning methods, specifically addressing continual learning, catastrophic forgetting prevention, and online adaptation learning for LLMs.

- **ARCHITECTURE**: The paper proposes novel neural architecture components, including memory-augmented approaches with codebook optimization and Key-Value Low-Rank Adaptation in attention layers.
[16.01.2026 11:22] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A memory-augmented continual learning approach for large language models that compresses memory banks through codebook optimization while maintaining retention accuracy.  					AI-generated summary 				 Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC."

[16.01.2026 11:22] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```

**Justification:**

- **OPTIMIZATION**: The paper focuses on optimizing memory efficiency through codebook optimization and compression strategies during continual learning, which directly relates to advancing training and operational optimization methods.

- **OPEN_SOURCE**: The paper explicitly states "Our code is publicly available at https://github.com/Thomkat/MBC," indicating the authors are releasing their code/framework to the public.
[16.01.2026 11:22] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "OPEN_SOURCE"]


**Justification:**

- **OPTIMIZATION**: The paper focuses on optimizing memory efficiency through codebook optimization and compression strategies during continual learning, which directly relates to advancing training and operational optimization methods.

- **OPEN_SOURCE**: The paper explicitly states "Our code is publicly available at https://github.com/Thomkat/MBC," indicating the authors are releasing their code/framework to the public.
[16.01.2026 11:22] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel approach called MBC, which enhances large language models (LLMs) by integrating a memory-augmented continual learning framework. It addresses the challenge of catastrophic forgetting by using a memory bank that is optimized through codebook compression, allowing the model to retain important information while adapting to new data. The proposed method includes an online resetting mechanism to maintain learning stability and prevent codebook collapse. Experimental results show that MBC significantly reduces the memory bank size while preserving high retention accuracy, making it efficient for real-world applications.","title":"Efficient Memory Management for Continual Learning in LLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a novel approach called MBC, which enhances large language models (LLMs) by integrating a memory-augmented continual learning framework. It addresses the challenge of catastrophic forgetting by using a memory bank that is optimized through codebook compression, allowing the model to retain important information while adapting to new data. The proposed method includes an online resetting mechanism to maintain learning stability and prevent codebook collapse. Experimental results show that MBC significantly reduces the memory bank size while preserving high retention accuracy, making it efficient for real-world applications.', title='Efficient Memory Management for Continual Learning in LLMs'))
[16.01.2026 11:22] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËÆ∞ÂøÜÂ¢ûÂº∫ÁöÑÊåÅÁª≠Â≠¶‰π†ÊñπÊ≥ïÔºåÊó®Âú®‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑËÆ∞ÂøÜÂ∫ì„ÄÇÈÄöËøá‰ª£Á†ÅÊú¨‰ºòÂåñÁ≠ñÁï•ÔºåMBCÊ®°ÂûãÂú®Âú®Á∫øÈÄÇÂ∫îÂ≠¶‰π†ËøáÁ®ã‰∏≠ÊúâÊïàÂéãÁº©ËÆ∞ÂøÜÂ∫ìÔºåÂêåÊó∂‰øùÊåÅÁü•ËØÜ‰øùÁïôÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ïÂºïÂÖ•‰∫ÜÂú®Á∫øÈáçÁΩÆÊú∫Âà∂Ôºå‰ª•Èò≤Ê≠¢‰ª£Á†ÅÊú¨Â¥©Ê∫ÉÔºåÂπ∂Âú®Ê≥®ÊÑèÂäõÂ±Ç‰∏≠ÈááÁî®‰∫ÜÈîÆÂÄº‰ΩéÁß©ÈÄÇÂ∫îÊäÄÊúØÔºåÊèêÂçá‰∫ÜÂéãÁº©ËÆ∞ÂøÜË°®Á§∫ÁöÑÂà©Áî®ÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMBCÂú®‰øùÊåÅÈ´ò‰øùÁïôÂáÜÁ°ÆÁéáÁöÑÂêåÊó∂ÔºåÂ∞ÜËÆ∞ÂøÜÂ∫ìÂ§ßÂ∞èÂáèÂ∞ëÂà∞0.3%„ÄÇ","title":"ËÆ∞ÂøÜÂ¢ûÂº∫ÁöÑÊåÅÁª≠Â≠¶‰π†Ôºå‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÁü•ËØÜ‰øùÁïô"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËÆ∞ÂøÜÂ¢ûÂº∫ÁöÑÊåÅÁª≠Â≠¶‰π†ÊñπÊ≥ïÔºåÊó®Âú®‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑËÆ∞ÂøÜÂ∫ì„ÄÇÈÄöËøá‰ª£Á†ÅÊú¨‰ºòÂåñÁ≠ñÁï•ÔºåMBCÊ®°ÂûãÂú®Âú®Á∫øÈÄÇÂ∫îÂ≠¶‰π†ËøáÁ®ã‰∏≠ÊúâÊïàÂéãÁº©ËÆ∞ÂøÜÂ∫ìÔºåÂêåÊó∂‰øùÊåÅÁü•ËØÜ‰øùÁïôÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ïÂºïÂÖ•‰∫ÜÂú®Á∫øÈáçÁΩÆÊú∫Âà∂Ôºå‰ª•Èò≤Ê≠¢‰ª£Á†ÅÊú¨Â¥©Ê∫ÉÔºåÂπ∂Âú®Ê≥®ÊÑèÂäõÂ±Ç‰∏≠ÈááÁî®‰∫ÜÈîÆÂÄº‰ΩéÁß©ÈÄÇÂ∫îÊäÄÊúØÔºåÊèêÂçá‰∫ÜÂéãÁº©ËÆ∞ÂøÜË°®Á§∫ÁöÑÂà©Áî®ÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMBCÂú®‰øùÊåÅÈ´ò‰øùÁïôÂáÜÁ°ÆÁéáÁöÑÂêåÊó∂ÔºåÂ∞ÜËÆ∞ÂøÜÂ∫ìÂ§ßÂ∞èÂáèÂ∞ëÂà∞0.3%„ÄÇ', title='ËÆ∞ÂøÜÂ¢ûÂº∫ÁöÑÊåÅÁª≠Â≠¶‰π†Ôºå‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÁü•ËØÜ‰øùÁïô'))
[16.01.2026 11:22] Renaming data file.
[16.01.2026 11:22] Renaming previous data. hf_papers.json to ./d/2026-01-16.json
[16.01.2026 11:22] Saving new data file.
[16.01.2026 11:22] Generating page.
[16.01.2026 11:22] Renaming previous page.
[16.01.2026 11:22] Renaming previous data. index.html to ./d/2026-01-16.html
[16.01.2026 11:22] Writing result.
[16.01.2026 11:22] Renaming log file.
[16.01.2026 11:22] Renaming previous data. log.txt to ./logs/2026-01-16_last_log.txt
