[17.02.2026 08:37] Read previous papers.
[17.02.2026 08:37] Generating top page (month).
[17.02.2026 08:37] Writing top page (month).
[17.02.2026 09:40] Read previous papers.
[17.02.2026 09:40] Get feed.
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14699
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14492
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14234
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14041
[17.02.2026 09:40] Extract page data from URL. URL: https://huggingface.co/papers/2602.14367
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13949
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13823
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14178
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12876
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14721
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14534
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14147
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13344
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09185
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14689
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13195
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12299
[17.02.2026 09:40] Extract page data from URL. URL: https://huggingface.co/papers/2602.09319
[17.02.2026 09:40] Extract page data from URL. URL: https://huggingface.co/papers/2602.07673
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14696
[17.02.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14560
[17.02.2026 09:40] Extract page data from URL. URL: https://huggingface.co/papers/2602.07824
[17.02.2026 09:40] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.02.2026 09:40] No deleted papers detected.
[17.02.2026 09:40] Downloading and parsing papers (pdf, html). Total: 22.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.14699.
[17.02.2026 09:40] Extra JSON file exists (./assets/json/2602.14699.json), skip PDF parsing.
[17.02.2026 09:40] Paper image links file exists (./assets/img_data/2602.14699.json), skip HTML parsing.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.14492.
[17.02.2026 09:40] Extra JSON file exists (./assets/json/2602.14492.json), skip PDF parsing.
[17.02.2026 09:40] Paper image links file exists (./assets/img_data/2602.14492.json), skip HTML parsing.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.14234.
[17.02.2026 09:40] Extra JSON file exists (./assets/json/2602.14234.json), skip PDF parsing.
[17.02.2026 09:40] Paper image links file exists (./assets/img_data/2602.14234.json), skip HTML parsing.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.14041.
[17.02.2026 09:40] Extra JSON file exists (./assets/json/2602.14041.json), skip PDF parsing.
[17.02.2026 09:40] Paper image links file exists (./assets/img_data/2602.14041.json), skip HTML parsing.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.14367.
[17.02.2026 09:40] Downloading paper 2602.14367 from https://arxiv.org/pdf/2602.14367v1...
[17.02.2026 09:40] Extracting affiliations from text.
[17.02.2026 09:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"InnoEval: On Research Idea Evaluation as Knowledge-Grounded, Multi-Perspective Reasoning Problem Shuofei Qiao 1 2 Yunxiang Wei 1 Xuehai Wang 1 Bin Wu 2 Boyang Xue 3 Ningyu Zhang 1 Hossein A. Rahmani 2 Yanshan Wang 1 Qiang Zhang 1 Keyan Ding 1 Jeff Z. Pan 4 Huajun Chen 1 Emine Yilmaz 2 zjunlp/InnoEval innoeval.zjukg.cn 6 2 0 2 6 ] . [ 1 7 6 3 4 1 . 2 0 6 2 : r Abstract The rapid evolution of Large Language Models has catalyzed surge in scientific idea production, yet this leap has not been accompanied by matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-aJudge. To address these, we regard idea evaluation as knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, deep innovation evaluation framework designed to emulate human-level idea assessment. We apply heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts. 1. Introduction The integration of Large Language Models (LLMs) into scientific discovery (Chen et al., 2025c; Schmidgall et al., 1Zhejiang University 2University College London 3The Chinese University of Hong Kong 4The University of Edinburgh. Correspondence to: Ningyu Zhang, Huajun Chen. Co"
[17.02.2026 09:40] Response: ```python
[
    "Zhejiang University",
    "University College London",
    "The Chinese University of Hong Kong",
    "The University of Edinburgh"
]
```
[17.02.2026 09:40] Deleting PDF ./assets/pdf/2602.14367.pdf.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.13949.
[17.02.2026 09:40] Extra JSON file exists (./assets/json/2602.13949.json), skip PDF parsing.
[17.02.2026 09:40] Paper image links file exists (./assets/img_data/2602.13949.json), skip HTML parsing.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.13823.
[17.02.2026 09:40] Extra JSON file exists (./assets/json/2602.13823.json), skip PDF parsing.
[17.02.2026 09:40] Paper image links file exists (./assets/img_data/2602.13823.json), skip HTML parsing.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.14178.
[17.02.2026 09:40] Extra JSON file exists (./assets/json/2602.14178.json), skip PDF parsing.
[17.02.2026 09:40] Paper image links file exists (./assets/img_data/2602.14178.json), skip HTML parsing.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.12876.
[17.02.2026 09:40] Extra JSON file exists (./assets/json/2602.12876.json), skip PDF parsing.
[17.02.2026 09:40] Paper image links file exists (./assets/img_data/2602.12876.json), skip HTML parsing.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.14721.
[17.02.2026 09:40] Extra JSON file exists (./assets/json/2602.14721.json), skip PDF parsing.
[17.02.2026 09:40] Paper image links file exists (./assets/img_data/2602.14721.json), skip HTML parsing.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.14534.
[17.02.2026 09:40] Extra JSON file exists (./assets/json/2602.14534.json), skip PDF parsing.
[17.02.2026 09:40] Paper image links file exists (./assets/img_data/2602.14534.json), skip HTML parsing.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.14147.
[17.02.2026 09:40] Extra JSON file exists (./assets/json/2602.14147.json), skip PDF parsing.
[17.02.2026 09:40] Paper image links file exists (./assets/img_data/2602.14147.json), skip HTML parsing.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.13344.
[17.02.2026 09:40] Extra JSON file exists (./assets/json/2602.13344.json), skip PDF parsing.
[17.02.2026 09:40] Paper image links file exists (./assets/img_data/2602.13344.json), skip HTML parsing.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.09185.
[17.02.2026 09:40] Extra JSON file exists (./assets/json/2602.09185.json), skip PDF parsing.
[17.02.2026 09:40] Paper image links file exists (./assets/img_data/2602.09185.json), skip HTML parsing.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.14689.
[17.02.2026 09:40] Extra JSON file exists (./assets/json/2602.14689.json), skip PDF parsing.
[17.02.2026 09:40] Paper image links file exists (./assets/img_data/2602.14689.json), skip HTML parsing.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.13195.
[17.02.2026 09:40] Extra JSON file exists (./assets/json/2602.13195.json), skip PDF parsing.
[17.02.2026 09:40] Paper image links file exists (./assets/img_data/2602.13195.json), skip HTML parsing.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.12299.
[17.02.2026 09:40] Extra JSON file exists (./assets/json/2602.12299.json), skip PDF parsing.
[17.02.2026 09:40] Paper image links file exists (./assets/img_data/2602.12299.json), skip HTML parsing.
[17.02.2026 09:40] Success.
[17.02.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2602.09319.
[17.02.2026 09:40] Downloading paper 2602.09319 from https://arxiv.org/pdf/2602.09319v2...
[17.02.2026 09:41] Extracting affiliations from text.
[17.02.2026 09:41] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation 6 2 0 2 2 1 ] . [ 2 9 1 3 9 0 . 2 0 6 2 : r a ABSTRACT Retrieval-Augmented Generation (RAG) has become cornerstone of knowledge-intensive applications, including enterprise chatbots, healthcare assistants, and agentic memory management. However, recent studies show that knowledge-extraction attacks can recover sensitive knowledge-base content through maliciously crafted queries, raising serious concerns about intellectual property theft and privacy leakage. While prior work has explored individual attack and defense techniques, the research landscape remains fragmented, spanning heterogeneous retrieval embeddings, diverse generation models, and evaluations based on non-standardized metrics and inconsistent datasets. To address this gap, we introduce the first systematic benchmark for knowledge-extraction attacks on RAG systems. Our benchmark covers broad spectrum of attack and defense strategies, representative retrieval embedding models, and both openand closed-source generators, all evaluated under unified experimental framework with standardized protocols across multiple datasets. By consolidating the experimental landscape and enabling reproducible, comparable evaluation, this benchmark provides actionable insights and practical foundation for developing privacy-preserving RAG systems in the face of emerging knowledge extraction threats. Our code is available here. KEYWORDS Retrieval-augmented Generation, Knowledge-Extraction Attack ACM Reference Format: Zhisheng Qi, Utkarsh Sahu, Li Ma, Haoyu Han, Ryan Rossi, Franck Dernoncourt, Mahantesh Halappanavar, Nesreen Ahmed, Yushun Dong, Yue Zhao, Yu Zhang, and Yu Wang. 2026. Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation. In Proceedings of ACM Conference (Conference17). ACM, New York, NY, USA, 17 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn Permission to make digital or hard copies of all or"
[17.02.2026 09:41] Response: ```python
[]
```
[17.02.2026 09:41] Extracting affiliations from text.
[17.02.2026 09:41] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation6 2 0 2 2 1 ] . [ 2 9 1 3 9 0 . 2 0 6 2 : r aABSTRACT Retrieval-Augmented Generation (RAG) has become cornerstone of knowledge-intensive applications, including enterprise chatbots, healthcare assistants, and agentic memory management. However, recent studies show that knowledge-extraction attacks can recover sensitive knowledge-base content through maliciously crafted queries, raising serious concerns about intellectual property theft and privacy leakage. While prior work has explored individual attack and defense techniques, the research landscape remains fragmented, spanning heterogeneous retrieval embeddings, diverse generation models, and evaluations based on non-standardized metrics and inconsistent datasets. To address this gap, we introduce the first systematic benchmark for knowledge-extraction attacks on RAG systems. Our benchmark covers broad spectrum of attack and defense strategies, representative retrieval embedding models, and both openand closed-source generators, all evaluated under unified experimental framework with standardized protocols across multiple datasets. By consolidating the experimental landscape and enabling reproducible, comparable evaluation, this benchmark provides actionable insights and practical foundation for developing privacy-preserving RAG systems in the face of emerging knowledge extraction threats. Our code is available here. KEYWORDS Retrieval-augmented Generation, Knowledge-Extraction Attack ACM Reference Format: Zhisheng Qi, Utkarsh Sahu, Li Ma, Haoyu Han, Ryan Rossi, Franck Dernoncourt, Mahantesh Halappanavar, Nesreen Ahmed, Yushun Dong, Yue Zhao, Yu Zhang, and Yu Wang. 2026. Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation. In Proceedings of ACM Conference (Conference17). ACM, New York, NY, USA, 17 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference17, July 2017, Washington, DC, USA 2026 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn Yu Zhang Texas A&M UniversityFigure 1: Knowledge extraction attack on RAG causes privacy/proprietary risks across pervasive high-stake domains.Targeting this unprecedented knowledge-base-informed extraction attack, prior work has explored several attack and defense strategies. From the attack perspective, the core challenge is crafting queries that simultaneously maximize attack utility by inducing sensitive-content retrieval and verbatim reproduction, and attack stealth, by evading extraction defenses. Existing methods achieve this via two complementary components [13]. The INFORMATION component steers retrieval toward sensitive content by inducing favorable embedding-space alignment, using random text [24], LLMgenerated fragments [25], or embedding-optimized queries [26], Conference17, July 2017, Washington, DC, USA Qi, et al. enabling (un)targeted extraction. The COMMAND component instructs the generator to explicitly reproduce retrieved content, typically through prompts requesting verbatim output [13, 24, 26, 27]. Operating jointly within single query, these two components adversarially drive RAG systems to both retrieve sensitive information and leak it through generated content. Beyond single-query attacks [13, 27], adversaries can further exploit the iterative queryresponse loop of RAG systems [2426] to progressively accumulate sensitive content. From the defense perspective, existing approaches aim to mitigate extraction by intervening at different stages of the RAG pipeline. Input defenses [25, 28] reject suspicious requests with malicious extraction intent before retrieval. Retrieval defenses [13, 24 26] constrain retrieval of sensitive content by limiting the quantity or relevance of retrieved documents. Generation defenses [13, 27] operate after retrieval, controlling what content is ultimately revealed to the user through techniques such as summarization or content filtering to prevent verbatim reproduction of sensitive passages. Despite the above progress , existing studies are typically conducted under heterogeneous yet inconsistent experimental settings, as in Table 1. These differences span dataset versions (e.g., HealthCareMagic origin [13, 25] versus (vs.) sampled instances [24, 26]) , retrieval embedding models (e.g., MiniLM [13] vs. MPNet [26] , generators (e.g., Llama [13] vs. Gemini [26]) , knowledge-base construction strategies (e.g., Knowledge instance [13, 25, 26] vs. Fixed chunk length [24]), assumptions about attacker and defender capabilities (e.g., embedding white box [26] vs. black box [13, 24, 25]), and non-uniformed evaluation metrics [2426]. This lack of unified design space and experimental settings makes it difficult to obtain comparable understanding of extraction attack and defense behaviors in RAG systems. To address this fragmentation, we introduce unified benchmark for systematic and fair evaluation that spans comprehensive RAG design space. It covers diverse retriever and generator architectures, knowledge-base construction strategies, and extraction attack querycrafting methods, ranging from simple random baselines to state-of-the-art adaptive attacks [24 26]. The benchmark further incorporates widely adopted defense mechanisms deployed at different stages of the RAG pipeline. All attacks and defenses are evaluated under unified experimental protocol across multiple datasets [21, 2931], ensuring consistent threat assumptions, comparable metrics, and fair assessment of effectiveness. Our contributions are as follows: Comprehensive review and unified design space. We systematically survey existing knowledge-extraction attack and defense methods for RAG systems [13, 2426, 26] in Table 1 and formalize unified design space that characterizes their unique design dimensions and assumptions in 3. Stand"
[17.02.2026 09:41] Mistral response. {"id": "92ca6ed451784cc3b0885cf3a26dda97", "created": 1771321263, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1432, "total_tokens": 1451, "completion_tokens": 19, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Texas A&M University\",\n    \"ACM\"\n]\n```"}}]}
[17.02.2026 09:41] Response: ```python
[
    "Texas A&M University",
    "ACM"
]
```
[17.02.2026 09:41] Deleting PDF ./assets/pdf/2602.09319.pdf.
[17.02.2026 09:41] Success.
[17.02.2026 09:41] Downloading and parsing paper https://huggingface.co/papers/2602.07673.
[17.02.2026 09:41] Downloading paper 2602.07673 from https://arxiv.org/pdf/2602.07673v1...
[17.02.2026 09:41] Extracting affiliations from text.
[17.02.2026 09:41] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 7 ] . [ 1 3 7 6 7 0 . 2 0 6 2 : r Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation Jiangnan Fang1, Cheng-Tse Liu2, Hanieh Deilamsalehy3, Nesreen K. Ahmed4, Puneet Mathur5, Nedim Lipka6, Franck Dernoncourt7, Ryan A. Rossi8 1,2Independent Researchers, 3,5,6,7,8Adobe Research, 4Cisco Research {1jfang53, 2cliu282}@ucsc.edu Abstract Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at more granular level in relation to well-defined overlap metric. In this work we provide an LLM judge bias analysis as function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond simple comparison. Keywords: LLMs, summarization As large language models (LLMs) continue to improve in their capabilities, LLM-as-a-judge has emerged as method of automating evaluation. Compared to traditional overlap-based metrics, LLM judges better capture semantic content of texts and are more robust to paraphrasing. As models can leverage reasoning capabilities gained from train"
[17.02.2026 09:41] Response: ```python
[
    "Independent Researchers",
    "Adobe Research",
    "Cisco Research"
]
```
[17.02.2026 09:41] Deleting PDF ./assets/pdf/2602.07673.pdf.
[17.02.2026 09:41] Success.
[17.02.2026 09:41] Downloading and parsing paper https://huggingface.co/papers/2602.14696.
[17.02.2026 09:41] Extra JSON file exists (./assets/json/2602.14696.json), skip PDF parsing.
[17.02.2026 09:41] Paper image links file exists (./assets/img_data/2602.14696.json), skip HTML parsing.
[17.02.2026 09:41] Success.
[17.02.2026 09:41] Downloading and parsing paper https://huggingface.co/papers/2602.14560.
[17.02.2026 09:41] Extra JSON file exists (./assets/json/2602.14560.json), skip PDF parsing.
[17.02.2026 09:41] Paper image links file exists (./assets/img_data/2602.14560.json), skip HTML parsing.
[17.02.2026 09:41] Success.
[17.02.2026 09:41] Downloading and parsing paper https://huggingface.co/papers/2602.07824.
[17.02.2026 09:41] Downloading paper 2602.07824 from https://arxiv.org/pdf/2602.07824v1...
[17.02.2026 09:41] Extracting affiliations from text.
[17.02.2026 09:41] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Data Darwinism Part Unlocking the Value of Scientific Data for Pre-training Yiwei Qin*1,2,4 Zhen Huang*1,3,4 Tiantian Mi*1,3,4 Weiye Si1,2,4 Chenyang Zhou1,2,4 Qipeng Guo1 Siyuan Feng1 Pengfei Liu1,2,4 1SII 2SJTU 3FDU 4GAIR Data-Darwinism daVinci-origin-3B ƒ± Darwin-Science ƒ± Darwin-Science-Eval daVinci-origin-7B "
[17.02.2026 09:41] Response: ```python
['SII', 'SJTU', 'FDU', 'GAIR']
```
[17.02.2026 09:41] Deleting PDF ./assets/pdf/2602.07824.pdf.
[17.02.2026 09:41] Success.
[17.02.2026 09:41] Enriching papers with extra data.
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 0. This paper envisions a quantum database (Qute) that treats quantum computation as a first-class execution option. Unlike prior simulation-based methods that either run quantum algorithms on classical machines or adapt existing databases for quantum simulation, Qute instead (i) compiles an extended f...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 1. A novel framework called Query-as-Anchor is introduced that transforms user modeling from static encoding to dynamic, query-aware synthesis using large language models with specialized architectures and training methods.  					AI-generated summary 				 Industrial-scale user representation learning r...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 2. REDSearcher presents a unified framework for optimizing search agents through improved task synthesis, tool-augmented queries, midtraining capability enhancement, and simulated environments to address challenges in long-horizon search tasks.  					AI-generated summary 				 Large language models are ...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 3. BitDance is a scalable autoregressive image generator that uses binary visual tokens and diffusion-based methods to achieve efficient high-resolution image generation with improved speed and performance.  					AI-generated summary 				 We present BitDance, a scalable autoregressive (AR) image genera...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 4. InnoEval is a deep innovation evaluation framework that emulates human-level idea assessment through knowledge-grounded, multi-perspective reasoning with heterogeneous deep knowledge search and multi-dimensional decoupled evaluation.  					AI-generated summary 				 The rapid evolution of Large Langu...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 5. Experiential Reinforcement Learning introduces an explicit experience-reflection-consolidation loop that improves learning efficiency and performance in sparse-reward environments by enabling structured behavioral revision without additional inference costs.  					AI-generated summary 				 Reinforce...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 6. A reasoning-driven universal multimodal embedding framework integrates embedder-guided reinforcement learning with traceability chain-of-thought to enhance cross-modal semantic consistency and retrieval performance.  					AI-generated summary 				 Leveraging Multimodal Large Language Models (MLLMs) ...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 7. UniWeTok introduces a unified discrete tokenizer with a massive binary codebook and novel training techniques to achieve superior performance in image generation and multimodal tasks while reducing computational requirements.  					AI-generated summary 				 Unified Multimodal Large Language Models (...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 8. A new benchmark called BrowseComp-V3 challenges multimodal large language models with complex, multi-hop reasoning tasks requiring deep search across text and visual modalities, revealing significant gaps in current capabilities.  					AI-generated summary 				 Multimodal large language models (MLLM...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 9. WebWorld is an open-web simulator trained on over one million interactions that supports long-horizon reasoning and multi-format data, achieving performance comparable to advanced models like Gemini-3-Pro and GPT-4o.  					AI-generated summary 				 Web agents require massive trajectories to generali...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 10. A unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards improves human motion understanding and generation through semantic alignment, reasoning coherence, and physical plausibility.  					AI-generated summary 				 Human motion underst...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 11. LaViDa-R1 is a multimodal reasoning diffusion language model that unifies supervised fine-tuning and multi-task reinforcement learning with novel training techniques for enhanced performance across visual reasoning and generation tasks.  					AI-generated summary 				 Diffusion language models (dLLM...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 12. FireRed-Image-Edit uses a diffusion transformer with optimized data curation and training methods to achieve state-of-the-art performance in instruction-based image editing, supported by a comprehensive benchmark and novel techniques for data efficiency and optimization stability.  					AI-generated...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 13. AIDev is a large-scale dataset of agent-authored pull requests from real-world GitHub repositories that captures AI coding agent usage in practical software development scenarios.  					AI-generated summary 				 AI coding agents are rapidly transforming software engineering by performing tasks such ...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 14. Prefill attacks represent a significant and underexplored vulnerability in open-weight language models, affecting major contemporary models despite some resistance from large reasoning models.  					AI-generated summary 				 As the capabilities of large language models continue to advance, so does t...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 15. Conversational image segmentation addresses functional and physical reasoning tasks by introducing a new benchmark and model that combines segmentation priors with language understanding.  					AI-generated summary 				 Conversational image segmentation grounds abstract, intent-driven concepts into ...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 16. Room acoustics analysis plays a central role in architectural design, audio engineering, speech intelligibility assessment, and hearing research. Despite the availability of standardized metrics such as reverberation time, clarity, and speech transmission index, accessible tools that combine rigorou...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 17. A systematic benchmark for evaluating knowledge-extraction attacks on Retrieval-Augmented Generation systems is introduced, covering diverse attack and defense strategies across multiple retrieval and generation models with standardized evaluation protocols.  					AI-generated summary 				 Retrieval...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 18. LLM judges exhibit bias toward summaries similar to their own generation, with performance deteriorating as summary overlap with human references decreases across multiple model sizes and architectures.  					AI-generated summary 				 Large language model (LLM) judges have often been used alongside ...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 19. Targeted instruction selection for LLM fine-tuning can be improved by systematically analyzing data representation and selection algorithms, with gradient-based representations and greedy round-robin selection performing best at low budgets.  					AI-generated summary 				 Instruction fine-tuning of...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 20. Parameter-mapping sonification of ENSO data preserves dynamical signatures through acoustic phase space analysis, revealing distinct coupling regimes in traditional musical scales.  					AI-generated summary 				 Sonification -- the mapping of data to non-speech audio -- offers an underexplored chan...
[17.02.2026 09:41] ********************************************************************************
[17.02.2026 09:41] Abstract 21. Data Darwinism presents a systematic framework for data-model co-evolution through a ten-level taxonomy, demonstrating that advanced processing techniques significantly improve foundation model performance on scientific text.  					AI-generated summary 				 Data quality determines foundation model p...
[17.02.2026 09:41] Read previous papers.
[17.02.2026 09:41] Generating reviews via LLM API.
[17.02.2026 09:41] Using data from previous issue: {"categories": [], "emoji": "‚öõÔ∏è", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–ø–æ—Ö–∏ –∫–≤–∞–Ω—Ç–æ–≤—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö Qute, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –∫–≤–∞–Ω—Ç–æ–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–∞–∫ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤. –°–∏—Å—Ç–µ–º–∞ –∫–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π SQL –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#dataset", "#multimodal"], "emoji": "‚öì", "ru": {"title": "–û—Ç —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –∫ –¥–∏–Ω–∞–º–∏—á–Ω—ã–º: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –∫ –∑–∞–ø—Ä–æ—Å—É", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Query-as-Anchor, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#rl", "#optimization", "#synthetic", "#multimodal", "#reasoning", "#open_source", "#agents", "#benchmark", "#dataset", "#long_context", "#training"], "emoji": "üîç", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ –∑–∞–¥–∞—á –∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "RE
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#architecture", "#multimodal", "#open_source", "#diffusion", "#small_models", "#inference", "#optimization"], "emoji": "üé®", "ru": {"title": "–ë–∏–Ω–∞—Ä–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏ –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "BitDance ‚Äî —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∞–≤—Ç–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ
[17.02.2026 09:41] Querying the API.
[17.02.2026 09:41] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

InnoEval is a deep innovation evaluation framework that emulates human-level idea assessment through knowledge-grounded, multi-perspective reasoning with heterogeneous deep knowledge search and multi-dimensional decoupled evaluation.  					AI-generated summary 				 The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts.
[17.02.2026 09:41] Response: ```json
{
  "desc": "InnoEval ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞—É—á–Ω—ã—Ö –∏–¥–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º—É–ª—è—Ü–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ —Å—É–∂–¥–µ–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∑–Ω–∞–Ω–∏–π –∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ –∑–∞–∑–µ–º–ª—è–µ—Ç –æ—Ü–µ–Ω–∫–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞, —á—Ç–æ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —É–∑–∫–∏—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ –∑–Ω–∞–Ω–∏–π —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤. –§—Ä–µ–π–º–≤–æ—Ä–∫ —Ä–µ–∞–ª–∏–∑—É–µ—Ç –º—É–ª—å—Ç–∏–¥imensional –æ—Ü–µ–Ω–∫—É —á–µ—Ä–µ–∑ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—É—é –∫–æ–º–∏—Å—Å–∏—é —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–º–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—è–º–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –º–Ω–æ–≥–æ–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ InnoEval –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –æ—Ü–µ–Ω–æ–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤.",
  "emoji": "üí°",
  "title": "–ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑—É–º –¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –Ω–∞—É—á–Ω—ã—Ö –∏–Ω–Ω–æ–≤–∞—Ü–∏–π"
}
```
[17.02.2026 09:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"InnoEval is a deep innovation evaluation framework that emulates human-level idea assessment through knowledge-grounded, multi-perspective reasoning with heterogeneous deep knowledge search and multi-dimensional decoupled evaluation.  					AI-generated summary 				 The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts."

[17.02.2026 09:41] Response: ```python
["BENCHMARK", "DATASET", "MULTIMODAL"]
```
[17.02.2026 09:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"InnoEval is a deep innovation evaluation framework that emulates human-level idea assessment through knowledge-grounded, multi-perspective reasoning with heterogeneous deep knowledge search and multi-dimensional decoupled evaluation.  					AI-generated summary 				 The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts."

[17.02.2026 09:41] Response: ```python
['REASONING', 'SCIENCE', 'OPEN_SOURCE']
```
[17.02.2026 09:41] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"InnoEval is a novel framework designed to evaluate scientific ideas by mimicking human-level assessment through advanced reasoning techniques. It utilizes a heterogeneous deep knowledge search to gather diverse evidence from various online sources, ensuring a well-rounded evaluation. The framework incorporates a multi-dimensional approach by involving reviewers from different academic backgrounds, which helps in achieving consensus and reducing bias. Experiments show that InnoEval outperforms traditional evaluation methods, aligning closely with expert human judgments across various evaluation tasks.","title":"Revolutionizing Idea Evaluation with InnoEval"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='InnoEval is a novel framework designed to evaluate scientific ideas by mimicking human-level assessment through advanced reasoning techniques. It utilizes a heterogeneous deep knowledge search to gather diverse evidence from various online sources, ensuring a well-rounded evaluation. The framework incorporates a multi-dimensional approach by involving reviewers from different academic backgrounds, which helps in achieving consensus and reducing bias. Experiments show that InnoEval outperforms traditional evaluation methods, aligning closely with expert human judgments across various evaluation tasks.', title='Revolutionizing Idea Evaluation with InnoEval'))
[17.02.2026 09:41] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"InnoEvalÊòØ‰∏Ä‰∏™Ê∑±Â∫¶ÂàõÊñ∞ËØÑ‰º∞Ê°ÜÊû∂ÔºåÊó®Âú®Ê®°Êãü‰∫∫Á±ªÁöÑÂàõÊÑèËØÑ‰º∞ËÉΩÂäõ„ÄÇÂÆÉÈÄöËøáÁü•ËØÜÈ©±Âä®ÁöÑÂ§öËßíÂ∫¶Êé®ÁêÜÔºåÁªìÂêàÂºÇÊûÑÊ∑±Â∫¶Áü•ËØÜÊêúÁ¥¢ÂíåÂ§öÁª¥Ëß£ËÄ¶ËØÑ‰º∞ÔºåÊù•Ëß£ÂÜ≥Áé∞ÊúâËØÑ‰º∞ÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®Êù•Ëá™‰∏çÂêåÂú®Á∫øÊù•Ê∫êÁöÑÂä®ÊÄÅËØÅÊçÆÔºåÁ°Æ‰øùËØÑ‰º∞ÁöÑÂÖ®Èù¢ÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåInnoEvalÂú®Â§öÁßçËØÑ‰º∞‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÂü∫ÂáÜÔºåËØÑÂà§Ê®°Âºè‰∏é‰∫∫Á±ª‰∏ìÂÆ∂È´òÂ∫¶‰∏ÄËá¥„ÄÇ","title":"ÂàõÊñ∞ËØÑ‰º∞ÁöÑÊñ∞Ê†áÂáÜÔºöInnoEval"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='InnoEvalÊòØ‰∏Ä‰∏™Ê∑±Â∫¶ÂàõÊñ∞ËØÑ‰º∞Ê°ÜÊû∂ÔºåÊó®Âú®Ê®°Êãü‰∫∫Á±ªÁöÑÂàõÊÑèËØÑ‰º∞ËÉΩÂäõ„ÄÇÂÆÉÈÄöËøáÁü•ËØÜÈ©±Âä®ÁöÑÂ§öËßíÂ∫¶Êé®ÁêÜÔºåÁªìÂêàÂºÇÊûÑÊ∑±Â∫¶Áü•ËØÜÊêúÁ¥¢ÂíåÂ§öÁª¥Ëß£ËÄ¶ËØÑ‰º∞ÔºåÊù•Ëß£ÂÜ≥Áé∞ÊúâËØÑ‰º∞ÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®Êù•Ëá™‰∏çÂêåÂú®Á∫øÊù•Ê∫êÁöÑÂä®ÊÄÅËØÅÊçÆÔºåÁ°Æ‰øùËØÑ‰º∞ÁöÑÂÖ®Èù¢ÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåInnoEvalÂú®Â§öÁßçËØÑ‰º∞‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÂü∫ÂáÜÔºåËØÑÂà§Ê®°Âºè‰∏é‰∫∫Á±ª‰∏ìÂÆ∂È´òÂ∫¶‰∏ÄËá¥„ÄÇ', title='ÂàõÊñ∞ËØÑ‰º∞ÁöÑÊñ∞Ê†áÂáÜÔºöInnoEval'))
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#agents", "#training", "#rl"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—è –∫–∞–∫ –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –≤–Ω–µ–¥—Ä—è–µ—Ç —è–≤–Ω—ã–π —Ü–∏–∫–ª –æ–ø—ã—Ç-—Ä–µ—Ñ–ª–µ–∫—Å–∏—è-–∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#rl", "#optimization", "#multimodal", "#reasoning", "#rag", "#benchmark"], "emoji": "üß†", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∫–∞–∫ –ø—É—Ç—å –∫ –ª—É—á—à–µ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#multimodal", "#open_source", "#training"], "emoji": "üé®", "ru": {"title": "–ï–¥–∏–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "UniWeTok –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –æ–≥—Ä–æ–º–Ω—ã–º –±–∏–Ω–∞—Ä–Ω—ã–º –∫–æ–¥–æ–≤—ã–º —Å–ª–æ
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#multimodal"], "emoji": "üîç", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –≥–ª—É–±–æ–∫–æ–º –≤–µ–±-–ø–æ–∏—Å–∫–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ BrowseComp-V3 –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Å–ª–æ–∂–Ω–æ–≥–æ –º–Ω–æ–≥–æ—à–∞–≥–æ
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multimodal", "#synthetic", "#agents", "#open_source", "#reasoning", "#long_context", "#training"], "emoji": "üåê", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –æ—Ç–∫—Ä—ã—Ç—ã–π –≤–µ–±-—Å–∏–º—É–ª—è—Ç–æ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "WebWorld ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π –≤–µ–±-—Å–∏–º—É–ª—è—Ç–æ—Ä, –æ–±—É—á–µ–Ω
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#open_source", "#rl", "#reasoning", "#rlhf", "#cv", "#robotics", "#optimization"], "emoji": "üé¨", "ru": {"title": "–õ–æ–≥–∏—á–Ω—ã–µ –∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–≤–∏–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç MoRL ‚Äî –µ–¥–∏–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#rl", "#optimization", "#architecture", "#multimodal", "#reasoning", "#training", "#diffusion"], "emoji": "üé®", "ru": {"title": "–ï–¥–∏–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "LaViDa-R1 ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#rlhf", "#open_source", "#data", "#benchmark", "#dataset", "#training", "#cv"], "emoji": "üé®", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ FireRed-Image-Edit ‚Äî –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#science", "#open_source", "#agents", "#dataset", "#plp"], "emoji": "ü§ñ", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∫–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AIDev ‚Äî –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–æ–ª–µ–µ —á–µ–º 932 —Ç—ã—Å—è—á–∏ pull requests, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö AI-–∞–≥–µ–Ω
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#open_source", "#security"], "emoji": "üîì", "ru": {"title": "–ü—Ä–µ—Ñ–∏–ª–ª–∏–Ω–≥-–∞—Ç–∞–∫–∏: —Å–∫—Ä—ã—Ç–∞—è —É—è–∑–≤–∏–º–æ—Å—Ç—å –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —É—è–∑–≤–∏–º–æ—Å—Ç—å –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, —Å–≤—è–∑–∞–Ω–Ω–∞—è —Å –∞—Ç–∞–∫–∞–º–∏ –Ω–∞ —ç—Ç–∞–ø –ø—Ä–µ—Ñ–∏–ª–ª–∏–Ω–≥–∞ ‚Äî –∫–æ–≥–¥–∞ –∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫ –ø—Ä–µ–¥–∑–∞–ø–æ–ª–Ω—è–µ—Ç –Ω–∞—á–∞–ª—å–Ω—ã–µ —Ç–æ
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#synthetic", "#open_source", "#reasoning", "#cv", "#dataset", "#multimodal", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–î–∏–∞–ª–æ–≥–æ–≤–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è: –æ—Ç –ø—Ä–æ—Å—Ç—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∫ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∑–∞–¥–∞—á–∞ –¥–∏–∞–ª–æ–≥–æ–≤–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –≤—ã—Ö–æ–¥
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#dataset", "#science", "#audio"], "emoji": "üéµ", "ru": {"title": "–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∞–Ω–∞–ª–∏–∑ –∞–∫—É—Å—Ç–∏–∫–∏ –ø–æ–º–µ—â–µ–Ω–∏–π —á–µ—Ä–µ–∑ –≤–µ–±-–ø–ª–∞—Ç—Ñ–æ—Ä–º—É", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ AcoustiVision Pro ‚Äî –≤–µ–±-–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–º–ø—É–ª—å—Å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç
[17.02.2026 09:41] Querying the API.
[17.02.2026 09:41] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A systematic benchmark for evaluating knowledge-extraction attacks on Retrieval-Augmented Generation systems is introduced, covering diverse attack and defense strategies across multiple retrieval and generation models with standardized evaluation protocols.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) has become a cornerstone of knowledge-intensive applications, including enterprise chatbots, healthcare assistants, and agentic memory management. However, recent studies show that knowledge-extraction attacks can recover sensitive knowledge-base content through maliciously crafted queries, raising serious concerns about intellectual property theft and privacy leakage. While prior work has explored individual attack and defense techniques, the research landscape remains fragmented, spanning heterogeneous retrieval embeddings, diverse generation models, and evaluations based on non-standardized metrics and inconsistent datasets. To address this gap, we introduce the first systematic benchmark for knowledge-extraction attacks on RAG systems. Our benchmark covers a broad spectrum of attack and defense strategies, representative retrieval embedding models, and both open- and closed-source generators, all evaluated under a unified experimental framework with standardized protocols across multiple datasets. By consolidating the experimental landscape and enabling reproducible, comparable evaluation, this benchmark provides actionable insights and a practical foundation for developing privacy-preserving RAG systems in the face of emerging knowledge extraction threats. Our code is available here.
[17.02.2026 09:41] Response: ```json
{
  "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—ã–π —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π benchmark –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞—Ç–∞–∫ –Ω–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –≤ —Å–∏—Å—Ç–µ–º–∞—Ö Retrieval-Augmented Generation (RAG). –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞—Ç–∞–∫ –∏ –∑–∞—â–∏—Ç—ã, —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ retrieval –∏ generation —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –µ–¥–∏–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫–∏ –º–æ–≥—É—Ç –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ crafted –∑–∞–ø—Ä–æ—Å—ã, —á—Ç–æ —Å–æ–∑–¥–∞—ë—Ç —Ä–∏—Å–∫–∏ –∫—Ä–∞–∂–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —É—Ç–µ—á–µ–∫ –ø—Ä–∏–≤–∞—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. Benchmark –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–µ –∏ —Å—Ä–∞–≤–Ω–∏–º—ã–µ –æ—Ü–µ–Ω–∫–∏, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é –±–∞–∑—É –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ privacy-preserving RAG —Å–∏—Å—Ç–µ–º.",
  "emoji": "üîì",
  "title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞—â–∏—Ç–∞ RAG —Å–∏—Å—Ç–µ–º –æ—Ç –∞—Ç–∞–∫ –Ω–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π"
}
```
[17.02.2026 09:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A systematic benchmark for evaluating knowledge-extraction attacks on Retrieval-Augmented Generation systems is introduced, covering diverse attack and defense strategies across multiple retrieval and generation models with standardized evaluation protocols.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) has become a cornerstone of knowledge-intensive applications, including enterprise chatbots, healthcare assistants, and agentic memory management. However, recent studies show that knowledge-extraction attacks can recover sensitive knowledge-base content through maliciously crafted queries, raising serious concerns about intellectual property theft and privacy leakage. While prior work has explored individual attack and defense techniques, the research landscape remains fragmented, spanning heterogeneous retrieval embeddings, diverse generation models, and evaluations based on non-standardized metrics and inconsistent datasets. To address this gap, we introduce the first systematic benchmark for knowledge-extraction attacks on RAG systems. Our benchmark covers a broad spectrum of attack and defense strategies, representative retrieval embedding models, and both open- and closed-source generators, all evaluated under a unified experimental framework with standardized protocols across multiple datasets. By consolidating the experimental landscape and enabling reproducible, comparable evaluation, this benchmark provides actionable insights and a practical foundation for developing privacy-preserving RAG systems in the face of emerging knowledge extraction threats. Our code is available here."

[17.02.2026 09:41] Response: ```python
["RAG", "BENCHMARK", "AGENTS"]
```
[17.02.2026 09:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A systematic benchmark for evaluating knowledge-extraction attacks on Retrieval-Augmented Generation systems is introduced, covering diverse attack and defense strategies across multiple retrieval and generation models with standardized evaluation protocols.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) has become a cornerstone of knowledge-intensive applications, including enterprise chatbots, healthcare assistants, and agentic memory management. However, recent studies show that knowledge-extraction attacks can recover sensitive knowledge-base content through maliciously crafted queries, raising serious concerns about intellectual property theft and privacy leakage. While prior work has explored individual attack and defense techniques, the research landscape remains fragmented, spanning heterogeneous retrieval embeddings, diverse generation models, and evaluations based on non-standardized metrics and inconsistent datasets. To address this gap, we introduce the first systematic benchmark for knowledge-extraction attacks on RAG systems. Our benchmark covers a broad spectrum of attack and defense strategies, representative retrieval embedding models, and both open- and closed-source generators, all evaluated under a unified experimental framework with standardized protocols across multiple datasets. By consolidating the experimental landscape and enabling reproducible, comparable evaluation, this benchmark provides actionable insights and a practical foundation for developing privacy-preserving RAG systems in the face of emerging knowledge extraction threats. Our code is available here."

[17.02.2026 09:41] Response: ```python
["SECURITY", "LEAKAGE", "BENCHMARK"]
```

Wait, let me reconsider - "BENCHMARK" is not in the provided topics list. Here's the corrected classification:

```python
["SECURITY", "LEAKAGE"]
```
[17.02.2026 09:41] Error. Failed to parse JSON from LLM. ["SECURITY", "LEAKAGE", "BENCHMARK"]


Wait, let me reconsider - "BENCHMARK" is not in the provided topics list. Here"s the corrected classification:


["SECURITY", "LEAKAGE"]
[17.02.2026 09:41] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper presents a comprehensive benchmark designed to evaluate knowledge-extraction attacks on Retrieval-Augmented Generation (RAG) systems. It addresses the vulnerabilities of RAG applications, such as chatbots and healthcare assistants, to attacks that can extract sensitive information through crafted queries. The benchmark includes a variety of attack and defense strategies, as well as different retrieval and generation models, all assessed using standardized evaluation protocols. By providing a unified framework for testing, this work aims to enhance the security and privacy of RAG systems against emerging threats.","title":"Strengthening RAG Systems Against Knowledge-Extraction Attacks"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a comprehensive benchmark designed to evaluate knowledge-extraction attacks on Retrieval-Augmented Generation (RAG) systems. It addresses the vulnerabilities of RAG applications, such as chatbots and healthcare assistants, to attacks that can extract sensitive information through crafted queries. The benchmark includes a variety of attack and defense strategies, as well as different retrieval and generation models, all assessed using standardized evaluation protocols. By providing a unified framework for testing, this work aims to enhance the security and privacy of RAG systems against emerging threats.', title='Strengthening RAG Systems Against Knowledge-Extraction Attacks'))
[17.02.2026 09:41] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Á≥ªÁªüÂåñÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Áü•ËØÜÊèêÂèñÊîªÂáªÂØπÂ¢ûÂº∫Ê£ÄÁ¥¢ÁîüÊàêÔºàRAGÔºâÁ≥ªÁªüÁöÑÂΩ±Âìç„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊÅ∂ÊÑèÊü•ËØ¢ÂèØ‰ª•ÊÅ¢Â§çÊïèÊÑüÁü•ËØÜÂ∫ìÂÜÖÂÆπÔºåÂØºËá¥Áü•ËØÜ‰∫ßÊùÉÁõóÁ™ÉÂíåÈöêÁßÅÊ≥ÑÈú≤ÁöÑ‰∏•ÈáçÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑÂü∫ÂáÜÊ∂µÁõñ‰∫ÜÂ§öÁßçÊîªÂáªÂíåÈò≤Âæ°Á≠ñÁï•Ôºå‰ª£Ë°®ÊÄßÁöÑÊ£ÄÁ¥¢ÂµåÂÖ•Ê®°ÂûãÔºå‰ª•ÂèäÂºÄÊîæÂíåÈó≠Ê∫êÁîüÊàêÂô®ÔºåÊâÄÊúâËØÑ‰º∞ÈÉΩÂú®Áªü‰∏ÄÁöÑÂÆûÈ™åÊ°ÜÊû∂‰∏ãËøõË°å„ÄÇÈÄöËøáÊï¥ÂêàÂÆûÈ™åÁéØÂ¢ÉÂπ∂ÂÆûÁé∞ÂèØÈáçÂ§ç„ÄÅÂèØÊØîËæÉÁöÑËØÑ‰º∞ÔºåËØ•Âü∫ÂáÜ‰∏∫ÂºÄÂèë‰øùÊä§ÈöêÁßÅÁöÑRAGÁ≥ªÁªüÊèê‰æõ‰∫ÜÂÆûÁî®ÁöÑÂü∫Á°Ä„ÄÇ","title":"ÊûÑÂª∫Áü•ËØÜÊèêÂèñÊîªÂáªÁöÑÁ≥ªÁªüÂåñËØÑ‰º∞Âü∫ÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Á≥ªÁªüÂåñÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Áü•ËØÜÊèêÂèñÊîªÂáªÂØπÂ¢ûÂº∫Ê£ÄÁ¥¢ÁîüÊàêÔºàRAGÔºâÁ≥ªÁªüÁöÑÂΩ±Âìç„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊÅ∂ÊÑèÊü•ËØ¢ÂèØ‰ª•ÊÅ¢Â§çÊïèÊÑüÁü•ËØÜÂ∫ìÂÜÖÂÆπÔºåÂØºËá¥Áü•ËØÜ‰∫ßÊùÉÁõóÁ™ÉÂíåÈöêÁßÅÊ≥ÑÈú≤ÁöÑ‰∏•ÈáçÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑÂü∫ÂáÜÊ∂µÁõñ‰∫ÜÂ§öÁßçÊîªÂáªÂíåÈò≤Âæ°Á≠ñÁï•Ôºå‰ª£Ë°®ÊÄßÁöÑÊ£ÄÁ¥¢ÂµåÂÖ•Ê®°ÂûãÔºå‰ª•ÂèäÂºÄÊîæÂíåÈó≠Ê∫êÁîüÊàêÂô®ÔºåÊâÄÊúâËØÑ‰º∞ÈÉΩÂú®Áªü‰∏ÄÁöÑÂÆûÈ™åÊ°ÜÊû∂‰∏ãËøõË°å„ÄÇÈÄöËøáÊï¥ÂêàÂÆûÈ™åÁéØÂ¢ÉÂπ∂ÂÆûÁé∞ÂèØÈáçÂ§ç„ÄÅÂèØÊØîËæÉÁöÑËØÑ‰º∞ÔºåËØ•Âü∫ÂáÜ‰∏∫ÂºÄÂèë‰øùÊä§ÈöêÁßÅÁöÑRAGÁ≥ªÁªüÊèê‰æõ‰∫ÜÂÆûÁî®ÁöÑÂü∫Á°Ä„ÄÇ', title='ÊûÑÂª∫Áü•ËØÜÊèêÂèñÊîªÂáªÁöÑÁ≥ªÁªüÂåñËØÑ‰º∞Âü∫ÂáÜ'))
[17.02.2026 09:41] Querying the API.
[17.02.2026 09:41] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LLM judges exhibit bias toward summaries similar to their own generation, with performance deteriorating as summary overlap with human references decreases across multiple model sizes and architectures.  					AI-generated summary 				 Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.
[17.02.2026 09:41] Response: ```json
{
  "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —Å–º–µ—â–µ–Ω–∏–µ LLM-—Å—É–¥–µ–π –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—é–º–µ. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞—é—Ç —Ä–µ–∑—é–º–µ, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥—Ä—É–≥–∏–º–∏ LLM, —Ä–µ–∑—é–º–µ, –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã–º –ª—é–¥—å–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –∫–æ–≥–¥–∞ —Å—Ö–æ–∂–µ—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤ (–∏–∑–º–µ—Ä—è–µ–º–∞—è –º–µ—Ç—Ä–∏–∫–∞–º–∏ ROUGE –∏ BLEU) –Ω–∏–∑–∫–∞—è. –≠—Ç–æ —è–≤–ª–µ–Ω–∏–µ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –≤–æ –≤—Å–µ—Ö –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –æ—Ç 1 –¥–æ 12 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø–æ–ª–∞–≥–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ LLM –∫–∞–∫ —Å—É–¥–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∑—é–º–µ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –≤—ã—Ö–æ–¥—è—â–∏—Ö –∑–∞ —Ä–∞–º–∫–∏ –ø—Ä–æ—Å—Ç–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤.",
  "emoji": "‚öñÔ∏è",
  "title": "–°—É–¥—å–∏-–º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞—é—Ç —Å–≤–æ–∏—Ö: —Å–º–µ—â–µ–Ω–∏–µ LLM –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ —Ä–µ–∑—é–º–µ"
}
```
[17.02.2026 09:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLM judges exhibit bias toward summaries similar to their own generation, with performance deteriorating as summary overlap with human references decreases across multiple model sizes and architectures.  					AI-generated summary 				 Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison."

[17.02.2026 09:41] Response: ```python
["BENCHMARK", "TRAINING"]
```
[17.02.2026 09:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLM judges exhibit bias toward summaries similar to their own generation, with performance deteriorating as summary overlap with human references decreases across multiple model sizes and architectures.  					AI-generated summary 				 Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison."

[17.02.2026 09:41] Response: ```python
['INTERPRETABILITY', 'ETHICS']
```
[17.02.2026 09:41] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper investigates the biases of large language model (LLM) judges when evaluating summaries. It reveals that LLM judges tend to favor summaries that are similar to their own outputs, particularly as the overlap with human-written summaries decreases. The study tests nine different LLMs, showing that their performance in judging summaries declines when the similarity, measured by metrics like ROUGE and BLEU, is low. The findings suggest that LLMs may need more sophisticated evaluation techniques rather than relying solely on direct comparisons to human summaries.","title":"LLM Judges Prefer Their Own: A Bias in Summary Evaluation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates the biases of large language model (LLM) judges when evaluating summaries. It reveals that LLM judges tend to favor summaries that are similar to their own outputs, particularly as the overlap with human-written summaries decreases. The study tests nine different LLMs, showing that their performance in judging summaries declines when the similarity, measured by metrics like ROUGE and BLEU, is low. The findings suggest that LLMs may need more sophisticated evaluation techniques rather than relying solely on direct comparisons to human summaries.', title='LLM Judges Prefer Their Own: A Bias in Summary Evaluation'))
[17.02.2026 09:41] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êú¨Á†îÁ©∂ÂàÜÊûê‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰Ωú‰∏∫ËØÑÂà§ËÄÖÂú®ÊëòË¶ÅÁîüÊàê‰ªªÂä°‰∏≠ÁöÑÂÅèËßÅ„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÂΩìÁîüÊàêÁöÑÊëòË¶Å‰∏é‰∫∫Á±ªÂèÇËÄÉÊëòË¶ÅÁöÑÈáçÂè†Â∫¶Èôç‰ΩéÊó∂ÔºåLLMËØÑÂà§ËÄÖÂØπÂÖ∂‰ªñLLMÁîüÊàêÁöÑÊëòË¶ÅÁöÑÂÅèÂ•Ω‰ºöÂ¢ûÂä†„ÄÇÈÄöËøáÊµãËØï‰πùÁßç‰∏çÂêåÂèÇÊï∞ËßÑÊ®°ÁöÑLLMÔºåÊàë‰ª¨ÂèëÁé∞ËøôÁßçÂÅèËßÅÂú®Â§ßÂ§öÊï∞Ê®°Âûã‰∏≠ÊôÆÈÅçÂ≠òÂú®ÔºåÂπ∂‰∏îÊ®°ÂûãÂú®ËØÑÂà§ÊëòË¶ÅÊó∂Âç≥‰ΩøÈáçÂè†Â∫¶ÊúâÈôê‰πüË°®Áé∞Âá∫Âõ∞Èöæ„ÄÇËøôË°®ÊòéÔºåÂú®ÊëòË¶ÅÈ¢ÜÂüü‰∏≠ÔºåLLM‰Ωú‰∏∫ËØÑÂà§ËÄÖÁöÑËØÑ‰º∞ÊñπÊ≥ïÈúÄË¶ÅË∂ÖË∂äÁÆÄÂçïÁöÑÊØîËæÉÊäÄÊúØ„ÄÇ","title":"LLMËØÑÂà§ËÄÖÁöÑÂÅèËßÅ‰∏éÊëòË¶ÅÈáçÂè†Â∫¶ÁöÑÂÖ≥Á≥ª"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Á†îÁ©∂ÂàÜÊûê‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰Ωú‰∏∫ËØÑÂà§ËÄÖÂú®ÊëòË¶ÅÁîüÊàê‰ªªÂä°‰∏≠ÁöÑÂÅèËßÅ„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÂΩìÁîüÊàêÁöÑÊëòË¶Å‰∏é‰∫∫Á±ªÂèÇËÄÉÊëòË¶ÅÁöÑÈáçÂè†Â∫¶Èôç‰ΩéÊó∂ÔºåLLMËØÑÂà§ËÄÖÂØπÂÖ∂‰ªñLLMÁîüÊàêÁöÑÊëòË¶ÅÁöÑÂÅèÂ•Ω‰ºöÂ¢ûÂä†„ÄÇÈÄöËøáÊµãËØï‰πùÁßç‰∏çÂêåÂèÇÊï∞ËßÑÊ®°ÁöÑLLMÔºåÊàë‰ª¨ÂèëÁé∞ËøôÁßçÂÅèËßÅÂú®Â§ßÂ§öÊï∞Ê®°Âûã‰∏≠ÊôÆÈÅçÂ≠òÂú®ÔºåÂπ∂‰∏îÊ®°ÂûãÂú®ËØÑÂà§ÊëòË¶ÅÊó∂Âç≥‰ΩøÈáçÂè†Â∫¶ÊúâÈôê‰πüË°®Áé∞Âá∫Âõ∞Èöæ„ÄÇËøôË°®ÊòéÔºåÂú®ÊëòË¶ÅÈ¢ÜÂüü‰∏≠ÔºåLLM‰Ωú‰∏∫ËØÑÂà§ËÄÖÁöÑËØÑ‰º∞ÊñπÊ≥ïÈúÄË¶ÅË∂ÖË∂äÁÆÄÂçïÁöÑÊØîËæÉÊäÄÊúØ„ÄÇ', title='LLMËØÑÂà§ËÄÖÁöÑÂÅèËßÅ‰∏éÊëòË¶ÅÈáçÂè†Â∫¶ÁöÑÂÖ≥Á≥ª'))
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#training", "#data"], "emoji": "üéØ", "ru": {"title": "–¶–µ–ª–µ–≤–æ–π –æ—Ç–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã–±–æ—Ä–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ –±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞
[17.02.2026 09:41] Using data from previous issue: {"categories": ["#science"], "emoji": "üéµ", "ru": {"title": "–ú—É–∑—ã–∫–∞ –∫–ª–∏–º–∞—Ç–∞: –∫–∞–∫ –∑–≤—É–∫ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É –≠–ª—å-–ù–∏–Ω—å–æ", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç —Å–æ–Ω–∏—Ñ–∏–∫–∞—Ü–∏—é ‚Äî –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∑–≤—É–∫ ‚Äî –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–≤–ª–µ–Ω–∏—è –≠–ª—å-–ù–∏–Ω—å–æ (ENSO). –û–Ω–∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –ø–æ
[17.02.2026 09:41] Querying the API.
[17.02.2026 09:41] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Data Darwinism presents a systematic framework for data-model co-evolution through a ten-level taxonomy, demonstrating that advanced processing techniques significantly improve foundation model performance on scientific text.  					AI-generated summary 				 Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.   To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.
[17.02.2026 09:41] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Data Darwinism ‚Äî —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –¥–µ—Å—è—Ç–∏—É—Ä–æ–≤–Ω–µ–≤—É—é —Ç–∞–∫—Å–æ–Ω–æ–º–∏—é –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π —ç–≤–æ–ª—é—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–µ–π, –≥–¥–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ LLM –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ Darwin-Science, –∫–æ—Ä–ø—É—Å –∏–∑ 900 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞—É—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤, –∏ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —É—Ä–æ–≤–Ω–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏, –≤–∫–ª—é—á–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ, —á—Ç–æ–±—ã –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å —Ä–∞–∑—Ä—ã–≤ –≤ –æ–±—É—á–∞–µ–º–æ—Å—Ç–∏ —Å—ã—Ä–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –û–Ω–∏ –æ–±—É—á–∏–ª–∏ baseline –º–æ–¥–µ–ª–∏ daVinci-origin —Å –Ω—É–ª—è, –∏—Å–∫–ª—é—á–∏–≤ –Ω–∞—É—á–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —á–∏—Å—Ç–æ—Ç—É —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ü–æ—Å–ª–µ –ø—Ä–æ–¥–æ–ª–∂–∞—é—â–µ–≥–æ—Å—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ 600 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤ Darwin-Science –ø–æ–∫–∞–∑–∞–ª –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 20 –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–∏—Ä–æ—Å—Ç–∞ –¥–æ +8.40 —Ç–æ—á–µ–∫ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.",
  "emoji": "üß¨",
  "title": "–≠–≤–æ–ª—é—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫—É –Ω–∞—É—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"
}
```
[17.02.2026 09:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Data Darwinism presents a systematic framework for data-model co-evolution through a ten-level taxonomy, demonstrating that advanced processing techniques significantly improve foundation model performance on scientific text.  					AI-generated summary 				 Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.   To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development."

[17.02.2026 09:41] Response: ```python
["DATASET", "DATA", "TRAINING", "SMALL_MODELS"]
```

**Justification:**

- **DATASET**: The paper introduces Darwin-Science, a new 900B-token corpus of processed scientific literature, which is a significant new dataset contribution.

- **DATA**: The core focus is on data processing and curation methodologies, presenting a ten-level taxonomy (L0-L9) for systematic data processing and refinement techniques (including Generative Refinement and Cognitive Completion).

- **TRAINING**: The paper involves pre-training foundation models from scratch and continued pre-training on processed data, demonstrating improvements in model training through better data.

- **SMALL_MODELS**: The paper explicitly evaluates daVinci-origin-3B and daVinci-origin-7B models, which are small models below the 1 billion parameter threshold (3B and 7B parameters).
[17.02.2026 09:41] Error. Failed to parse JSON from LLM. ["DATASET", "DATA", "TRAINING", "SMALL_MODELS"]


**Justification:**

- **DATASET**: The paper introduces Darwin-Science, a new 900B-token corpus of processed scientific literature, which is a significant new dataset contribution.

- **DATA**: The core focus is on data processing and curation methodologies, presenting a ten-level taxonomy (L0-L9) for systematic data processing and refinement techniques (including Generative Refinement and Cognitive Completion).

- **TRAINING**: The paper involves pre-training foundation models from scratch and continued pre-training on processed data, demonstrating improvements in model training through better data.

- **SMALL_MODELS**: The paper explicitly evaluates daVinci-origin-3B and daVinci-origin-7B models, which are small models below the 1 billion parameter threshold (3B and 7B parameters).
[17.02.2026 09:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Data Darwinism presents a systematic framework for data-model co-evolution through a ten-level taxonomy, demonstrating that advanced processing techniques significantly improve foundation model performance on scientific text.  					AI-generated summary 				 Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.   To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development."

[17.02.2026 09:41] Response: ```python
['SCIENCE', 'SYNTHETIC', 'OPEN_SOURCE']
```
[17.02.2026 09:42] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"The paper introduces \'Data Darwinism\', a framework that outlines how data and models can evolve together through a ten-level taxonomy. It highlights the importance of data quality in enhancing the performance of foundation models, particularly in the context of scientific literature. By creating the Darwin-Science corpus, which consists of 900 billion tokens, the authors demonstrate that advanced processing techniques like Generative Refinement and Cognitive Completion can significantly improve model learnability. The results show that models trained on this refined data outperform traditional baselines, confirming that systematic data processing can unlock greater value in machine learning applications.","title":"Unlocking Model Potential through Data Evolution"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="The paper introduces 'Data Darwinism', a framework that outlines how data and models can evolve together through a ten-level taxonomy. It highlights the importance of data quality in enhancing the performance of foundation models, particularly in the context of scientific literature. By creating the Darwin-Science corpus, which consists of 900 billion tokens, the authors demonstrate that advanced processing techniques like Generative Refinement and Cognitive Completion can significantly improve model learnability. The results show that models trained on this refined data outperform traditional baselines, confirming that systematic data processing can unlock greater value in machine learning applications.", title='Unlocking Model Potential through Data Evolution'))
[17.02.2026 09:42] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êï∞ÊçÆËææÂ∞îÊñá‰∏ª‰πâÊèêÂá∫‰∫Ü‰∏Ä‰∏™Á≥ªÁªüÊ°ÜÊû∂ÔºåÁî®‰∫éÊï∞ÊçÆ‰∏éÊ®°ÂûãÁöÑÂÖ±ÂêåËøõÂåñÔºåÂåÖÂê´ÂçÅ‰∏™Â±ÇÁ∫ßÁöÑÂàÜÁ±ªÊ≥ï„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂÖàËøõÁöÑÂ§ÑÁêÜÊäÄÊúØÊòæËëóÊèêÂçá‰∫ÜÂü∫Á°ÄÊ®°ÂûãÂú®ÁßëÂ≠¶ÊñáÊú¨‰∏äÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ÈÄöËøáÊûÑÂª∫Darwin-ScienceÔºå‰∏Ä‰∏™ÂåÖÂê´9000‰∫ø‰∏™Ê†áËÆ∞ÁöÑËØ≠ÊñôÂ∫ìÔºåÈ™åËØÅ‰∫ÜËøô‰∏ÄÁêÜËÆ∫ÔºåÂπ∂ÂèëÁé∞ÂéüÂßãÁßëÂ≠¶ÊñáÊú¨Â≠òÂú®ÂèØÂ≠¶‰π†ÊÄßÂ∑ÆË∑ù„ÄÇÈÄöËøá‰ΩøÁî®ÂâçÊ≤øÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËøõË°åÁîüÊàêÊÄßÁ≤æÁÇºÂíåËÆ§Áü•Ë°•ÂÖ®ÔºåÊàë‰ª¨ÊàêÂäüÂº•Ë°•‰∫ÜËøô‰∏ÄÂ∑ÆË∑ùÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ","title":"Êï∞ÊçÆ‰∏éÊ®°ÂûãÁöÑÂÖ±ÂêåËøõÂåñ‰πãÈÅì"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êï∞ÊçÆËææÂ∞îÊñá‰∏ª‰πâÊèêÂá∫‰∫Ü‰∏Ä‰∏™Á≥ªÁªüÊ°ÜÊû∂ÔºåÁî®‰∫éÊï∞ÊçÆ‰∏éÊ®°ÂûãÁöÑÂÖ±ÂêåËøõÂåñÔºåÂåÖÂê´ÂçÅ‰∏™Â±ÇÁ∫ßÁöÑÂàÜÁ±ªÊ≥ï„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂÖàËøõÁöÑÂ§ÑÁêÜÊäÄÊúØÊòæËëóÊèêÂçá‰∫ÜÂü∫Á°ÄÊ®°ÂûãÂú®ÁßëÂ≠¶ÊñáÊú¨‰∏äÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ÈÄöËøáÊûÑÂª∫Darwin-ScienceÔºå‰∏Ä‰∏™ÂåÖÂê´9000‰∫ø‰∏™Ê†áËÆ∞ÁöÑËØ≠ÊñôÂ∫ìÔºåÈ™åËØÅ‰∫ÜËøô‰∏ÄÁêÜËÆ∫ÔºåÂπ∂ÂèëÁé∞ÂéüÂßãÁßëÂ≠¶ÊñáÊú¨Â≠òÂú®ÂèØÂ≠¶‰π†ÊÄßÂ∑ÆË∑ù„ÄÇÈÄöËøá‰ΩøÁî®ÂâçÊ≤øÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËøõË°åÁîüÊàêÊÄßÁ≤æÁÇºÂíåËÆ§Áü•Ë°•ÂÖ®ÔºåÊàë‰ª¨ÊàêÂäüÂº•Ë°•‰∫ÜËøô‰∏ÄÂ∑ÆË∑ùÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ', title='Êï∞ÊçÆ‰∏éÊ®°ÂûãÁöÑÂÖ±ÂêåËøõÂåñ‰πãÈÅì'))
[17.02.2026 09:42] Renaming data file.
[17.02.2026 09:42] Renaming previous data. hf_papers.json to ./d/2026-02-17.json
[17.02.2026 09:42] Saving new data file.
[17.02.2026 09:42] Generating page.
[17.02.2026 09:42] Renaming previous page.
[17.02.2026 09:42] Renaming previous data. index.html to ./d/2026-02-17.html
[17.02.2026 09:42] Writing result.
[17.02.2026 09:42] Renaming log file.
[17.02.2026 09:42] Renaming previous data. log.txt to ./logs/2026-02-17_last_log.txt
