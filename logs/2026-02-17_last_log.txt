[17.02.2026 01:21] Read previous papers.
[17.02.2026 01:21] Generating top page (month).
[17.02.2026 01:21] Writing top page (month).
[17.02.2026 04:12] Read previous papers.
[17.02.2026 04:12] Get feed.
[17.02.2026 04:12] Extract page data from URL. URL: https://huggingface.co/papers/2602.14699
[17.02.2026 04:12] Extract page data from URL. URL: https://huggingface.co/papers/2602.14234
[17.02.2026 04:12] Extract page data from URL. URL: https://huggingface.co/papers/2602.13823
[17.02.2026 04:12] Extract page data from URL. URL: https://huggingface.co/papers/2602.14178
[17.02.2026 04:12] Extract page data from URL. URL: https://huggingface.co/papers/2602.14147
[17.02.2026 04:12] Extract page data from URL. URL: https://huggingface.co/papers/2602.12876
[17.02.2026 04:12] Extract page data from URL. URL: https://huggingface.co/papers/2602.13344
[17.02.2026 04:12] Extract page data from URL. URL: https://huggingface.co/papers/2602.09185
[17.02.2026 04:12] Extract page data from URL. URL: https://huggingface.co/papers/2602.14696
[17.02.2026 04:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.02.2026 04:12] Downloading and parsing papers (pdf, html). Total: 9.
[17.02.2026 04:12] Downloading and parsing paper https://huggingface.co/papers/2602.14699.
[17.02.2026 04:12] Downloading paper 2602.14699 from https://arxiv.org/pdf/2602.14699v1...
[17.02.2026 04:12] Extracting affiliations from text.
[17.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Qute: Towards Quantum-Native Database Muzhi Chen Shanghai Jiao Tong University inefable@sjtu.edu.cn Xuanhe Zhou Shanghai Jiao Tong University zhouxuanhe@sjtu.edu.cn Wei Zhou Shanghai Jiao Tong University wzdb@sjtu.edu.cn Bangrui Xu Shanghai Jiao Tong University dreameter@sjtu.edu.cn Surui Tang Shanghai Jiao Tong University tangsurui@sjtu.edu.cn Guoliang Li Tsinghua University liguoliang@ tsinghua.edu.cn Bingsheng He National University of Singapore dcsheb@nus.edu.sg Yeye He Microsoft Corporation yeyehe@microsoft.com Yitong Song Hong Kong Baptist University ytsong@hkbu.edu.hk Fan Wu Shanghai Jiao Tong University fwu@cs.sjtu.edu.cn 6 2 0 2 6 1 ] . [ 1 9 9 6 4 1 . 2 0 6 2 : r Abstract This paper envisions quantum database (Qute) that treats quantum computation as first-class execution option. Unlike prior simulation-based methods that either run quantum algorithms on classical machines or adapt existing databases for quantum simulation, Qute instead (i) compiles an extended form of SQL into gate-efficient quantum circuits, (ii) employs hybrid optimizer to dynamically select between quantum and classical execution plans, (iii) introduces selective quantum indexing, and (iv) designs fidelity-preserving storage to mitigate current qubit constraints. We also present three-stage evolution roadmap toward quantumnative database. Finally, by deploying Qute on real quantum processor (origin_wukong), we show that it outperforms classical baseline at scale, and we release an open-source prototype at https:// github.com/ weAIDB/ Qute. 1 Introduction Quantum databases aim to leverage quantum computers to accelerate workloads that are increasingly infeasible for classical machines. For example, analytical query processing in columnar OLAP systems often depends on linear scans over unindexed tables [46]. By leveraging entangled qubit superpositions and quantum algorithms such as Grovers search [45], quantum databases can substantially accelerate these queries. Recent research follows"
[17.02.2026 04:12] Response: ```python
[
    "Shanghai Jiao Tong University",
    "Tsinghua University",
    "National University of Singapore",
    "Microsoft Corporation",
    "Hong Kong Baptist University"
]
```
[17.02.2026 04:12] Deleting PDF ./assets/pdf/2602.14699.pdf.
[17.02.2026 04:12] Success.
[17.02.2026 04:12] Downloading and parsing paper https://huggingface.co/papers/2602.14234.
[17.02.2026 04:12] Downloading paper 2602.14234 from https://arxiv.org/pdf/2602.14234v1...
[17.02.2026 04:13] Extracting affiliations from text.
[17.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 5 1 ] . [ 1 4 3 2 4 1 . 2 0 6 2 : r REDSearcher: Scalable and Cost-Efficient Framework for Long-Horizon Search Agents REDSearcher Team Project Page: redsearchagent.github.io "
[17.02.2026 04:13] Response: ```python
[]
```
[17.02.2026 04:13] Extracting affiliations from text.
[17.02.2026 04:13] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 5 1 ] . [ 1 4 3 2 4 1 . 2 0 6 2 : r REDSearcher: Scalable and Cost-Efficient Framework for Long-Horizon Search Agents REDSearcher Team Project Page: redsearchagent.github.ioLarge language models are transitioning from general-purpose knowledge engines to real-world problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of high-quality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interaction-heavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, unified framework that co-designs complex task synthesis, mid-training, and post-training for scalable search-agent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as dual-constrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, high-quality tasks. (2) We introduce tool-augmented queries to encourage proactive tool use rather than passive recall.(3) During mid-training, we strengthen core atomic capabilitiesknowledge, planning, and function callingsubstantially reducing the cost of collecting high-quality trajectories for downstream training. (4) We build local simulated environment that enables rapid, low-cost algorithmic iteration for reinforcement learning experiments. Across both text-only and multimodal searchagent benchmarks, our approach achieves state-of-the-art performance. To facilitate future research on long-horizon search agents, we will release 10K high-quality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints. Figure 1: Benchmark performance of REDSearcher.Large language models (LLMs) [1, 38, 33] are transitioning from static, parametric knowledge engines into dynamic agents [23, 46, 35] capable of navigating the open world. While current models excel at simple retrieval tasks [17], they struggle with deep searchan interactive, long-horizon setting in which an agent must iteratively acquire evidence, maintain competing hypotheses, and synthesize information across multiple sources. In contrast to standard RAG [4], which typically relies on static one-shot retrieval, deep search requires closed-loop search-and-reason behavior that adapts to newly found evidence [31]. However, optimizing LLMs for such depth is hindered by critical bottleneck: the extreme sparsity of effective supervision signals [21, 20, 37]. Scaling these agents is currently intractable due to two prohibitive barriers: the difficulty of synthesizing complex, high-quality reasoning tasks at scale, and the immense computational and temporal cost of collecting interaction-heavy trajectories involving extensive external tool usage. Accordingly, we propose REDSearcher, framework for training tool-augmented deep-search agents across text-only and multimodal (image-text) settings, jointly optimizing task synthesis, mid-training, and post-training to enable scalable, controllable, and cost-effective optimization of long-horizon search behavior. REDSearcher introduces the following technical components: Dual-Constrained Task Synthesis. We mitigate the scarcity of challenging supervision by formulating query generation as constraint satisfaction problem over latent knowledge graph. Unlike standard QA datasets that predominantly admit linear, tree-like reasoning, we construct instances with higher structural complexity (e.g., cycles and interlocking constraints), which increases the effective reasoning load and requires maintaining multiple competing hypotheses rather than simple sequential deduction. In addition, we introduce an explicit evidence-dispersion constraint to discourage single-page shortcut solutions: logically coupled facts are deliberately placed in disjoint sources, encouraging iterative planning and cross-document synthesis under realistic search settings. Proactive Tool-Augmented Queries. Learning to use tools purely via sparse trial-and-error exploration is sample-inefficient. We therefore tool-ground the synthesized queries by rewriting key facts into tool-resolvable constraints that cannot be satisfied by text retrieval alone. Concretely, we replace explicit entities with operationalized specificationse.g., turning place name into routing/distance constraint resolved by map tool, or swapping named entity for visual cue that requires image understanding. This design makes successful task completion contingent on invoking the appropriate tool, thereby densifying learning signals for targeted tool usage during long-horizon rollouts. Cost-Efficient Mid-Training. Bridging the gap between static pre-training and dynamic agent deployment requires dedicated transitional phase. We adopt two-stage mid-training regimen that separates the acquisition of atomic subskills from interactive execution. In the first stage, synthetic data strengthens core competenciesintent-anchored grounding (filtering noise to find evidence) and hierarchical planning (structuring ambiguous goals)at scale without costly environment interaction. The second stage introduces simulated tool-use loops and long-horizon trajectories to capture environmental feedback and state retention. By warm-starting the model with these capabilities before real-world exposure, we significantly improve initial exploration success and reduce the sample complexity and computational cost of collecting high-quality trajectories for downstream training. Functionally Equivalent Simulation Environment. To facilitate rapid algorithmic iteration, we construct lightweight, local simulated environment that mimics real-world web dynamics while eliminating the latency and expense of live API calls. Crucially, this environment is engineered to balance guaranteed solvability with high-interference noise: it ensures that all necessary evidence is present within the closed corpus, yet physically dispersed and buried amidst extensive distractor documents. This design rigorously stress-tests the agents ability to discriminate valid signals from noise, providing high-throughput sandbox that enables efficient reinforcement learning experiments and scalable evaluation without the bottlenecks of external network interactions.2.1 Problem Formulation We model web-enabled question an"
[17.02.2026 04:13] Mistral response. {"id": "e9f2d4d1697243c7910c54c830c4e777", "created": 1771301590, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1253, "total_tokens": 1259, "completion_tokens": 6, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}}]}
[17.02.2026 04:13] Response: ```python
[]
```
[17.02.2026 04:13] Deleting PDF ./assets/pdf/2602.14234.pdf.
[17.02.2026 04:13] Success.
[17.02.2026 04:13] Downloading and parsing paper https://huggingface.co/papers/2602.13823.
[17.02.2026 04:13] Downloading paper 2602.13823 from https://arxiv.org/pdf/2602.13823v1...
[17.02.2026 04:13] Extracting affiliations from text.
[17.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 4 1 ] . [ 1 3 2 8 3 1 . 2 0 6 2 : r Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings Haonan Jiang1,2, Yuji Wang1,2, Yongjie Zhu2, Xin Lu2, Wenyu Qin2, Meng Wang2, Pengfei Wan2, and Yansong Tang1 1Tsinghua Shenzhen International Graduate School, Tsinghua University 2Kling Team, Kuaishou Technology {jiang-hn24@mails, yuji-wan24@mails, tang.yansong@sz}.tsinghua.edu.cn {zhuyongjie, luxin09, qinwenyu, wangmeng46, wanpengfei}@kuaishou.com Abstract. Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks. Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner, ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment, effectively strengthens cross-modal semantic consistency and boosts the models fine-grained matching capability as well as its generalization across complex scenarios. Our work demonstrat"
[17.02.2026 04:13] Response: ```python
[
    "Tsinghua Shenzhen International Graduate School, Tsinghua University",
    "Kling Team, Kuaishou Technology"
]
```
[17.02.2026 04:13] Deleting PDF ./assets/pdf/2602.13823.pdf.
[17.02.2026 04:13] Success.
[17.02.2026 04:13] Downloading and parsing paper https://huggingface.co/papers/2602.14178.
[17.02.2026 04:13] Downloading paper 2602.14178 from https://arxiv.org/pdf/2602.14178v1...
[17.02.2026 04:13] Extracting affiliations from text.
[17.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"UniWeTok: An Unified Binary Tokenizer with Codebook Size 2 128 for Unified Multimodal Large Language Model Shaobin Zhuang1,2, Yuang Ai1,3, Jiaming Han1,3, Weijia Mao1,5, Xiaohui Li2 Fangyikang Wang6, Xiao Wang1, Yan Li1, Shanchuan Lin1, Kun Xu1 Zhenheng Yang1, Huaibo Huang4, Xiangyu Yue3, Hao Chen1, Yali Wang7 1ByteDance, 2Shanghai Jiao Tong University, 3MMLab, The Chinese University of Hong Kong 4Institute of Automation, Chinese Academy of Sciences, 5National University of Singapore 6Zhejiang University, 7Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences Equal contribution, Corresponding Author, Project lead "
[17.02.2026 04:13] Response: ```python
[
    "ByteDance",
    "Shanghai Jiao Tong University",
    "MMLab, The Chinese University of Hong Kong",
    "Institute of Automation, Chinese Academy of Sciences",
    "National University of Singapore",
    "Zhejiang University",
    "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences"
]
```
[17.02.2026 04:13] Deleting PDF ./assets/pdf/2602.14178.pdf.
[17.02.2026 04:13] Success.
[17.02.2026 04:13] Downloading and parsing paper https://huggingface.co/papers/2602.14147.
[17.02.2026 04:13] Downloading paper 2602.14147 from https://arxiv.org/pdf/2602.14147v1...
[17.02.2026 04:13] Extracting affiliations from text.
[17.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 5 1 ] . [ 1 7 4 1 4 1 . 2 0 6 2 : r LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models Shufan Li1,2,,, Yuchen Zhu1,3,,, Jiuxiang Gu1, Kangning Liu1, Zhe Lin1 Yongxin Chen3, Molei Tao3, Aditya Grover2, Jason Kuen1 1Adobe 2UCLA 3Georgia Tech * Equal Contribution Work done primarily during internship at Adobe Research Figure 1. We introduce LaViDa-R1, multimodal diffusion language model with strong reasoning capabilities across diverse tasks. LaViDa-R1incorporates novel unified post-training that significantly improves upon the base model LaViDa-O (Li et al., 2025b) and SFT baseline on visual math reasoning, visual question answering, image editing, and object grounding tasks. "
[17.02.2026 04:13] Response: ```python
['Adobe', 'UCLA', 'Georgia Tech']
```
[17.02.2026 04:13] Deleting PDF ./assets/pdf/2602.14147.pdf.
[17.02.2026 04:13] Success.
[17.02.2026 04:13] Downloading and parsing paper https://huggingface.co/papers/2602.12876.
[17.02.2026 04:13] Downloading paper 2602.12876 from https://arxiv.org/pdf/2602.12876v1...
[17.02.2026 04:13] Extracting affiliations from text.
[17.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 3 1 ] . [ 1 6 7 8 2 1 . 2 0 6 2 : r BrowseComp-V3: Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents Huanyao Zhang1,,, Jiepeng Zhou2,, Bo Li1,, Bowen Zhou1,, Yanzhe Dan3,, Haishan Lu1, Zhiyong Cao4, Jiaoyang Chen5, Yuqian Han1, Zinan Sheng1, Zhengwei Tao1, Hao Liang1, Jialong Wu1, Yang Shi1, Yuanpeng He1, Jiaye Lin6, Qintong Zhang1, Guochen Yan1, Runhao Zhao1, Zhengpin Li1, Xiaohan Yu7, Lang Mei7, Chong Chen7,, Wentao Zhang1,, Bin Cui1, 1PKU 2HKUST(GZ) 3OUC 4CASIA 5HITSZ 6THU 7Huawei Cloud BU Core Contributor Project Leader Corresponding author "
[17.02.2026 04:13] Response: ```python
[
    "PKU",
    "HKUST(GZ)",
    "OUC",
    "CASIA",
    "HITSZ",
    "THU",
    "Huawei Cloud BU"
]
```
[17.02.2026 04:13] Deleting PDF ./assets/pdf/2602.12876.pdf.
[17.02.2026 04:13] Success.
[17.02.2026 04:13] Downloading and parsing paper https://huggingface.co/papers/2602.13344.
[17.02.2026 04:13] Downloading paper 2602.13344 from https://arxiv.org/pdf/2602.13344v1...
[17.02.2026 04:14] Extracting affiliations from text.
[17.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"FireRed-Image-Edit-1.0 Technical Report Super Intelligence Team, Xiaohongshu Inc. "
[17.02.2026 04:14] Response: ```python
["Xiaohongshu Inc."]
```
[17.02.2026 04:14] Deleting PDF ./assets/pdf/2602.13344.pdf.
[17.02.2026 04:14] Success.
[17.02.2026 04:14] Downloading and parsing paper https://huggingface.co/papers/2602.09185.
[17.02.2026 04:14] Downloading paper 2602.09185 from https://arxiv.org/pdf/2602.09185v1...
[17.02.2026 04:14] Extracting affiliations from text.
[17.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 ] . [ 1 5 8 1 9 0 . 2 0 6 2 : r AIDev: Studying AI Coding Agents on GitHub Hao Li Haoxiang Zhang Queens University Queens University Kingston, ON, Canada Kingston, ON, Canada hao.li@queensu.ca haoxiang.zhang@queensu.ca Ahmed E. Hassan Queens University Kingston, ON, Canada ahmed@cs.queensu.ca Abstract AI coding agents are rapidly transforming software engineering by performing tasks such as feature development, debugging, and testing. Despite their growing impact, the research community lacks comprehensive dataset capturing how these agents are used in realworld projects. To address this gap, we introduce AIDev, largescale dataset focused on agent-authored pull requests (AgenticPRs) in real-world GitHub repositories. AIDev aggregates 932,791 Agentic-PRs produced by five agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These PRs span 116,211 repositories and involve 72,189 developers. In addition, AIDev includes curated subset of 33,596 Agentic-PRs from 2,807 repositories with over 100 stars, providing further information such as comments, reviews, commits, and related issues. This dataset offers foundation for future research on AI adoption, developer productivity, and human-AI collaboration in the new era of software engineering. CCS Concepts Software and its engineering; Information systems Data mining; Keywords AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering Agent Figure 1 illustrates how Coding Agent operates within real GitHub workflow. In this example, the agent (GitHub Copilot) is assigned an issue, generates code patch, and submits PR with detailed description. human reviewer provides feedback, which the agent addresses in follow-up commit and replies.2 This interaction showcases the emerging dynamics of human-AI collaboration in software development, where Coding Agents not only contribute code but also remain engaged in the review process. 1We refer to PRs authored by Coding Agents as Agentic-PRs. 2No"
[17.02.2026 04:14] Response: ```python
["Queens University"]
```
[17.02.2026 04:14] Deleting PDF ./assets/pdf/2602.09185.pdf.
[17.02.2026 04:14] Success.
[17.02.2026 04:14] Downloading and parsing paper https://huggingface.co/papers/2602.14696.
[17.02.2026 04:14] Downloading paper 2602.14696 from https://arxiv.org/pdf/2602.14696v1...
[17.02.2026 04:14] Extracting affiliations from text.
[17.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Nihal V. Nayak 1 Paula Rodriguez-Diaz 1 Neha Hulkund 2 Sara Beery 2 David Alvarez-Melis "
[17.02.2026 04:14] Response: ```python
[]
```
[17.02.2026 04:14] Extracting affiliations from text.
[17.02.2026 04:14] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Nihal V. Nayak 1 Paula Rodriguez-Diaz 1 Neha Hulkund 2 Sara Beery 2 David Alvarez-Melis1. Introduction 6 2 0 2 6 1 ] . [ 1 6 9 6 4 1 . 2 0 6 2 : r Instruction fine-tuning of large language models (LLMs) often involves selecting subset of instruction training data from large candidate pool, using small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradientbased representations paired with greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/tar geted-instruction-selection. *Equal contribution 3Kempner Institute. <nnayak@seas.harvard.edu>. 2MIT 1Harvard University Correspondence to: Nihal V. Nayak Preprint. February 17, 2026. 1 Large language models (LLMs) have demonstrated remarkable ability to follow complex instructions through instruction fine-tuning on curated datasets of instructionresponse pairs (Hurst et al., 2024; Olmo et al., 2025; Agarwal et al., 2025). Constructing such datasets, however, typically requires careful experimentation and extensive ablation studies, making them both time-consuming and computationally expensive (Guha et al., 2025; Magnusson et al., 2025). This challenge is further exacerbated by the growing need to adapt LLMs to specialized downstream tasks under limited data or compute budgets (Thulke et al., 2024; Zhao et al., 2024b). As result, growing body of work studies how to automatically select subset of examples from large candidate pool that is most useful for given target taska problem commonly referred to as targeted instruction selection (Xia et al., 2024). Despite increasing interest, the literature on targeted instruction selection remains fragmented and difficult to interpret (Appendix A). Proposed methods vary widely in their formulations, often involve multiple design choices (e.g., representations, similarity metrics, selection algorithms), and lack key baselines, such as the zero-shot baseline. Empirical results are inconsistent, and it remains unclear which techniques actually drive performance, when, and why. In this work, we bring clarity to this space through disentangled framework that separates the two core ingredients of targeted instruction selection: (i) the representation used to encode the data and (ii) the algorithm used to select examples based on these representations. This disentangled view allows us to isolate the effects of each component and enables controlled comparisons across models, datasets, and budgets. We further show that, despite differing in algorithmic detail, many selection methods can be unified under the view of approximate distance minimization between the selected subset and the target task distributions. We support this perspective with new generalization bounds that characterize when distance-based selection helpsand when it doesnt. Our empirical findings are mixed but revealing. Gradientbased representations are the only ones whose similarity Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) (or distance) to the query set reliably predicts performance (Section 5.1, Section 5.2), but even these fail to improve over zero-shot inference in some regimes (Appendix M). While no single method dominates, greedy round-robin selection tends to perform best at small budgets, while optimal transport-based methods offer modest gains at larger ones (Section 5.2). Surprisingly, randomly sampled subsets often match or outperform many popular selection methods (TyDiQA and MMLU-Pro in Figure 5), especially as the budget increases, highlighting the brittleness of current practice. Our work makes the following contributions: We propose disentangled framework for targeted instruction selection that isolates the effects of (i) data representations and (ii) selection algorithms, enabling controlled comparisons. We conduct systematic empirical analyses across multiple target tasks and LLMs, showing that gradient-based data representations correlate most strongly with the loss and that greedy round-robin selection performs best at low budgets, while optimal transport-based methods perform best at high budgets. We develop unified theoretical perspective that interprets many selection algorithms as approximate distance minimization, and prove generalization bounds that explain both the benefits and limitations of this view. 2. Targeted Instruction Selection Let = {zi = (xi, yi)}N i=1 be large candidate pool and = {qj = (xj, yj)}M j=1 be the query set drawn from the distribution PT of the target task . We consider model fθ parameterized by θ. For sample = (x, y), we define the per-example loss as ℓ(θ; z) := ℓ(fθ(x), y). The goal of targeted instruction selection is to choose subset of examples of size (training budget) and train the model fθ using only examples in so as to minimize the expected loss on the target task : = arg min SD S=B EzPT [ℓ(θS ; z)] . Since we do not have access to the target tasks during training and selection, we use as proxy for selection and choose to solve the objective: ˆS = arg min SD S=B 1 M (cid:88) j=1 ℓ(θS ; qj) . Optimizing this objective is impractical and combinatorially expensive, which has motivated compute efficient instruction selection me"
[17.02.2026 04:14] Mistral response. {"id": "2c451338c5154764ab8445bbb9a01b4b", "created": 1771301660, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1412, "total_tokens": 1436, "completion_tokens": 24, "prompt_tokens_details": {"cached_tokens": 32}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Kempner Institute\",\n    \"MIT\",\n    \"Harvard University\"\n]\n```"}}]}
[17.02.2026 04:14] Response: ```python
[
    "Kempner Institute",
    "MIT",
    "Harvard University"
]
```
[17.02.2026 04:14] Deleting PDF ./assets/pdf/2602.14696.pdf.
[17.02.2026 04:14] Success.
[17.02.2026 04:14] Enriching papers with extra data.
[17.02.2026 04:14] ********************************************************************************
[17.02.2026 04:14] Abstract 0. This paper envisions a quantum database (Qute) that treats quantum computation as a first-class execution option. Unlike prior simulation-based methods that either run quantum algorithms on classical machines or adapt existing databases for quantum simulation, Qute instead (i) compiles an extended f...
[17.02.2026 04:14] ********************************************************************************
[17.02.2026 04:14] Abstract 1. REDSearcher presents a unified framework for optimizing search agents through improved task synthesis, tool-augmented queries, midtraining capability enhancement, and simulated environments to address challenges in long-horizon search tasks.  					AI-generated summary 				 Large language models are ...
[17.02.2026 04:14] ********************************************************************************
[17.02.2026 04:14] Abstract 2. A reasoning-driven universal multimodal embedding framework integrates embedder-guided reinforcement learning with traceability chain-of-thought to enhance cross-modal semantic consistency and retrieval performance.  					AI-generated summary 				 Leveraging Multimodal Large Language Models (MLLMs) ...
[17.02.2026 04:14] ********************************************************************************
[17.02.2026 04:14] Abstract 3. UniWeTok introduces a unified discrete tokenizer with a massive binary codebook and novel training techniques to achieve superior performance in image generation and multimodal tasks while reducing computational requirements.  					AI-generated summary 				 Unified Multimodal Large Language Models (...
[17.02.2026 04:14] ********************************************************************************
[17.02.2026 04:14] Abstract 4. LaViDa-R1 is a multimodal reasoning diffusion language model that unifies supervised fine-tuning and multi-task reinforcement learning with novel training techniques for enhanced performance across visual reasoning and generation tasks.  					AI-generated summary 				 Diffusion language models (dLLM...
[17.02.2026 04:14] ********************************************************************************
[17.02.2026 04:14] Abstract 5. A new benchmark called BrowseComp-V3 challenges multimodal large language models with complex, multi-hop reasoning tasks requiring deep search across text and visual modalities, revealing significant gaps in current capabilities.  					AI-generated summary 				 Multimodal large language models (MLLM...
[17.02.2026 04:14] ********************************************************************************
[17.02.2026 04:14] Abstract 6. FireRed-Image-Edit uses a diffusion transformer with optimized data curation and training methods to achieve state-of-the-art performance in instruction-based image editing, supported by a comprehensive benchmark and novel techniques for data efficiency and optimization stability.  					AI-generated...
[17.02.2026 04:14] ********************************************************************************
[17.02.2026 04:14] Abstract 7. AIDev is a large-scale dataset of agent-authored pull requests from real-world GitHub repositories that captures AI coding agent usage in practical software development scenarios.  					AI-generated summary 				 AI coding agents are rapidly transforming software engineering by performing tasks such ...
[17.02.2026 04:14] ********************************************************************************
[17.02.2026 04:14] Abstract 8. Targeted instruction selection for LLM fine-tuning can be improved by systematically analyzing data representation and selection algorithms, with gradient-based representations and greedy round-robin selection performing best at low budgets.  					AI-generated summary 				 Instruction fine-tuning of...
[17.02.2026 04:14] Read previous papers.
[17.02.2026 04:14] Generating reviews via LLM API.
[17.02.2026 04:14] Querying the API.
[17.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

This paper envisions a quantum database (Qute) that treats quantum computation as a first-class execution option. Unlike prior simulation-based methods that either run quantum algorithms on classical machines or adapt existing databases for quantum simulation, Qute instead (i) compiles an extended form of SQL into gate-efficient quantum circuits, (ii) employs a hybrid optimizer to dynamically select between quantum and classical execution plans, (iii) introduces selective quantum indexing, and (iv) designs fidelity-preserving storage to mitigate current qubit constraints. We also present a three-stage evolution roadmap toward quantum-native database. Finally, by deploying Qute on a real quantum processor (origin_wukong), we show that it outperforms a classical baseline at scale, and we release an open-source prototype at https://github.com/weAIDB/Qute.
[17.02.2026 04:14] Response: ```json
{
  "desc": "В статье представлена квантовая база данных Qute, которая рассматривает квантовые вычисления как полноценный вариант выполнения запросов. Система компилирует расширенный SQL в эффективные квантовые схемы и использует гибридный оптимизатор для динамического выбора между квантовым и классическим выполнением операций. Авторы внедряют селективное квантовое индексирование и разработали методы хранения данных, сохраняющие верность вычислений при ограниченном числе кубитов. Экспериментальная валидация на реальном квантовом процессоре показывает превосходство подхода над классическими базами данных.",
  "emoji": "⚛️",
  "title": "Гибридная база данных для эпохи квантовых вычислений"
}
```
[17.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This paper envisions a quantum database (Qute) that treats quantum computation as a first-class execution option. Unlike prior simulation-based methods that either run quantum algorithms on classical machines or adapt existing databases for quantum simulation, Qute instead (i) compiles an extended form of SQL into gate-efficient quantum circuits, (ii) employs a hybrid optimizer to dynamically select between quantum and classical execution plans, (iii) introduces selective quantum indexing, and (iv) designs fidelity-preserving storage to mitigate current qubit constraints. We also present a three-stage evolution roadmap toward quantum-native database. Finally, by deploying Qute on a real quantum processor (origin_wukong), we show that it outperforms a classical baseline at scale, and we release an open-source prototype at https://github.com/weAIDB/Qute."

[17.02.2026 04:14] Response: ```python
["BENCHMARK", "DATASET"]
```

**Reasoning:**
- **BENCHMARK**: The paper presents Qute, a quantum database system with evaluation on a real quantum processor (origin_wukong), comparing performance against classical baselines. This constitutes a model/system evaluation framework.
- **DATASET**: The paper releases an open-source prototype, which represents a significant contribution that could be considered a resource/tool for the community.

Note: While the paper involves novel architecture and optimization techniques, these are secondary to the core contribution of the quantum database system itself. The primary contributions align most directly with benchmark evaluation and the release of a new system/resource.
[17.02.2026 04:14] Error. Failed to parse JSON from LLM. ["BENCHMARK", "DATASET"]


**Reasoning:**
- **BENCHMARK**: The paper presents Qute, a quantum database system with evaluation on a real quantum processor (origin_wukong), comparing performance against classical baselines. This constitutes a model/system evaluation framework.
- **DATASET**: The paper releases an open-source prototype, which represents a significant contribution that could be considered a resource/tool for the community.

Note: While the paper involves novel architecture and optimization techniques, these are secondary to the core contribution of the quantum database system itself. The primary contributions align most directly with benchmark evaluation and the release of a new system/resource.
[17.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This paper envisions a quantum database (Qute) that treats quantum computation as a first-class execution option. Unlike prior simulation-based methods that either run quantum algorithms on classical machines or adapt existing databases for quantum simulation, Qute instead (i) compiles an extended form of SQL into gate-efficient quantum circuits, (ii) employs a hybrid optimizer to dynamically select between quantum and classical execution plans, (iii) introduces selective quantum indexing, and (iv) designs fidelity-preserving storage to mitigate current qubit constraints. We also present a three-stage evolution roadmap toward quantum-native database. Finally, by deploying Qute on a real quantum processor (origin_wukong), we show that it outperforms a classical baseline at scale, and we release an open-source prototype at https://github.com/weAIDB/Qute."

[17.02.2026 04:14] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```

**Reasoning:**
- **OPTIMIZATION**: The paper discusses a hybrid optimizer that dynamically selects between quantum and classical execution plans, and presents optimization techniques for compiling SQL into gate-efficient quantum circuits.
- **OPEN_SOURCE**: The paper explicitly states "we release an open-source prototype at https://github.com/weAIDB/Qute.
[17.02.2026 04:14] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "OPEN_SOURCE"]


**Reasoning:**
- **OPTIMIZATION**: The paper discusses a hybrid optimizer that dynamically selects between quantum and classical execution plans, and presents optimization techniques for compiling SQL into gate-efficient quantum circuits.
- **OPEN_SOURCE**: The paper explicitly states "we release an open-source prototype at https://github.com/weAIDB/Qute.
[17.02.2026 04:14] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper introduces Qute, a quantum database that integrates quantum computation directly into database operations. It innovatively compiles SQL into quantum circuits, allowing for efficient execution of quantum algorithms. Qute also features a hybrid optimizer that intelligently chooses between quantum and classical processing, along with selective quantum indexing to enhance performance. The authors demonstrate Qute\'s effectiveness on a real quantum processor, showing superior performance compared to classical methods, and provide an open-source prototype for further exploration.","title":"Qute: Revolutionizing Databases with Quantum Computing"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces Qute, a quantum database that integrates quantum computation directly into database operations. It innovatively compiles SQL into quantum circuits, allowing for efficient execution of quantum algorithms. Qute also features a hybrid optimizer that intelligently chooses between quantum and classical processing, along with selective quantum indexing to enhance performance. The authors demonstrate Qute's effectiveness on a real quantum processor, showing superior performance compared to classical methods, and provide an open-source prototype for further exploration.", title='Qute: Revolutionizing Databases with Quantum Computing'))
[17.02.2026 04:14] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"本文提出了一种量子数据库（Qute），将量子计算视为一种优先执行选项。与以往在经典机器上运行量子算法或调整现有数据库进行量子模拟的方法不同，Qute通过将扩展的SQL编译为高效的量子电路，动态选择量子和经典执行计划，并引入选择性量子索引来优化性能。此外，Qute设计了保真度存储以应对当前量子比特的限制，并展示了在真实量子处理器上部署的效果超越经典基线。","title":"量子数据库：量子计算的新选择"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种量子数据库（Qute），将量子计算视为一种优先执行选项。与以往在经典机器上运行量子算法或调整现有数据库进行量子模拟的方法不同，Qute通过将扩展的SQL编译为高效的量子电路，动态选择量子和经典执行计划，并引入选择性量子索引来优化性能。此外，Qute设计了保真度存储以应对当前量子比特的限制，并展示了在真实量子处理器上部署的效果超越经典基线。', title='量子数据库：量子计算的新选择'))
[17.02.2026 04:14] Querying the API.
[17.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

REDSearcher presents a unified framework for optimizing search agents through improved task synthesis, tool-augmented queries, midtraining capability enhancement, and simulated environments to address challenges in long-horizon search tasks.  					AI-generated summary 				 Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.
[17.02.2026 04:14] Response: ```json
{
  "desc": "REDSearcher представляет унифицированный фреймворк для оптимизации поисковых агентов, построенных на основе больших языковых моделей. Основная проблема заключается в нехватке высокачественных траекторий поиска и сигналов вознаграждения для задач с длинным горизонтом планирования. Авторы предлагают решение через синтез сложных задач с контролируемой сложностью, улучшение возможностей модели через промежуточное обучение и создание локальной симулированной среды. Подход достигает лучших результатов на бенчмарках текстового и мультимодального поиска и будет сопровождаться выпуском набора высокачественных данных для обучения.",
  "emoji": "🔍",
  "title": "Масштабируемая оптимизация поисковых агентов через синтез задач и промежуточное обучение"
}
```
[17.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"REDSearcher presents a unified framework for optimizing search agents through improved task synthesis, tool-augmented queries, midtraining capability enhancement, and simulated environments to address challenges in long-horizon search tasks.  					AI-generated summary 				 Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints."

[17.02.2026 04:14] Response: ```python
['AGENTS', 'DATASET', 'TRAINING', 'BENCHMARK', 'MULTIMODAL', 'RL']
```
[17.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"REDSearcher presents a unified framework for optimizing search agents through improved task synthesis, tool-augmented queries, midtraining capability enhancement, and simulated environments to address challenges in long-horizon search tasks.  					AI-generated summary 				 Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints."

[17.02.2026 04:14] Response: ```python
['REASONING', 'OPTIMIZATION', 'OPEN_SOURCE', 'LONG_CONTEXT', 'SYNTHETIC']
```
[17.02.2026 04:14] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"REDSearcher is a new framework designed to improve the performance of search agents in complex tasks. It tackles the problem of sparse high-quality search trajectories by optimizing task synthesis and enhancing training processes. The framework introduces innovative methods like dual-constrained task synthesis and tool-augmented queries to promote effective tool usage. Additionally, it provides a simulated environment for efficient reinforcement learning, leading to state-of-the-art results in both text and multimodal search tasks.","title":"Optimizing Search Agents with REDSearcher: A Unified Approach"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='REDSearcher is a new framework designed to improve the performance of search agents in complex tasks. It tackles the problem of sparse high-quality search trajectories by optimizing task synthesis and enhancing training processes. The framework introduces innovative methods like dual-constrained task synthesis and tool-augmented queries to promote effective tool usage. Additionally, it provides a simulated environment for efficient reinforcement learning, leading to state-of-the-art results in both text and multimodal search tasks.', title='Optimizing Search Agents with REDSearcher: A Unified Approach'))
[17.02.2026 04:14] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"REDSearcher是一个统一框架，旨在通过改进任务合成、工具增强查询、中期训练能力提升和模拟环境来优化搜索代理，以应对长时间搜索任务中的挑战。该框架通过双约束优化来精确控制任务难度，从而可扩展地生成复杂的高质量任务。REDSearcher还引入了工具增强查询，鼓励主动使用工具，而不是被动回忆。在中期训练中，REDSearcher显著增强了核心原子能力，降低了收集高质量轨迹的成本，从而提高了下游训练的效率。","title":"REDSearcher：优化搜索代理的统一框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='REDSearcher是一个统一框架，旨在通过改进任务合成、工具增强查询、中期训练能力提升和模拟环境来优化搜索代理，以应对长时间搜索任务中的挑战。该框架通过双约束优化来精确控制任务难度，从而可扩展地生成复杂的高质量任务。REDSearcher还引入了工具增强查询，鼓励主动使用工具，而不是被动回忆。在中期训练中，REDSearcher显著增强了核心原子能力，降低了收集高质量轨迹的成本，从而提高了下游训练的效率。', title='REDSearcher：优化搜索代理的统一框架'))
[17.02.2026 04:14] Querying the API.
[17.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A reasoning-driven universal multimodal embedding framework integrates embedder-guided reinforcement learning with traceability chain-of-thought to enhance cross-modal semantic consistency and retrieval performance.  					AI-generated summary 				 Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks. Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose a reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner, ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment, effectively strengthens cross-modal semantic consistency and boosts the fine-grained matching capability of the model as well as the generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development.
[17.02.2026 04:14] Response: ```json
{
  "desc": "Статья представляет новый подход к созданию универсальных мультимодальных эмбеддингов, который объединяет направленное подкреплением обучение с цепочкой рассуждений для улучшения согласованности семантики между модальностями. Авторы предлагают框架EG-RL, где эмбеддер явно направляет обучение модели рассуждений, заставляя её генерировать релевантные для поиска объяснения. Ключевой инновацией является введение трассируемой цепочки мыслей (T-CoT), которая извлекает критические мультимодальные признаки и фокусируется на элементах, важных для поиска. Экспериментальные результаты показывают, что метод превосходит существующие подходы на бенчмарках MMEB-V2 и UVRB, обеспечивая более эффективное кроссмодальное сопоставление и лучшую обобщаемость.",
  "emoji": "🧠",
  "title": "Рассуждение как путь к лучшему пониманию между модальностями"
}
```
[17.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A reasoning-driven universal multimodal embedding framework integrates embedder-guided reinforcement learning with traceability chain-of-thought to enhance cross-modal semantic consistency and retrieval performance.  					AI-generated summary 				 Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks. Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose a reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner, ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment, effectively strengthens cross-modal semantic consistency and boosts the fine-grained matching capability of the model as well as the generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development."

[17.02.2026 04:14] Response: ```python
["MULTIMODAL", "RL", "BENCHMARK", "RAG"]
```
[17.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A reasoning-driven universal multimodal embedding framework integrates embedder-guided reinforcement learning with traceability chain-of-thought to enhance cross-modal semantic consistency and retrieval performance.  					AI-generated summary 				 Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks. Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose a reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner, ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment, effectively strengthens cross-modal semantic consistency and boosts the fine-grained matching capability of the model as well as the generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development."

[17.02.2026 04:14] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[17.02.2026 04:14] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper presents a new framework for Universal Multimodal Embeddings (UME) that enhances the consistency and performance of retrieving information across different types of data, like text and images. It combines Embedder-Guided Reinforcement Learning (EG-RL) with a novel reasoning approach called Traceability Chain-of-Thought (T-CoT) to improve how models understand and relate different modalities. The framework allows the reasoning process to be guided by the embedder, ensuring that the generated reasoning is relevant to the retrieval tasks. Overall, this approach shows significant improvements in embedding quality and retrieval effectiveness on benchmark datasets, demonstrating the power of structured reasoning in multimodal contexts.","title":"Enhancing Multimodal Retrieval with Reasoning-Driven Embeddings"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new framework for Universal Multimodal Embeddings (UME) that enhances the consistency and performance of retrieving information across different types of data, like text and images. It combines Embedder-Guided Reinforcement Learning (EG-RL) with a novel reasoning approach called Traceability Chain-of-Thought (T-CoT) to improve how models understand and relate different modalities. The framework allows the reasoning process to be guided by the embedder, ensuring that the generated reasoning is relevant to the retrieval tasks. Overall, this approach shows significant improvements in embedding quality and retrieval effectiveness on benchmark datasets, demonstrating the power of structured reasoning in multimodal contexts.', title='Enhancing Multimodal Retrieval with Reasoning-Driven Embeddings'))
[17.02.2026 04:15] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"本文提出了一种基于推理的通用多模态嵌入框架，结合了嵌入引导的强化学习和可追溯的思维链，以增强跨模态的语义一致性和检索性能。通过引入生成的思维链推理，显著提升了任务特定的表示能力。我们设计了一个EG-RL框架，使嵌入器为推理器提供明确的监督，确保生成的思维链与嵌入任务对齐。实验结果表明，该框架在资源有限的情况下，超越了现有的多模态嵌入模型，提升了模型在复杂场景中的匹配能力和泛化能力。","title":"推理驱动的多模态嵌入新框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种基于推理的通用多模态嵌入框架，结合了嵌入引导的强化学习和可追溯的思维链，以增强跨模态的语义一致性和检索性能。通过引入生成的思维链推理，显著提升了任务特定的表示能力。我们设计了一个EG-RL框架，使嵌入器为推理器提供明确的监督，确保生成的思维链与嵌入任务对齐。实验结果表明，该框架在资源有限的情况下，超越了现有的多模态嵌入模型，提升了模型在复杂场景中的匹配能力和泛化能力。', title='推理驱动的多模态嵌入新框架'))
[17.02.2026 04:15] Querying the API.
[17.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

UniWeTok introduces a unified discrete tokenizer with a massive binary codebook and novel training techniques to achieve superior performance in image generation and multimodal tasks while reducing computational requirements.  					AI-generated summary 				 Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook (2^{128}). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.
[17.02.2026 04:15] Response: ```json
{
  "desc": "UniWeTok представляет унифицированный дискретный токенизатор с огромным бинарным кодовым словарём размером 2^128, который одновременно обеспечивает высокую точность реконструкции изображений, сложное семантическое извлечение и пригодность для генеративных задач. Авторы предлагают новые методы обучения, включая Pre-Post Distillation и Generative-Aware Prior, а также гибридную архитектуру свёрточной сети с функцией активации SigLu, которая стабилизирует процесс дистилляции и разрешает конфликт оптимизации между энтропией токенов и loss приверженности. Трёхэтапная схема обучения повышает адаптивность модели к различным разрешениям изображений и чувствительным к восприятию сценариям с лицами и текстом. UniWeTok достигает лучших результатов в генерации изображений с минимальными вычислительными затратами и демонстрирует конкурентные способности в мультимодальном понимании, генерации и редактировании изображений.",
  "emoji": "🎨",
  "title": "Единый токенизатор для универсальной генерации и понимания изображений"
}
```
[17.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"UniWeTok introduces a unified discrete tokenizer with a massive binary codebook and novel training techniques to achieve superior performance in image generation and multimodal tasks while reducing computational requirements.  					AI-generated summary 				 Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook (2^{128}). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM."

[17.02.2026 04:15] Response: ```python
['MULTIMODAL', 'ARCHITECTURE', 'TRAINING']
```
[17.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"UniWeTok introduces a unified discrete tokenizer with a massive binary codebook and novel training techniques to achieve superior performance in image generation and multimodal tasks while reducing computational requirements.  					AI-generated summary 				 Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook (2^{128}). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM."

[17.02.2026 04:15] Response: ```python
['OPTIMIZATION', 'OPEN_SOURCE']
```
[17.02.2026 04:15] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"UniWeTok is a novel discrete tokenizer that enhances image generation and multimodal tasks by utilizing a large binary codebook and innovative training methods. It addresses the challenges of high-fidelity reconstruction and complex semantic extraction within a single framework. The paper introduces a unique training approach called Pre-Post Distillation and a Generative-Aware Prior, which improve the performance of discrete tokens. Additionally, a convolution-attention hybrid architecture with the SigLu activation function is proposed to optimize the training process and adaptability across various scenarios.","title":"UniWeTok: Bridging the Gap in Multimodal Tokenization"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='UniWeTok is a novel discrete tokenizer that enhances image generation and multimodal tasks by utilizing a large binary codebook and innovative training methods. It addresses the challenges of high-fidelity reconstruction and complex semantic extraction within a single framework. The paper introduces a unique training approach called Pre-Post Distillation and a Generative-Aware Prior, which improve the performance of discrete tokens. Additionally, a convolution-attention hybrid architecture with the SigLu activation function is proposed to optimize the training process and adaptability across various scenarios.', title='UniWeTok: Bridging the Gap in Multimodal Tokenization'))
[17.02.2026 04:15] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"UniWeTok是一种统一的离散分词器，采用了一个巨大的二进制代码本和新颖的训练技术，以在图像生成和多模态任务中实现卓越性能，同时降低计算需求。它通过引入预后蒸馏和生成感知先验来增强离散标记的语义提取和生成能力。UniWeTok的模型架构结合了卷积和注意力机制，并使用SigLu激活函数来稳定语义蒸馏过程。经过三阶段训练框架的优化，UniWeTok在各种图像分辨率和感知敏感场景中表现出色，尤其在ImageNet上达到了最先进的图像生成性能。","title":"统一离散分词器，提升多模态任务性能"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='UniWeTok是一种统一的离散分词器，采用了一个巨大的二进制代码本和新颖的训练技术，以在图像生成和多模态任务中实现卓越性能，同时降低计算需求。它通过引入预后蒸馏和生成感知先验来增强离散标记的语义提取和生成能力。UniWeTok的模型架构结合了卷积和注意力机制，并使用SigLu激活函数来稳定语义蒸馏过程。经过三阶段训练框架的优化，UniWeTok在各种图像分辨率和感知敏感场景中表现出色，尤其在ImageNet上达到了最先进的图像生成性能。', title='统一离散分词器，提升多模态任务性能'))
[17.02.2026 04:15] Querying the API.
[17.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LaViDa-R1 is a multimodal reasoning diffusion language model that unifies supervised fine-tuning and multi-task reinforcement learning with novel training techniques for enhanced performance across visual reasoning and generation tasks.  					AI-generated summary 				 Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing, tree search, and complementary likelihood estimation, to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks, including visual math reasoning, reason-intensive grounding, and image editing.
[17.02.2026 04:15] Response: ```json
{
  "desc": "LaViDa-R1 — это мультимодальная диффузионная языковая модель, которая объединяет различные визуальные задачи в едином подходе. В отличие от предыдущих работ, она интегрирует контролируемую тонкую настройку и многозадачное обучение с подкреплением в единую систему обучения. Модель использует инновационные техники, такие как принудительные ответы, поиск по дереву и дополнительная оценка правдоподобия, чтобы повысить эффективность и масштабируемость. LaViDa-R1 демонстрирует сильные результаты на разнообразных задачах: визуальное математическое рассуждение, детальное позиционирование объектов и редактирование изображений.",
  "emoji": "🎨",
  "title": "Единая диффузионная модель для мультимодального рассуждения и генерации"
}
```
[17.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LaViDa-R1 is a multimodal reasoning diffusion language model that unifies supervised fine-tuning and multi-task reinforcement learning with novel training techniques for enhanced performance across visual reasoning and generation tasks.  					AI-generated summary 				 Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing, tree search, and complementary likelihood estimation, to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks, including visual math reasoning, reason-intensive grounding, and image editing."

[17.02.2026 04:15] Response: ```python
['MULTIMODAL', 'RL', 'TRAINING', 'ARCHITECTURE']
```
[17.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LaViDa-R1 is a multimodal reasoning diffusion language model that unifies supervised fine-tuning and multi-task reinforcement learning with novel training techniques for enhanced performance across visual reasoning and generation tasks.  					AI-generated summary 				 Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing, tree search, and complementary likelihood estimation, to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks, including visual math reasoning, reason-intensive grounding, and image editing."

[17.02.2026 04:15] Response: ```python
['DIFFUSION', 'REASONING', 'OPTIMIZATION']
```
[17.02.2026 04:15] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"LaViDa-R1 is a new type of language model that combines different ways of learning to improve how machines understand and generate information from both text and images. It uses a unique training approach that merges supervised fine-tuning with multi-task reinforcement learning, allowing it to tackle various tasks at once. This model introduces innovative techniques like answer-forcing and tree search to boost its performance and adaptability. Experiments show that LaViDa-R1 excels in complex tasks such as visual reasoning and image editing, making it a powerful tool for multimodal applications.","title":"Unifying Multimodal Reasoning with LaViDa-R1"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LaViDa-R1 is a new type of language model that combines different ways of learning to improve how machines understand and generate information from both text and images. It uses a unique training approach that merges supervised fine-tuning with multi-task reinforcement learning, allowing it to tackle various tasks at once. This model introduces innovative techniques like answer-forcing and tree search to boost its performance and adaptability. Experiments show that LaViDa-R1 excels in complex tasks such as visual reasoning and image editing, making it a powerful tool for multimodal applications.', title='Unifying Multimodal Reasoning with LaViDa-R1'))
[17.02.2026 04:15] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"LaViDa-R1是一种多模态推理扩散语言模型，结合了监督微调和多任务强化学习的新训练技术，以提高视觉推理和生成任务的性能。与现有的通过特定任务强化学习构建推理扩散语言模型的方法不同，LaViDa-R1以统一的方式整合了多种多模态理解和生成任务。该模型采用了一种新颖的统一后训练框架，能够无缝结合监督微调和多任务强化学习。通过多项实验，LaViDa-R1在视觉数学推理、推理密集的基础和图像编辑等多种多模态任务上表现出色。","title":"LaViDa-R1：多模态推理的统一解决方案"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LaViDa-R1是一种多模态推理扩散语言模型，结合了监督微调和多任务强化学习的新训练技术，以提高视觉推理和生成任务的性能。与现有的通过特定任务强化学习构建推理扩散语言模型的方法不同，LaViDa-R1以统一的方式整合了多种多模态理解和生成任务。该模型采用了一种新颖的统一后训练框架，能够无缝结合监督微调和多任务强化学习。通过多项实验，LaViDa-R1在视觉数学推理、推理密集的基础和图像编辑等多种多模态任务上表现出色。', title='LaViDa-R1：多模态推理的统一解决方案'))
[17.02.2026 04:15] Querying the API.
[17.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A new benchmark called BrowseComp-V3 challenges multimodal large language models with complex, multi-hop reasoning tasks requiring deep search across text and visual modalities, revealing significant gaps in current capabilities.  					AI-generated summary 				 Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-V^3, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings.
[17.02.2026 04:15] Response: ```json
{
  "desc": "В работе представлен новый бенчмарк BrowseComp-V3 для оценки мультимодальных больших языковых моделей на задачах сложного многошагового рассуждения с поиском информации в текстовых и визуальных модальностях. Авторы показывают, что современные модели достигают только 36% точности, выявляя критические пробелы в интеграции информации из разных источников и в восприятии визуальных деталей. Кроме того, предложена архитектура OmniSeeker — унифицированный агент для мультимодального веб-поиска, интегрирующий инструменты поиска и компьютерного зрения. Исследование подчеркивает значительный разрыв между возможностями современных моделей и требованиями для надежного глубокого поиска в реальных сценариях.",
  "emoji": "🔍",
  "title": "Преодоление границ мультимодального рассуждения в глубоком веб-поиске"
}
```
[17.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new benchmark called BrowseComp-V3 challenges multimodal large language models with complex, multi-hop reasoning tasks requiring deep search across text and visual modalities, revealing significant gaps in current capabilities.  					AI-generated summary 				 Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-V^3, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings."

[17.02.2026 04:15] Response: ```python
["BENCHMARK", "MULTIMODAL", "AGENTS"]
```
[17.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new benchmark called BrowseComp-V3 challenges multimodal large language models with complex, multi-hop reasoning tasks requiring deep search across text and visual modalities, revealing significant gaps in current capabilities.  					AI-generated summary 				 Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-V^3, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings."

[17.02.2026 04:15] Response: ```python
['REASONING', 'BENCHMARK']
```

Wait, let me reconsider. Looking at the available topics list, 'BENCHMARK' is not included. Let me classify based only on the provided topics:

```python
['REASONING']
```

The paper primarily focuses on multi-hop reasoning tasks and reasoning capabilities of multimodal large language models in complex search scenarios. While it introduces a benchmark, "BENCHMARK" is not in the provided topics list, so it should not be included.
[17.02.2026 04:15] Error. Failed to parse JSON from LLM. ["REASONING", "BENCHMARK"]


Wait, let me reconsider. Looking at the available topics list, "BENCHMARK" is not included. Let me classify based only on the provided topics:


["REASONING"]


The paper primarily focuses on multi-hop reasoning tasks and reasoning capabilities of multimodal large language models in complex search scenarios. While it introduces a benchmark, "BENCHMARK" is not in the provided topics list, so it should not be included.
[17.02.2026 04:15] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"The paper introduces BrowseComp-V3, a new benchmark designed to test multimodal large language models (MLLMs) on complex reasoning tasks that require deep searching across both text and visual information. It highlights the limitations of existing benchmarks in evaluating the capabilities of MLLMs, particularly in terms of task complexity and evidence accessibility. BrowseComp-V3 consists of 300 challenging questions that necessitate multi-hop reasoning across different modalities, ensuring that all evidence is publicly searchable for fairness. The study reveals that even the best current models only achieve 36% accuracy, indicating significant gaps in their ability to integrate multimodal information effectively.","title":"Bridging the Gap in Multimodal Reasoning with BrowseComp-V3"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces BrowseComp-V3, a new benchmark designed to test multimodal large language models (MLLMs) on complex reasoning tasks that require deep searching across both text and visual information. It highlights the limitations of existing benchmarks in evaluating the capabilities of MLLMs, particularly in terms of task complexity and evidence accessibility. BrowseComp-V3 consists of 300 challenging questions that necessitate multi-hop reasoning across different modalities, ensuring that all evidence is publicly searchable for fairness. The study reveals that even the best current models only achieve 36% accuracy, indicating significant gaps in their ability to integrate multimodal information effectively.', title='Bridging the Gap in Multimodal Reasoning with BrowseComp-V3'))
[17.02.2026 04:15] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"本文介绍了一个新的基准测试BrowseComp-V3，旨在挑战多模态大型语言模型（MLLMs）在复杂的多跳推理任务中的能力。这些任务需要在文本和视觉模态中进行深入搜索，揭示了当前模型能力的显著差距。新基准包含300个精心策划的问题，强调深层次的多级跨模态推理，并要求所有证据必须是公开可搜索的，以确保公平性和可重复性。此外，研究还提出了OmniSeeker框架，整合了多种网络搜索和视觉感知工具，以评估模型在真实世界中的表现。","title":"挑战多模态推理的新基准测试"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了一个新的基准测试BrowseComp-V3，旨在挑战多模态大型语言模型（MLLMs）在复杂的多跳推理任务中的能力。这些任务需要在文本和视觉模态中进行深入搜索，揭示了当前模型能力的显著差距。新基准包含300个精心策划的问题，强调深层次的多级跨模态推理，并要求所有证据必须是公开可搜索的，以确保公平性和可重复性。此外，研究还提出了OmniSeeker框架，整合了多种网络搜索和视觉感知工具，以评估模型在真实世界中的表现。', title='挑战多模态推理的新基准测试'))
[17.02.2026 04:15] Querying the API.
[17.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

FireRed-Image-Edit uses a diffusion transformer with optimized data curation and training methods to achieve state-of-the-art performance in instruction-based image editing, supported by a comprehensive benchmark and novel techniques for data efficiency and optimization stability.  					AI-generated summary 				 We present FireRed-Image-Edit, a diffusion transformer for instruction-based image editing that achieves state-of-the-art performance through systematic optimization of data curation, training methodology, and evaluation design. We construct a 1.6B-sample training corpus, comprising 900M text-to-image and 700M image editing pairs from diverse sources. After rigorous cleaning, stratification, auto-labeling, and two-stage filtering, we retain over 100M high-quality samples balanced between generation and editing, ensuring strong semantic coverage and instruction alignment. Our multi-stage training pipeline progressively builds editing capability via pre-training, supervised fine-tuning, and reinforcement learning. To improve data efficiency, we introduce a Multi-Condition Aware Bucket Sampler for variable-resolution batching and Stochastic Instruction Alignment with dynamic prompt re-indexing. To stabilize optimization and enhance controllability, we propose Asymmetric Gradient Optimization for DPO, DiffusionNFT with layout-aware OCR rewards for text editing, and a differentiable Consistency Loss for identity preservation. We further establish REDEdit-Bench, a comprehensive benchmark spanning 15 editing categories, including newly introduced beautification and low-level enhancement tasks. Extensive experiments on REDEdit-Bench and public benchmarks (ImgEdit and GEdit) demonstrate competitive or superior performance against both open-source and proprietary systems. We release code, models, and the benchmark suite to support future research.
[17.02.2026 04:15] Response: ```json
{
  "desc": "Представлена FireRed-Image-Edit — диффузионный трансформер для редактирования изображений по текстовым инструкциям, достигающий лучших результатов через оптимизацию данных и методов обучения. Авторы создали корпус из 1.6 млрд образцов с тщательной очисткой и фильтрацией, оставив более 100 млн высококачественных примеров для обучения моделей редактирования и генерации. Предложены новые техники для повышения эффективности: многоусловный сэмплер для переменного разрешения, стохастическое выравнивание инструкций и асимметричная оптимизация градиентов. Разработана комплексная бенчмарк-система REDEdit-Bench с 15 категориями задач редактирования для оценки качества работы моделей.",
  "emoji": "🎨",
  "title": "Масштабное редактирование изображений через диффузионные трансформеры"
}
```
[17.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FireRed-Image-Edit uses a diffusion transformer with optimized data curation and training methods to achieve state-of-the-art performance in instruction-based image editing, supported by a comprehensive benchmark and novel techniques for data efficiency and optimization stability.  					AI-generated summary 				 We present FireRed-Image-Edit, a diffusion transformer for instruction-based image editing that achieves state-of-the-art performance through systematic optimization of data curation, training methodology, and evaluation design. We construct a 1.6B-sample training corpus, comprising 900M text-to-image and 700M image editing pairs from diverse sources. After rigorous cleaning, stratification, auto-labeling, and two-stage filtering, we retain over 100M high-quality samples balanced between generation and editing, ensuring strong semantic coverage and instruction alignment. Our multi-stage training pipeline progressively builds editing capability via pre-training, supervised fine-tuning, and reinforcement learning. To improve data efficiency, we introduce a Multi-Condition Aware Bucket Sampler for variable-resolution batching and Stochastic Instruction Alignment with dynamic prompt re-indexing. To stabilize optimization and enhance controllability, we propose Asymmetric Gradient Optimization for DPO, DiffusionNFT with layout-aware OCR rewards for text editing, and a differentiable Consistency Loss for identity preservation. We further establish REDEdit-Bench, a comprehensive benchmark spanning 15 editing categories, including newly introduced beautification and low-level enhancement tasks. Extensive experiments on REDEdit-Bench and public benchmarks (ImgEdit and GEdit) demonstrate competitive or superior performance against both open-source and proprietary systems. We release code, models, and the benchmark suite to support future research."

[17.02.2026 04:15] Response: ```python
["CV", "DATASET", "BENCHMARK", "DATA", "TRAINING", "RLHF", "ARCHITECTURE"]
```
[17.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FireRed-Image-Edit uses a diffusion transformer with optimized data curation and training methods to achieve state-of-the-art performance in instruction-based image editing, supported by a comprehensive benchmark and novel techniques for data efficiency and optimization stability.  					AI-generated summary 				 We present FireRed-Image-Edit, a diffusion transformer for instruction-based image editing that achieves state-of-the-art performance through systematic optimization of data curation, training methodology, and evaluation design. We construct a 1.6B-sample training corpus, comprising 900M text-to-image and 700M image editing pairs from diverse sources. After rigorous cleaning, stratification, auto-labeling, and two-stage filtering, we retain over 100M high-quality samples balanced between generation and editing, ensuring strong semantic coverage and instruction alignment. Our multi-stage training pipeline progressively builds editing capability via pre-training, supervised fine-tuning, and reinforcement learning. To improve data efficiency, we introduce a Multi-Condition Aware Bucket Sampler for variable-resolution batching and Stochastic Instruction Alignment with dynamic prompt re-indexing. To stabilize optimization and enhance controllability, we propose Asymmetric Gradient Optimization for DPO, DiffusionNFT with layout-aware OCR rewards for text editing, and a differentiable Consistency Loss for identity preservation. We further establish REDEdit-Bench, a comprehensive benchmark spanning 15 editing categories, including newly introduced beautification and low-level enhancement tasks. Extensive experiments on REDEdit-Bench and public benchmarks (ImgEdit and GEdit) demonstrate competitive or superior performance against both open-source and proprietary systems. We release code, models, and the benchmark suite to support future research."

[17.02.2026 04:15] Response: ```python
['DIFFUSION', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[17.02.2026 04:15] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"FireRed-Image-Edit is a cutting-edge diffusion transformer designed for instruction-based image editing, achieving top performance through advanced data curation and training techniques. It utilizes a massive training dataset of 1.6 billion samples, ensuring high-quality and diverse image editing capabilities. The model employs a multi-stage training approach, incorporating pre-training, supervised fine-tuning, and reinforcement learning to enhance its editing skills. Additionally, it introduces innovative methods for data efficiency and optimization stability, validated through extensive benchmarking against existing systems.","title":"Revolutionizing Image Editing with FireRed-Image-Edit"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FireRed-Image-Edit is a cutting-edge diffusion transformer designed for instruction-based image editing, achieving top performance through advanced data curation and training techniques. It utilizes a massive training dataset of 1.6 billion samples, ensuring high-quality and diverse image editing capabilities. The model employs a multi-stage training approach, incorporating pre-training, supervised fine-tuning, and reinforcement learning to enhance its editing skills. Additionally, it introduces innovative methods for data efficiency and optimization stability, validated through extensive benchmarking against existing systems.', title='Revolutionizing Image Editing with FireRed-Image-Edit'))
[17.02.2026 04:15] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"FireRed-Image-Edit 是一种基于扩散变换器的图像编辑模型，专注于指令驱动的图像编辑任务。该模型通过优化数据整理和训练方法，构建了一个包含16亿样本的训练数据集，确保了高质量样本的平衡和语义覆盖。为了提高数据效率，模型引入了多条件感知的桶采样器和动态提示重索引技术。通过多阶段训练流程和创新的优化技术，FireRed-Image-Edit 在多个基准测试中表现出色，超越了许多开源和专有系统。","title":"基于指令的图像编辑新突破"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FireRed-Image-Edit 是一种基于扩散变换器的图像编辑模型，专注于指令驱动的图像编辑任务。该模型通过优化数据整理和训练方法，构建了一个包含16亿样本的训练数据集，确保了高质量样本的平衡和语义覆盖。为了提高数据效率，模型引入了多条件感知的桶采样器和动态提示重索引技术。通过多阶段训练流程和创新的优化技术，FireRed-Image-Edit 在多个基准测试中表现出色，超越了许多开源和专有系统。', title='基于指令的图像编辑新突破'))
[17.02.2026 04:15] Querying the API.
[17.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

AIDev is a large-scale dataset of agent-authored pull requests from real-world GitHub repositories that captures AI coding agent usage in practical software development scenarios.  					AI-generated summary 				 AI coding agents are rapidly transforming software engineering by performing tasks such as feature development, debugging, and testing. Despite their growing impact, the research community lacks a comprehensive dataset capturing how these agents are used in real-world projects. To address this gap, we introduce AIDev, a large-scale dataset focused on agent-authored pull requests (Agentic-PRs) in real-world GitHub repositories. AIDev aggregates 932,791 Agentic-PRs produced by five agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These PRs span 116,211 repositories and involve 72,189 developers. In addition, AIDev includes a curated subset of 33,596 Agentic-PRs from 2,807 repositories with over 100 stars, providing further information such as comments, reviews, commits, and related issues. This dataset offers a foundation for future research on AI adoption, developer productivity, and human-AI collaboration in the new era of software engineering.   > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Agentic Software Engineering, Agentic Engineering
[17.02.2026 04:15] Response: ```json
{
  "desc": "Статья представляет AIDev — большой набор данных из более чем 932 тысячи pull requests, созданных AI-агентами в реальных проектах на GitHub. Набор включает данные от пяти различных агентов (Codex, Devin, GitHub Copilot, Cursor и Claude Code) из более чем 116 тысяч репозиториев. Исследователи собрали информацию о коммитах, комментариях, ревью и связанных issues для анализа использования AI в практической разработке. Этот ресурс позволяет изучать внедрение ИИ, продуктивность разработчиков и взаимодействие человека с машиной в современной инженерии программного обеспечения.",
  "emoji": "🤖",
  "title": "Масштабный датасет реальной практики применения AI-агентов в разработке кода"
}
```
[17.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AIDev is a large-scale dataset of agent-authored pull requests from real-world GitHub repositories that captures AI coding agent usage in practical software development scenarios.  					AI-generated summary 				 AI coding agents are rapidly transforming software engineering by performing tasks such as feature development, debugging, and testing. Despite their growing impact, the research community lacks a comprehensive dataset capturing how these agents are used in real-world projects. To address this gap, we introduce AIDev, a large-scale dataset focused on agent-authored pull requests (Agentic-PRs) in real-world GitHub repositories. AIDev aggregates 932,791 Agentic-PRs produced by five agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These PRs span 116,211 repositories and involve 72,189 developers. In addition, AIDev includes a curated subset of 33,596 Agentic-PRs from 2,807 repositories with over 100 stars, providing further information such as comments, reviews, commits, and related issues. This dataset offers a foundation for future research on AI adoption, developer productivity, and human-AI collaboration in the new era of software engineering.   > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Agentic Software Engineering, Agentic Engineering"

[17.02.2026 04:15] Response: ```python
['DATASET', 'AGENTS', 'PLP']
```
[17.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AIDev is a large-scale dataset of agent-authored pull requests from real-world GitHub repositories that captures AI coding agent usage in practical software development scenarios.  					AI-generated summary 				 AI coding agents are rapidly transforming software engineering by performing tasks such as feature development, debugging, and testing. Despite their growing impact, the research community lacks a comprehensive dataset capturing how these agents are used in real-world projects. To address this gap, we introduce AIDev, a large-scale dataset focused on agent-authored pull requests (Agentic-PRs) in real-world GitHub repositories. AIDev aggregates 932,791 Agentic-PRs produced by five agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These PRs span 116,211 repositories and involve 72,189 developers. In addition, AIDev includes a curated subset of 33,596 Agentic-PRs from 2,807 repositories with over 100 stars, providing further information such as comments, reviews, commits, and related issues. This dataset offers a foundation for future research on AI adoption, developer productivity, and human-AI collaboration in the new era of software engineering.   > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Agentic Software Engineering, Agentic Engineering"

[17.02.2026 04:15] Response: ```python
['OPEN_SOURCE', 'SCIENCE']
```
[17.02.2026 04:15] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"AIDev is a comprehensive dataset that focuses on pull requests authored by AI coding agents in real-world software development. It includes 932,791 Agentic-PRs from five different AI agents, showcasing their contributions across 116,211 GitHub repositories. This dataset not only highlights the usage of AI in coding tasks like feature development and debugging but also provides insights into developer interactions with these agents. AIDev serves as a valuable resource for studying AI\'s impact on software engineering, developer productivity, and collaboration between humans and AI.","title":"Unlocking AI\'s Role in Software Development with AIDev"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="AIDev is a comprehensive dataset that focuses on pull requests authored by AI coding agents in real-world software development. It includes 932,791 Agentic-PRs from five different AI agents, showcasing their contributions across 116,211 GitHub repositories. This dataset not only highlights the usage of AI in coding tasks like feature development and debugging but also provides insights into developer interactions with these agents. AIDev serves as a valuable resource for studying AI's impact on software engineering, developer productivity, and collaboration between humans and AI.", title="Unlocking AI's Role in Software Development with AIDev"))
[17.02.2026 04:16] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"AIDev是一个大规模的数据集，专注于真实世界GitHub仓库中由AI编写的拉取请求（Agentic-PRs）。该数据集汇总了932,791个由五个不同的AI编码代理生成的拉取请求，涵盖了116,211个仓库和72,189名开发者。AIDev还包含一个经过整理的子集，提供了更详细的信息，如评论、审查、提交和相关问题。这一数据集为未来关于AI应用、开发者生产力和人机协作的研究提供了基础。","title":"AIDev：AI编码代理的真实世界数据集"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='AIDev是一个大规模的数据集，专注于真实世界GitHub仓库中由AI编写的拉取请求（Agentic-PRs）。该数据集汇总了932,791个由五个不同的AI编码代理生成的拉取请求，涵盖了116,211个仓库和72,189名开发者。AIDev还包含一个经过整理的子集，提供了更详细的信息，如评论、审查、提交和相关问题。这一数据集为未来关于AI应用、开发者生产力和人机协作的研究提供了基础。', title='AIDev：AI编码代理的真实世界数据集'))
[17.02.2026 04:16] Querying the API.
[17.02.2026 04:16] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Targeted instruction selection for LLM fine-tuning can be improved by systematically analyzing data representation and selection algorithms, with gradient-based representations and greedy round-robin selection performing best at low budgets.  					AI-generated summary 				 Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.
[17.02.2026 04:16] Response: ```json
{
  "desc": "Работа посвящена оптимизации выбора инструкций для дообучения больших языковых моделей из большого набора кандидатов. Авторы систематически анализируют два ключевых компонента: представление данных и алгоритмы отбора, предлагая единую структуру для контролируемого сравнения. Они показывают, что градиентные представления данных обеспечивают наиболее надежное предсказание качества на разных датасетах и моделях. Наилучшие результаты при малых бюджетах достигаются комбинацией градиентных представлений с жадным алгоритмом round-robin отбора.",
  "emoji": "🎯",
  "title": "Целевой отбор инструкций для эффективного дообучения больших языковых моделей"
}
```
[17.02.2026 04:16] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Targeted instruction selection for LLM fine-tuning can be improved by systematically analyzing data representation and selection algorithms, with gradient-based representations and greedy round-robin selection performing best at low budgets.  					AI-generated summary 				 Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection."

[17.02.2026 04:16] Response: ```python
["DATA", "TRAINING"]
```
[17.02.2026 04:16] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Targeted instruction selection for LLM fine-tuning can be improved by systematically analyzing data representation and selection algorithms, with gradient-based representations and greedy round-robin selection performing best at low budgets.  					AI-generated summary 				 Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection."

[17.02.2026 04:16] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```

**Justification:**

- **OPTIMIZATION**: The paper focuses on improving instruction fine-tuning of LLMs through systematic analysis of data representation and selection algorithms, which is fundamentally about optimizing the training process and selection methods.

- **OPEN_SOURCE**: The paper explicitly states "The code is available at https://github.com/dcml-lab/targeted-instruction-selection," indicating the authors are releasing code/framework to the public.
[17.02.2026 04:16] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "OPEN_SOURCE"]


**Justification:**

- **OPTIMIZATION**: The paper focuses on improving instruction fine-tuning of LLMs through systematic analysis of data representation and selection algorithms, which is fundamentally about optimizing the training process and selection methods.

- **OPEN_SOURCE**: The paper explicitly states "The code is available at https://github.com/dcml-lab/targeted-instruction-selection," indicating the authors are releasing code/framework to the public.
[17.02.2026 04:16] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper focuses on improving the process of selecting training instructions for fine-tuning large language models (LLMs). It systematically analyzes how data representation and selection algorithms impact the effectiveness of instruction selection. The authors find that using gradient-based representations along with a greedy round-robin selection method yields the best results, especially when working with limited resources. Their work clarifies existing methods and provides a framework for better data selection practices in LLM fine-tuning.","title":"Optimizing Instruction Selection for Effective LLM Fine-Tuning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper focuses on improving the process of selecting training instructions for fine-tuning large language models (LLMs). It systematically analyzes how data representation and selection algorithms impact the effectiveness of instruction selection. The authors find that using gradient-based representations along with a greedy round-robin selection method yields the best results, especially when working with limited resources. Their work clarifies existing methods and provides a framework for better data selection practices in LLM fine-tuning.', title='Optimizing Instruction Selection for Effective LLM Fine-Tuning'))
[17.02.2026 04:16] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"本论文探讨了大语言模型（LLM）指令微调中的目标指令选择问题。我们系统分析了数据表示和选择算法，发现基于梯度的数据表示能够有效预测不同数据集和模型的性能。研究表明，在低预算情况下，结合贪婪轮询选择算法的梯度表示方法表现最佳，但在高预算时效果减弱。我们的工作为LLM微调中的数据选择提供了清晰的指导和理论基础。","title":"优化LLM微调的指令选择策略"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文探讨了大语言模型（LLM）指令微调中的目标指令选择问题。我们系统分析了数据表示和选择算法，发现基于梯度的数据表示能够有效预测不同数据集和模型的性能。研究表明，在低预算情况下，结合贪婪轮询选择算法的梯度表示方法表现最佳，但在高预算时效果减弱。我们的工作为LLM微调中的数据选择提供了清晰的指导和理论基础。', title='优化LLM微调的指令选择策略'))
[17.02.2026 04:16] Renaming data file.
[17.02.2026 04:16] Renaming previous data. hf_papers.json to ./d/2026-02-17.json
[17.02.2026 04:16] Saving new data file.
[17.02.2026 04:16] Generating page.
[17.02.2026 04:16] Renaming previous page.
[17.02.2026 04:16] Renaming previous data. index.html to ./d/2026-02-17.html
[17.02.2026 04:16] Writing result.
[17.02.2026 04:16] Renaming log file.
[17.02.2026 04:16] Renaming previous data. log.txt to ./logs/2026-02-17_last_log.txt
