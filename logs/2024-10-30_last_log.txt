[29.10.2024 22:11] [Experimental] Generating an image for paper Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation.
[29.10.2024 22:11] [Experimental] Image for paper Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation already exists.
[29.10.2024 22:11] [Experimental] Generating an image for paper GPT-4o System Card.
[29.10.2024 22:11] [Experimental] Image for paper GPT-4o System Card already exists.
[29.10.2024 22:11] [Experimental] Generating an image for paper AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant.
[29.10.2024 22:11] [Experimental] Image for paper AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant already exists.
[29.10.2024 22:11] [Experimental] Generating an image for paper Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction.
[29.10.2024 22:11] [Experimental] Image for paper Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction already exists.
[29.10.2024 22:11] [Experimental] Generating an image for paper MarDini: Masked Autoregressive Diffusion for Video Generation at Scale.
[29.10.2024 22:11] [Experimental] Image for paper MarDini: Masked Autoregressive Diffusion for Video Generation at Scale already exists.
[29.10.2024 22:11] [Experimental] Generating an image for paper LongReward: Improving Long-context Large Language Models with AI Feedback.
[29.10.2024 22:11] [Experimental] Image for paper LongReward: Improving Long-context Large Language Models with AI Feedback already exists.
[29.10.2024 22:11] [Experimental] Generating an image for paper A Survey of Small Language Models.
[29.10.2024 22:11] [Experimental] Image for paper A Survey of Small Language Models already exists.
[29.10.2024 22:11] [Experimental] Generating an image for paper GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation.
[29.10.2024 22:11] [Experimental] Image for paper GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation already exists.
[29.10.2024 22:11] [Experimental] Generating an image for paper DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation.
[29.10.2024 22:11] [Experimental] Image for paper DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation already exists.
[29.10.2024 22:11] [Experimental] Generating an image for paper COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training.
[29.10.2024 22:11] [Experimental] Image for paper COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training already exists.
[30.10.2024 00:58] Read previous papers.
[30.10.2024 00:58] Get feed.
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18565
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.21276
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18603
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.21169
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18666
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20280
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.21252
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20011
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20474
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19313
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20290
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.21220
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.21264
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19100
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18481
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20220
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2406.10615
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20672
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20636
[30.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.01968
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 0. We introduce Bielik 7B v0.1, a 7-billion-parameter generative text model for Polish language processing. Trained on curated Polish corpora, this model addresses key challenges in language model development through innovative techniques. These include Weighted Instruction Cross-Entropy Loss, which ba...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 1. GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural netw...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 2. Digital agents capable of automating complex computer tasks have attracted considerable attention due to their immense potential to enhance human-computer interaction. However, existing agent methods exhibit deficiencies in their generalization and specialization capabilities, especially in handling...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 3. Document parsing is essential for converting unstructured and semi-structured documents-such as contracts, academic papers, and invoices-into structured, machine-readable data. Document parsing extract reliable structured data from unstructured inputs, providing huge convenience for numerous applica...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 4. Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets. To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (Di...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 5. We introduce MarDini, a new family of video diffusion models that integrate the advantages of masked auto-regression (MAR) into a unified diffusion model (DM) framework. Here, MAR handles temporal planning, while DM focuses on spatial generation in an asymmetric network design: i) a MAR-based planni...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 6. Though significant advancements have been achieved in developing long-context large language models (LLMs), the compromised quality of LLM-synthesized data for supervised fine-tuning (SFT) often affects the long-context performance of SFT models and leads to inherent limitations. In principle, reinf...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 7. Small Language Models (SLMs) have become increasingly important due to their efficiency and performance to perform various language tasks with minimal computational resources, making them ideal for various settings including on-device, mobile, edge devices, among many others. In this article, we pre...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 8. We introduce a novel training-free spatial grounding technique for text-to-image generation using Diffusion Transformers (DiT). Spatial grounding with bounding boxes has gained attention for its simplicity and versatility, allowing for enhanced user control in image generation. However, prior traini...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 9. FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 10. The safe and effective deployment of Large Language Models (LLMs) involves a critical step called alignment, which ensures that the model's responses are in accordance with human preferences. Prevalent alignment techniques, such as DPO, PPO and their variants, align LLMs by changing the pre-trained ...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 11. Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This challenge is particularly pronounced for large vision-lang...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 12. We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization s...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 13. Videos are often used to learn or extract the necessary information to complete tasks in ways different than what text and static imagery alone can provide. However, many existing agent benchmarks neglect long-context video understanding, instead focusing on text or static image inputs. To bridge th...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 14. Efficiently deriving structured workflows from unannotated dialogs remains an underexplored and formidable challenge in computational linguistics. Automating this process could significantly accelerate the manual design of workflows in new domains and enable the grounding of large language models in...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 15. Neural Fields have emerged as a transformative approach for 3D scene representation in computer vision and robotics, enabling accurate inference of geometry, 3D semantics, and dynamics from posed 2D data. Leveraging differentiable rendering, Neural Fields encompass both continuous implicit and expli...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 16. Given the high cost of collecting robotic data in the real world, sample efficiency is a consistently compelling pursuit in robotics. In this paper, we introduce SGRv2, an imitation learning framework that enhances sample efficiency through improved visual and action representations. Central to the ...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 17. Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit "layer tying" as form of parameter sharing in Transformers, and introduce novel m...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 18. This research tests the role of Large Language Models (LLMs) as formal second opinion tools in professional decision-making, particularly focusing on complex medical cases where even experienced physicians seek peer consultation. The work analyzed 183 challenging medical cases from Medscape over a 2...
[30.10.2024 00:58] ********************************************************************************
[30.10.2024 00:58] Abstract 19. Imitation learning from human motion capture (MoCap) data provides a promising way to train humanoid robots. However, due to differences in morphology, such as varying degrees of joint freedom and force limits, exact replication of human behaviors may not be feasible for humanoid robots. Consequentl...
[30.10.2024 00:58] Read previous papers.
[30.10.2024 00:58] Generating reviews via LLM API.
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark", "#multilingual", "#alignment", "#reasoning"], "emoji": "üáµüá±", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ–ª—å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞: –º–æ—â–Ω–∞—è 7B-–º–æ–¥–µ–ª—å Bielik", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Bielik 7B v0.1 - —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–ª—å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–æ–¥–µ–ª—å 
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#multimodal", "#alignment", "#medicine", "#interpretability", "#ethics"], "emoji": "ü§ñ", "ru": {"title": "GPT-4o: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ò–ò", "desc": "GPT-4o - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω–∞—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –∞—É–¥–∏–æ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≤–∏–¥–µ–æ. –û–Ω–∞ –æ–±—É—á–µ–Ω–∞ —Å–∫–≤–æ–∑–Ω
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "AgentStore: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "AgentStore - —ç—Ç–æ –Ω–æ–≤–∞—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∑–∞–¥–∞—á, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –º–∞–≥–∞–∑–∏–Ω–∞ 
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#dataset", "#data", "#survey", "#multimodal"], "emoji": "üìÑ", "ru": {"title": "–ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: –æ—Ç –º–æ–¥—É–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏, –æ—Ç –º–æ–¥—É–ª—å–Ω
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#dataset", "#data", "#cv", "#diffusion"], "emoji": "üñºÔ∏è", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –æ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–æ —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç GenIR
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#video", "#diffusion"], "emoji": "üé¨", "ru": {"title": "MarDini: –ì–∏–±–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π –∏ –¥–∏—Ñ—Ñ—É–∑–∏–µ–π", "desc": "MarDini - —ç—Ç–æ –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ, –æ–±—ä–µ–¥–∏–Ω—è—é—â–µ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (MAR) –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (DM).
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#rl", "#rlhf", "#long_context"], "emoji": "üìè", "ru": {"title": "LongReward: –£–ª—É—á—à–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ LongReward –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–æ—Ç–æ–≤—É—é –±–æ–ª—å—à—É—é —è
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#survey", "#architecture", "#inference", "#benchmark", "#edge_computing"], "emoji": "üß†", "ru": {"title": "–ú–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π –æ–±–∑–æ—Ä –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (SLM), –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –≤—Å–µ –±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#benchmark"], "emoji": "üñºÔ∏è", "ru": {"title": "–¢–æ—á–Ω–∞—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –ø—Ä–∏–≤—è–∑–∫–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Diffusion Transf
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#optimization"], "emoji": "üöÄ", "ru": {"title": "COAT: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ –ø–∞–º—è—Ç–∏", "desc": "COAT - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ FP8, –∫–æ—Ç–æ—Ä–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –≤
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#rlhf", "#alignment", "#inference"], "emoji": "üöÄ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ LLM –±–µ–∑ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Speculative Rejection. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#rag", "#agents", "#cv", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ó—Ä–µ–Ω–∏–µ –ò–ò: –æ—Ç –Ω–µ–∑–Ω–∞–Ω–∏—è –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Vision Search Assistant - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –û–Ω –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#video", "#multimodal", "#benchmark", "#architecture"], "emoji": "üé¨", "ru": {"title": "LARP: –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "LARP - —ç—Ç–æ –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤–∏–¥–µ–æ, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –≤ 
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#video", "#agents"], "emoji": "üé•", "ru": {"title": "VideoWebArena: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ VideoWebArena (VideoWA) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤ –ø
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark", "#multimodal"], "emoji": "üó∫Ô∏è", "ru": {"title": "D2F: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∫–∞—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏–∞–ª–æ–≥–æ–≤ –≤ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏–∑ –Ω–µ–∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ —Å –ø–æ–º–æ—â—å—é embeddin
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#3d", "#robots", "#survey"], "emoji": "ü§ñ", "ru": {"title": "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ –ø–æ–ª—è: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-–≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏ —Ä–æ–±–æ—Ç–æ–≤", "desc": "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ –ø–æ–ª—è - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é 3D-—Å—Ü–µ–Ω –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏ –∏ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ. –û–Ω–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç —Ç–æ—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –≥–µ–æ–º–µ—Ç—Ä–∏—é, 3D-—Å–µ–º–∞–Ω—Ç–∏–∫—É –∏ –¥–∏–Ω–∞–º–∏–∫—É
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#rl", "#agents", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–õ–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π - –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Ä–æ–±–æ—Ç–æ–≤", "desc": "SGRv2 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∏–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏, –ø–æ–≤—ã—à–∞—é—â–∏–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö. –û–Ω –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#inference", "#architecture", "#optimization"], "emoji": "üîÅ", "ru": {"title": "–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã: –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç—å –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –Ω–æ–≤–æ–º—É –º–µ—Ç–æ–¥—É —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –ø–æ–º–æ—â—å—é —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã—Ö
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#medicine", "#benchmark", "#alignment"], "emoji": "ü©∫", "ru": {"title": "LLM –∫–∞–∫ –≤—Ç–æ—Ä–æ–µ –º–Ω–µ–Ω–∏–µ: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫—É—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Ç–æ—Ä–æ–≥
[30.10.2024 00:58] Using data from previous issue: {"categories": ["#agents", "#robotics", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∏–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π —Ä–æ–±–æ—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞—Ö–≤–∞—Ç–∞ –¥–≤–∏–∂–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥
[30.10.2024 00:58] Loading Chinese text from previous data.
[30.10.2024 00:58] Renaming data file.
[30.10.2024 00:58] Renaming previous data. hf_papers.json to ./d/2024-10-30.json
[30.10.2024 00:58] Saving new data file.
[30.10.2024 00:58] Generating page.
[30.10.2024 00:58] Renaming previous page.
[30.10.2024 00:58] Renaming previous data. index.html to ./d/2024-10-30.html
[30.10.2024 00:58] [Experimental] Generating Chinese page for reading.
[30.10.2024 00:58] Chinese vocab [{'word': '‰ªãÁªç', 'pinyin': 'ji√®sh√†o', 'trans': 'introduce'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generate'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'}, {'word': 'ÂàõÊñ∞', 'pinyin': 'chu√†ngxƒ´n', 'trans': 'innovative'}, {'word': 'ÊäÄÊúØ', 'pinyin': 'j√¨sh√π', 'trans': 'technology'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πnli√†n', 'trans': 'train'}, {'word': 'ÊùÉÈáç', 'pinyin': 'qu√°nzh√≤ng', 'trans': 'weight'}, {'word': 'Êåá‰ª§', 'pinyin': 'zh«êl√¨ng', 'trans': 'instruction'}, {'word': '‰∫§ÂèâÁÜµ', 'pinyin': 'jiƒÅochƒÅ shƒÅng', 'trans': 'cross-entropy'}, {'word': 'ÊçüÂ§±', 'pinyin': 's«înshƒ´', 'trans': 'loss'}, {'word': 'Ëá™ÈÄÇÂ∫î', 'pinyin': 'z√¨sh√¨y√¨ng', 'trans': 'adaptive'}, {'word': 'Â≠¶‰π†Áéá', 'pinyin': 'xu√©x√≠l«ú', 'trans': 'learning rate'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ngg≈´', 'trans': 'evaluate'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√≠ngn√©ng', 'trans': 'performance'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®nw√π', 'trans': 'task'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'}, {'word': 'ÊîπËøõ', 'pinyin': 'g«éij√¨n', 'trans': 'improvement'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'}, {'word': 'ËßíËâ≤', 'pinyin': 'ju√©s√®', 'trans': 'role'}, {'word': 'ÊâÆÊºî', 'pinyin': 'b√†ny«én', 'trans': 'play'}, {'word': 'È¢ÜÂüü', 'pinyin': 'l«êngy√π', 'trans': 'field'}, {'word': 'ÈáçÂ§ß', 'pinyin': 'zh√≤ngd√†', 'trans': 'major'}, {'word': 'ËøõÂ±ï', 'pinyin': 'j√¨nzh«én', 'trans': 'progress'}]
[30.10.2024 00:58] Renaming previous Chinese page.
[30.10.2024 00:58] Renaming previous data. zh.html to ./d/2024-10-29_zh_reading_task.html
[30.10.2024 00:58] Writing result.
[30.10.2024 00:58] Writing Chinese reading task.
[30.10.2024 00:58] Renaming log file.
[30.10.2024 00:58] Renaming previous data. log.txt to ./logs/2024-10-30_last_log.txt
