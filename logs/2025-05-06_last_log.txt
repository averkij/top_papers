[06.05.2025 12:23] Read previous papers.
[06.05.2025 12:23] Generating top page (month).
[06.05.2025 12:23] Writing top page (month).
[06.05.2025 13:26] Read previous papers.
[06.05.2025 13:26] Get feed.
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02707
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02387
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2504.20752
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02819
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02735
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02391
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02835
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02222
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.01658
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02156
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02094
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.01441
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02370
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.01043
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02471
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02625
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.01583
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02823
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02005
[06.05.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2505.01456
[06.05.2025 13:26] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.05.2025 13:26] No deleted papers detected.
[06.05.2025 13:26] Downloading and parsing papers (pdf, html). Total: 20.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.02707.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.02707.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.02707.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.02387.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.02387.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.02387.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2504.20752.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2504.20752.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2504.20752.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.02819.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.02819.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.02819.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.02735.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.02735.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.02735.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.02391.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.02391.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.02391.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.02835.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.02835.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.02835.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.02222.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.02222.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.02222.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.01658.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.01658.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.01658.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.02156.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.02156.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.02156.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.02094.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.02094.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.02094.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.01441.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.01441.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.01441.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.02370.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.02370.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.02370.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.01043.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.01043.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.01043.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.02471.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.02471.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.02471.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.02625.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.02625.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.02625.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.01583.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.01583.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.01583.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.02823.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.02823.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.02823.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.02005.
[06.05.2025 13:26] Downloading paper 2505.02005 from http://arxiv.org/pdf/2505.02005v1...
[06.05.2025 13:26] Extracting affiliations from text.
[06.05.2025 13:26] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 ] . [ 1 5 0 0 2 0 . 5 0 5 2 : r JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 1 Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields Zhenxing Mi, Ping Yin, Xue Xiao, and Dan Xu Member, IEEE AbstractRecent Neural Radiance Field (NeRF) methods on large-scale scenes have demonstrated promising results and underlined the importance of scene decomposition for scalable NeRFs. Although these methods achieved reasonable scalability, there are several critical problems remaining unexplored in the existing large-scale NeRF modeling methods, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within unified framework. Our framework is highly scalable NeRF that learns heterogeneous decomposition and heterogeneous Neural Radiance Fields efficiently for large-scale scenes in an end-to-end manner. In our framework, gating network learns to decomposes scenes into partitions and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts, by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. Our network architecture incorporates hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges. This enables effective learning of the heterogeneous representation of different decomposed scene parts within large-scale complex scenes. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets. Additionally, we also introduce new dataset with very large-sca"
[06.05.2025 13:26] Response: ```python
[]
```
[06.05.2025 13:26] Extracting affiliations from text.
[06.05.2025 13:26] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 ] . [ 1 5 0 0 2 0 . 5 0 5 2 : r JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 1 Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields Zhenxing Mi, Ping Yin, Xue Xiao, and Dan Xu Member, IEEE AbstractRecent Neural Radiance Field (NeRF) methods on large-scale scenes have demonstrated promising results and underlined the importance of scene decomposition for scalable NeRFs. Although these methods achieved reasonable scalability, there are several critical problems remaining unexplored in the existing large-scale NeRF modeling methods, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within unified framework. Our framework is highly scalable NeRF that learns heterogeneous decomposition and heterogeneous Neural Radiance Fields efficiently for large-scale scenes in an end-to-end manner. In our framework, gating network learns to decomposes scenes into partitions and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts, by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. Our network architecture incorporates hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges. This enables effective learning of the heterogeneous representation of different decomposed scene parts within large-scale complex scenes. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets. Additionally, we also introduce new dataset with very large-scale scenes (> 6.5km2) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency gains, with an 8x acceleration in training and 16x acceleration in rendering compared to the best-performing competitor Switch-NeRF. The codes and trained models will be released in https://github.com/MiZhenxing/Switch-NeRF. Index TermsNeural Radiance Fields; Mixture of Experts; Large-scale 3D reconstruction; Novel view synthesisNeural Radiance Fields (NeRF) [1] have shown the powerfulness for high-fidelity 3D scene modeling. Recent methods [2], [3], [4] have scaled NeRF to large-scale scenes with thousands of high-resolution images. The scalability of them essentially comes from the decomposition of the 3D scenes. Several sub-NeRF-networks handle different partitions of large scene. Mega-NeRF [2] and Block-NeRF [3] both incorporate hand-crafted decomposition rules for different scenes. They decompose 3D scenes by clustering based on 3D physical distances or street blocks. Despite the promising performances achieved by current large-scale NeRF methods, several critical challenges remain unexplored. Firstly, design universal decomposition rule is extremely challenging for different large-scale scenes. The real-world large-scale scenes tend to have very different and complex scene structures. Hand-crafted rules will inevitably brings adaptation issues for distinct scenarios. Moreover, the non-learnable nature of hand-crafted decomposition restricts the networks ability to optimize the scene decomposition and radiance fields together within an end-to-end framework. To handle this problem, we need learnable decomposition method to automatically learn different decomposition for different scenes and we can optimize the Zhenxing Mi and Dan Xu are with the Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR. E-mail: zmiaa@connect.ust.hk, danxu@cse.ust.hk Ping Yin and Xue Xiao are with Inspur Cloud Information Technology Co, Ltd. E-mail: yinping@inspur.com, xiaoxue@inspur.com decomposition together with the scene representation. Secondly, large-scale scene is typically irregular and complex, with different scene parts exhibiting diverse appearance and geometry distributions, which requires heterogeneous modeling capabilities to produce high-quality scene representation. However, current large-scale NeRF methods often employ identical networks, such as Multilayer Perceptrons (MLPs), for different parts of the scene, lacking explicit designs for heterogeneous modeling. Thirdly, the long training time and high GPU memory usage of existing methods is severe bottleneck that limits their scalability when dealing with much larger scenes. Although several methods such as Instant-NGP [5] have been proposed to enhance the NeRF representation and accelerate the NeRF training on smalland large-scale scenes [6], [7], [8], [9], they still attempt to fit an entire scene into single global network, overlooking the crucial aspect of heterogeneous scene representation from distinct scene decompositions. Consequently, these methods suffer from clear limitations in scalability and representation effectiveness, particularly when dealing with very large scenes. In this paper, to simultaneously tackle these critical challenges, we propose novel and highly scalable Heterogeneous Mixture of Hash Experts framework for NeRF, coined as Switch-NeRF++. The fundamental designs of SwitchNeRF++ are fourfold. 1) An end-to-end framework to learn scene decomposition in trainable manner, together with the scene representation. 2) An efficient hash-based gating network to learn scene decompositions. 3) heterogeneous JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2 Fig. 1: The birds-eye view of scene in UrbanBIS [10], rendered using INGP [5] and our Switch-NeRF++, showcasing the impressive scalability of our approach. This scene contains 13k high-quality images, capturing an urban region of around 6.5km2. Experiments on this scene demonstrate the superiority of our method in scaling to large-scale urban scenes with exceptional rendering quality. modeling of scene decomposition and each decomposed scene part. 4) highly scalable and efficient learning framework that can handle extremely large-scale scenes. With these key designs, Switch-NeRF++ can "
[06.05.2025 13:26] Mistral response. {"id": "39ba20c221e746eea50d429d5562915f", "object": "chat.completion", "created": 1746537972, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR\",\n    \"Inspur Cloud Information Technology Co, Ltd\"\n]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1559, "total_tokens": 1607, "completion_tokens": 48}}
[06.05.2025 13:26] Response: ```python
[
    "Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR",
    "Inspur Cloud Information Technology Co, Ltd"
]
```
[06.05.2025 13:26] Deleting PDF ./assets/pdf/2505.02005.pdf.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2505.01456.
[06.05.2025 13:26] Extra JSON file exists (./assets/json/2505.01456.json), skip PDF parsing.
[06.05.2025 13:26] Paper image links file exists (./assets/img_data/2505.01456.json), skip HTML parsing.
[06.05.2025 13:26] Success.
[06.05.2025 13:26] Enriching papers with extra data.
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 0. A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner. Rather than merely reacting to commands, it would continuously listen, reason, and respond proactively, fostering fluid, dynamic, and emotionally resonan...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 1. Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assig...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 2. Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generali...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 3. We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 4. Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olymp...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 5. Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically ...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 6. Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 7. We demonstrate that Muon, the simplest instantiation of a second-order optimizer, explicitly expands the Pareto frontier over AdamW on the compute-time tradeoff. We find that Muon is more effective than AdamW in retaining data efficiency at large batch sizes, far beyond the so-called critical batch ...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 8. Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workloads such as chain-of-thought, complex reasoning, and agent services significantly increase the inference cost by invoking the model repeatedly. Optimization methods such as parallelism, compressio...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 9. Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, re...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 10. We address a fundamental challenge in Reinforcement Learning from Interaction Demonstration (RLID): demonstration noise and coverage limitations. While existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that ...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 11. Large language models (LLMs) have achieved remarkable progress in complex reasoning tasks, yet they remain fundamentally limited by their reliance on static internal knowledge and text-only reasoning. Real-world problem solving often demands dynamic, multi-step reasoning, adaptive decision making, a...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 12. Due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. Recent efforts attempt to improve...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 13. Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widel...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 14. We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries ...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 15. Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a s...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 16. Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event bound...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 17. Current multi-subject customization approaches encounter two critical challenges: the difficulty in acquiring diverse multi-subject training data, and attribute entanglement across different subjects. To bridge these gaps, we propose MUSAR - a simple yet effective framework to achieve robust multi-s...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 18. Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficienc...
[06.05.2025 13:26] ********************************************************************************
[06.05.2025 13:26] Abstract 19. LLMs trained on massive datasets may inadvertently acquire sensitive information such as personal details and potentially harmful content. This risk is further heightened in multimodal LLMs as they integrate information from multiple modalities (image and text). Adversaries can exploit this knowledg...
[06.05.2025 13:26] Read previous papers.
[06.05.2025 13:26] Generating reviews via LLM API.
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#open_source", "#agi", "#multimodal", "#audio", "#architecture", "#agents", "#reasoning", "#multilingual", "#machine_translation"], "emoji": "üó£Ô∏è", "ru": {"title": "Voila: –ò–ò-—Å–æ–±–µ—Å–µ–¥–Ω–∏–∫ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –≥–æ–ª–æ—Å–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Voila - —Å–µ–º–µ–π—Å—Ç–≤–æ –≥–æ–ª–æ—Å–æ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#training", "#benchmark", "#rlhf", "#alignment", "#interpretability"], "emoji": "üß†", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–∞—é—â–∏–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#benchmark", "#training", "#multimodal", "#dataset", "#reasoning", "#synthetic", "#interpretability"], "emoji": "üß†", "ru": {"title": "–ì—Ä–æ–∫–∏–Ω–≥ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –¥–≤–µ—Ä—å –∫ –Ω–∞–¥–µ–∂–Ω–æ–º—É —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–Ω–æ–≥–æ
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#architecture", "#open_source", "#training", "#inference", "#optimization"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "ReplaceMe - —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–±—Ä–µ–∑–∫–∏ –≥–ª—É–±–∏–Ω—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω –∑–∞–º–µ–Ω—è–µ—Ç –±–ª–æ–∫–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#math", "#benchmark"], "emoji": "üßÆ", "ru": {"title": "FormalMATH: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏ –¥–ª—è –ò–ò", "desc": "FormalMATH - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –Ω–∞ Lean4, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 5560 —Ñ–æ—Ä–º–∞–ª—å–Ω–æ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –æ—Ç –æ–ª–∏–º–ø–∏–∞–¥–Ω–æ–≥–æ –¥–æ —É–Ω–∏–≤–µ—Ä
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#math", "#training", "#rlhf", "#reasoning", "#optimization"], "emoji": "üß†", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º GVM-RAFT –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –∑–∞–¥–∞
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#training", "#multimodal", "#rl", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization"], "emoji": "üöÄ", "ru": {"title": "Muon: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä Muon, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç AdamW –ø–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—é –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –º–æ—â–Ω–æ—Å—Ç–∏ –∏ –≤—Ä–µ–º–µ–Ω
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#inference", "#open_source", "#optimization", "#survey"], "emoji": "üöÄ", "ru": {"title": "–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–≤–∏–∂–∫–æ–≤ –≤—ã–≤–æ–¥–∞ LLM: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –±—É–¥—É—â–µ–µ", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ 25 –¥–≤–∏–∂–∫–æ–≤ –≤—ã–≤–æ–¥–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LL
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#optimization", "#training"], "emoji": "üß†", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞–º –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Adaptive Mode Learning (AML) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#games", "#training", "#rl", "#optimization", "#data"], "emoji": "ü§ñ", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è (RLID). –ê–≤—Ç–æ—Ä—ã –ø—Ä
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#agents", "#rl", "#reasoning", "#training", "#benchmark", "#optimization", "#rlhf"], "emoji": "ü§ñ", "ru": {"title": "ARTIST: –ê–≥–µ–Ω—Ç–Ω—ã–π –ò–ò —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ARTIST - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –∞–≥–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#data", "#benchmark", "#dataset", "#training", "#cv"], "emoji": "‚úèÔ∏è", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#optimization", "#inference", "#survey", "#training"], "emoji": "üî¨", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Å –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π —Å –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. 
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#agi", "#multimodal", "#open_source", "#cv"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ –≤ –µ–¥–∏–Ω–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ", "desc": "Ming-Lite-Uni - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#audio", "#small_models", "#benchmark"], "emoji": "üó£Ô∏è", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–º –ò–ò: –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≥–∏–≥–∞–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å LLaMA-Omni 2 - —Å–µ—Ä–∏—è —Ä–µ—á–µ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –ú–æ–¥–µ–ª—å –æ—Å–Ω–æ–≤–∞–Ω–∞
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#video", "#benchmark"], "emoji": "üé¨", "ru": {"title": "TEMPURA: –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ–±—ã—Ç–∏–π", "desc": "TEMPURA - —ç—Ç–æ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–æ
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#dataset", "#multimodal", "#data"], "emoji": "üñºÔ∏è", "ru": {"title": "MUSAR: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–Ω–æ–≥–æ—Å—É–±—ä–µ–∫—Ç–Ω–∞—è –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "MUSAR - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –º–Ω–æ–≥–æ—Å—É–±—ä–µ–∫—Ç–Ω–æ–π –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#benchmark", "#3d", "#dataset"], "emoji": "üèôÔ∏è", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π NeRF –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —Å—Ü–µ–Ω", "desc": "Switch-NeRF++ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö —Å—Ü–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π –∏–∑–ª—É—á–µ–Ω–∏—è (NeRF). –≠—Ç–∞ –º–æ–¥–µ
[06.05.2025 13:26] Using data from previous issue: {"categories": ["#hallucinations", "#benchmark", "#interpretability", "#security", "#multimodal", "#dataset"], "emoji": "üß†", "ru": {"title": "–ó–∞–±—ã–≤–∞–Ω–∏–µ –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∑–∞–±—ã–≤–∞–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –º—É–ª—å—Ç
[06.05.2025 13:26] Loading Chinese text from previous data.
[06.05.2025 13:26] Renaming data file.
[06.05.2025 13:26] Renaming previous data. hf_papers.json to ./d/2025-05-06.json
[06.05.2025 13:26] Saving new data file.
[06.05.2025 13:26] Generating page.
[06.05.2025 13:26] Renaming previous page.
[06.05.2025 13:26] Renaming previous data. index.html to ./d/2025-05-06.html
[06.05.2025 13:26] [Experimental] Generating Chinese page for reading.
[06.05.2025 13:26] Chinese vocab [{'word': 'ËØ≠Èü≥', 'pinyin': 'y«îyƒ´n', 'trans': 'voice'}, {'word': 'AI', 'pinyin': 'ƒìi-√†i', 'trans': 'artificial intelligence'}, {'word': '‰ª£ÁêÜ', 'pinyin': 'd√†il«ê', 'trans': 'agent'}, {'word': 'Ëá™Âä®', 'pinyin': 'z√¨d√≤ng', 'trans': 'automatic'}, {'word': 'ÂÆûÊó∂', 'pinyin': 'sh√≠sh√≠', 'trans': 'real-time'}, {'word': 'ÂØåÊúâ', 'pinyin': 'f√πy«íu', 'trans': 'rich in'}, {'word': 'ÊÉÖÊÑü', 'pinyin': 'q√≠ngg«én', 'trans': 'emotion'}, {'word': '‰∫íÂä®', 'pinyin': 'h√πd√≤ng', 'trans': 'interaction'}, {'word': 'Á´ØÂà∞Á´Ø', 'pinyin': 'duƒÅnd√†oduƒÅn', 'trans': 'end-to-end'}, {'word': 'Êû∂ÊûÑ', 'pinyin': 'ji√†g√≤u', 'trans': 'architecture'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠xi√†n', 'trans': 'achieve'}, {'word': '‰ΩéÂª∂Ëøü', 'pinyin': 'dƒ´ y√°nch√≠', 'trans': 'low latency'}, {'word': 'ÂÖ®ÂèåÂ∑•', 'pinyin': 'qu√°n shuƒÅngg≈çng', 'trans': 'full duplex'}, {'word': 'ÂØπËØù', 'pinyin': 'du√¨hu√†', 'trans': 'dialogue'}, {'word': 'ÁªìÂêà', 'pinyin': 'ji√©h√©', 'trans': 'combine'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«îy√°n m√≥x√≠ng', 'trans': 'large language model'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ngl√¨', 'trans': 'ability'}, {'word': 'Âº∫Â§ß', 'pinyin': 'qi√°ngd√†', 'trans': 'powerful'}, {'word': 'Â£∞Â≠¶', 'pinyin': 'shƒìngxu√©', 'trans': 'acoustics'}, {'word': 'Âª∫Ê®°', 'pinyin': 'ji√†nm√≥', 'trans': 'modeling'}, {'word': 'ÊîØÊåÅ', 'pinyin': 'zhƒ´ch√≠', 'trans': 'support'}, {'word': 'Ëá™ÁÑ∂', 'pinyin': 'z√¨r√°n', 'trans': 'natural'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generation'}, {'word': 'Ë∂ÖËøá', 'pinyin': 'chƒÅogu√≤', 'trans': 'exceed'}, {'word': 'È¢ÑËÆæ', 'pinyin': 'y√πsh√®', 'trans': 'preset'}, {'word': 'Â£∞Èü≥', 'pinyin': 'shƒìngyƒ´n', 'trans': 'sound'}, {'word': 'È´òÊïà', 'pinyin': 'gƒÅoxi√†o', 'trans': 'efficient'}, {'word': 'ÂÆöÂà∂', 'pinyin': 'd√¨ngzh√¨', 'trans': 'customize'}, {'word': 'Êñ∞', 'pinyin': 'xƒ´n', 'trans': 'new'}]
[06.05.2025 13:26] Renaming previous Chinese page.
[06.05.2025 13:26] Renaming previous data. zh.html to ./d/2025-05-05_zh_reading_task.html
[06.05.2025 13:26] Writing Chinese reading task.
[06.05.2025 13:26] Writing result.
[06.05.2025 13:26] Renaming log file.
[06.05.2025 13:26] Renaming previous data. log.txt to ./logs/2025-05-06_last_log.txt
