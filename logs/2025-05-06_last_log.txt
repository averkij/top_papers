[06.05.2025 06:17] Read previous papers.
[06.05.2025 06:17] Generating top page (month).
[06.05.2025 06:17] Writing top page (month).
[06.05.2025 07:12] Read previous papers.
[06.05.2025 07:12] Get feed.
[06.05.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02707
[06.05.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02387
[06.05.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02735
[06.05.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02835
[06.05.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02391
[06.05.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02156
[06.05.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02094
[06.05.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.01441
[06.05.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02370
[06.05.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.01043
[06.05.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.20752
[06.05.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02471
[06.05.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.01583
[06.05.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.01658
[06.05.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02823
[06.05.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02625
[06.05.2025 07:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.05.2025 07:12] No deleted papers detected.
[06.05.2025 07:12] Downloading and parsing papers (pdf, html). Total: 16.
[06.05.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2505.02707.
[06.05.2025 07:12] Extra JSON file exists (./assets/json/2505.02707.json), skip PDF parsing.
[06.05.2025 07:12] Paper image links file exists (./assets/img_data/2505.02707.json), skip HTML parsing.
[06.05.2025 07:12] Success.
[06.05.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2505.02387.
[06.05.2025 07:12] Extra JSON file exists (./assets/json/2505.02387.json), skip PDF parsing.
[06.05.2025 07:12] Paper image links file exists (./assets/img_data/2505.02387.json), skip HTML parsing.
[06.05.2025 07:12] Success.
[06.05.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2505.02735.
[06.05.2025 07:12] Extra JSON file exists (./assets/json/2505.02735.json), skip PDF parsing.
[06.05.2025 07:12] Paper image links file exists (./assets/img_data/2505.02735.json), skip HTML parsing.
[06.05.2025 07:12] Success.
[06.05.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2505.02835.
[06.05.2025 07:12] Extra JSON file exists (./assets/json/2505.02835.json), skip PDF parsing.
[06.05.2025 07:12] Paper image links file exists (./assets/img_data/2505.02835.json), skip HTML parsing.
[06.05.2025 07:12] Success.
[06.05.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2505.02391.
[06.05.2025 07:12] Extra JSON file exists (./assets/json/2505.02391.json), skip PDF parsing.
[06.05.2025 07:12] Paper image links file exists (./assets/img_data/2505.02391.json), skip HTML parsing.
[06.05.2025 07:12] Success.
[06.05.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2505.02156.
[06.05.2025 07:12] Extra JSON file exists (./assets/json/2505.02156.json), skip PDF parsing.
[06.05.2025 07:12] Paper image links file exists (./assets/img_data/2505.02156.json), skip HTML parsing.
[06.05.2025 07:12] Success.
[06.05.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2505.02094.
[06.05.2025 07:12] Extra JSON file exists (./assets/json/2505.02094.json), skip PDF parsing.
[06.05.2025 07:12] Paper image links file exists (./assets/img_data/2505.02094.json), skip HTML parsing.
[06.05.2025 07:12] Success.
[06.05.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2505.01441.
[06.05.2025 07:12] Extra JSON file exists (./assets/json/2505.01441.json), skip PDF parsing.
[06.05.2025 07:12] Paper image links file exists (./assets/img_data/2505.01441.json), skip HTML parsing.
[06.05.2025 07:12] Success.
[06.05.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2505.02370.
[06.05.2025 07:12] Extra JSON file exists (./assets/json/2505.02370.json), skip PDF parsing.
[06.05.2025 07:12] Paper image links file exists (./assets/img_data/2505.02370.json), skip HTML parsing.
[06.05.2025 07:12] Success.
[06.05.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2505.01043.
[06.05.2025 07:12] Extra JSON file exists (./assets/json/2505.01043.json), skip PDF parsing.
[06.05.2025 07:12] Paper image links file exists (./assets/img_data/2505.01043.json), skip HTML parsing.
[06.05.2025 07:12] Success.
[06.05.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2504.20752.
[06.05.2025 07:12] Extra JSON file exists (./assets/json/2504.20752.json), skip PDF parsing.
[06.05.2025 07:12] Paper image links file exists (./assets/img_data/2504.20752.json), skip HTML parsing.
[06.05.2025 07:12] Success.
[06.05.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2505.02471.
[06.05.2025 07:12] Extra JSON file exists (./assets/json/2505.02471.json), skip PDF parsing.
[06.05.2025 07:12] Paper image links file exists (./assets/img_data/2505.02471.json), skip HTML parsing.
[06.05.2025 07:12] Success.
[06.05.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2505.01583.
[06.05.2025 07:12] Extra JSON file exists (./assets/json/2505.01583.json), skip PDF parsing.
[06.05.2025 07:12] Paper image links file exists (./assets/img_data/2505.01583.json), skip HTML parsing.
[06.05.2025 07:12] Success.
[06.05.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2505.01658.
[06.05.2025 07:12] Extra JSON file exists (./assets/json/2505.01658.json), skip PDF parsing.
[06.05.2025 07:12] Paper image links file exists (./assets/img_data/2505.01658.json), skip HTML parsing.
[06.05.2025 07:12] Success.
[06.05.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2505.02823.
[06.05.2025 07:12] Extra JSON file exists (./assets/json/2505.02823.json), skip PDF parsing.
[06.05.2025 07:12] Paper image links file exists (./assets/img_data/2505.02823.json), skip HTML parsing.
[06.05.2025 07:12] Success.
[06.05.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2505.02625.
[06.05.2025 07:12] Extra JSON file exists (./assets/json/2505.02625.json), skip PDF parsing.
[06.05.2025 07:12] Paper image links file exists (./assets/img_data/2505.02625.json), skip HTML parsing.
[06.05.2025 07:12] Success.
[06.05.2025 07:12] Enriching papers with extra data.
[06.05.2025 07:12] ********************************************************************************
[06.05.2025 07:12] Abstract 0. A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner. Rather than merely reacting to commands, it would continuously listen, reason, and respond proactively, fostering fluid, dynamic, and emotionally resonan...
[06.05.2025 07:12] ********************************************************************************
[06.05.2025 07:12] Abstract 1. Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assig...
[06.05.2025 07:12] ********************************************************************************
[06.05.2025 07:12] Abstract 2. Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olymp...
[06.05.2025 07:12] ********************************************************************************
[06.05.2025 07:12] Abstract 3. Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long...
[06.05.2025 07:12] ********************************************************************************
[06.05.2025 07:12] Abstract 4. Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically ...
[06.05.2025 07:12] ********************************************************************************
[06.05.2025 07:12] Abstract 5. Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, re...
[06.05.2025 07:12] ********************************************************************************
[06.05.2025 07:12] Abstract 6. We address a fundamental challenge in Reinforcement Learning from Interaction Demonstration (RLID): demonstration noise and coverage limitations. While existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that ...
[06.05.2025 07:12] ********************************************************************************
[06.05.2025 07:12] Abstract 7. Large language models (LLMs) have achieved remarkable progress in complex reasoning tasks, yet they remain fundamentally limited by their reliance on static internal knowledge and text-only reasoning. Real-world problem solving often demands dynamic, multi-step reasoning, adaptive decision making, a...
[06.05.2025 07:12] ********************************************************************************
[06.05.2025 07:12] Abstract 8. Due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. Recent efforts attempt to improve...
[06.05.2025 07:12] ********************************************************************************
[06.05.2025 07:12] Abstract 9. Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widel...
[06.05.2025 07:12] ********************************************************************************
[06.05.2025 07:12] Abstract 10. Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generali...
[06.05.2025 07:12] ********************************************************************************
[06.05.2025 07:12] Abstract 11. We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries ...
[06.05.2025 07:12] ********************************************************************************
[06.05.2025 07:12] Abstract 12. Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event bound...
[06.05.2025 07:12] ********************************************************************************
[06.05.2025 07:12] Abstract 13. Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workloads such as chain-of-thought, complex reasoning, and agent services significantly increase the inference cost by invoking the model repeatedly. Optimization methods such as parallelism, compressio...
[06.05.2025 07:12] ********************************************************************************
[06.05.2025 07:12] Abstract 14. Current multi-subject customization approaches encounter two critical challenges: the difficulty in acquiring diverse multi-subject training data, and attribute entanglement across different subjects. To bridge these gaps, we propose MUSAR - a simple yet effective framework to achieve robust multi-s...
[06.05.2025 07:12] ********************************************************************************
[06.05.2025 07:12] Abstract 15. Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a s...
[06.05.2025 07:12] Read previous papers.
[06.05.2025 07:12] Generating reviews via LLM API.
[06.05.2025 07:12] Using data from previous issue: {"categories": ["#open_source", "#agi", "#multimodal", "#audio", "#architecture", "#agents", "#reasoning", "#multilingual", "#machine_translation"], "emoji": "üó£Ô∏è", "ru": {"title": "Voila: –ò–ò-—Å–æ–±–µ—Å–µ–¥–Ω–∏–∫ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –≥–æ–ª–æ—Å–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Voila - —Å–µ–º–µ–π—Å—Ç–≤–æ –≥–æ–ª–æ—Å–æ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[06.05.2025 07:12] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#training", "#benchmark", "#rlhf", "#alignment", "#interpretability"], "emoji": "üß†", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–∞—é—â–∏–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω
[06.05.2025 07:12] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#math", "#benchmark"], "emoji": "üßÆ", "ru": {"title": "FormalMATH: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏ –¥–ª—è –ò–ò", "desc": "FormalMATH - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –Ω–∞ Lean4, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 5560 —Ñ–æ—Ä–º–∞–ª—å–Ω–æ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –æ—Ç –æ–ª–∏–º–ø–∏–∞–¥–Ω–æ–≥–æ –¥–æ —É–Ω–∏–≤–µ—Ä
[06.05.2025 07:12] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#training", "#multimodal", "#rl", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
[06.05.2025 07:12] Using data from previous issue: {"categories": ["#math", "#training", "#rlhf", "#reasoning", "#optimization"], "emoji": "üß†", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º GVM-RAFT –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –∑–∞–¥–∞
[06.05.2025 07:12] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#optimization", "#training"], "emoji": "üß†", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞–º –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Adaptive Mode Learning (AML) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å
[06.05.2025 07:12] Using data from previous issue: {"categories": ["#games", "#training", "#rl", "#optimization", "#data"], "emoji": "ü§ñ", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è (RLID). –ê–≤—Ç–æ—Ä—ã –ø—Ä
[06.05.2025 07:12] Using data from previous issue: {"categories": ["#agents", "#rl", "#reasoning", "#training", "#benchmark", "#optimization", "#rlhf"], "emoji": "ü§ñ", "ru": {"title": "ARTIST: –ê–≥–µ–Ω—Ç–Ω—ã–π –ò–ò —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ARTIST - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –∞–≥–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥
[06.05.2025 07:12] Using data from previous issue: {"categories": ["#data", "#benchmark", "#dataset", "#training", "#cv"], "emoji": "‚úèÔ∏è", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç
[06.05.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#inference", "#survey", "#training"], "emoji": "üî¨", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Å –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π —Å –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. 
[06.05.2025 07:12] Using data from previous issue: {"categories": ["#benchmark", "#training", "#multimodal", "#dataset", "#reasoning", "#synthetic", "#interpretability"], "emoji": "üß†", "ru": {"title": "–ì—Ä–æ–∫–∏–Ω–≥ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –¥–≤–µ—Ä—å –∫ –Ω–∞–¥–µ–∂–Ω–æ–º—É —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–Ω–æ–≥–æ
[06.05.2025 07:12] Using data from previous issue: {"categories": ["#agi", "#multimodal", "#open_source", "#cv"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ –≤ –µ–¥–∏–Ω–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ", "desc": "Ming-Lite-Uni - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ
[06.05.2025 07:12] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#video", "#benchmark"], "emoji": "üé¨", "ru": {"title": "TEMPURA: –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ–±—ã—Ç–∏–π", "desc": "TEMPURA - —ç—Ç–æ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–æ
[06.05.2025 07:12] Using data from previous issue: {"categories": ["#inference", "#open_source", "#optimization", "#survey"], "emoji": "üöÄ", "ru": {"title": "–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–≤–∏–∂–∫–æ–≤ –≤—ã–≤–æ–¥–∞ LLM: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –±—É–¥—É—â–µ–µ", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ 25 –¥–≤–∏–∂–∫–æ–≤ –≤—ã–≤–æ–¥–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LL
[06.05.2025 07:12] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#dataset", "#multimodal", "#data"], "emoji": "üñºÔ∏è", "ru": {"title": "MUSAR: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–Ω–æ–≥–æ—Å—É–±—ä–µ–∫—Ç–Ω–∞—è –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "MUSAR - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –º–Ω–æ–≥–æ—Å—É–±—ä–µ–∫—Ç–Ω–æ–π –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø
[06.05.2025 07:12] Using data from previous issue: {"categories": ["#audio", "#small_models", "#benchmark"], "emoji": "üó£Ô∏è", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–º –ò–ò: –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≥–∏–≥–∞–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å LLaMA-Omni 2 - —Å–µ—Ä–∏—è —Ä–µ—á–µ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –ú–æ–¥–µ–ª—å –æ—Å–Ω–æ–≤–∞–Ω–∞
[06.05.2025 07:12] Loading Chinese text from previous data.
[06.05.2025 07:12] Renaming data file.
[06.05.2025 07:12] Renaming previous data. hf_papers.json to ./d/2025-05-06.json
[06.05.2025 07:12] Saving new data file.
[06.05.2025 07:12] Generating page.
[06.05.2025 07:12] Renaming previous page.
[06.05.2025 07:12] Renaming previous data. index.html to ./d/2025-05-06.html
[06.05.2025 07:12] [Experimental] Generating Chinese page for reading.
[06.05.2025 07:12] Chinese vocab [{'word': 'ÂõæÂÉè‰øÆÂ§ç', 'pinyin': 't√∫ xi√†ng xi≈´ f√π', 'trans': 'image inpainting'}, {'word': 'È¢ÜÂüü', 'pinyin': 'l«êng y√π', 'trans': 'field'}, {'word': 'Ê≥®ÊÑèÂäõÊú∫Âà∂', 'pinyin': 'zh√π y√¨ l√¨ jƒ´ zh√¨', 'trans': 'attention mechanism'}, {'word': 'ËΩªÈáèÊû∂ÊûÑ', 'pinyin': 'qƒ´ng li√†ng ji√† g√≤u', 'trans': 'lightweight architecture'}, {'word': '‰∏ä‰∏ãÊñáÊÑüÁü•', 'pinyin': 'sh√†ng xi√† w√©n g«én zhƒ´', 'trans': 'context-aware'}, {'word': 'Âª∫Ê®°', 'pinyin': 'ji√†n m√≥', 'trans': 'modeling'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«én zh√π', 'trans': 'significant'}, {'word': 'ËøõÂ±ï', 'pinyin': 'j√¨n zh«én', 'trans': 'progress'}, {'word': 'Â§çÊùÇÁªìÊûÑ', 'pinyin': 'f√π z√° ji√© g√≤u', 'trans': 'complex structures'}, {'word': 'ËØ≠‰πâ', 'pinyin': 'y«î y√¨', 'trans': 'semantics'}, {'word': 'ÊåëÊàò', 'pinyin': 'ti«éo zh√†n', 'trans': 'challenge'}, {'word': '‰º™ÂΩ±', 'pinyin': 'wƒõi y«êng', 'trans': 'artifacts'}, {'word': '‰∏çÊÅ∞ÂΩì', 'pinyin': 'b√π qi√† d√†ng', 'trans': 'inappropriate'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generation'}, {'word': 'ÊΩúÂú®Á±ªÂà´', 'pinyin': 'qi√°n z√†i l√®i bi√©', 'trans': 'latent category'}, {'word': 'ÊåáÂØº', 'pinyin': 'zh«ê d«éo', 'trans': 'guidance'}, {'word': 'ËåÉÂºè', 'pinyin': 'f√†n sh√¨', 'trans': 'paradigm'}, {'word': 'ÁÆÄÂçïÊúâÊïà', 'pinyin': 'ji«én dƒÅn y«íu xi√†o', 'trans': 'simple and effective'}, {'word': 'Êâ©Êï£', 'pinyin': 'ku√≤ s√†n', 'trans': 'diffusion'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥ x√≠ng', 'trans': 'model'}, {'word': 'ÂëΩÂêç', 'pinyin': 'm√¨ng m√≠ng', 'trans': 'named'}, {'word': 'ÂâçÊôØ', 'pinyin': 'qi√°n j«êng', 'trans': 'foreground'}, {'word': 'ËÉåÊôØ', 'pinyin': 'b√®i j«êng', 'trans': 'background'}, {'word': 'Ë°®Á§∫', 'pinyin': 'bi«éo sh√¨', 'trans': 'representation'}, {'word': 'Ê≥®ÂÖ•', 'pinyin': 'zh√π r√π', 'trans': 'inject'}, {'word': 'ÁâπÂæÅ', 'pinyin': 't√® zhƒìng', 'trans': 'features'}, {'word': 'ÂéªÂô™', 'pinyin': 'q√π z√†o', 'trans': 'denoising'}, {'word': 'ËøáÁ®ã', 'pinyin': 'gu√≤ ch√©ng', 'trans': 'process'}, {'word': 'ÁºñÁ†Å', 'pinyin': 'biƒÅn m«é', 'trans': 'encode'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': '‰ºòÂºÇ', 'pinyin': 'y≈çu y√¨', 'trans': 'excellent'}, {'word': '‰∏ÄËá¥ÊÄß', 'pinyin': 'yƒ´ zh√¨ x√¨ng', 'trans': 'consistency'}, {'word': 'È°πÁõÆ', 'pinyin': 'xi√†ng m√π', 'trans': 'project'}, {'word': 'È°µÈù¢', 'pinyin': 'y√® mi√†n', 'trans': 'page'}]
[06.05.2025 07:12] Renaming previous Chinese page.
[06.05.2025 07:12] Renaming previous data. zh.html to ./d/2025-05-05_zh_reading_task.html
[06.05.2025 07:12] Writing Chinese reading task.
[06.05.2025 07:12] Writing result.
[06.05.2025 07:12] Renaming log file.
[06.05.2025 07:12] Renaming previous data. log.txt to ./logs/2025-05-06_last_log.txt
