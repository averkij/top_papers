[06.05.2025 05:13] Read previous papers.
[06.05.2025 05:13] Generating top page (month).
[06.05.2025 05:13] Writing top page (month).
[06.05.2025 06:16] Read previous papers.
[06.05.2025 06:16] Get feed.
[06.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02707
[06.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02387
[06.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02735
[06.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02835
[06.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02156
[06.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02094
[06.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02370
[06.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.01043
[06.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.01441
[06.05.2025 06:16] Extract page data from URL. URL: https://huggingface.co/papers/2505.02391
[06.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02471
[06.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.01583
[06.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.01658
[06.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2504.20752
[06.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02823
[06.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02625
[06.05.2025 06:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.05.2025 06:16] No deleted papers detected.
[06.05.2025 06:16] Downloading and parsing papers (pdf, html). Total: 16.
[06.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.02707.
[06.05.2025 06:16] Extra JSON file exists (./assets/json/2505.02707.json), skip PDF parsing.
[06.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.02707.json), skip HTML parsing.
[06.05.2025 06:16] Success.
[06.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.02387.
[06.05.2025 06:16] Extra JSON file exists (./assets/json/2505.02387.json), skip PDF parsing.
[06.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.02387.json), skip HTML parsing.
[06.05.2025 06:16] Success.
[06.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.02735.
[06.05.2025 06:16] Extra JSON file exists (./assets/json/2505.02735.json), skip PDF parsing.
[06.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.02735.json), skip HTML parsing.
[06.05.2025 06:16] Success.
[06.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.02835.
[06.05.2025 06:16] Extra JSON file exists (./assets/json/2505.02835.json), skip PDF parsing.
[06.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.02835.json), skip HTML parsing.
[06.05.2025 06:16] Success.
[06.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.02156.
[06.05.2025 06:16] Extra JSON file exists (./assets/json/2505.02156.json), skip PDF parsing.
[06.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.02156.json), skip HTML parsing.
[06.05.2025 06:16] Success.
[06.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.02094.
[06.05.2025 06:16] Extra JSON file exists (./assets/json/2505.02094.json), skip PDF parsing.
[06.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.02094.json), skip HTML parsing.
[06.05.2025 06:16] Success.
[06.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.02370.
[06.05.2025 06:16] Extra JSON file exists (./assets/json/2505.02370.json), skip PDF parsing.
[06.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.02370.json), skip HTML parsing.
[06.05.2025 06:16] Success.
[06.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.01043.
[06.05.2025 06:16] Extra JSON file exists (./assets/json/2505.01043.json), skip PDF parsing.
[06.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.01043.json), skip HTML parsing.
[06.05.2025 06:16] Success.
[06.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.01441.
[06.05.2025 06:16] Extra JSON file exists (./assets/json/2505.01441.json), skip PDF parsing.
[06.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.01441.json), skip HTML parsing.
[06.05.2025 06:16] Success.
[06.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.02391.
[06.05.2025 06:16] Downloading paper 2505.02391 from http://arxiv.org/pdf/2505.02391v1...
[06.05.2025 06:17] Extracting affiliations from text.
[06.05.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 ] . [ 1 1 9 3 2 0 . 5 0 5 2 : r Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL Jiarui Yao1 Yifan Hao1 Hanning Zhang1 Hanze Dong2 Wei Xiong1 Nan Jiang1 Tong Zhang1 1University of Illinois Urbana-Champaign 2Salesforce AI Research "
[06.05.2025 06:17] Response: ```python
["University of Illinois Urbana-Champaign", "Salesforce AI Research"]
```
[06.05.2025 06:17] Deleting PDF ./assets/pdf/2505.02391.pdf.
[06.05.2025 06:17] Success.
[06.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.02471.
[06.05.2025 06:17] Extra JSON file exists (./assets/json/2505.02471.json), skip PDF parsing.
[06.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.02471.json), skip HTML parsing.
[06.05.2025 06:17] Success.
[06.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.01583.
[06.05.2025 06:17] Extra JSON file exists (./assets/json/2505.01583.json), skip PDF parsing.
[06.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.01583.json), skip HTML parsing.
[06.05.2025 06:17] Success.
[06.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.01658.
[06.05.2025 06:17] Extra JSON file exists (./assets/json/2505.01658.json), skip PDF parsing.
[06.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.01658.json), skip HTML parsing.
[06.05.2025 06:17] Success.
[06.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2504.20752.
[06.05.2025 06:17] Extra JSON file exists (./assets/json/2504.20752.json), skip PDF parsing.
[06.05.2025 06:17] Paper image links file exists (./assets/img_data/2504.20752.json), skip HTML parsing.
[06.05.2025 06:17] Success.
[06.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.02823.
[06.05.2025 06:17] Extra JSON file exists (./assets/json/2505.02823.json), skip PDF parsing.
[06.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.02823.json), skip HTML parsing.
[06.05.2025 06:17] Success.
[06.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.02625.
[06.05.2025 06:17] Extra JSON file exists (./assets/json/2505.02625.json), skip PDF parsing.
[06.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.02625.json), skip HTML parsing.
[06.05.2025 06:17] Success.
[06.05.2025 06:17] Enriching papers with extra data.
[06.05.2025 06:17] ********************************************************************************
[06.05.2025 06:17] Abstract 0. A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner. Rather than merely reacting to commands, it would continuously listen, reason, and respond proactively, fostering fluid, dynamic, and emotionally resonan...
[06.05.2025 06:17] ********************************************************************************
[06.05.2025 06:17] Abstract 1. Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assig...
[06.05.2025 06:17] ********************************************************************************
[06.05.2025 06:17] Abstract 2. Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olymp...
[06.05.2025 06:17] ********************************************************************************
[06.05.2025 06:17] Abstract 3. Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long...
[06.05.2025 06:17] ********************************************************************************
[06.05.2025 06:17] Abstract 4. Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, re...
[06.05.2025 06:17] ********************************************************************************
[06.05.2025 06:17] Abstract 5. We address a fundamental challenge in Reinforcement Learning from Interaction Demonstration (RLID): demonstration noise and coverage limitations. While existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that ...
[06.05.2025 06:17] ********************************************************************************
[06.05.2025 06:17] Abstract 6. Due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. Recent efforts attempt to improve...
[06.05.2025 06:17] ********************************************************************************
[06.05.2025 06:17] Abstract 7. Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widel...
[06.05.2025 06:17] ********************************************************************************
[06.05.2025 06:17] Abstract 8. Large language models (LLMs) have achieved remarkable progress in complex reasoning tasks, yet they remain fundamentally limited by their reliance on static internal knowledge and text-only reasoning. Real-world problem solving often demands dynamic, multi-step reasoning, adaptive decision making, a...
[06.05.2025 06:17] ********************************************************************************
[06.05.2025 06:17] Abstract 9. Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically ...
[06.05.2025 06:17] ********************************************************************************
[06.05.2025 06:17] Abstract 10. We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries ...
[06.05.2025 06:17] ********************************************************************************
[06.05.2025 06:17] Abstract 11. Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event bound...
[06.05.2025 06:17] ********************************************************************************
[06.05.2025 06:17] Abstract 12. Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workloads such as chain-of-thought, complex reasoning, and agent services significantly increase the inference cost by invoking the model repeatedly. Optimization methods such as parallelism, compressio...
[06.05.2025 06:17] ********************************************************************************
[06.05.2025 06:17] Abstract 13. Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generali...
[06.05.2025 06:17] ********************************************************************************
[06.05.2025 06:17] Abstract 14. Current multi-subject customization approaches encounter two critical challenges: the difficulty in acquiring diverse multi-subject training data, and attribute entanglement across different subjects. To bridge these gaps, we propose MUSAR - a simple yet effective framework to achieve robust multi-s...
[06.05.2025 06:17] ********************************************************************************
[06.05.2025 06:17] Abstract 15. Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a s...
[06.05.2025 06:17] Read previous papers.
[06.05.2025 06:17] Generating reviews via LLM API.
[06.05.2025 06:17] Using data from previous issue: {"categories": ["#open_source", "#agi", "#multimodal", "#audio", "#architecture", "#agents", "#reasoning", "#multilingual", "#translation"], "emoji": "ğŸ—£ï¸", "ru": {"title": "Voila: Ğ˜Ğ˜-ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸Ğº Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Voila - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±
[06.05.2025 06:17] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#training", "#benchmark", "#rlhf", "#alignment", "#interpretability"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½
[06.05.2025 06:17] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#math", "#benchmark"], "emoji": "ğŸ§®", "ru": {"title": "FormalMATH: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ˜Ğ˜", "desc": "FormalMATH - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ½Ğ° Lean4, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 5560 Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ñ‚ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€
[06.05.2025 06:17] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#training", "#multimodal", "#rl", "#optimization"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
[06.05.2025 06:17] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#optimization", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Adaptive Mode Learning (AML) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ
[06.05.2025 06:17] Using data from previous issue: {"categories": ["#games", "#training", "#rl", "#optimization", "#data"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (RLID). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€
[06.05.2025 06:17] Using data from previous issue: {"categories": ["#data", "#benchmark", "#dataset", "#training", "#cv"], "emoji": "âœï¸", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚
[06.05.2025 06:17] Using data from previous issue: {"categories": ["#optimization", "#inference", "#survey", "#training"], "emoji": "ğŸ”¬", "ru": {"title": "Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. 
[06.05.2025 06:17] Using data from previous issue: {"categories": ["#agents", "#rl", "#reasoning", "#training", "#benchmark", "#optimization", "#rlhf"], "emoji": "ğŸ¤–", "ru": {"title": "ARTIST: ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ARTIST - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´
[06.05.2025 06:17] Querying the API.
[06.05.2025 06:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.
[06.05.2025 06:17] Response: {
  "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GVM-RAFT Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (CoT) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ½ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ 2-4-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ RAFT.",
  "emoji": "ğŸ§ ",
  "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼"
}
[06.05.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM."

[06.05.2025 06:17] Response: ```python
['TRAINING', 'RLHF', 'MATH']
```
[06.05.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM."

[06.05.2025 06:17] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[06.05.2025 06:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenge of improving chain-of-thought (CoT) reasoning in large language models (LLMs) by treating it as a latent variable problem. The authors introduce GVM-RAFT, a new method that dynamically allocates computational resources based on the difficulty of prompts, rather than using a fixed strategy. This approach minimizes the variance in stochastic gradient estimation, leading to faster convergence and better performance in reasoning tasks. Experimental results demonstrate that GVM-RAFT significantly outperforms previous methods, achieving notable speedups and accuracy gains.","title":"Dynamic Resource Allocation for Enhanced Reasoning in LLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenge of improving chain-of-thought (CoT) reasoning in large language models (LLMs) by treating it as a latent variable problem. The authors introduce GVM-RAFT, a new method that dynamically allocates computational resources based on the difficulty of prompts, rather than using a fixed strategy. This approach minimizes the variance in stochastic gradient estimation, leading to faster convergence and better performance in reasoning tasks. Experimental results demonstrate that GVM-RAFT significantly outperforms previous methods, achieving notable speedups and accuracy gains.', title='Dynamic Resource Allocation for Enhanced Reasoning in LLMs'))
[06.05.2025 06:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é“¾å¼æ¨ç†ï¼ˆCoTï¼‰å¦‚ä½•è¢«å½¢å¼åŒ–ä¸ºæ½œå˜é‡é—®é¢˜ï¼Œæ¨¡å‹éœ€è¦ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ã€‚ä»¥å¾€çš„æ–¹æ³•å¦‚è¿­ä»£å¥–åŠ±æ’åå¾®è°ƒï¼ˆRAFTï¼‰åœ¨å¤„ç†ä¸åŒéš¾åº¦çš„æç¤ºæ—¶ï¼Œé€šå¸¸é‡‡ç”¨ç»Ÿä¸€çš„æ¨ç†é¢„ç®—ï¼Œè¿™å¯¼è‡´äº†æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†GVM-RAFTï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ç‰¹å®šæç¤ºçš„åŠ¨æ€æ ·æœ¬åˆ†é…ç­–ç•¥ï¼Œæ—¨åœ¨åœ¨è®¡ç®—é¢„ç®—é™åˆ¶ä¸‹æœ€å°åŒ–éšæœºæ¢¯åº¦æ–¹å·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGVM-RAFTåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å®ç°äº†2-4å€çš„åŠ é€Ÿå’Œæ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ã€‚","title":"åŠ¨æ€æ ·æœ¬åˆ†é…ï¼Œæå‡æ¨ç†æ•ˆç‡ï¼"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é“¾å¼æ¨ç†ï¼ˆCoTï¼‰å¦‚ä½•è¢«å½¢å¼åŒ–ä¸ºæ½œå˜é‡é—®é¢˜ï¼Œæ¨¡å‹éœ€è¦ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ã€‚ä»¥å¾€çš„æ–¹æ³•å¦‚è¿­ä»£å¥–åŠ±æ’åå¾®è°ƒï¼ˆRAFTï¼‰åœ¨å¤„ç†ä¸åŒéš¾åº¦çš„æç¤ºæ—¶ï¼Œé€šå¸¸é‡‡ç”¨ç»Ÿä¸€çš„æ¨ç†é¢„ç®—ï¼Œè¿™å¯¼è‡´äº†æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†GVM-RAFTï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ç‰¹å®šæç¤ºçš„åŠ¨æ€æ ·æœ¬åˆ†é…ç­–ç•¥ï¼Œæ—¨åœ¨åœ¨è®¡ç®—é¢„ç®—é™åˆ¶ä¸‹æœ€å°åŒ–éšæœºæ¢¯åº¦æ–¹å·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGVM-RAFTåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å®ç°äº†2-4å€çš„åŠ é€Ÿå’Œæ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ã€‚', title='åŠ¨æ€æ ·æœ¬åˆ†é…ï¼Œæå‡æ¨ç†æ•ˆç‡ï¼'))
[06.05.2025 06:17] Using data from previous issue: {"categories": ["#agi", "#multimodal", "#open_source", "#cv"], "emoji": "ğŸ¤–", "ru": {"title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ", "desc": "Ming-Lite-Uni - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµ
[06.05.2025 06:17] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#video", "#benchmark"], "emoji": "ğŸ¬", "ru": {"title": "TEMPURA: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹", "desc": "TEMPURA - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¾
[06.05.2025 06:17] Using data from previous issue: {"categories": ["#inference", "#open_source", "#optimization", "#survey"], "emoji": "ğŸš€", "ru": {"title": "ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° LLM: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ", "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· 25 Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LL
[06.05.2025 06:17] Using data from previous issue: {"categories": ["#benchmark", "#training", "#multimodal", "#dataset", "#reasoning", "#synthetic", "#interpretability"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ“Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ²ĞµÑ€ÑŒ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¼Ñƒ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾
[06.05.2025 06:17] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#dataset", "#multimodal", "#data"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "MUSAR: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ½Ğ°Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "MUSAR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾Ğ¹ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿
[06.05.2025 06:17] Using data from previous issue: {"categories": ["#audio", "#small_models", "#benchmark"], "emoji": "ğŸ—£ï¸", "ru": {"title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ğ¾Ğ¼ Ğ˜Ğ˜: ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ¾Ğ²", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LLaMA-Omni 2 - ÑĞµÑ€Ğ¸Ñ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ°
[06.05.2025 06:17] Loading Chinese text from previous data.
[06.05.2025 06:17] Renaming data file.
[06.05.2025 06:17] Renaming previous data. hf_papers.json to ./d/2025-05-06.json
[06.05.2025 06:17] Saving new data file.
[06.05.2025 06:17] Generating page.
[06.05.2025 06:17] Renaming previous page.
[06.05.2025 06:17] Renaming previous data. index.html to ./d/2025-05-06.html
[06.05.2025 06:17] [Experimental] Generating Chinese page for reading.
[06.05.2025 06:17] Chinese vocab [{'word': 'å›¾åƒä¿®å¤', 'pinyin': 'tÃº xiÃ ng xiÅ« fÃ¹', 'trans': 'image inpainting'}, {'word': 'é¢†åŸŸ', 'pinyin': 'lÇng yÃ¹', 'trans': 'field'}, {'word': 'æ³¨æ„åŠ›æœºåˆ¶', 'pinyin': 'zhÃ¹ yÃ¬ lÃ¬ jÄ« zhÃ¬', 'trans': 'attention mechanism'}, {'word': 'è½»é‡æ¶æ„', 'pinyin': 'qÄ«ng liÃ ng jiÃ  gÃ²u', 'trans': 'lightweight architecture'}, {'word': 'ä¸Šä¸‹æ–‡æ„ŸçŸ¥', 'pinyin': 'shÃ ng xiÃ  wÃ©n gÇn zhÄ«', 'trans': 'context-aware'}, {'word': 'å»ºæ¨¡', 'pinyin': 'jiÃ n mÃ³', 'trans': 'modeling'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'}, {'word': 'è¿›å±•', 'pinyin': 'jÃ¬n zhÇn', 'trans': 'progress'}, {'word': 'å¤æ‚ç»“æ„', 'pinyin': 'fÃ¹ zÃ¡ jiÃ© gÃ²u', 'trans': 'complex structures'}, {'word': 'è¯­ä¹‰', 'pinyin': 'yÇ” yÃ¬', 'trans': 'semantics'}, {'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'}, {'word': 'ä¼ªå½±', 'pinyin': 'wÄ›i yÇng', 'trans': 'artifacts'}, {'word': 'ä¸æ°å½“', 'pinyin': 'bÃ¹ qiÃ  dÃ ng', 'trans': 'inappropriate'}, {'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generation'}, {'word': 'æ½œåœ¨ç±»åˆ«', 'pinyin': 'qiÃ¡n zÃ i lÃ¨i biÃ©', 'trans': 'latent category'}, {'word': 'æŒ‡å¯¼', 'pinyin': 'zhÇ dÇo', 'trans': 'guidance'}, {'word': 'èŒƒå¼', 'pinyin': 'fÃ n shÃ¬', 'trans': 'paradigm'}, {'word': 'ç®€å•æœ‰æ•ˆ', 'pinyin': 'jiÇn dÄn yÇ’u xiÃ o', 'trans': 'simple and effective'}, {'word': 'æ‰©æ•£', 'pinyin': 'kuÃ² sÃ n', 'trans': 'diffusion'}, {'word': 'æ¨¡å‹', 'pinyin': 'mÃ³ xÃ­ng', 'trans': 'model'}, {'word': 'å‘½å', 'pinyin': 'mÃ¬ng mÃ­ng', 'trans': 'named'}, {'word': 'å‰æ™¯', 'pinyin': 'qiÃ¡n jÇng', 'trans': 'foreground'}, {'word': 'èƒŒæ™¯', 'pinyin': 'bÃ¨i jÇng', 'trans': 'background'}, {'word': 'è¡¨ç¤º', 'pinyin': 'biÇo shÃ¬', 'trans': 'representation'}, {'word': 'æ³¨å…¥', 'pinyin': 'zhÃ¹ rÃ¹', 'trans': 'inject'}, {'word': 'ç‰¹å¾', 'pinyin': 'tÃ¨ zhÄ“ng', 'trans': 'features'}, {'word': 'å»å™ª', 'pinyin': 'qÃ¹ zÃ o', 'trans': 'denoising'}, {'word': 'è¿‡ç¨‹', 'pinyin': 'guÃ² chÃ©ng', 'trans': 'process'}, {'word': 'ç¼–ç ', 'pinyin': 'biÄn mÇ', 'trans': 'encode'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'}, {'word': 'ä¼˜å¼‚', 'pinyin': 'yÅu yÃ¬', 'trans': 'excellent'}, {'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ« zhÃ¬ xÃ¬ng', 'trans': 'consistency'}, {'word': 'é¡¹ç›®', 'pinyin': 'xiÃ ng mÃ¹', 'trans': 'project'}, {'word': 'é¡µé¢', 'pinyin': 'yÃ¨ miÃ n', 'trans': 'page'}]
[06.05.2025 06:17] Renaming previous Chinese page.
[06.05.2025 06:17] Renaming previous data. zh.html to ./d/2025-05-05_zh_reading_task.html
[06.05.2025 06:17] Writing Chinese reading task.
[06.05.2025 06:17] Writing result.
[06.05.2025 06:17] Renaming log file.
[06.05.2025 06:17] Renaming previous data. log.txt to ./logs/2025-05-06_last_log.txt
