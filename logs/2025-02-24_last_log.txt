[24.02.2025 09:12] Read previous papers.
[24.02.2025 09:12] Generating top page (month).
[24.02.2025 09:12] Writing top page (month).
[24.02.2025 10:12] Read previous papers.
[24.02.2025 10:12] Get feed.
[24.02.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14776
[24.02.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11663
[24.02.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13449
[24.02.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15007
[24.02.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14397
[24.02.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14922
[24.02.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12084
[24.02.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15589
[24.02.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14494
[24.02.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15027
[24.02.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15657
[24.02.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15631
[24.02.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14905
[24.02.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15681
[24.02.2025 10:12] Extract page data from URL. URL: https://huggingface.co/papers/2502.13189
[24.02.2025 10:12] Extract page data from URL. URL: https://huggingface.co/papers/2502.13407
[24.02.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15011
[24.02.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15082
[24.02.2025 10:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[24.02.2025 10:12] No deleted papers detected.
[24.02.2025 10:12] Downloading and parsing papers (pdf, html). Total: 18.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.14776.
[24.02.2025 10:12] Extra JSON file exists (./assets/json/2502.14776.json), skip PDF parsing.
[24.02.2025 10:12] Paper image links file exists (./assets/img_data/2502.14776.json), skip HTML parsing.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.11663.
[24.02.2025 10:12] Extra JSON file exists (./assets/json/2502.11663.json), skip PDF parsing.
[24.02.2025 10:12] Paper image links file exists (./assets/img_data/2502.11663.json), skip HTML parsing.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.13449.
[24.02.2025 10:12] Extra JSON file exists (./assets/json/2502.13449.json), skip PDF parsing.
[24.02.2025 10:12] Paper image links file exists (./assets/img_data/2502.13449.json), skip HTML parsing.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.15007.
[24.02.2025 10:12] Extra JSON file exists (./assets/json/2502.15007.json), skip PDF parsing.
[24.02.2025 10:12] Paper image links file exists (./assets/img_data/2502.15007.json), skip HTML parsing.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.14397.
[24.02.2025 10:12] Extra JSON file exists (./assets/json/2502.14397.json), skip PDF parsing.
[24.02.2025 10:12] Paper image links file exists (./assets/img_data/2502.14397.json), skip HTML parsing.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.14922.
[24.02.2025 10:12] Extra JSON file exists (./assets/json/2502.14922.json), skip PDF parsing.
[24.02.2025 10:12] Paper image links file exists (./assets/img_data/2502.14922.json), skip HTML parsing.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.12084.
[24.02.2025 10:12] Extra JSON file exists (./assets/json/2502.12084.json), skip PDF parsing.
[24.02.2025 10:12] Paper image links file exists (./assets/img_data/2502.12084.json), skip HTML parsing.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.15589.
[24.02.2025 10:12] Extra JSON file exists (./assets/json/2502.15589.json), skip PDF parsing.
[24.02.2025 10:12] Paper image links file exists (./assets/img_data/2502.15589.json), skip HTML parsing.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.14494.
[24.02.2025 10:12] Extra JSON file exists (./assets/json/2502.14494.json), skip PDF parsing.
[24.02.2025 10:12] Paper image links file exists (./assets/img_data/2502.14494.json), skip HTML parsing.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.15027.
[24.02.2025 10:12] Extra JSON file exists (./assets/json/2502.15027.json), skip PDF parsing.
[24.02.2025 10:12] Paper image links file exists (./assets/img_data/2502.15027.json), skip HTML parsing.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.15657.
[24.02.2025 10:12] Extra JSON file exists (./assets/json/2502.15657.json), skip PDF parsing.
[24.02.2025 10:12] Paper image links file exists (./assets/img_data/2502.15657.json), skip HTML parsing.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.15631.
[24.02.2025 10:12] Extra JSON file exists (./assets/json/2502.15631.json), skip PDF parsing.
[24.02.2025 10:12] Paper image links file exists (./assets/img_data/2502.15631.json), skip HTML parsing.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.14905.
[24.02.2025 10:12] Extra JSON file exists (./assets/json/2502.14905.json), skip PDF parsing.
[24.02.2025 10:12] Paper image links file exists (./assets/img_data/2502.14905.json), skip HTML parsing.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.15681.
[24.02.2025 10:12] Extra JSON file exists (./assets/json/2502.15681.json), skip PDF parsing.
[24.02.2025 10:12] Paper image links file exists (./assets/img_data/2502.15681.json), skip HTML parsing.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.13189.
[24.02.2025 10:12] Downloading paper 2502.13189 from http://arxiv.org/pdf/2502.13189v1...
[24.02.2025 10:12] Extracting affiliations from text.
[24.02.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 9 8 1 3 1 . 2 0 5 2 : r MOBA: MIXTURE OF BLOCK ATTENTION FOR LONG-CONTEXT LLMS Enzhe Lu1 Zhejun Jiang1 Jingyuan Liu1 Yulun Du1 Tao Jiang1 Chao Hong1 Shaowei Liu1 Weiran He1 Enming Yuan1 Yuzhi Wang1 Zhiqi Huang1 Huan Yuan1 Suting Xu1 Xinran Xu1 Guokun Lai1 Yanru Chen1 Huabin Zheng1 Junjie Yan1 Jianlin Su1 Yuxin Wu1 Neo Y. Zhang1 Zhilin Yang1 Jiezhong Qiu3, Xinyu Zhou1, Mingxing Zhang2, 1 Moonshot AI 2 Tsinghua University 3 Zhejiang Lab/Zhejiang University "
[24.02.2025 10:12] Response: ```python
["Moonshot AI", "Tsinghua University", "Zhejiang Lab/Zhejiang University"]
```
[24.02.2025 10:12] Deleting PDF ./assets/pdf/2502.13189.pdf.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.13407.
[24.02.2025 10:12] Downloading paper 2502.13407 from http://arxiv.org/pdf/2502.13407v1...
[24.02.2025 10:12] Extracting affiliations from text.
[24.02.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 1 JL1-CD: New Benchmark for Remote Sensing Change Detection and Robust Multi-Teacher Knowledge Distillation Framework Ziyuan Liu, Ruifei Zhu, Long Gao, Yuanxiu Zhou, Jingyu Ma, and Yuantao Gu, Senior Member, IEEE 5 2 0 2 9 ] . [ 1 7 0 4 3 1 . 2 0 5 2 : r AbstractDeep learning has achieved significant success in the field of remote sensing image change detection (CD), yet two major challenges remain: the scarcity of sub-meter, all-inclusive open-source CD datasets, and the difficulty of achieving consistent and satisfactory detection results across images with varying change areas. To address these issues, we introduce the JL1-CD dataset, which contains 5,000 pairs of 512 512 pixel images with resolution of 0.5 to 0.75 meters. Additionally, we propose multi-teacher knowledge distillation (MTKD) framework for CD. Experimental results on the JL1-CD and SYSU-CD datasets demonstrate that the MTKD framework significantly improves the performance of CD models with various network architectures and parameter sizes, achieving new state-of-the-art results. The code is available at https://github.com/circleLZY/MTKD-CD. Index TermsKnowledge distillation, change detection, remote sensing. I. INTRODUCTION Remote sensing image change detection (CD) is technique used to detect and analyze surface changes by leveraging multi-temporal data [1]. Over the past few decades, it has been extensively studied and has become crucial tool for Earth surface observation. CD plays significant role in various fields, including land-use change updates, natural disaster assessment, environmental monitoring, and urban planning. Early traditional CD methods primarily relied on image processing techniques, detecting changes by directly comparing pixel values or spectral features between multi-temporal images. Examples include Image Differencing [2], Change Vector Analysis (CVA) [3], Principal Component Analysis (PCA) [4], KauthThomas (KT) transforms ["
[24.02.2025 10:12] Response: ```python
[]
```
[24.02.2025 10:12] Extracting affiliations from text.
[24.02.2025 10:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 1 JL1-CD: New Benchmark for Remote Sensing Change Detection and Robust Multi-Teacher Knowledge Distillation Framework Ziyuan Liu, Ruifei Zhu, Long Gao, Yuanxiu Zhou, Jingyu Ma, and Yuantao Gu, Senior Member, IEEE 5 2 0 2 9 ] . [ 1 7 0 4 3 1 . 2 0 5 2 : r AbstractDeep learning has achieved significant success in the field of remote sensing image change detection (CD), yet two major challenges remain: the scarcity of sub-meter, all-inclusive open-source CD datasets, and the difficulty of achieving consistent and satisfactory detection results across images with varying change areas. To address these issues, we introduce the JL1-CD dataset, which contains 5,000 pairs of 512 512 pixel images with resolution of 0.5 to 0.75 meters. Additionally, we propose multi-teacher knowledge distillation (MTKD) framework for CD. Experimental results on the JL1-CD and SYSU-CD datasets demonstrate that the MTKD framework significantly improves the performance of CD models with various network architectures and parameter sizes, achieving new state-of-the-art results. The code is available at https://github.com/circleLZY/MTKD-CD. Index TermsKnowledge distillation, change detection, remote sensing. I. INTRODUCTION Remote sensing image change detection (CD) is technique used to detect and analyze surface changes by leveraging multi-temporal data [1]. Over the past few decades, it has been extensively studied and has become crucial tool for Earth surface observation. CD plays significant role in various fields, including land-use change updates, natural disaster assessment, environmental monitoring, and urban planning. Early traditional CD methods primarily relied on image processing techniques, detecting changes by directly comparing pixel values or spectral features between multi-temporal images. Examples include Image Differencing [2], Change Vector Analysis (CVA) [3], Principal Component Analysis (PCA) [4], KauthThomas (KT) transforms [5], and Multivariate Alteration Detection (MAD) [6]. While these methods are simple and intuitive, they exhibit limited performance when handling complex change patterns, such as those affected by significant noise, lighting variations, or seasonal differences. With the rise of machine learning (ML), CD methods began to incorporate feature extraction and classifiers. Common techniques include Support Vector Machines (SVM) [7], Random Forest (RF) [8], K-means clustering [9] and so on. These machine learning approaches significantly improved detection Ziyuan Liu and Yuantao Gu are with the Department of Electronic Engineering, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China (e-mail: liuziyua22@mails.tsinghua.edu.cn; gyt@tsinghua.edu.cn). Ruifei Zhu, Long Gao, Yuanxiu Zhou, and Jingyu Ma are with Chang Guang Satellite Technology Co., Ltd. (CGSTL) Changchun 130102, China (e-mail: zhuruifei@jl1.cn; gaolong1056@jl1.cn; zhouyuanxiu@jl1.cn; majingyu@jl1.cn). (Corresponding author: Yuantao Gu.) accuracy but heavily relied on high-quality labeled data and manually designed features. In recent years, the rapid advancement of deep learning (DL) has revolutionized remote sensing CD, delivering substantial performance breakthroughs. DL-based CD methods generally involve three steps: (1) extracting change features from image pairs, (2) generating change maps based on the extracted features, and (3) predicting labels based on the feature maps. Convolutional Neural Networks (CNNs), which achieved remarkable success in image processing, were the first neural network architecture applied to remote sensing CD and remain widely optimized and utilized today [10] [12]. With the introduction of Transformers, some studies have explored their application in CD tasks [13], [14]. More recently, Foundational Model (FM) has emerged as novel paradigm, aiming to achieve multi-task and multidomain generalization through large-scale pretraining [15]. There have been works utilizing remote sensing data to finetune pretrained models such as Vision Transformers (ViT) [16], Segment Anything Model (SAM) [17], and Contrastive Language-Image Pretraining (CLIP) [18], achieving higher performance in CD tasks [19][22]. Several semi-supervised methods have also been proposed, further improving CD performance under limited labeled data. Zhang et al. [23] propose novel multilevel consistency-regularization-based semi-supervised CD approach, incorporating Fourier-based frequency transformation and dynamic pseudolabel selection scheme to mitigate background noise and improve unlabeled data utilization. Xu et al. [24] introduce semi-supervised label and embedding consistency network (SS-LEC) for ORSI scene classification, which strategically enforces consistency across augmentations and stages of training. Li et al. [25] propose SemiCD-VL, VLM-guided semi-supervised change detection method that synthesizes pseudo labels via mixed change event generation strategy, achieving significant performance gains over FixMatch and SOTA unsupervised methods. However, DL-based CD methods generally face two major challenges: the scarcity of high-quality, high-resolution, all-inclusive CD datasets and limitations in handling highly dynamic change areas. Although numerous CD datasets have been constructed and proposed, they are often tailored to specific scenarios, which restricts the generalization capabilities of the algorithms. For instance, models trained on datasets focused on human-induced changes often fail to perform effectively when confronted with natural change scenarios. On the other hand, the learning capacity of these models IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 2 Fig. 1. Timeline of the development of mainstream DL-based CD methods. is inherently limited. Most existing algorithms rely on single-phase training approach, typically end-to-end training, supplemented by techniques such as learning rate decay. While such training strategies can achieve satisfactory results within the models performance constrained range of changes, significantly degrades when addressing scenarios with wide variations in change areas, ranging from no change to complete change. To address the aforementioned challenges, we construct new large-scale, high-resolution, all-inclusive open-source CD dataset, named JL1-CD (named after the Jilin-1 satellite). This dataset comprises 5,000 pairs of 512 512 images captured"
[24.02.2025 10:12] Mistral response. {"id": "54109cce3ac44fadb72b325dcba6df1e", "object": "chat.completion", "created": 1740391937, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Department of Electronic Engineering, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China\",\n    \"Chang Guang Satellite Technology Co., Ltd. (CGSTL) Changchun 130102, China\"\n]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1649, "total_tokens": 1729, "completion_tokens": 80}}
[24.02.2025 10:12] Response: ```python
[
    "Department of Electronic Engineering, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China",
    "Chang Guang Satellite Technology Co., Ltd. (CGSTL) Changchun 130102, China"
]
```
[24.02.2025 10:12] Deleting PDF ./assets/pdf/2502.13407.pdf.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.15011.
[24.02.2025 10:12] Extra JSON file exists (./assets/json/2502.15011.json), skip PDF parsing.
[24.02.2025 10:12] Paper image links file exists (./assets/img_data/2502.15011.json), skip HTML parsing.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2502.15082.
[24.02.2025 10:12] Extra JSON file exists (./assets/json/2502.15082.json), skip PDF parsing.
[24.02.2025 10:12] Paper image links file exists (./assets/img_data/2502.15082.json), skip HTML parsing.
[24.02.2025 10:12] Success.
[24.02.2025 10:12] Enriching papers with extra data.
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 0. Large Language Models (LLMs) have demonstrated exceptional comprehension capabilities and a vast knowledge base, suggesting that LLMs can serve as efficient tools for automated survey generation. However, recent research related to automated survey generation remains constrained by some critical lim...
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 1. World models that forecast environmental changes from actions are vital for autonomous driving models with strong generalization. The prevailing driving world model mainly build on video prediction model. Although these models can produce high-fidelity video sequences with advanced diffusion-based g...
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 2. Understanding molecules is key to understanding organisms and driving advances in drug discovery, requiring interdisciplinary knowledge across chemistry and biology. Although large molecular language models have achieved notable success in interpreting molecular structures, their instruction dataset...
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 3. We introduce methods to quantify how Large Language Models (LLMs) encode and store contextual information, revealing that tokens often seen as minor (e.g., determiners, punctuation) carry surprisingly high context. Notably, removing these tokens -- especially stopwords, articles, and commas -- consi...
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 4. We introduce PhotoDoodle, a novel image editing framework designed to facilitate photo doodling by enabling artists to overlay decorative elements onto photographs. Photo doodling is challenging because the inserted elements must appear seamlessly integrated with the background, requiring realistic ...
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 5. This paper identifies the misinterpretation of the context can be a significant issue during the reasoning process of large language models, spanning from smaller models like Llama3.2-3B-Instruct to cutting-edge ones like DeepSeek-R1. For example, in the phrase "10 dollars per kilo," LLMs might not ...
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 6. Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are...
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 7. Large language models (LLMs) have shown remarkable performance in complex reasoning tasks, but their efficiency is hindered by the substantial memory and computational costs associated with generating lengthy tokens. In this paper, we propose LightThinker, a novel method that enables LLMs to dynamic...
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 8. Multi-turn instruction following capability constitutes a core competency of large language models (LLMs) in real-world applications. Existing evaluation benchmarks predominantly focus on fine-grained constraint satisfaction and domain-specific capability assessment, yet overlook the crucial structu...
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 9. Existing benchmarks do not test Large Multimodal Models (LMMs) on their interactive intelligence with human users which is vital for developing general-purpose AI assistants. We design InterFeedback, an interactive framework, which can be applied to any LMM and dataset to assess this ability autonom...
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 10. The leading AI companies are increasingly focused on building generalist AI agents -- systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite how useful these systems might be, unchecked AI agency poses significant risks to public safety and...
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 11. Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token usage and accuracy gains. In particular, when comparing models across ...
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 12. In this paper, we address the challenge of enforcing strict schema adherence in large language model (LLM) generation by leveraging LLM reasoning capabilities. Building on the DeepSeek R1 reinforcement learning framework, our approach trains structured reasoning skills of a 1.5B parameter model thro...
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 13. Sampling from diffusion models involves a slow iterative process that hinders their practical deployment, especially for interactive applications. To accelerate generation speed, recent approaches distill a multi-step diffusion model into a single-step student generator via variational score distill...
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 14. Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches eit...
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 15. Deep learning has achieved significant success in the field of remote sensing image change detection (CD), yet two major challenges remain: the scarcity of sub-meter, all-inclusive open-source CD datasets, and the difficulty of achieving consistent and satisfactory detection results across images wi...
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 16. Multi-modal 3D object understanding has gained significant attention, yet current approaches often assume complete data availability and rigid alignment across all modalities. We present CrossOver, a novel framework for cross-modal 3D scene understanding via flexible, scene-level modality alignment....
[24.02.2025 10:12] ********************************************************************************
[24.02.2025 10:12] Abstract 17. User specifications or legal frameworks often require information to be removed from pretrained models, including large language models (LLMs). This requires deleting or "forgetting" a set of data points from an already-trained model, which typically degrades its performance on other data points. Th...
[24.02.2025 10:12] Read previous papers.
[24.02.2025 10:12] Generating reviews via LLM API.
[24.02.2025 10:12] Using data from previous issue: {"categories": ["#data", "#survey", "#benchmark", "#dataset", "#multimodal"], "emoji": "üìä", "ru": {"title": "SurveyX: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Å–æ–∑–¥–∞–Ω–∏–∏ –Ω–∞—É—á–Ω—ã—Ö –æ–±–∑–æ—Ä–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ SurveyX - —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –æ–±–∑–æ—Ä–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ
[24.02.2025 10:12] Using data from previous issue: {"categories": ["#video", "#diffusion", "#architecture", "#long_context", "#games", "#dataset", "#benchmark"], "emoji": "üöó", "ru": {"title": "MaskGWM: –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å MaskGWM –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã –≤ 
[24.02.2025 10:12] Using data from previous issue: {"categories": ["#data", "#dataset", "#science", "#agi", "#architecture", "#multimodal"], "emoji": "üß¨", "ru": {"title": "Mol-LLaMA: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –Ø–ú –¥–ª—è –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –º–æ–ª–µ–∫—É–ª", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Mol-LLaMA - –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –º–æ–ª–µ–∫—É–ª, –æ–±—É—á–µ–Ω–Ω—É—é –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É
[24.02.2025 10:12] Using data from previous issue: {"categories": ["#data", "#architecture", "#long_context", "#interpretability", "#training", "#open_source"], "emoji": "üî¨", "ru": {"title": "–°–∫—Ä—ã—Ç–∞—è —Å–∏–ª–∞ –º–µ–ª–æ—á–µ–π: –∫–∞–∫ –Ω–µ–∑–∞–º–µ—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤–ª–∏—è—é—Ç –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –º–µ—Ç–æ–¥—ã –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ç–æ–≥–æ, –∫–∞–∫ –±–æ–ª—å—à–∏–µ —è–∑—ã
[24.02.2025 10:12] Using data from previous issue: {"categories": ["#training", "#data", "#cv", "#dataset"], "emoji": "üé®", "ru": {"title": "PhotoDoodle: –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π", "desc": "PhotoDoodle - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è —Ö—É–¥–æ–∂–Ω–∏–∫–∞–º –Ω–∞–∫–ª–∞–¥—ã–≤–∞—Ç—å –¥–µ–∫–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞ —Ñ–æ—Ç–æ–≥—Ä
[24.02.2025 10:12] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#inference", "#math", "#training", "#reasoning", "#open_source"], "emoji": "üß†", "ru": {"title": "SIFT: –ù–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º SIFT (Stick to the Facts) –¥–ª—è —É
[24.02.2025 10:12] Using data from previous issue: {"categories": ["#benchmark", "#alignment", "#interpretability", "#multimodal", "#cv"], "emoji": "üëÅÔ∏è", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É –º–∞—à–∏–Ω–Ω—ã–º –∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –∑—Ä–µ–Ω–∏–µ–º –≤ –∑–∞–¥–∞—á–∞—Ö —Å–≤—è–∑—ã–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VLM^2-Bench - –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏
[24.02.2025 10:12] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#reasoning", "#long_context", "#optimization", "#data"], "emoji": "üß†", "ru": {"title": "LightThinker: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –º—ã—Å–ª–µ–π –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "LightThinker - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º
[24.02.2025 10:12] Using data from previous issue: {"categories": ["#alignment", "#open_source", "#benchmark"], "emoji": "üîÄ", "ru": {"title": "–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ LLM –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ StructFlowBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å–ª–µ–¥–æ–≤–∞—Ç—å –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –í –æ—Ç–ª–∏
[24.02.2025 10:12] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#rlhf", "#dataset", "#agi", "#multimodal", "#open_source"], "emoji": "ü§ñ", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç LMM: –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –æ—Ü–µ–Ω–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ InterFeedback –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç
[24.02.2025 10:12] Using data from previous issue: {"categories": ["#training", "#inference", "#agents", "#security", "#science", "#agi", "#ethics"], "emoji": "üî¨", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ò–ò: –æ—Ç –∞–≥–µ–Ω—Ç–æ–≤ –∫ —É—á–µ–Ω—ã–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –æ–±—Å—É–∂–¥–∞—é—Ç—Å—è —Ä–∏—Å–∫–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–æ–π –≥–µ–Ω–µ—Ä–∞–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –∏ –¥–µ–π—Å—Ç–≤–æ–≤
[24.02.2025 10:12] Using data from previous issue: {"categories": ["#reasoning", "#math", "#training", "#benchmark"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–∞–∂–Ω–µ–µ –∏—Ö –¥–ª–∏–Ω—ã –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É –¥–ª–∏–Ω–æ–π —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö 
[24.02.2025 10:12] Using data from previous issue: {"categories": ["#training", "#rl", "#synthetic", "#dataset", "#reasoning"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å—Ç—Ä–æ–≥–æ–≥–æ —Å–æ–±–ª—é–¥–µ–Ω–∏—è —Å—Ö–µ–º—ã –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è
[24.02.2025 10:12] Using data from previous issue: {"categories": ["#optimization", "#cv", "#diffusion", "#inference"], "emoji": "üñºÔ∏è", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—é f-–¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π f-distill. –ê–≤—Ç–æ—Ä—ã 
[24.02.2025 10:12] Querying the API.
[24.02.2025 10:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored.   In this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi's long-context requests and demonstrates significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA.
[24.02.2025 10:12] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –¥–ª–∏–Ω—ã –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Mixture of Block Attention (MoBA), –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø—ã Mixture of Experts –∫ –º–µ—Ö–∞–Ω–∏–∑–º—É –≤–Ω–∏–º–∞–Ω–∏—è. MoBA –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å, –Ω–∞ —á—Ç–æ –æ–±—Ä–∞—â–∞—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ, –±–µ–∑ –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–ª–∞–≤–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –º–µ–∂–¥—É –ø–æ–ª–Ω—ã–º –∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º.",

  "emoji": "üß†",

  "title": "MoBA: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è LLM –±–µ–∑ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤"
}
[24.02.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored.   In this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi's long-context requests and demonstrates significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA."

[24.02.2025 10:12] Response: ```python
['ARCHITECTURE', 'TRAINING']
```
[24.02.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored.   In this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi's long-context requests and demonstrates significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA."

[24.02.2025 10:12] Response: ```python
['AGI', 'LONG_CONTEXT', 'OPTIMIZATION']
```
[24.02.2025 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenge of scaling context length in large language models (LLMs) while managing computational complexity. Traditional attention mechanisms face a quadratic increase in resource demands, which can hinder performance. The authors propose Mixture of Block Attention (MoBA), a new method that allows models to autonomously determine their attention focus without imposing rigid structures. MoBA combines the principles of Mixture of Experts with attention, enabling efficient processing of long contexts and improving performance on complex reasoning tasks.","title":"Empowering LLMs with Efficient Context Scaling through MoBA"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenge of scaling context length in large language models (LLMs) while managing computational complexity. Traditional attention mechanisms face a quadratic increase in resource demands, which can hinder performance. The authors propose Mixture of Block Attention (MoBA), a new method that allows models to autonomously determine their attention focus without imposing rigid structures. MoBA combines the principles of Mixture of Experts with attention, enabling efficient processing of long contexts and improving performance on complex reasoning tasks.', title='Empowering LLMs with Efficient Context Scaling through MoBA'))
[24.02.2025 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁß∞‰∏∫Ê∑∑ÂêàÂùóÊ≥®ÊÑèÂäõÔºàMoBAÔºâÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜÈïø‰∏ä‰∏ãÊñáÊó∂ÁöÑÊïàÁéá„ÄÇ‰º†ÁªüÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Âú®ËÆ°ÁÆóÂ§çÊùÇÂ∫¶‰∏äÂëàÁé∞‰∫åÊ¨°Â¢ûÈïøÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊâ©Â±ïÊÄß„ÄÇMoBAÈÅµÂæ™‚ÄúÂ∞ëÁªìÊûÑ‚ÄùÂéüÂàôÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üËá™‰∏ªÂÜ≥ÂÆöÂÖ≥Ê≥®ÁöÑÂÜÖÂÆπÔºåËÄå‰∏çÊòØ‰æùËµñ‰∫éÈ¢ÑÂÆö‰πâÁöÑÂÅèËßÅ„ÄÇËØ•ÊñπÊ≥ïÂú®Èïø‰∏ä‰∏ãÊñá‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂ËÉΩÂ§üÂú®ÂÖ®Ê≥®ÊÑèÂäõÂíåÁ®ÄÁñèÊ≥®ÊÑèÂäõ‰πãÈó¥Êó†ÁºùÂàáÊç¢ÔºåÊèêÂçá‰∫ÜËÆ°ÁÆóÊïàÁéá„ÄÇ","title":"ÊèêÂçáÈïø‰∏ä‰∏ãÊñáÂ§ÑÁêÜÊïàÁéáÁöÑÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁß∞‰∏∫Ê∑∑ÂêàÂùóÊ≥®ÊÑèÂäõÔºàMoBAÔºâÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜÈïø‰∏ä‰∏ãÊñáÊó∂ÁöÑÊïàÁéá„ÄÇ‰º†ÁªüÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Âú®ËÆ°ÁÆóÂ§çÊùÇÂ∫¶‰∏äÂëàÁé∞‰∫åÊ¨°Â¢ûÈïøÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊâ©Â±ïÊÄß„ÄÇMoBAÈÅµÂæ™‚ÄúÂ∞ëÁªìÊûÑ‚ÄùÂéüÂàôÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üËá™‰∏ªÂÜ≥ÂÆöÂÖ≥Ê≥®ÁöÑÂÜÖÂÆπÔºåËÄå‰∏çÊòØ‰æùËµñ‰∫éÈ¢ÑÂÆö‰πâÁöÑÂÅèËßÅ„ÄÇËØ•ÊñπÊ≥ïÂú®Èïø‰∏ä‰∏ãÊñá‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂ËÉΩÂ§üÂú®ÂÖ®Ê≥®ÊÑèÂäõÂíåÁ®ÄÁñèÊ≥®ÊÑèÂäõ‰πãÈó¥Êó†ÁºùÂàáÊç¢ÔºåÊèêÂçá‰∫ÜËÆ°ÁÆóÊïàÁéá„ÄÇ', title='ÊèêÂçáÈïø‰∏ä‰∏ãÊñáÂ§ÑÁêÜÊïàÁéáÁöÑÊñ∞ÊñπÊ≥ï'))
[24.02.2025 10:12] Querying the API.
[24.02.2025 10:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Deep learning has achieved significant success in the field of remote sensing image change detection (CD), yet two major challenges remain: the scarcity of sub-meter, all-inclusive open-source CD datasets, and the difficulty of achieving consistent and satisfactory detection results across images with varying change areas. To address these issues, we introduce the JL1-CD dataset, which contains 5,000 pairs of 512 x 512 pixel images with a resolution of 0.5 to 0.75 meters. Additionally, we propose a multi-teacher knowledge distillation (MTKD) framework for CD. Experimental results on the JL1-CD and SYSU-CD datasets demonstrate that the MTKD framework significantly improves the performance of CD models with various network architectures and parameter sizes, achieving new state-of-the-art results. The code is available at https://github.com/circleLZY/MTKD-CD.
[24.02.2025 10:12] Response: {
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö JL1-CD –¥–ª—è –∑–∞–¥–∞—á–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö —Å–Ω–∏–º–∫–∞—Ö –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –º–Ω–æ–≥–æ—É—á–∏—Ç–µ–ª—å–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π (MTKD) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MTKD –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –î–æ—Å—Ç–∏–≥–Ω—É—Ç—ã –Ω–æ–≤—ã–µ –ø–µ—Ä–µ–¥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö JL1-CD –∏ SYSU-CD.",
  "emoji": "üõ∞Ô∏è",
  "title": "–ü—Ä–æ—Ä—ã–≤ –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö —Å–Ω–∏–º–∫–∞—Ö —Å –ø–æ–º–æ—â—å—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π"
}
[24.02.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Deep learning has achieved significant success in the field of remote sensing image change detection (CD), yet two major challenges remain: the scarcity of sub-meter, all-inclusive open-source CD datasets, and the difficulty of achieving consistent and satisfactory detection results across images with varying change areas. To address these issues, we introduce the JL1-CD dataset, which contains 5,000 pairs of 512 x 512 pixel images with a resolution of 0.5 to 0.75 meters. Additionally, we propose a multi-teacher knowledge distillation (MTKD) framework for CD. Experimental results on the JL1-CD and SYSU-CD datasets demonstrate that the MTKD framework significantly improves the performance of CD models with various network architectures and parameter sizes, achieving new state-of-the-art results. The code is available at https://github.com/circleLZY/MTKD-CD."

[24.02.2025 10:12] Response: ```python
["DATASET", "TRAINING"]
```
[24.02.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Deep learning has achieved significant success in the field of remote sensing image change detection (CD), yet two major challenges remain: the scarcity of sub-meter, all-inclusive open-source CD datasets, and the difficulty of achieving consistent and satisfactory detection results across images with varying change areas. To address these issues, we introduce the JL1-CD dataset, which contains 5,000 pairs of 512 x 512 pixel images with a resolution of 0.5 to 0.75 meters. Additionally, we propose a multi-teacher knowledge distillation (MTKD) framework for CD. Experimental results on the JL1-CD and SYSU-CD datasets demonstrate that the MTKD framework significantly improves the performance of CD models with various network architectures and parameter sizes, achieving new state-of-the-art results. The code is available at https://github.com/circleLZY/MTKD-CD."

[24.02.2025 10:12] Response: ```python
['OPEN_SOURCE', 'TRANSFER_LEARNING']
```
[24.02.2025 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses challenges in remote sensing image change detection (CD) by introducing the JL1-CD dataset, which consists of 5,000 pairs of high-resolution images. The dataset aims to provide a comprehensive resource for training models, overcoming the issue of limited open-source datasets. Additionally, the authors propose a multi-teacher knowledge distillation (MTKD) framework that enhances the performance of CD models across different architectures. Experimental results show that this framework achieves state-of-the-art performance on both the JL1-CD and SYSU-CD datasets.","title":"Enhancing Change Detection with JL1-CD and MTKD Framework"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses challenges in remote sensing image change detection (CD) by introducing the JL1-CD dataset, which consists of 5,000 pairs of high-resolution images. The dataset aims to provide a comprehensive resource for training models, overcoming the issue of limited open-source datasets. Additionally, the authors propose a multi-teacher knowledge distillation (MTKD) framework that enhances the performance of CD models across different architectures. Experimental results show that this framework achieves state-of-the-art performance on both the JL1-CD and SYSU-CD datasets.', title='Enhancing Change Detection with JL1-CD and MTKD Framework'))
[24.02.2025 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Ê∑±Â∫¶Â≠¶‰π†Âú®ÈÅ•ÊÑüÂõæÂÉèÂèòÂåñÊ£ÄÊµãÈ¢ÜÂüüÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºå‰ΩÜ‰ªçÈù¢‰∏¥‰∏§‰∏™‰∏ªË¶ÅÊåëÊàòÔºöÁº∫‰πè‰∫öÁ±≥Á∫ß„ÄÅÂÖ®Èù¢ÁöÑÂºÄÊ∫êÂèòÂåñÊ£ÄÊµãÊï∞ÊçÆÈõÜÔºå‰ª•ÂèäÂú®ÂèòÂåñÂå∫Âüü‰∏çÂêåÁöÑÂõæÂÉè‰∏≠ÂÆûÁé∞‰∏ÄËá¥‰∏î‰ª§‰∫∫Êª°ÊÑèÁöÑÊ£ÄÊµãÁªìÊûúÁöÑÂõ∞Èöæ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜJL1-CDÊï∞ÊçÆÈõÜÔºåÂåÖÂê´5000ÂØπ512 x 512ÂÉèÁ¥†ÁöÑÂõæÂÉèÔºåÂàÜËæ®Áéá‰∏∫0.5Âà∞0.75Á±≥„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§öÊïôÂ∏àÁü•ËØÜËí∏È¶èÔºàMTKDÔºâÊ°ÜÊû∂Áî®‰∫éÂèòÂåñÊ£ÄÊµã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMTKDÊ°ÜÊû∂ÊòæËëóÊèêÈ´ò‰∫ÜÂêÑÁßçÁΩëÁªúÊû∂ÊûÑÂíåÂèÇÊï∞ËßÑÊ®°ÁöÑÂèòÂåñÊ£ÄÊµãÊ®°ÂûãÁöÑÊÄßËÉΩÔºåËææÂà∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÁªìÊûú„ÄÇ","title":"ÊèêÂçáÈÅ•ÊÑüÂèòÂåñÊ£ÄÊµãÁöÑÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Ê∑±Â∫¶Â≠¶‰π†Âú®ÈÅ•ÊÑüÂõæÂÉèÂèòÂåñÊ£ÄÊµãÈ¢ÜÂüüÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºå‰ΩÜ‰ªçÈù¢‰∏¥‰∏§‰∏™‰∏ªË¶ÅÊåëÊàòÔºöÁº∫‰πè‰∫öÁ±≥Á∫ß„ÄÅÂÖ®Èù¢ÁöÑÂºÄÊ∫êÂèòÂåñÊ£ÄÊµãÊï∞ÊçÆÈõÜÔºå‰ª•ÂèäÂú®ÂèòÂåñÂå∫Âüü‰∏çÂêåÁöÑÂõæÂÉè‰∏≠ÂÆûÁé∞‰∏ÄËá¥‰∏î‰ª§‰∫∫Êª°ÊÑèÁöÑÊ£ÄÊµãÁªìÊûúÁöÑÂõ∞Èöæ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜJL1-CDÊï∞ÊçÆÈõÜÔºåÂåÖÂê´5000ÂØπ512 x 512ÂÉèÁ¥†ÁöÑÂõæÂÉèÔºåÂàÜËæ®Áéá‰∏∫0.5Âà∞0.75Á±≥„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§öÊïôÂ∏àÁü•ËØÜËí∏È¶èÔºàMTKDÔºâÊ°ÜÊû∂Áî®‰∫éÂèòÂåñÊ£ÄÊµã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMTKDÊ°ÜÊû∂ÊòæËëóÊèêÈ´ò‰∫ÜÂêÑÁßçÁΩëÁªúÊû∂ÊûÑÂíåÂèÇÊï∞ËßÑÊ®°ÁöÑÂèòÂåñÊ£ÄÊµãÊ®°ÂûãÁöÑÊÄßËÉΩÔºåËææÂà∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÁªìÊûú„ÄÇ', title='ÊèêÂçáÈÅ•ÊÑüÂèòÂåñÊ£ÄÊµãÁöÑÊñ∞ÊñπÊ≥ï'))
[24.02.2025 10:12] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#3d", "#training"], "emoji": "üåê", "ru": {"title": "–ì–∏–±–∫–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è 3D-—Å—Ü–µ–Ω", "desc": "CrossOver - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è 3D-—Å—Ü–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≥–∏–±–∫–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å—Ü–µ–Ω—ã. –û–Ω–∞ —Å–æ–∑
[24.02.2025 10:12] Using data from previous issue: {"categories": ["#training", "#inference", "#leakage", "#optimization", "#data"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∑–∞–±—ã–≤–∞–Ω–∏–µ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∑–Ω–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç–æ–¥ UPCORE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —É–¥–∞–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –æ–±—É—á–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. UPCORE –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç —É—Ö—É–¥—à–µ–Ω–∏–µ —Ä–∞
[24.02.2025 10:12] Loading Chinese text from previous data.
[24.02.2025 10:12] Renaming data file.
[24.02.2025 10:12] Renaming previous data. hf_papers.json to ./d/2025-02-24.json
[24.02.2025 10:12] Saving new data file.
[24.02.2025 10:12] Generating page.
[24.02.2025 10:12] Renaming previous page.
[24.02.2025 10:12] Renaming previous data. index.html to ./d/2025-02-24.html
[24.02.2025 10:12] [Experimental] Generating Chinese page for reading.
[24.02.2025 10:12] Chinese vocab [{'word': 'Ëá™Âä®Âåñ', 'pinyin': 'z√¨d√≤nghu√†', 'trans': 'automated'}, {'word': 'ÈóÆÂç∑', 'pinyin': 'w√®nju√†n', 'trans': 'questionnaire'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generate'}, {'word': 'Á≥ªÁªü', 'pinyin': 'x√¨t«íng', 'trans': 'system'}, {'word': 'ÂºïÂÖ•', 'pinyin': 'y«ênr√π', 'trans': 'introduce'}, {'word': 'Âú®Á∫ø', 'pinyin': 'z√†ixi√†n', 'trans': 'online'}, {'word': 'ÂèÇËÄÉ', 'pinyin': 'cƒÅnk«éo', 'trans': 'reference'}, {'word': 'Ê£ÄÁ¥¢', 'pinyin': 'ji«énsu«í', 'trans': 'retrieval'}, {'word': 'È¢ÑÂ§ÑÁêÜ', 'pinyin': 'y√πch«îl«ê', 'trans': 'preprocessing'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'}, {'word': 'AttributeTree', 'pinyin': '', 'trans': 'AttributeTree'}, {'word': 'Ê∂¶Ëâ≤', 'pinyin': 'r√πns√®', 'trans': 'polish'}, {'word': 'ËøáÁ®ã', 'pinyin': 'gu√≤ch√©ng', 'trans': 'process'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠gƒÅo', 'trans': 'improve'}, {'word': 'ÊïàÊûú', 'pinyin': 'xi√†ogu«í', 'trans': 'effect'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√©gu«í', 'trans': 'result'}, {'word': 'Ë°®Êòé', 'pinyin': 'bi«éom√≠ng', 'trans': 'indicate'}, {'word': 'ÂÜÖÂÆπ', 'pinyin': 'n√®ir√≥ng', 'trans': 'content'}, {'word': 'Ë¥®Èáè', 'pinyin': 'zh√¨li√†ng', 'trans': 'quality'}, {'word': 'ÂºïÁî®', 'pinyin': 'y«êny√≤ng', 'trans': 'citation'}, {'word': 'Áé∞Êúâ', 'pinyin': 'xi√†ny«íu', 'trans': 'existing'}, {'word': 'Êé•Ëøë', 'pinyin': 'jiƒìj√¨n', 'trans': 'close to'}, {'word': '‰∫∫Á±ª', 'pinyin': 'r√©nl√®i', 'trans': 'human'}, {'word': '‰∏ìÂÆ∂', 'pinyin': 'zhuƒÅnjiƒÅ', 'trans': 'expert'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éoxi√†n', 'trans': 'performance'}, {'word': 'ÊèêÂà∞', 'pinyin': 't√≠d√†o', 'trans': 'mention'}, {'word': 'Á§∫‰æã', 'pinyin': 'sh√¨l√¨', 'trans': 'example'}, {'word': 'ÊâæÂà∞', 'pinyin': 'zh«éod√†o', 'trans': 'find'}]
[24.02.2025 10:12] Renaming previous Chinese page.
[24.02.2025 10:12] Renaming previous data. zh.html to ./d/2025-02-23_zh_reading_task.html
[24.02.2025 10:12] Writing Chinese reading task.
[24.02.2025 10:12] Writing result.
[24.02.2025 10:12] Renaming log file.
[24.02.2025 10:12] Renaming previous data. log.txt to ./logs/2025-02-24_last_log.txt
