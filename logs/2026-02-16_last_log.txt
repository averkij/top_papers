[16.02.2026 11:35] Read previous papers.
[16.02.2026 11:35] Generating top page (month).
[16.02.2026 11:35] Writing top page (month).
[16.02.2026 12:43] Read previous papers.
[16.02.2026 12:43] Get feed.
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10388
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11858
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12705
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08683
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12395
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12628
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11236
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13013
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11865
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12617
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04163
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12829
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12506
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11715
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13191
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12984
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13022
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12684
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12612
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11910
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11769
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11757
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04315
[16.02.2026 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03120
[16.02.2026 12:43] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.02.2026 12:43] No deleted papers detected.
[16.02.2026 12:43] Downloading and parsing papers (pdf, html). Total: 24.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.10388.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.10388.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.10388.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.11858.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.11858.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.11858.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.12705.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.12705.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.12705.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.08683.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.08683.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.08683.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.12395.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.12395.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.12395.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.12628.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.12628.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.12628.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.11236.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.11236.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.11236.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.13013.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.13013.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.13013.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.11865.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.11865.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.11865.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.12617.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.12617.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.12617.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.04163.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.04163.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.04163.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.12829.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.12829.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.12829.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.12506.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.12506.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.12506.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.11715.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.11715.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.11715.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.13191.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.13191.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.13191.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.12984.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.12984.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.12984.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.13022.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.13022.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.13022.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.12684.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.12684.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.12684.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.12612.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.12612.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.12612.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.11910.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.11910.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.11910.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.11769.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.11769.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.11769.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.11757.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.11757.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.11757.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.04315.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.04315.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.04315.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Downloading and parsing paper https://huggingface.co/papers/2602.03120.
[16.02.2026 12:43] Extra JSON file exists (./assets/json/2602.03120.json), skip PDF parsing.
[16.02.2026 12:43] Paper image links file exists (./assets/img_data/2602.03120.json), skip HTML parsing.
[16.02.2026 12:43] Success.
[16.02.2026 12:43] Enriching papers with extra data.
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 0. Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures.  					AI-generated summary 				 The diversity of post-training data is critical for eff...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 1. Region-to-Image Distillation enables fine-grained visual perception in MLLMs by training models to internally perform iterative zooming during inference, eliminating the need for repeated tool calls and visual re-encoding while maintaining high performance across multiple benchmarks.  					AI-genera...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 2. MedXIAOHE is a medical vision-language foundation model that enhances clinical understanding through entity-aware continual pretraining, reinforcement learning, and tool-augmented agentic training for reliable diagnostic reasoning.  					AI-generated summary 				 We present MedXIAOHE, a medical visi...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 3. Visual understanding can be improved by aligning architectures with information-theoretic principles of video compression, using sparsity-driven encoding that outperforms traditional approaches in efficiency and accuracy.  					AI-generated summary 				 Hypothesis. Artificial general intelligence is...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 4. Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benc...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 5. Reinforcement learning-based sim-real co-training framework improves vision-language-action policy performance through interactive simulation and real-world data anchoring.  					AI-generated summary 				 Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, ...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 6. ABot-M0 presents a unified framework for embodied agent development that standardizes diverse robotic data and employs action manifold learning to improve prediction efficiency and stability.  					AI-generated summary 				 Building general-purpose embodied agents across diverse hardware remains a c...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 7. A large-scale dataset and model for fine-grained audiovisual understanding are introduced, demonstrating improved caption quality and reduced hallucinations through structured annotations and supervised fine-tuning.  					AI-generated summary 				 Universal video understanding requires modeling fine...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 8. AI agents require adaptive frameworks for task decomposition and delegation that can dynamically respond to environmental changes and handle unexpected failures through structured authority transfer and trust mechanisms.  					AI-generated summary 				 AI agents are able to tackle increasingly compl...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 9. GeoAgent achieves superior geolocation reasoning performance through a specialized dataset and reward mechanisms that ensure geographic accuracy and reasoning consistency.  					AI-generated summary 				 This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 10. Bit-Plane Decomposition Quantization (BPDQ) improves low-bit quantization by using variable quantization grids derived from bit-planes and scalar coefficients, achieving better accuracy than traditional methods in resource-constrained LLM inference.  					AI-generated summary 				 Large language mod...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 11. Field Least-Energy Actor-Critic (FLAC) addresses challenges in maximum entropy reinforcement learning with iterative generative policies by using kinetic energy as a proxy for policy stochasticity regulation through a generalized Schr√∂dinger bridge formulation.  					AI-generated summary 				 Iterat...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 12. Reinforcement learning (RL) fine-tuning has become a key technique for enhancing large language models (LLMs) on reasoning-intensive tasks, motivating its extension to vision language models (VLMs). While RL-tuned VLMs improve on visual reasoning benchmarks, they remain vulnerable to weak visual gro...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 13. Diffusion large language models (dLLMs) for CUDA kernel generation achieve superior performance through a specialized dataset and reinforcement learning framework.  					AI-generated summary 				 Diffusion large language models (dLLMs) have emerged as a compelling alternative to autoregressive (AR) ...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 14. Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, proce...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 15. SciAgentGym and SciAgentBench enable evaluation of scientific tool-use capabilities, while SciForge improves agent performance through dependency graph modeling of tool interactions.  					AI-generated summary 				 Scientific reasoning inherently demands integrating sophisticated toolkits to navigat...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 16. Mapping individual tree crowns is essential for tasks such as maintaining urban tree inventories and monitoring forest health, which help us understand and care for our environment. However, automatically separating the crowns from each other in aerial imagery is challenging due to factors such as t...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 17. A vision-language-action model for robotics combines large-scale pretraining with specialized training techniques to enable real-time execution and high-performance manipulation tasks.  					AI-generated summary 				 In this report, we introduce Xiaomi-Robotics-0, an advanced vision-language-action ...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 18. Traditional methods for automating recommender system design, such as Neural Architecture Search (NAS), are often constrained by a fixed search space defined by human priors, limiting innovation to pre-defined operators. While recent LLM-driven code evolution frameworks shift fixed search space targ...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 19. Research reveals that specific attention layers in audio diffusion models control distinct musical concepts, enabling precise manipulation of audio features through activation steering.  					AI-generated summary 				 Audio diffusion models can synthesize high-fidelity music from text, yet their int...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 20. Light4D enables consistent 4D video synthesis under target illumination through disentangled flow guidance and temporal consistent attention mechanisms.  					AI-generated summary 				 Recent advances in diffusion-based generative models have established a new paradigm for image and video relighting...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 21. Code2Worlds enables 4D dynamic scene generation by formulating it as language-to-simulation code generation with a dual-stream architecture and physics-aware closed-loop refinement.  					AI-generated summary 				 Achieving spatial intelligence requires moving beyond visual plausibility to build wor...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 22. GeneralVLA is a hierarchical vision-language-action model that enables zero-shot robotic manipulation through knowledge-guided trajectory planning without requiring real-world data collection.  					AI-generated summary 				 Large foundation models have shown strong open-world generalization to comp...
[16.02.2026 12:43] ********************************************************************************
[16.02.2026 12:43] Abstract 23. Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on memory-constrained devices, yet it renders models static and difficult to fine-tune. Standard fine-tuning paradigms, including Reinforcement Learning (RL), fundamentally rely on backpropagation and high-preci...
[16.02.2026 12:43] Read previous papers.
[16.02.2026 12:43] Generating reviews via LLM API.
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#training", "#interpretability", "#optimization", "#data", "#multimodal", "#synthetic"], "emoji": "üß©", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞ Feature Activation Coverage (FAC) –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–∞–Ω
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#multimodal", "#dataset", "#training"], "emoji": "üîç", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –º–µ–ª–∫–æ–∑—ë—Ä–Ω–∏—Å—Ç–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –µ–¥–∏–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –º–æ–¥–µ–ª–∏", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Region-to-Image Distillation –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ 
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#hallucinations", "#reasoning", "#benchmark", "#science", "#open_source", "#agents", "#cv", "#multimodal", "#rl", "#training", "#healthcare"], "emoji": "üè•", "ru": {"title": "–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –ò–ò —Å –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–º –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º", "desc": "MedXIAOHE ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è foundatio
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#cv", "#video", "#multimodal", "#architecture"], "emoji": "üé¨", "ru": {"title": "–°–∂–∞—Ç–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∫–∞–∫ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —Ç–µ–æ—Ä–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ –≤–∏–¥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#benchmark", "#multimodal", "#rl"], "emoji": "üîç", "ru": {"title": "–§—Ä–∞–Ω–∫–µ–Ω—à—Ç–µ–π–Ω-–∞–Ω–∞–ª–∏–∑: RL —É–ª—É—á—à–∞–µ—Ç –≥–ª—É–±–æ–∫–∏–µ —Å–ª–æ–∏, –∞ –Ω–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è, –∫–∞–∫–∏–µ –∏–º–µ–Ω–Ω–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —É–ª—É—á—à–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –≤ vision-langua
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#training", "#optimization", "#robotics", "#transfer_learning", "#multimodal", "#rl", "#synthetic"], "emoji": "ü§ñ", "ru": {"title": "–°–∏–º—É–ª—è—Ü–∏—è –∏ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å –≤–º–µ—Å—Ç–µ: —É—Å–∏–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∏ —è–∫–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#architecture", "#data", "#multimodal", "#3d", "#dataset", "#training", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–û–¥–∏–Ω –º–æ–∑–≥ –¥–ª—è –º–Ω–æ–≥–∏—Ö —Ç–µ–ª: —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏—è—Ö –¥–µ–π—Å—Ç–≤–∏–π", "desc": "ABot-M0 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Ä–∞–∑—Ä–∞–±
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#audio", "#hallucinations", "#synthetic", "#video", "#open_source", "#multimodal", "#dataset", "#training", "#data"], "emoji": "üé¨", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –¥–æ–≤–æ–¥–∫–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∏–¥–µ–æ –±–µ–∑ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#agents"], "emoji": "ü§ù", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö —Å –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏ –¥–æ–≤–µ—Ä–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–¥–∞—á –º–µ–∂–¥—É AI –∞–≥–µ–Ω—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Ä–µ–∞–≥–∏—Ä—É–µ—Ç –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã –∏ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#synthetic", "#reasoning", "#open_source", "#agents", "#rl", "#dataset", "#training"], "emoji": "üó∫Ô∏è", "ru": {"title": "–ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω GeoAgent ‚Äî –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω–∞—è —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –æ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization", "#open_source"], "emoji": "‚ö°", "ru": {"title": "–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ BPDQ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö —Å –æ–≥
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#architecture", "#diffusion", "#optimization", "#rl", "#training"], "emoji": "‚ö°", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —ç–Ω—Ç—Ä–æ–ø–∏–∏ —á–µ—Ä–µ–∑ –∫–∏–Ω–µ—Ç–∏—á–µ—Å–∫—É—é —ç–Ω–µ—Ä–≥–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Field Least-Energy Actor-Critic (FLAC) ‚Äî –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ —ç–Ω—Ç—Ä–æ–ø–∏–∏
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#hallucinations", "#interpretability", "#security", "#reasoning", "#benchmark", "#multimodal", "#rlhf", "#rl", "#alignment"], "emoji": "üéØ", "ru": {"title": "–í–µ—Ä–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏: –≤—ã—è–≤–ª–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#training", "#dataset", "#plp", "#architecture", "#rl"], "emoji": "‚ö°", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞: –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö CUDA —è–¥–µ—Ä", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (dLLM) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ CUDA —è–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#inference", "#training", "#video", "#multimodal", "#architecture"], "emoji": "üé¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –∫–æ–¥–µ–∫–æ–≤—ã–µ –ø—Ä–∏–º–∏—Ç–∏–≤—ã –≤–º–µ—Å—Ç–æ –ø–æ–ª–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∏–¥–µ–æ –≤ –≤–∏–¥–µ–æ—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–º
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#synthetic", "#reasoning", "#benchmark", "#science", "#agents", "#dataset", "#training"], "emoji": "üß™", "ru": {"title": "–ù–∞—É—á–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã SciAgentGym –∏ SciAgentBench ‚Äî —Å—Ä–µ–¥–∞ –∏ –Ω–∞–±–æ—Ä –±–µ–Ω—á–º
[16.02.2026 12:43] Using data from previous issue: {"categories": [], "emoji": "üå≥", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∫—Ä–æ–Ω –¥–µ—Ä–µ–≤—å–µ–≤ —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ –¥–∞–Ω–Ω—ã—Ö –ª–∞–∑–µ—Ä–Ω–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–æ–¥–µ–ª–µ–π –Ω—É–ª–µ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫—Ä–æ–Ω –¥–µ—Ä–µ–≤—å–µ–≤ –Ω–∞ –∞—ç—Ä–æ—Ñ–æ—Ç–æ—Å–Ω–∏–º–∫–∞—Ö. –ê–≤—Ç–æ—Ä—ã –∏
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#training", "#inference", "#robotics", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –≤–∏–¥–µ–Ω–∏–µ-—è–∑—ã–∫-–¥–µ–π—Å—Ç–≤–∏–µ –¥–ª—è –≤—ã—Å–æ–∫–æ—Å–∫–æ—Ä–æ—Å—Ç–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å –≤–∏–¥–µ–Ω–∏–µ-—è–∑—ã–∫-–¥–µ–π—Å—Ç–≤–∏–µ (VLA) –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏, –∫–æ—Ç–æ—Ä–∞
[16.02.2026 12:43] Using data from previous issue: {"categories": [], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —á–µ—Ä–µ–∑ –¥–∏–∞–ª–æ–≥ –º–µ–∂–¥—É —Å–∏–º—É–ª—è—Ç–æ—Ä–æ–º –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Self-EvolveRec ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç–≤–æ–ª—é—Ü–∏–∏ –∫–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#interpretability", "#diffusion"], "emoji": "üéµ", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º—É–∑—ã–∫–∞–ª—å–Ω—ã–º–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏ —á–µ—Ä–µ–∑ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ —Å–ª–æ–∏ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –∞—É–¥–∏–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—é—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º—É–∑—ã–∫–∞–ª—å–Ω—ã–µ
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#open_source", "#diffusion"], "emoji": "üí°", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ 4D –≤–∏–¥–µ–æ —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–º –æ—Å–≤–µ—â–µ–Ω–∏–µ–º –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "Light4D ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã—Ö 4D –≤–∏–¥–µ–æ —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –æ—Å–≤–µ—â–µ–Ω–∏–µ–º, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ú–µ—Ç–æ–¥ –≤–≤–æ–¥–∏—Ç —Ç–µ—Ö–Ω–∏–∫—É Disentangled
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#plp", "#benchmark", "#agents", "#multimodal", "#3d"], "emoji": "üåç", "ru": {"title": "–û—Ç —Ç–µ–∫—Å—Ç–∞ –∫ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Å—Ü–µ–Ω–µ —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –∫–æ–¥", "desc": "Code2Worlds ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 4D —Å—Ü–µ–Ω –ø—É—Ç—ë–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –≤ –∫–æ–¥ —Å
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#cv", "#robotics", "#multimodal", "#3d"], "emoji": "ü§ñ", "ru": {"title": "–†–æ–±–æ—Ç, –æ–±—É—á–∞—é—â–∏–π—Å—è –∏–∑ —Å–ª–æ–≤ –∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –±–µ–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤", "desc": "GeneralVLA ‚Äî —ç—Ç–æ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞-–¥–µ–π—Å—Ç–≤–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –±–æ–ª—å—à–∏–µ foundation models –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–æ–π –±–µ–∑
[16.02.2026 12:43] Using data from previous issue: {"categories": ["#open_source", "#inference", "#training", "#optimization", "#rl"], "emoji": "‚ö°", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Quantized Evolution Strategies (QES) –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã—Ö
[16.02.2026 12:43] Renaming data file.
[16.02.2026 12:43] Renaming previous data. hf_papers.json to ./d/2026-02-16.json
[16.02.2026 12:43] Saving new data file.
[16.02.2026 12:43] Generating page.
[16.02.2026 12:43] Renaming previous page.
[16.02.2026 12:43] Renaming previous data. index.html to ./d/2026-02-16.html
[16.02.2026 12:43] Writing result.
[16.02.2026 12:43] Renaming log file.
[16.02.2026 12:43] Renaming previous data. log.txt to ./logs/2026-02-16_last_log.txt
