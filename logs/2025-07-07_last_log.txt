[07.07.2025 16:15] Read previous papers.
[07.07.2025 16:15] Generating top page (month).
[07.07.2025 16:15] Writing top page (month).
[07.07.2025 17:11] Read previous papers.
[07.07.2025 17:11] Get feed.
[07.07.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.01955
[07.07.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02608
[07.07.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.01853
[07.07.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.00769
[07.07.2025 17:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[07.07.2025 17:11] No deleted papers detected.
[07.07.2025 17:11] Downloading and parsing papers (pdf, html). Total: 4.
[07.07.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2507.01955.
[07.07.2025 17:11] Extra JSON file exists (./assets/json/2507.01955.json), skip PDF parsing.
[07.07.2025 17:11] Paper image links file exists (./assets/img_data/2507.01955.json), skip HTML parsing.
[07.07.2025 17:11] Success.
[07.07.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2507.02608.
[07.07.2025 17:11] Extra JSON file exists (./assets/json/2507.02608.json), skip PDF parsing.
[07.07.2025 17:11] Paper image links file exists (./assets/img_data/2507.02608.json), skip HTML parsing.
[07.07.2025 17:11] Success.
[07.07.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2507.01853.
[07.07.2025 17:11] Extra JSON file exists (./assets/json/2507.01853.json), skip PDF parsing.
[07.07.2025 17:11] Paper image links file exists (./assets/img_data/2507.01853.json), skip HTML parsing.
[07.07.2025 17:11] Success.
[07.07.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2507.00769.
[07.07.2025 17:11] Downloading paper 2507.00769 from http://arxiv.org/pdf/2507.00769v1...
[07.07.2025 17:11] Extracting affiliations from text.
[07.07.2025 17:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 ] . [ 1 9 6 7 0 0 . 7 0 5 2 : r LitBench: Benchmark and Dataset for Reliable Evaluation of Creative Writing Daniel Fein Sebastian Russo Violet Xiang Kabir Jolly Rafael Rafailov Nick Haber "
[07.07.2025 17:11] Response: []
[07.07.2025 17:11] Extracting affiliations from text.
[07.07.2025 17:11] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 ] . [ 1 9 6 7 0 0 . 7 0 5 2 : r LitBench: Benchmark and Dataset for Reliable Evaluation of Creative Writing Daniel Fein Sebastian Russo Violet Xiang Kabir Jolly Rafael Rafailov Nick HaberEvaluating creative writing generated by large language models (LLMs) remains challenging because open-ended narratives lack ground truths. Without performant automated evaluation methods, off-the-shelf (OTS) language models are employed as zero-shot judges, yet their reliability is unclear in this context. In pursuit of robust evaluation for creative writing, we introduce LitBench, the first standardized benchmark and paired dataset for creative writing verification, comprising heldout test set of 2,480 debiased, human-labeled story comparisons drawn from Reddit and 43,827-pair training corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot LLM judges, (ii) train BradleyTerry and generative reward models, and (iii) conduct an online human study to validate reward model rankings on newly LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the strongest off-the-shelf judge, reaching 73% agreement with human preferences; among trained reward models, Bradley-Terry and Generative reward models both attain an accuracy of 78%, outperforming all off-the-shelf judges. An online human study further confirms that our trained reward models consistently align with human preferences in novel LLM-generated stories. We release LitBench and reward models here, providing vetted resource for reliable, automated evaluation and optimization of creative-writing systems.Automated verification with oracles or learned verifiers has catalyzed rapid progress in math and code generation [Hendrycks et al., 2021, Gao et al., 2024, Jimenez et al., 2023, Pan et al., 2024]. By contrast, creative writing is inherently divergent: given the same prompt, authors may produce different yet equally valid stories. The lack of ground truth labels hinders verification and, consequently, progress in creative writing generation. Evaluation by human experts with structured rubric is reliable, but it is expensive to collect such judgements, particularly at the scale of AIgenerated text [Chakrabarty et al., 2024]. In domains where human where ground truth are usually collected from human raters, LLM judges are often used ([Badshah and Sajjad, 2024]; [Son et al., 2024]). The agreement between LLM judgments and human preferences has been found to be reasonable in the contexts of dialog, helpfulness, and summarization tasks [Zheng et al., 2023]. But, they exhibit biases, such as favoring lengthy text [Wang et al., 2023], and lack of internal consistency [Wei et al., 2025]. Feuer et al. [2025] found that within these tasks, stylistic choices account for the judgments of language models more often than substance. This raises questions about the reliability of judges in the context of creative writing, where form and content are paramount. *Equal contribution. Correspondence: drfein@stanford.edu. Preprint. Under review. We introduce LitBench, the first standardized benchmark of high-quality, pairwise creative writing samples, derived from Reddits r/WritingPrompts. LitBench is designed to both evaluate existing zero-shot judges and enable the development of learned verifiers that better align with human preferences. Then, to study the gap between LLM-judges and trained reward models, we curate dataset of 43k further pairwise examples from r/WritingPrompts. LitBench evaluation reveals that small and open-source LLM-judges fail to evaluate creative writing accurately, but that some leading proprietary models are competitive with trained verifiers. Our investigation of various reward models reveals that generative reward models (GenRMs) are on-par with Bradley-Terry reward models in this domain. While [Mahan et al., 2024] found that training GenRMs with chain-of-thought (CoT) can lead to similar or even improved performance on some preference based benchmarks, such as RewardBench, we find it hinders performance in creative writing verification, even when CoTs are distilled from much stronger out-of-the-box verifier model. Additional human evaluation on LLM-generated stories validates that well-performing reward models on our benchmark can indeed judge creative quality. Our contributions are as follows. benchmark of 2.5k pairwise comparisons of human-written stories, coupled with filtered and labeled training dataset for verifiers consisting of 43k pairwise examples, along with generated rationales. Benchmarking of current approaches of creative writing verification, revealing that the best zero-shot LLM judge (Claude-Sonnet-3.7) underperformed small reward models (1B-7B) trained on our training set, suggesting we can get higher quality reward models at lower cost. Study of GenRMs showing that distilled chain-of-thought degrades performance for creative writing verification. Human evaluation validating that verifier performing well on LitBench can be used to select higher-quality creative writing.2.1 Verification Recently, math and coding benchmarks with ground truth labels have facilitated progress in these domains [Gao et al., 2024, Jimenez et al., 2023]. Cobbe et al. [2021] first used inference-time verification to bootstrap language model performance on GSM8K by ranking candidate solutions from generator. More recently, Costello et al. [2025] and Zelikman et al. [2024] have shown that ground truth pruning of generations and retraining can improve the latent ability to correctly solve math problems. Without ground truth labels, verification is difficult. Reinforcement learning from human feedback (RLHF) has emerged as the dominant paradigm to align model language and behavior with human taste and steer models to follow instructions [Ouyang et al., 2022, Stiennon et al., 2020]. Bai et al. [2022] developed lower-cost method of alignment with human preferences by substituting humans for language models in the feedback process. LLM-judges have been found to agree with human preferences in some contexts Zheng et al. [2023], Liu et al. [2023], though their agreement with humans in the evaluation of creative writing or other forms of artistic expression has not been systematically evaluated. In another attempt to avoid costly human preferences, [Ethayarajh et al., 2022a] released The Stanford Human Preferences (SHP) dataset, leveraging Reddit to distill human preferences for helpfulness using post and comm"
[07.07.2025 17:11] Mistral response. {"id": "498edea30fb1416fba98f8f9f40ac817", "object": "chat.completion", "created": 1751908281, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Stanford University\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1641, "total_tokens": 1654, "completion_tokens": 13}}
[07.07.2025 17:11] Response: ```python
["Stanford University"]
```
[07.07.2025 17:11] Deleting PDF ./assets/pdf/2507.00769.pdf.
[07.07.2025 17:11] Success.
[07.07.2025 17:11] Enriching papers with extra data.
[07.07.2025 17:11] ********************************************************************************
[07.07.2025 17:11] Abstract 0. Multimodal foundation models, despite being primarily trained on image-text tasks, demonstrate respectable performance across various vision tasks when adapted through prompt chaining, though they fall short compared to specialized models.  					AI-generated summary 				 Multimodal foundation models...
[07.07.2025 17:11] ********************************************************************************
[07.07.2025 17:11] Abstract 1. The use of latent space diffusion models for faster and accurate emulation of dynamical systems is viable, offering robustness to high compression rates and improved prediction diversity compared to non-generative approaches.  					AI-generated summary 				 The steep computational cost of diffusion ...
[07.07.2025 17:11] ********************************************************************************
[07.07.2025 17:11] Abstract 2. EKA-EVAL is a comprehensive multilingual evaluation framework for large language models, supporting diverse benchmarks and features for efficient distributed inference and GPU usage.  					AI-generated summary 				 The rapid advancement of Large Language Models (LLMs) has intensified the need for ev...
[07.07.2025 17:11] ********************************************************************************
[07.07.2025 17:11] Abstract 3. LitBench introduces a standardized benchmark for evaluating creative writing generated by language models, using human-labeled story comparisons and training reward models to assess and validate automated evaluation methods.  					AI-generated summary 				 Evaluating creative writing generated by la...
[07.07.2025 17:11] Read previous papers.
[07.07.2025 17:11] Generating reviews via LLM API.
[07.07.2025 17:11] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#multimodal", "#games", "#cv", "#reasoning", "#hallucinations"], "emoji": "🧠", "ru": {"title": "Универсальность мультимодальных моделей в компьютерном зрении: потенциал и ограничения", "desc": "Статья исследует эффективность мультимодальных фундаме
[07.07.2025 17:11] Using data from previous issue: {"categories": ["#diffusion", "#inference", "#data", "#architecture", "#training", "#optimization"], "emoji": "🌀", "ru": {"title": "Латентная диффузия для быстрой и точной эмуляции физических систем", "desc": "Статья исследует применение моделей диффузии в латентном пространстве для эмуляции динамич
[07.07.2025 17:11] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#open_source", "#benchmark"], "emoji": "🌏", "ru": {"title": "EKA-EVAL: Универсальная платформа для оценки многоязычных языковых моделей", "desc": "EKA-EVAL - это комплексная многоязычная система оценки больших языковых моделей (LLM), поддерживающая 
[07.07.2025 17:11] Using data from previous issue: {"categories": ["#story_generation", "#dataset", "#alignment", "#benchmark", "#optimization"], "emoji": "📝", "ru": {"title": "LitBench: новый стандарт оценки генеративного творческого письма", "desc": "LitBench представляет собой стандартизированный бенчмарк для оценки творческого письма, сгенериров
[07.07.2025 17:11] Renaming data file.
[07.07.2025 17:11] Renaming previous data. hf_papers.json to ./d/2025-07-07.json
[07.07.2025 17:11] Saving new data file.
[07.07.2025 17:11] Generating page.
[07.07.2025 17:11] Renaming previous page.
[07.07.2025 17:11] Renaming previous data. index.html to ./d/2025-07-07.html
[07.07.2025 17:11] Writing result.
[07.07.2025 17:11] Renaming log file.
[07.07.2025 17:11] Renaming previous data. log.txt to ./logs/2025-07-07_last_log.txt
