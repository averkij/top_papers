[12.08.2025 03:45] Read previous papers.
[12.08.2025 03:45] Generating top page (month).
[12.08.2025 03:45] Writing top page (month).
[12.08.2025 04:20] Read previous papers.
[12.08.2025 04:20] Get feed.
[12.08.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2508.07999
[12.08.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2508.07629
[12.08.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2508.05614
[12.08.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2508.06600
[12.08.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2508.08134
[12.08.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2508.07917
[12.08.2025 04:20] Extract page data from URL. URL: https://huggingface.co/papers/2508.06026
[12.08.2025 04:20] Extract page data from URL. URL: https://huggingface.co/papers/2508.07101
[12.08.2025 04:20] Extract page data from URL. URL: https://huggingface.co/papers/2508.07050
[12.08.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2508.06601
[12.08.2025 04:20] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[12.08.2025 04:20] No deleted papers detected.
[12.08.2025 04:20] Downloading and parsing papers (pdf, html). Total: 10.
[12.08.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2508.07999.
[12.08.2025 04:20] Extra JSON file exists (./assets/json/2508.07999.json), skip PDF parsing.
[12.08.2025 04:20] Paper image links file exists (./assets/img_data/2508.07999.json), skip HTML parsing.
[12.08.2025 04:20] Success.
[12.08.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2508.07629.
[12.08.2025 04:20] Extra JSON file exists (./assets/json/2508.07629.json), skip PDF parsing.
[12.08.2025 04:20] Paper image links file exists (./assets/img_data/2508.07629.json), skip HTML parsing.
[12.08.2025 04:20] Success.
[12.08.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2508.05614.
[12.08.2025 04:20] Extra JSON file exists (./assets/json/2508.05614.json), skip PDF parsing.
[12.08.2025 04:20] Paper image links file exists (./assets/img_data/2508.05614.json), skip HTML parsing.
[12.08.2025 04:20] Success.
[12.08.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2508.06600.
[12.08.2025 04:20] Extra JSON file exists (./assets/json/2508.06600.json), skip PDF parsing.
[12.08.2025 04:20] Paper image links file exists (./assets/img_data/2508.06600.json), skip HTML parsing.
[12.08.2025 04:20] Success.
[12.08.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2508.08134.
[12.08.2025 04:20] Extra JSON file exists (./assets/json/2508.08134.json), skip PDF parsing.
[12.08.2025 04:20] Paper image links file exists (./assets/img_data/2508.08134.json), skip HTML parsing.
[12.08.2025 04:20] Success.
[12.08.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2508.07917.
[12.08.2025 04:20] Extra JSON file exists (./assets/json/2508.07917.json), skip PDF parsing.
[12.08.2025 04:20] Paper image links file exists (./assets/img_data/2508.07917.json), skip HTML parsing.
[12.08.2025 04:20] Success.
[12.08.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2508.06026.
[12.08.2025 04:20] Downloading paper 2508.06026 from http://arxiv.org/pdf/2508.06026v1...
[12.08.2025 04:21] Extracting affiliations from text.
[12.08.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future Yidong Wang1*, Xin Wang1*, Cunxiang Wang2*, Junfeng Fang3, Qiufeng Wang4, Jianing Chu5, Xuran Meng6, Shuxun Yang7, Libo Qin8, Yue Zhang9, Wei Ye1, Shikun Zhang1 1Peking University, 2Tsinghua University, 3National University of Singapore, 4Southeast University, 5North Carolina State University, 6University of Michigan, 7Beijing Institute of Technology, 8Central South University 9Westlake University, 5 2 0 2 8 ] . [ 1 6 2 0 6 0 . 8 0 5 2 : r a "
[12.08.2025 04:21] Response: ```python
[
    "Peking University",
    "Tsinghua University",
    "National University of Singapore",
    "Southeast University",
    "North Carolina State University",
    "University of Michigan",
    "Beijing Institute of Technology",
    "Central South University",
    "Westlake University"
]
```
[12.08.2025 04:21] Deleting PDF ./assets/pdf/2508.06026.pdf.
[12.08.2025 04:21] Success.
[12.08.2025 04:21] Downloading and parsing paper https://huggingface.co/papers/2508.07101.
[12.08.2025 04:21] Downloading paper 2508.07101 from http://arxiv.org/pdf/2508.07101v1...
[12.08.2025 04:21] Extracting affiliations from text.
[12.08.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 ] . [ 1 1 0 1 7 0 . 8 0 5 2 : r a LESS IS MORE: TRAINING-FREE SPARSE ATTENTION WITH GLOBAL LOCALITY FOR EFFICIENT REASONING Lijie Yang Zhihao Zhang Arti Jain Shijie Cao Baihong Yuan Yiwei Chen Princeton University Zhihao Jia Ravi Netravali Carnegie Mellon University Microsoft Research ly3223@princeton.edu, zhihaoz3@cs.cmu.edu, shijiecao@microsoft.com zhihao@cmu.edu, rnetravali@cs.princeton.edu "
[12.08.2025 04:21] Response: ```python
["Princeton University", "Carnegie Mellon University", "Microsoft Research"]
```
[12.08.2025 04:21] Deleting PDF ./assets/pdf/2508.07101.pdf.
[12.08.2025 04:21] Success.
[12.08.2025 04:21] Downloading and parsing paper https://huggingface.co/papers/2508.07050.
[12.08.2025 04:21] Downloading paper 2508.07050 from http://arxiv.org/pdf/2508.07050v1...
[12.08.2025 04:21] Extracting affiliations from text.
[12.08.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability Wenhan Liu1, Xinyu Ma2, Weiwei Sun3, Yutao Zhu1, Yuchen Li2, Dawei Yin2, Zhicheng Dou1*, 1Gaoling School of Artificial Intelligence, Renmin University of China 2Baidu Inc. 3Carnegie Mellon University lwh@ruc.edu.cn, xinyuma2016@gmail.com, dou@ruc.edu.cn 5 2 0 2 9 ] I . [ 1 0 5 0 7 0 . 8 0 5 2 : r a "
[12.08.2025 04:21] Response: ```python
["Gaoling School of Artificial Intelligence, Renmin University of China", "Baidu Inc.", "Carnegie Mellon University"]
```
[12.08.2025 04:21] Deleting PDF ./assets/pdf/2508.07050.pdf.
[12.08.2025 04:21] Success.
[12.08.2025 04:21] Downloading and parsing paper https://huggingface.co/papers/2508.06601.
[12.08.2025 04:21] Extra JSON file exists (./assets/json/2508.06601.json), skip PDF parsing.
[12.08.2025 04:21] Paper image links file exists (./assets/img_data/2508.06601.json), skip HTML parsing.
[12.08.2025 04:21] Success.
[12.08.2025 04:21] Enriching papers with extra data.
[12.08.2025 04:21] ********************************************************************************
[12.08.2025 04:21] Abstract 0. WideSearch is a new benchmark evaluating the reliability of automated search agents in large-scale information collection tasks, revealing significant deficiencies in current systems.  					AI-generated summary 				 From professional research to everyday planning, many tasks are bottlenecked by wide...
[12.08.2025 04:21] ********************************************************************************
[12.08.2025 04:21] Abstract 1. Klear-Reasoner, a model with long reasoning capabilities, achieves high performance across benchmarks through detailed post-training workflows, including long Chain-of-Thought supervised fine-tuning and reinforcement learning with Gradient-Preserving clipping Policy Optimization.  					AI-generated ...
[12.08.2025 04:21] ********************************************************************************
[12.08.2025 04:21] Abstract 2. OmniEAR evaluates language models' embodied reasoning capabilities in physical interactions, tool usage, and multi-agent coordination, revealing performance degradation under constraints and highlighting architectural limitations.  					AI-generated summary 				 Large language models excel at abstra...
[12.08.2025 04:21] ********************************************************************************
[12.08.2025 04:21] Abstract 3. BrowseComp-Plus, a curated benchmark, enables controlled evaluation of deep research agents and retrieval methods, providing insights into their performance and effectiveness.  					AI-generated summary 				 Deep-Research agents, which integrate large language models (LLMs) with search tools, have s...
[12.08.2025 04:21] ********************************************************************************
[12.08.2025 04:21] Abstract 4. Follow-Your-Shape framework uses a Trajectory Divergence Map and Scheduled KV Injection to enable precise and controllable shape editing in images while preserving non-target content.  					AI-generated summary 				 While recent flow-based image editing models demonstrate general-purpose capabilitie...
[12.08.2025 04:21] ********************************************************************************
[12.08.2025 04:21] Abstract 5. Action Reasoning Models (ARMs) integrate perception, planning, and control to enable adaptable and explainable robotic behavior, achieving superior performance across various tasks and settings.  					AI-generated summary 				 Reasoning is central to purposeful action, yet most robotic foundation mo...
[12.08.2025 04:21] ********************************************************************************
[12.08.2025 04:21] Abstract 6. Temporal Self-Rewarding Language Models improve generative capabilities by strategically using past and future model outputs, enhancing preference learning and out-of-distribution generalization.  					AI-generated summary 				 Self-Rewarding Language Models propose an architecture in which the Larg...
[12.08.2025 04:21] ********************************************************************************
[12.08.2025 04:21] Abstract 7. LessIsMore is a training-free sparse attention mechanism that improves efficiency and generalization in reasoning tasks by aggregating token selections from local attention heads.  					AI-generated summary 				 Large reasoning models achieve strong performance through test-time scaling but incur su...
[12.08.2025 04:21] ********************************************************************************
[12.08.2025 04:21] Abstract 8. A reasoning-intensive reranker, ReasonRank, achieves state-of-the-art performance in passage ranking tasks by using synthesized training data and a two-stage post-training approach with reinforcement learning.  					AI-generated summary 				 Large Language Model (LLM) based listwise ranking has show...
[12.08.2025 04:21] ********************************************************************************
[12.08.2025 04:21] Abstract 9. Data filtering during pretraining enhances LLM resistance to adversarial fine-tuning attacks without degrading unrelated capabilities, offering a promising defense mechanism for open-weight AI systems.  					AI-generated summary 				 Open-weight AI systems offer unique benefits, including enhanced t...
[12.08.2025 04:21] Read previous papers.
[12.08.2025 04:21] Generating reviews via LLM API.
[12.08.2025 04:21] Using data from previous issue: {"categories": ["#science", "#benchmark", "#agents", "#dataset", "#survey"], "emoji": "🔍", "ru": {"title": "Поисковые агенты на основе ИИ пока не справляются с масштабными задачами", "desc": "WideSearch - это новый эталонный тест для оценки надежности автоматизированных поисковых агентов в задачах с
[12.08.2025 04:21] Using data from previous issue: {"categories": ["#training", "#long_context", "#optimization", "#math", "#plp", "#rl", "#reasoning", "#benchmark"], "emoji": "🧠", "ru": {"title": "Klear-Reasoner: Мощный ИИ для длительных рассуждений", "desc": "Klear-Reasoner - это модель с длительными способностями к рассуждению, достигающая высоко
[12.08.2025 04:21] Using data from previous issue: {"categories": ["#architecture", "#open_source", "#agents", "#reasoning", "#benchmark"], "emoji": "🤖", "ru": {"title": "OmniEAR: раскрывая ограничения языковых моделей в воплощенном рассуждении", "desc": "OmniEAR - это комплексная система оценки способностей языковых моделей к воплощенному рассужден
[12.08.2025 04:21] Using data from previous issue: {"categories": ["#interpretability", "#rag", "#ethics", "#benchmark", "#agents", "#open_source"], "emoji": "🔬", "ru": {"title": "BrowseComp-Plus: Прозрачная оценка агентов глубокого исследования", "desc": "BrowseComp-Plus - это новый эталонный тест для оценки агентов глубокого исследования и методов
[12.08.2025 04:21] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#cv", "#games"], "emoji": "🔄", "ru": {"title": "Точное редактирование форм объектов с сохранением фона", "desc": "Статья представляет новый фреймворк Follow-Your-Shape для точного и контролируемого редактирования форм объектов на изображениях. Метод ис
[12.08.2025 04:21] Using data from previous issue: {"categories": ["#open_source", "#robotics", "#agents", "#dataset", "#reasoning", "#training"], "emoji": "🤖", "ru": {"title": "MolmoAct: Разумные действия роботов через структурированное рассуждение", "desc": "Модель Action Reasoning Models (ARM) интегрирует восприятие, планирование и управление для
[12.08.2025 04:21] Querying the API.
[12.08.2025 04:21] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Temporal Self-Rewarding Language Models improve generative capabilities by strategically using past and future model outputs, enhancing preference learning and out-of-distribution generalization.  					AI-generated summary 				 Self-Rewarding Language Models propose an architecture in which the Large Language Models(LLMs) both generates responses and evaluates its own outputs via LLM-as-a-Judge prompting, dynamically improving its generative capabilities through iterative Direct Preference Optimization (DPO). However, our analysis reveals a critical limitation in existing Self-Rewarding paradigms: the synchronized improvement of chosen and rejected responses progressively narrows the representational difference between contrasting samples, undermining effective preference learning. We propose Temporal Self-Rewarding Language Models that strategically coordinate past, present, and future model generations to sustain learning signals. Our dual-phase framework introduces: (1) Anchored Rejection - fixing rejected responses using the past initial model's outputs and (2) Future-Guided Chosen - dynamically curating chosen samples using next-generation model predictions. Extensive experiments across three model families (Llama, Qwen, Mistral) and different model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained with our method compared to Self-Rewarding using same computation resources. For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our method also demonstrates superior out-of-distribution generalization across mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code generation (HumanEval) tasks, even though we do not specifically collect such training data.
[12.08.2025 04:21] Response: {
  "desc": "Предложена новая архитектура языковых моделей под названием Temporal Self-Rewarding Language Models. Эта архитектура улучшает генеративные способности моделей, стратегически используя прошлые и будущие выходные данные модели. Метод включает в себя две фазы: фиксацию отвергнутых ответов с использованием выходных данных начальной модели и динамический отбор выбранных образцов с использованием предсказаний модели следующего поколения. Эксперименты показали значительные улучшения по сравнению с базовыми методами самовознаграждения, особенно в задачах вне распределения обучающих данных.",
  "emoji": "⏳",
  "title": "Временная самокоррекция: новый шаг в развитии языковых моделей"
}
[12.08.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Temporal Self-Rewarding Language Models improve generative capabilities by strategically using past and future model outputs, enhancing preference learning and out-of-distribution generalization.  					AI-generated summary 				 Self-Rewarding Language Models propose an architecture in which the Large Language Models(LLMs) both generates responses and evaluates its own outputs via LLM-as-a-Judge prompting, dynamically improving its generative capabilities through iterative Direct Preference Optimization (DPO). However, our analysis reveals a critical limitation in existing Self-Rewarding paradigms: the synchronized improvement of chosen and rejected responses progressively narrows the representational difference between contrasting samples, undermining effective preference learning. We propose Temporal Self-Rewarding Language Models that strategically coordinate past, present, and future model generations to sustain learning signals. Our dual-phase framework introduces: (1) Anchored Rejection - fixing rejected responses using the past initial model's outputs and (2) Future-Guided Chosen - dynamically curating chosen samples using next-generation model predictions. Extensive experiments across three model families (Llama, Qwen, Mistral) and different model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained with our method compared to Self-Rewarding using same computation resources. For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our method also demonstrates superior out-of-distribution generalization across mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code generation (HumanEval) tasks, even though we do not specifically collect such training data."

[12.08.2025 04:21] Response: ```python
["RL", "RLHF", "ARCHITECTURE", "TRAINING"]
```
[12.08.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Temporal Self-Rewarding Language Models improve generative capabilities by strategically using past and future model outputs, enhancing preference learning and out-of-distribution generalization.  					AI-generated summary 				 Self-Rewarding Language Models propose an architecture in which the Large Language Models(LLMs) both generates responses and evaluates its own outputs via LLM-as-a-Judge prompting, dynamically improving its generative capabilities through iterative Direct Preference Optimization (DPO). However, our analysis reveals a critical limitation in existing Self-Rewarding paradigms: the synchronized improvement of chosen and rejected responses progressively narrows the representational difference between contrasting samples, undermining effective preference learning. We propose Temporal Self-Rewarding Language Models that strategically coordinate past, present, and future model generations to sustain learning signals. Our dual-phase framework introduces: (1) Anchored Rejection - fixing rejected responses using the past initial model's outputs and (2) Future-Guided Chosen - dynamically curating chosen samples using next-generation model predictions. Extensive experiments across three model families (Llama, Qwen, Mistral) and different model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained with our method compared to Self-Rewarding using same computation resources. For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our method also demonstrates superior out-of-distribution generalization across mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code generation (HumanEval) tasks, even though we do not specifically collect such training data."

[12.08.2025 04:21] Response: ```python
["AGI", "OPTIMIZATION", "REASONING"]
```
[12.08.2025 04:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Temporal Self-Rewarding Language Models, which enhance the generative abilities of language models by effectively utilizing outputs from past and future generations. The proposed architecture allows models to generate responses and evaluate them through a self-judging mechanism, improving performance via Direct Preference Optimization. A key innovation is the dual-phase framework that includes Anchored Rejection and Future-Guided Chosen strategies, which help maintain diverse learning signals and improve preference learning. Experimental results show that this approach significantly outperforms traditional Self-Rewarding methods across various tasks and model sizes, demonstrating better generalization capabilities.","title":"Enhancing Language Models with Temporal Self-Rewarding Strategies"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Temporal Self-Rewarding Language Models, which enhance the generative abilities of language models by effectively utilizing outputs from past and future generations. The proposed architecture allows models to generate responses and evaluate them through a self-judging mechanism, improving performance via Direct Preference Optimization. A key innovation is the dual-phase framework that includes Anchored Rejection and Future-Guided Chosen strategies, which help maintain diverse learning signals and improve preference learning. Experimental results show that this approach significantly outperforms traditional Self-Rewarding methods across various tasks and model sizes, demonstrating better generalization capabilities.', title='Enhancing Language Models with Temporal Self-Rewarding Strategies'))
[12.08.2025 04:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种时间自奖励语言模型，通过战略性地利用过去和未来的模型输出，提升生成能力和偏好学习。该模型采用了自我评估机制，使大型语言模型（LLMs）能够生成响应并评估自身输出，从而通过直接偏好优化（DPO）不断改进生成能力。研究发现，现有自奖励模型存在一个关键限制，即选择和拒绝响应的同步改进会逐渐缩小对比样本之间的表示差异，影响有效的偏好学习。为此，本文提出了时间自奖励语言模型，通过协调过去、现在和未来的生成，保持学习信号，从而显著提高模型的性能。","title":"时间自奖励模型：提升生成能力的新策略"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种时间自奖励语言模型，通过战略性地利用过去和未来的模型输出，提升生成能力和偏好学习。该模型采用了自我评估机制，使大型语言模型（LLMs）能够生成响应并评估自身输出，从而通过直接偏好优化（DPO）不断改进生成能力。研究发现，现有自奖励模型存在一个关键限制，即选择和拒绝响应的同步改进会逐渐缩小对比样本之间的表示差异，影响有效的偏好学习。为此，本文提出了时间自奖励语言模型，通过协调过去、现在和未来的生成，保持学习信号，从而显著提高模型的性能。', title='时间自奖励模型：提升生成能力的新策略'))
[12.08.2025 04:21] Querying the API.
[12.08.2025 04:21] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LessIsMore is a training-free sparse attention mechanism that improves efficiency and generalization in reasoning tasks by aggregating token selections from local attention heads.  					AI-generated summary 				 Large reasoning models achieve strong performance through test-time scaling but incur substantial computational overhead, particularly from excessive token generation when processing short input prompts. While sparse attention mechanisms can reduce latency and memory usage, existing approaches suffer from significant accuracy degradation due to accumulated errors during long-generation reasoning. These methods generally require either high token retention rates or expensive retraining. We introduce LessIsMore, a training-free sparse attention mechanism for reasoning tasks, which leverages global attention patterns rather than relying on traditional head-specific local optimizations. LessIsMore aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers. This unified selection improves generalization and efficiency by avoiding the need to maintain separate token subsets per head. Evaluation across diverse reasoning tasks and benchmarks shows that LessIsMore preserves -- and in some cases improves -- accuracy while achieving a 1.1times average decoding speed-up compared to full attention. Moreover, LessIsMore attends to 2times fewer tokens without accuracy loss, achieving a 1.13times end-to-end speed-up compared to existing sparse attention methods.
[12.08.2025 04:21] Response: {
  "desc": "LessIsMore - это механизм разреженного внимания, не требующий дополнительного обучения, который улучшает эффективность и обобщающую способность в задачах рассуждения. Он агрегирует выбор токенов из локальных голов внимания с недавней контекстной информацией, что позволяет проводить единый ранжирование токенов для будущих слоев декодирования. Этот подход улучшает обобщение и эффективность, избегая необходимости поддерживать отдельные подмножества токенов для каждой головы. Оценка на различных задачах рассуждения показывает, что LessIsMore сохраняет или даже улучшает точность, при этом ускоряя декодирование в среднем в 1.1 раза по сравнению с полным вниманием.",
  "emoji": "🧠",
  "title": "Меньше значит больше: эффективное разреженное внимание для улучшенного рассуждения"
}
[12.08.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LessIsMore is a training-free sparse attention mechanism that improves efficiency and generalization in reasoning tasks by aggregating token selections from local attention heads.  					AI-generated summary 				 Large reasoning models achieve strong performance through test-time scaling but incur substantial computational overhead, particularly from excessive token generation when processing short input prompts. While sparse attention mechanisms can reduce latency and memory usage, existing approaches suffer from significant accuracy degradation due to accumulated errors during long-generation reasoning. These methods generally require either high token retention rates or expensive retraining. We introduce LessIsMore, a training-free sparse attention mechanism for reasoning tasks, which leverages global attention patterns rather than relying on traditional head-specific local optimizations. LessIsMore aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers. This unified selection improves generalization and efficiency by avoiding the need to maintain separate token subsets per head. Evaluation across diverse reasoning tasks and benchmarks shows that LessIsMore preserves -- and in some cases improves -- accuracy while achieving a 1.1times average decoding speed-up compared to full attention. Moreover, LessIsMore attends to 2times fewer tokens without accuracy loss, achieving a 1.13times end-to-end speed-up compared to existing sparse attention methods."

[12.08.2025 04:21] Response: ```python
['TRAINING', 'BENCHMARK', 'ARCHITECTURE']
```
[12.08.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LessIsMore is a training-free sparse attention mechanism that improves efficiency and generalization in reasoning tasks by aggregating token selections from local attention heads.  					AI-generated summary 				 Large reasoning models achieve strong performance through test-time scaling but incur substantial computational overhead, particularly from excessive token generation when processing short input prompts. While sparse attention mechanisms can reduce latency and memory usage, existing approaches suffer from significant accuracy degradation due to accumulated errors during long-generation reasoning. These methods generally require either high token retention rates or expensive retraining. We introduce LessIsMore, a training-free sparse attention mechanism for reasoning tasks, which leverages global attention patterns rather than relying on traditional head-specific local optimizations. LessIsMore aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers. This unified selection improves generalization and efficiency by avoiding the need to maintain separate token subsets per head. Evaluation across diverse reasoning tasks and benchmarks shows that LessIsMore preserves -- and in some cases improves -- accuracy while achieving a 1.1times average decoding speed-up compared to full attention. Moreover, LessIsMore attends to 2times fewer tokens without accuracy loss, achieving a 1.13times end-to-end speed-up compared to existing sparse attention methods."

[12.08.2025 04:21] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[12.08.2025 04:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LessIsMore is a novel sparse attention mechanism designed to enhance efficiency and generalization in reasoning tasks without the need for retraining. It aggregates token selections from local attention heads, utilizing global attention patterns to improve the decision-making process during decoding. This approach allows for a unified ranking of tokens across different heads, which reduces the number of tokens processed while maintaining or even improving accuracy. Evaluations demonstrate that LessIsMore achieves faster decoding speeds and lower token usage compared to traditional sparse attention methods, making it a significant advancement in the field.","title":"LessIsMore: Efficient Reasoning with Sparse Attention"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LessIsMore is a novel sparse attention mechanism designed to enhance efficiency and generalization in reasoning tasks without the need for retraining. It aggregates token selections from local attention heads, utilizing global attention patterns to improve the decision-making process during decoding. This approach allows for a unified ranking of tokens across different heads, which reduces the number of tokens processed while maintaining or even improving accuracy. Evaluations demonstrate that LessIsMore achieves faster decoding speeds and lower token usage compared to traditional sparse attention methods, making it a significant advancement in the field.', title='LessIsMore: Efficient Reasoning with Sparse Attention'))
[12.08.2025 04:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LessIsMore是一种无训练的稀疏注意力机制，旨在提高推理任务的效率和泛化能力。它通过聚合来自局部注意力头的标记选择，利用全局注意力模式，而不是依赖传统的头特定局部优化。该方法在推理过程中避免了维护每个头的独立标记子集，从而提高了通用性和效率。评估结果表明，LessIsMore在保持或提高准确性的同时，解码速度平均提升了1.1倍，并且在不损失准确性的情况下，关注的标记数量减少了2倍。","title":"LessIsMore：高效推理的新选择"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LessIsMore是一种无训练的稀疏注意力机制，旨在提高推理任务的效率和泛化能力。它通过聚合来自局部注意力头的标记选择，利用全局注意力模式，而不是依赖传统的头特定局部优化。该方法在推理过程中避免了维护每个头的独立标记子集，从而提高了通用性和效率。评估结果表明，LessIsMore在保持或提高准确性的同时，解码速度平均提升了1.1倍，并且在不损失准确性的情况下，关注的标记数量减少了2倍。', title='LessIsMore：高效推理的新选择'))
[12.08.2025 04:21] Querying the API.
[12.08.2025 04:21] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A reasoning-intensive reranker, ReasonRank, achieves state-of-the-art performance in passage ranking tasks by using synthesized training data and a two-stage post-training approach with reinforcement learning.  					AI-generated summary 				 Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker ReasonRank outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\footnote{https://brightbenchmark.github.io/.} Our codes are available at https://github.com/8421BCD/ReasonRank.
[12.08.2025 04:21] Response: {
  "desc": "ReasonRank - это новый ранжировщик пассажей, использующий синтезированные данные для обучения и двухэтапный подход с обучением с подкреплением. Он применяет пошаговое рассуждение во время тестирования для улучшения производительности ранжирования списков. Модель использует автоматизированную систему синтеза обучающих данных с рассуждениями и механизм фильтрации для обеспечения качества данных. ReasonRank достигает наилучших результатов в задачах ранжирования пассажей, превосходя существующие базовые модели.",
  "emoji": "🧠",
  "title": "ReasonRank: Рассуждающий ранжировщик нового поколения"
}
[12.08.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A reasoning-intensive reranker, ReasonRank, achieves state-of-the-art performance in passage ranking tasks by using synthesized training data and a two-stage post-training approach with reinforcement learning.  					AI-generated summary 				 Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker ReasonRank outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\footnote{https://brightbenchmark.github.io/.} Our codes are available at https://github.com/8421BCD/ReasonRank."

[12.08.2025 04:21] Response: ```python
['DATASET', 'RL', 'TRAINING']
```
[12.08.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A reasoning-intensive reranker, ReasonRank, achieves state-of-the-art performance in passage ranking tasks by using synthesized training data and a two-stage post-training approach with reinforcement learning.  					AI-generated summary 				 Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker ReasonRank outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\footnote{https://brightbenchmark.github.io/.} Our codes are available at https://github.com/8421BCD/ReasonRank."

[12.08.2025 04:21] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[12.08.2025 04:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces ReasonRank, a novel reranker that excels in passage ranking tasks by leveraging synthesized training data and a two-stage post-training method involving reinforcement learning. It addresses the challenge of limited reasoning-intensive training data by creating a framework that generates high-quality training labels from diverse domains. The two-stage approach consists of a supervised fine-tuning phase to learn reasoning patterns, followed by a reinforcement learning phase that enhances ranking capabilities using a multi-view ranking reward. Experimental results show that ReasonRank achieves state-of-the-art performance while maintaining lower latency compared to traditional pointwise rerankers.","title":"ReasonRank: Elevating Passage Ranking with Advanced Reasoning Techniques"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces ReasonRank, a novel reranker that excels in passage ranking tasks by leveraging synthesized training data and a two-stage post-training method involving reinforcement learning. It addresses the challenge of limited reasoning-intensive training data by creating a framework that generates high-quality training labels from diverse domains. The two-stage approach consists of a supervised fine-tuning phase to learn reasoning patterns, followed by a reinforcement learning phase that enhances ranking capabilities using a multi-view ranking reward. Experimental results show that ReasonRank achieves state-of-the-art performance while maintaining lower latency compared to traditional pointwise rerankers.', title='ReasonRank: Elevating Passage Ranking with Advanced Reasoning Techniques'))
[12.08.2025 04:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种名为ReasonRank的推理强化重排序模型，旨在提高段落排名任务的性能。通过合成训练数据和两阶段的后训练方法，ReasonRank在复杂的排名场景中表现出色。我们设计了一种自动化的推理训练数据合成框架，并引入了自一致性数据过滤机制，以确保数据质量。最终，ReasonRank在BRIGHT排行榜上达到了40.6的最新性能，显著优于现有基线模型。","title":"推理强化重排序，提升段落排名性能！"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种名为ReasonRank的推理强化重排序模型，旨在提高段落排名任务的性能。通过合成训练数据和两阶段的后训练方法，ReasonRank在复杂的排名场景中表现出色。我们设计了一种自动化的推理训练数据合成框架，并引入了自一致性数据过滤机制，以确保数据质量。最终，ReasonRank在BRIGHT排行榜上达到了40.6的最新性能，显著优于现有基线模型。', title='推理强化重排序，提升段落排名性能！'))
[12.08.2025 04:21] Using data from previous issue: {"categories": ["#training", "#open_source", "#security", "#data"], "emoji": "🛡️", "ru": {"title": "Фильтрация данных как защита от атак на открытые ИИ-системы", "desc": "Статья исследует возможность фильтрации текстов о двойном назначении из обучающих данных для предотвращения нежелательных способн
[12.08.2025 04:21] Renaming data file.
[12.08.2025 04:21] Renaming previous data. hf_papers.json to ./d/2025-08-12.json
[12.08.2025 04:21] Saving new data file.
[12.08.2025 04:21] Generating page.
[12.08.2025 04:21] Renaming previous page.
[12.08.2025 04:21] Renaming previous data. index.html to ./d/2025-08-12.html
[12.08.2025 04:21] Writing result.
[12.08.2025 04:21] Renaming log file.
[12.08.2025 04:21] Renaming previous data. log.txt to ./logs/2025-08-12_last_log.txt
