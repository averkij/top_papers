[17.06.2025 05:12] Read previous papers.
[17.06.2025 05:12] Generating top page (month).
[17.06.2025 05:12] Writing top page (month).
[17.06.2025 06:17] Read previous papers.
[17.06.2025 06:17] Get feed.
[17.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13585
[17.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10521
[17.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.11763
[17.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13759
[17.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08343
[17.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13654
[17.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12915
[17.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03968
[17.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.07961
[17.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13750
[17.06.2025 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2506.09050
[17.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12450
[17.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12189
[17.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06366
[17.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12953
[17.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12623
[17.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13752
[17.06.2025 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2506.13172
[17.06.2025 06:17] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.06.2025 06:17] No deleted papers detected.
[17.06.2025 06:17] Downloading and parsing papers (pdf, html). Total: 18.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.13585.
[17.06.2025 06:17] Extra JSON file exists (./assets/json/2506.13585.json), skip PDF parsing.
[17.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.13585.json), skip HTML parsing.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.10521.
[17.06.2025 06:17] Extra JSON file exists (./assets/json/2506.10521.json), skip PDF parsing.
[17.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.10521.json), skip HTML parsing.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.11763.
[17.06.2025 06:17] Extra JSON file exists (./assets/json/2506.11763.json), skip PDF parsing.
[17.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.11763.json), skip HTML parsing.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.13759.
[17.06.2025 06:17] Extra JSON file exists (./assets/json/2506.13759.json), skip PDF parsing.
[17.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.13759.json), skip HTML parsing.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.08343.
[17.06.2025 06:17] Extra JSON file exists (./assets/json/2506.08343.json), skip PDF parsing.
[17.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.08343.json), skip HTML parsing.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.13654.
[17.06.2025 06:17] Extra JSON file exists (./assets/json/2506.13654.json), skip PDF parsing.
[17.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.13654.json), skip HTML parsing.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.12915.
[17.06.2025 06:17] Extra JSON file exists (./assets/json/2506.12915.json), skip PDF parsing.
[17.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.12915.json), skip HTML parsing.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.03968.
[17.06.2025 06:17] Extra JSON file exists (./assets/json/2506.03968.json), skip PDF parsing.
[17.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.03968.json), skip HTML parsing.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.07961.
[17.06.2025 06:17] Extra JSON file exists (./assets/json/2506.07961.json), skip PDF parsing.
[17.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.07961.json), skip HTML parsing.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.13750.
[17.06.2025 06:17] Extra JSON file exists (./assets/json/2506.13750.json), skip PDF parsing.
[17.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.13750.json), skip HTML parsing.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.09050.
[17.06.2025 06:17] Downloading paper 2506.09050 from http://arxiv.org/pdf/2506.09050v1...
[17.06.2025 06:17] Extracting affiliations from text.
[17.06.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 1 ] . [ 1 0 5 0 9 0 . 6 0 5 2 : r ALE-Bench: Benchmark for Long-Horizon Objective-Driven Algorithm Engineering Yuki Imajuku1, Kohki Horie1,2, Yoichi Iwata3, Kensho Aoki3, Naohiro Takahashi3, Takuya Akiba1 2The University of Tokyo, Japan {imajuku, takiba}@sakana.ai 3AtCoder, Japan 1Sakana AI, Japan "
[17.06.2025 06:17] Response: ```python
["The University of Tokyo, Japan", "AtCoder, Japan", "Sakana AI, Japan"]
```
[17.06.2025 06:17] Deleting PDF ./assets/pdf/2506.09050.pdf.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.12450.
[17.06.2025 06:17] Extra JSON file exists (./assets/json/2506.12450.json), skip PDF parsing.
[17.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.12450.json), skip HTML parsing.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.12189.
[17.06.2025 06:17] Extra JSON file exists (./assets/json/2506.12189.json), skip PDF parsing.
[17.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.12189.json), skip HTML parsing.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.06366.
[17.06.2025 06:17] Extra JSON file exists (./assets/json/2506.06366.json), skip PDF parsing.
[17.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.06366.json), skip HTML parsing.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.12953.
[17.06.2025 06:17] Extra JSON file exists (./assets/json/2506.12953.json), skip PDF parsing.
[17.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.12953.json), skip HTML parsing.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.12623.
[17.06.2025 06:17] Extra JSON file exists (./assets/json/2506.12623.json), skip PDF parsing.
[17.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.12623.json), skip HTML parsing.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.13752.
[17.06.2025 06:17] Extra JSON file exists (./assets/json/2506.13752.json), skip PDF parsing.
[17.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.13752.json), skip HTML parsing.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.13172.
[17.06.2025 06:17] Downloading paper 2506.13172 from http://arxiv.org/pdf/2506.13172v1...
[17.06.2025 06:17] Extracting affiliations from text.
[17.06.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging Unsubstantiated Claims and Ambiguous Pronouns Evgeny Markhasin Lobachevsky State University of Nizhny Novgorod https://orcid.org/0000-0002-7419-3605 https://linkedin.com/in/evgenymarkhasin "
[17.06.2025 06:17] Response: ```python
["Lobachevsky State University of Nizhny Novgorod"]
```
[17.06.2025 06:17] Deleting PDF ./assets/pdf/2506.13172.pdf.
[17.06.2025 06:17] Success.
[17.06.2025 06:17] Enriching papers with extra data.
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 0. A hybrid-attention reasoning model called MiniMax-M1, featuring a Mixture-of-Experts architecture and lightning attention mechanism, is introduced for efficient long-input processing and reinforcement learning.  					AI-generated summary 				 We introduce MiniMax-M1, the world's first open-weight, l...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 1. Scientists' First Exam (SFE) benchmark assesses scientific cognitive capacities of Multimodal Large Language Models through perception, understanding, and comparative reasoning.  					AI-generated summary 				 Scientific discoveries increasingly rely on complex multimodal reasoning based on informat...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 2. DeepResearch Bench offers a benchmark framework to evaluate the capabilities of Deep Research Agents in terms of research quality and information retrieval accuracy across multiple fields.  					AI-generated summary 				 Deep Research Agents are a prominent category of LLM-based agents. By autonomou...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 3. Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs) enable parallel generation and faster inference compared to autoregressive models through denoising-based strategies and full attention mechanisms.  					AI-generated summary 				 In this work, we p...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 4. NoWait suppresses explicit self-reflection tokens during inference to enhance efficiency in multimodal reasoning without reducing model utility.  					AI-generated summary 				 Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant over...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 5. Ego-R1, a reinforcement learning-based framework, uses a structured tool-augmented chain-of-thought process to reason over ultra-long egocentric videos, achieving better performance than existing methods by extending time coverage to a week.  					AI-generated summary 				 We introduce Ego-R1, a nov...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 6. A new benchmark, PersonaFeedback, evaluates Large Language Models' ability to generate personalized responses given explicit user personas, revealing limitations in current systems.  					AI-generated summary 				 With the rapid improvement in the general capabilities of LLMs, LLM personalization, i...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 7. The paper presents a method for generating diverse and complex instruction data for large language models using attributed grounding, achieving top performance on benchmarks with a large synthesized dataset.  					AI-generated summary 				 The pursuit of diverse, complex, and large-scale instruction...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 8. BridgeVLA is a 3D vision-language-action model that projects 3D inputs to 2D images and uses 2D heatmaps for efficient and effective action prediction, outperforming baselines in various benchmarks.  					AI-generated summary 				 Recently, leveraging pre-trained vision-language models (VLMs) for bu...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 9. Test3R, a test-time learning technique for 3D reconstruction, enhances geometric accuracy by optimizing network consistency using self-supervised learning on image triplets.  					AI-generated summary 				 Dense matching methods like DUSt3R regress pairwise pointmaps for 3D reconstruction. However, ...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 10. ALE-Bench evaluates AI systems on score-based algorithmic programming contests drawn from AtCoder, focusing on long-term iterative problem-solving in domains like package-delivery routing, crew scheduling, factory production, and power-grid balancing.  					AI-generated summary 				 How well do AI s...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 11. Research confirms natural representation alignment in large language models and introduces Inference-Time Language Control to enhance cross-lingual performance.  					AI-generated summary 				 Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across tasks and lang...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 12. The study evaluates various LLMs on diverse text tasks using a new dataset, revealing distinct personality traits and improving model interpretability.  					AI-generated summary 				 Large Language Models (LLMs) are increasingly integrated into everyday applications. As their influence grows, under...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 13. A new field, AI Agent Behavioral Science, is proposed to systematically study the behaviors of AI agents in diverse contexts, emphasizing external factors and their interactions, and addressing responsible AI aspects.  					AI-generated summary 				 Recent advances in large language models (LLMs) ha...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 14. PatchInstruct enhances LLM forecasting quality through specialized prompting methods that include time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) have demonstrated new pos...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 15. A novel benchmark and dataset are proposed for multi-modal summarization of UI instructional videos, addressing the need for step-by-step executable instructions and key video frames.  					AI-generated summary 				 We study multi-modal summarization for instructional videos, whose goal is to provid...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 16. Budget guidance is a method that steers LLM reasoning within a targeted budget without fine-tuning and achieves improved efficiency and performance on math benchmarks.  					AI-generated summary 				 Recent deep-thinking large language models often reason extensively to improve performance, but such...
[17.06.2025 06:17] ********************************************************************************
[17.06.2025 06:17] Abstract 17. Structured workflow prompts improve hierarchical reasoning in LLMs for scholarly manuscript analysis, but their effectiveness varies with the model, task type, and context.  					AI-generated summary 				 We present and evaluate a suite of proof-of-concept (PoC), structured workflow prompts designed...
[17.06.2025 06:17] Read previous papers.
[17.06.2025 06:17] Generating reviews via LLM API.
[17.06.2025 06:17] Using data from previous issue: {"categories": ["#rl", "#training", "#open_source", "#architecture", "#reasoning", "#long_context", "#optimization"], "emoji": "🧠", "ru": {"title": "MiniMax-M1: Гибридный ИИ для эффективной обработки сложных задач", "desc": "MiniMax-M1 - это гибридная модель рассуждений с архитектурой Mixture-of-Exp
[17.06.2025 06:17] Using data from previous issue: {"categories": ["#reasoning", "#science", "#multimodal", "#benchmark"], "emoji": "🔬", "ru": {"title": "Новый бенчмарк для оценки научного мышления ИИ", "desc": "Учёные разработали новый бенчмарк под названием Scientists' First Exam (SFE) для оценки научных когнитивных способностей мультимодальных бо
[17.06.2025 06:17] Using data from previous issue: {"categories": ["#open_source", "#science", "#agents", "#alignment", "#benchmark"], "emoji": "🔬", "ru": {"title": "Комплексная оценка ИИ-агентов для глубоких исследований", "desc": "DeepResearch Bench - это система оценки возможностей агентов глубоких исследований в области качества исследований и т
[17.06.2025 06:17] Using data from previous issue: {"categories": ["#math", "#training", "#diffusion", "#inference", "#multimodal", "#survey"], "emoji": "🧠", "ru": {"title": "Революция в языковом моделировании: дискретные диффузионные модели", "desc": "Статья представляет собой систематический обзор дискретных диффузионных языковых моделей (dLLMs) и
[17.06.2025 06:17] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#inference", "#multimodal"], "emoji": "🧠", "ru": {"title": "Эффективное рассуждение без лишних слов", "desc": "Исследование представляет метод NoWait, который подавляет токены явной саморефлексии в больших языковых моделях во время вывода. Это позволяет с
[17.06.2025 06:17] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#benchmark", "#training", "#multimodal", "#long_context", "#rl"], "emoji": "🎥", "ru": {"title": "Ego-R1: Революция в анализе длительных эгоцентрических видео", "desc": "Ego-R1 - это новая система обработки сверхдлинных эгоцентрических видео, использующая ст
[17.06.2025 06:17] Using data from previous issue: {"categories": ["#alignment", "#interpretability", "#multimodal", "#benchmark"], "emoji": "🎭", "ru": {"title": "PersonaFeedback: новый стандарт оценки персонализации языковых моделей", "desc": "Представлен новый бенчмарк PersonaFeedback для оценки способности больших языковых моделей (LLM) генериров
[17.06.2025 06:17] Using data from previous issue: {"categories": ["#dataset", "#synthetic", "#alignment", "#data", "#benchmark"], "emoji": "🧠", "ru": {"title": "Синтез сложных инструкций для эффективного обучения языковых моделей", "desc": "Статья представляет метод генерации разнообразных и сложных инструкций для обучения больших языковых моделей 
[17.06.2025 06:17] Using data from previous issue: {"categories": ["#rl", "#3d", "#games", "#optimization", "#agents", "#benchmark"], "emoji": "🤖", "ru": {"title": "BridgeVLA: Эффективное обучение роботов через проекцию 3D в 2D", "desc": "BridgeVLA - это новая модель машинного обучения для роботизированных манипуляций, объединяющая 3D-зрение, язык и
[17.06.2025 06:17] Using data from previous issue: {"categories": ["#3d", "#training", "#optimization"], "emoji": "🏛️", "ru": {"title": "Test3R: Повышение точности 3D-реконструкции через самообучение на тестовых данных", "desc": "Test3R - это новая техника обучения во время тестирования для 3D-реконструкции, которая улучшает геометрическую точность.
[17.06.2025 06:17] Querying the API.
[17.06.2025 06:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ALE-Bench evaluates AI systems on score-based algorithmic programming contests drawn from AtCoder, focusing on long-term iterative problem-solving in domains like package-delivery routing, crew scheduling, factory production, and power-grid balancing.  					AI-generated summary 				 How well do AI systems perform in algorithm engineering for hard optimization problems in domains such as package-delivery routing, crew scheduling, factory production planning, and power-grid balancing? We introduce ALE-Bench, a new benchmark for evaluating AI systems on score-based algorithmic programming contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench presents optimization problems that are computationally hard and admit no known exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench encourages iterative solution refinement over long time horizons. Our software framework supports interactive agent architectures that leverage test-run feedback and visualizations. Our evaluation of frontier LLMs revealed that while they demonstrate high performance on specific problems, a notable gap remains compared to humans in terms of consistency across problems and long-horizon problem-solving capabilities. This highlights the need for this benchmark to foster future AI advancements.
[17.06.2025 06:17] Response: {
  "desc": "ALE-Bench - это новый эталонный тест для оценки систем искусственного интеллекта на основе алгоритмических соревнований по программированию. Он фокусируется на задачах оптимизации в таких областях, как маршрутизация доставки, планирование экипажей и балансировка электросетей. В отличие от кратковременных тестов на прохождение/непрохождение, ALE-Bench поощряет итеративное улучшение решений в течение длительного времени. Оценка современных языковых моделей показала, что хотя они демонстрируют высокую производительность на отдельных задачах, по сравнению с людьми остается заметный разрыв в последовательности решения задач и способности к долгосрочному решению проблем.",
  "emoji": "🧠",
  "title": "ALE-Bench: Испытание ИИ в алгоритмической оптимизации"
}
[17.06.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ALE-Bench evaluates AI systems on score-based algorithmic programming contests drawn from AtCoder, focusing on long-term iterative problem-solving in domains like package-delivery routing, crew scheduling, factory production, and power-grid balancing.  					AI-generated summary 				 How well do AI systems perform in algorithm engineering for hard optimization problems in domains such as package-delivery routing, crew scheduling, factory production planning, and power-grid balancing? We introduce ALE-Bench, a new benchmark for evaluating AI systems on score-based algorithmic programming contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench presents optimization problems that are computationally hard and admit no known exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench encourages iterative solution refinement over long time horizons. Our software framework supports interactive agent architectures that leverage test-run feedback and visualizations. Our evaluation of frontier LLMs revealed that while they demonstrate high performance on specific problems, a notable gap remains compared to humans in terms of consistency across problems and long-horizon problem-solving capabilities. This highlights the need for this benchmark to foster future AI advancements."

[17.06.2025 06:17] Response: ```python
['BENCHMARK', 'AGENTS']
```
[17.06.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ALE-Bench evaluates AI systems on score-based algorithmic programming contests drawn from AtCoder, focusing on long-term iterative problem-solving in domains like package-delivery routing, crew scheduling, factory production, and power-grid balancing.  					AI-generated summary 				 How well do AI systems perform in algorithm engineering for hard optimization problems in domains such as package-delivery routing, crew scheduling, factory production planning, and power-grid balancing? We introduce ALE-Bench, a new benchmark for evaluating AI systems on score-based algorithmic programming contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench presents optimization problems that are computationally hard and admit no known exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench encourages iterative solution refinement over long time horizons. Our software framework supports interactive agent architectures that leverage test-run feedback and visualizations. Our evaluation of frontier LLMs revealed that while they demonstrate high performance on specific problems, a notable gap remains compared to humans in terms of consistency across problems and long-horizon problem-solving capabilities. This highlights the need for this benchmark to foster future AI advancements."

[17.06.2025 06:17] Response: ```python
["OPTIMIZATION"]
```
[17.06.2025 06:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ALE-Bench is a benchmark designed to assess AI systems on complex optimization problems derived from real-world algorithmic programming contests. It focuses on long-term iterative problem-solving in various domains, such as package delivery and factory production. The benchmark emphasizes the importance of refining solutions over extended periods, rather than just achieving quick pass/fail results. Our findings indicate that while advanced language models perform well on certain tasks, they still lag behind human consistency and long-term problem-solving abilities, underscoring the need for further development in AI.","title":"Evaluating AI\'s Long-Term Problem-Solving with ALE-Bench"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ALE-Bench is a benchmark designed to assess AI systems on complex optimization problems derived from real-world algorithmic programming contests. It focuses on long-term iterative problem-solving in various domains, such as package delivery and factory production. The benchmark emphasizes the importance of refining solutions over extended periods, rather than just achieving quick pass/fail results. Our findings indicate that while advanced language models perform well on certain tasks, they still lag behind human consistency and long-term problem-solving abilities, underscoring the need for further development in AI.', title="Evaluating AI's Long-Term Problem-Solving with ALE-Bench"))
[17.06.2025 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ALE-Bench是一个新的基准，用于评估人工智能系统在基于分数的算法编程竞赛中的表现，特别是在复杂的优化问题上。它关注于长期的迭代问题解决，涉及包裹投递、人员调度、工厂生产和电网平衡等领域。与短期的通过/不通过编码基准不同，ALE-Bench鼓励在较长时间内对解决方案进行细化。我们的评估显示，尽管前沿的大型语言模型在特定问题上表现良好，但在一致性和长期问题解决能力方面与人类相比仍存在显著差距。","title":"ALE-Bench：推动AI在复杂优化问题上的进步"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ALE-Bench是一个新的基准，用于评估人工智能系统在基于分数的算法编程竞赛中的表现，特别是在复杂的优化问题上。它关注于长期的迭代问题解决，涉及包裹投递、人员调度、工厂生产和电网平衡等领域。与短期的通过/不通过编码基准不同，ALE-Bench鼓励在较长时间内对解决方案进行细化。我们的评估显示，尽管前沿的大型语言模型在特定问题上表现良好，但在一致性和长期问题解决能力方面与人类相比仍存在显著差距。', title='ALE-Bench：推动AI在复杂优化问题上的进步'))
[17.06.2025 06:18] Using data from previous issue: {"categories": ["#alignment", "#machine_translation", "#inference", "#multilingual"], "emoji": "🌐", "ru": {"title": "Улучшение кросс-языковых возможностей LLM через контроль скрытых представлений", "desc": "Исследование подтверждает естественное выравнивание представлений в больших языковых моделях 
[17.06.2025 06:18] Using data from previous issue: {"categories": ["#dataset", "#small_models", "#reasoning", "#long_context", "#interpretability", "#multimodal", "#benchmark"], "emoji": "🧠", "ru": {"title": "Раскрывая личность искусственного интеллекта: новый подход к интерпретации языковых моделей", "desc": "Исследование оценивает различные больши
[17.06.2025 06:18] Using data from previous issue: {"categories": ["#agi", "#healthcare", "#ethics", "#multimodal", "#agents", "#interpretability"], "emoji": "🧠", "ru": {"title": "От модели к поведению: новый взгляд на изучение ИИ-агентов", "desc": "Предлагается новая область исследований - поведенческая наука ИИ-агентов. Она направлена на системати
[17.06.2025 06:18] Using data from previous issue: {"categories": ["#data", "#training", "#optimization"], "emoji": "📈", "ru": {"title": "Точное прогнозирование временных рядов с помощью языковых моделей", "desc": "Статья представляет метод PatchInstruct для улучшения качества прогнозирования временных рядов с помощью больших языковых моделей (LLM).
[17.06.2025 06:18] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#video", "#dataset"], "emoji": "🎬", "ru": {"title": "Новый подход к сумматизации обучающих видео по UI", "desc": "Предложен новый бенчмарк и набор данных для мультимодальной сумматизации обучающих видео по пользовательским интерфейсам. Исследование напра
[17.06.2025 06:18] Using data from previous issue: {"categories": ["#optimization", "#inference", "#training", "#reasoning", "#math"], "emoji": "💡", "ru": {"title": "Эффективное управление рассуждениями ИИ в рамках бюджета", "desc": "Метод 'бюджетного руководства' позволяет управлять рассуждениями языковых моделей в рамках заданного бюджета без допо
[17.06.2025 06:18] Querying the API.
[17.06.2025 06:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Structured workflow prompts improve hierarchical reasoning in LLMs for scholarly manuscript analysis, but their effectiveness varies with the model, task type, and context.  					AI-generated summary 				 We present and evaluate a suite of proof-of-concept (PoC), structured workflow prompts designed to elicit human-like hierarchical reasoning while guiding Large Language Models (LLMs) in high-level semantic and linguistic analysis of scholarly manuscripts. The prompts target two non-trivial analytical tasks: identifying unsubstantiated claims in summaries (informational integrity) and flagging ambiguous pronoun references (linguistic clarity). We conducted a systematic, multi-run evaluation on two frontier models (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for the informational integrity task reveal a significant divergence in model performance: while both models successfully identified an unsubstantiated head of a noun phrase (95% success), ChatGPT consistently failed (0% success) to identify an unsubstantiated adjectival modifier that Gemini correctly flagged (95% success), raising a question regarding potential influence of the target's syntactic role. For the linguistic analysis task, both models performed well (80-90% success) with full manuscript context. In a summary-only setting, however, ChatGPT achieved a perfect (100%) success rate, while Gemini's performance was substantially degraded. Our findings suggest that structured prompting is a viable methodology for complex textual analysis but show that prompt performance may be highly dependent on the interplay between the model, task type, and context, highlighting the need for rigorous, model-specific testing.
[17.06.2025 06:18] Response: {
  "desc": "Исследователи разработали структурированные рабочие процессы для улучшения иерархического рассуждения в больших языковых моделях (LLM) при анализе научных рукописей. Они оценили эффективность этих процессов на двух моделях (Gemini Pro 2.5 Pro и ChatGPT Plus o3) для задач выявления необоснованных утверждений и неоднозначных местоимений. Результаты показали, что эффективность структурированных промптов зависит от конкретной модели, типа задачи и контекста. Исследование подчеркивает необходимость тщательного тестирования для каждой конкретной модели и задачи.",
  "emoji": "🧠",
  "title": "Структурированные промпты улучшают анализ текста, но требуют индивидуального подхода"
}
[17.06.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Structured workflow prompts improve hierarchical reasoning in LLMs for scholarly manuscript analysis, but their effectiveness varies with the model, task type, and context.  					AI-generated summary 				 We present and evaluate a suite of proof-of-concept (PoC), structured workflow prompts designed to elicit human-like hierarchical reasoning while guiding Large Language Models (LLMs) in high-level semantic and linguistic analysis of scholarly manuscripts. The prompts target two non-trivial analytical tasks: identifying unsubstantiated claims in summaries (informational integrity) and flagging ambiguous pronoun references (linguistic clarity). We conducted a systematic, multi-run evaluation on two frontier models (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for the informational integrity task reveal a significant divergence in model performance: while both models successfully identified an unsubstantiated head of a noun phrase (95% success), ChatGPT consistently failed (0% success) to identify an unsubstantiated adjectival modifier that Gemini correctly flagged (95% success), raising a question regarding potential influence of the target's syntactic role. For the linguistic analysis task, both models performed well (80-90% success) with full manuscript context. In a summary-only setting, however, ChatGPT achieved a perfect (100%) success rate, while Gemini's performance was substantially degraded. Our findings suggest that structured prompting is a viable methodology for complex textual analysis but show that prompt performance may be highly dependent on the interplay between the model, task type, and context, highlighting the need for rigorous, model-specific testing."

[17.06.2025 06:18] Response: ```python
['MULTIMODAL', 'TRAINING']
```
[17.06.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Structured workflow prompts improve hierarchical reasoning in LLMs for scholarly manuscript analysis, but their effectiveness varies with the model, task type, and context.  					AI-generated summary 				 We present and evaluate a suite of proof-of-concept (PoC), structured workflow prompts designed to elicit human-like hierarchical reasoning while guiding Large Language Models (LLMs) in high-level semantic and linguistic analysis of scholarly manuscripts. The prompts target two non-trivial analytical tasks: identifying unsubstantiated claims in summaries (informational integrity) and flagging ambiguous pronoun references (linguistic clarity). We conducted a systematic, multi-run evaluation on two frontier models (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for the informational integrity task reveal a significant divergence in model performance: while both models successfully identified an unsubstantiated head of a noun phrase (95% success), ChatGPT consistently failed (0% success) to identify an unsubstantiated adjectival modifier that Gemini correctly flagged (95% success), raising a question regarding potential influence of the target's syntactic role. For the linguistic analysis task, both models performed well (80-90% success) with full manuscript context. In a summary-only setting, however, ChatGPT achieved a perfect (100%) success rate, while Gemini's performance was substantially degraded. Our findings suggest that structured prompting is a viable methodology for complex textual analysis but show that prompt performance may be highly dependent on the interplay between the model, task type, and context, highlighting the need for rigorous, model-specific testing."

[17.06.2025 06:18] Response: ```python
['REASONING', 'INTERPRETABILITY', 'SCIENCE']
```
[17.06.2025 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores how structured workflow prompts can enhance hierarchical reasoning in Large Language Models (LLMs) when analyzing scholarly manuscripts. The authors designed prompts to help models identify unsubstantiated claims and ambiguous pronoun references, which are crucial for maintaining informational integrity and linguistic clarity. They evaluated two advanced models, Gemini Pro 2.5 Pro and ChatGPT Plus o3, and found significant differences in their performance based on the task and context. The results indicate that while structured prompting can improve analysis, its effectiveness varies greatly depending on the model and the specific analytical task.","title":"Enhancing Scholarly Analysis with Structured Prompts in LLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores how structured workflow prompts can enhance hierarchical reasoning in Large Language Models (LLMs) when analyzing scholarly manuscripts. The authors designed prompts to help models identify unsubstantiated claims and ambiguous pronoun references, which are crucial for maintaining informational integrity and linguistic clarity. They evaluated two advanced models, Gemini Pro 2.5 Pro and ChatGPT Plus o3, and found significant differences in their performance based on the task and context. The results indicate that while structured prompting can improve analysis, its effectiveness varies greatly depending on the model and the specific analytical task.', title='Enhancing Scholarly Analysis with Structured Prompts in LLMs'))
[17.06.2025 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本研究探讨了结构化工作流程提示在大型语言模型（LLMs）中促进层次推理的效果，特别是在学术手稿分析中。我们设计了一系列概念验证的提示，旨在引导模型进行高水平的语义和语言分析，重点关注识别未证实的主张和模糊的代词引用。通过对两种前沿模型的系统评估，我们发现模型在不同任务和上下文中的表现差异显著，尤其是在处理句法角色时。研究结果表明，结构化提示是一种有效的复杂文本分析方法，但其效果受模型、任务类型和上下文的相互影响。","title":"结构化提示提升学术分析中的层次推理"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本研究探讨了结构化工作流程提示在大型语言模型（LLMs）中促进层次推理的效果，特别是在学术手稿分析中。我们设计了一系列概念验证的提示，旨在引导模型进行高水平的语义和语言分析，重点关注识别未证实的主张和模糊的代词引用。通过对两种前沿模型的系统评估，我们发现模型在不同任务和上下文中的表现差异显著，尤其是在处理句法角色时。研究结果表明，结构化提示是一种有效的复杂文本分析方法，但其效果受模型、任务类型和上下文的相互影响。', title='结构化提示提升学术分析中的层次推理'))
[17.06.2025 06:18] Renaming data file.
[17.06.2025 06:18] Renaming previous data. hf_papers.json to ./d/2025-06-17.json
[17.06.2025 06:18] Saving new data file.
[17.06.2025 06:18] Generating page.
[17.06.2025 06:18] Renaming previous page.
[17.06.2025 06:18] Renaming previous data. index.html to ./d/2025-06-17.html
[17.06.2025 06:18] Writing result.
[17.06.2025 06:18] Renaming log file.
[17.06.2025 06:18] Renaming previous data. log.txt to ./logs/2025-06-17_last_log.txt
