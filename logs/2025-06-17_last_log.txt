[17.06.2025 04:22] Read previous papers.
[17.06.2025 04:22] Generating top page (month).
[17.06.2025 04:22] Writing top page (month).
[17.06.2025 05:12] Read previous papers.
[17.06.2025 05:12] Get feed.
[17.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13585
[17.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10521
[17.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13759
[17.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.11763
[17.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08343
[17.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12915
[17.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13654
[17.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03968
[17.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.07961
[17.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13750
[17.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12450
[17.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12189
[17.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06366
[17.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12953
[17.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12623
[17.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13752
[17.06.2025 05:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.06.2025 05:12] No deleted papers detected.
[17.06.2025 05:12] Downloading and parsing papers (pdf, html). Total: 16.
[17.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.13585.
[17.06.2025 05:12] Extra JSON file exists (./assets/json/2506.13585.json), skip PDF parsing.
[17.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.13585.json), skip HTML parsing.
[17.06.2025 05:12] Success.
[17.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.10521.
[17.06.2025 05:12] Extra JSON file exists (./assets/json/2506.10521.json), skip PDF parsing.
[17.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.10521.json), skip HTML parsing.
[17.06.2025 05:12] Success.
[17.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.13759.
[17.06.2025 05:12] Extra JSON file exists (./assets/json/2506.13759.json), skip PDF parsing.
[17.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.13759.json), skip HTML parsing.
[17.06.2025 05:12] Success.
[17.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.11763.
[17.06.2025 05:12] Extra JSON file exists (./assets/json/2506.11763.json), skip PDF parsing.
[17.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.11763.json), skip HTML parsing.
[17.06.2025 05:12] Success.
[17.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.08343.
[17.06.2025 05:12] Extra JSON file exists (./assets/json/2506.08343.json), skip PDF parsing.
[17.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.08343.json), skip HTML parsing.
[17.06.2025 05:12] Success.
[17.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.12915.
[17.06.2025 05:12] Extra JSON file exists (./assets/json/2506.12915.json), skip PDF parsing.
[17.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.12915.json), skip HTML parsing.
[17.06.2025 05:12] Success.
[17.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.13654.
[17.06.2025 05:12] Extra JSON file exists (./assets/json/2506.13654.json), skip PDF parsing.
[17.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.13654.json), skip HTML parsing.
[17.06.2025 05:12] Success.
[17.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.03968.
[17.06.2025 05:12] Extra JSON file exists (./assets/json/2506.03968.json), skip PDF parsing.
[17.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.03968.json), skip HTML parsing.
[17.06.2025 05:12] Success.
[17.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.07961.
[17.06.2025 05:12] Extra JSON file exists (./assets/json/2506.07961.json), skip PDF parsing.
[17.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.07961.json), skip HTML parsing.
[17.06.2025 05:12] Success.
[17.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.13750.
[17.06.2025 05:12] Extra JSON file exists (./assets/json/2506.13750.json), skip PDF parsing.
[17.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.13750.json), skip HTML parsing.
[17.06.2025 05:12] Success.
[17.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.12450.
[17.06.2025 05:12] Extra JSON file exists (./assets/json/2506.12450.json), skip PDF parsing.
[17.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.12450.json), skip HTML parsing.
[17.06.2025 05:12] Success.
[17.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.12189.
[17.06.2025 05:12] Extra JSON file exists (./assets/json/2506.12189.json), skip PDF parsing.
[17.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.12189.json), skip HTML parsing.
[17.06.2025 05:12] Success.
[17.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.06366.
[17.06.2025 05:12] Extra JSON file exists (./assets/json/2506.06366.json), skip PDF parsing.
[17.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.06366.json), skip HTML parsing.
[17.06.2025 05:12] Success.
[17.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.12953.
[17.06.2025 05:12] Extra JSON file exists (./assets/json/2506.12953.json), skip PDF parsing.
[17.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.12953.json), skip HTML parsing.
[17.06.2025 05:12] Success.
[17.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.12623.
[17.06.2025 05:12] Extra JSON file exists (./assets/json/2506.12623.json), skip PDF parsing.
[17.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.12623.json), skip HTML parsing.
[17.06.2025 05:12] Success.
[17.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.13752.
[17.06.2025 05:12] Extra JSON file exists (./assets/json/2506.13752.json), skip PDF parsing.
[17.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.13752.json), skip HTML parsing.
[17.06.2025 05:12] Success.
[17.06.2025 05:12] Enriching papers with extra data.
[17.06.2025 05:12] ********************************************************************************
[17.06.2025 05:12] Abstract 0. A hybrid-attention reasoning model called MiniMax-M1, featuring a Mixture-of-Experts architecture and lightning attention mechanism, is introduced for efficient long-input processing and reinforcement learning.  					AI-generated summary 				 We introduce MiniMax-M1, the world's first open-weight, l...
[17.06.2025 05:12] ********************************************************************************
[17.06.2025 05:12] Abstract 1. Scientists' First Exam (SFE) benchmark assesses scientific cognitive capacities of Multimodal Large Language Models through perception, understanding, and comparative reasoning.  					AI-generated summary 				 Scientific discoveries increasingly rely on complex multimodal reasoning based on informat...
[17.06.2025 05:12] ********************************************************************************
[17.06.2025 05:12] Abstract 2. Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs) enable parallel generation and faster inference compared to autoregressive models through denoising-based strategies and full attention mechanisms.  					AI-generated summary 				 In this work, we p...
[17.06.2025 05:12] ********************************************************************************
[17.06.2025 05:12] Abstract 3. DeepResearch Bench offers a benchmark framework to evaluate the capabilities of Deep Research Agents in terms of research quality and information retrieval accuracy across multiple fields.  					AI-generated summary 				 Deep Research Agents are a prominent category of LLM-based agents. By autonomou...
[17.06.2025 05:12] ********************************************************************************
[17.06.2025 05:12] Abstract 4. NoWait suppresses explicit self-reflection tokens during inference to enhance efficiency in multimodal reasoning without reducing model utility.  					AI-generated summary 				 Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant over...
[17.06.2025 05:12] ********************************************************************************
[17.06.2025 05:12] Abstract 5. A new benchmark, PersonaFeedback, evaluates Large Language Models' ability to generate personalized responses given explicit user personas, revealing limitations in current systems.  					AI-generated summary 				 With the rapid improvement in the general capabilities of LLMs, LLM personalization, i...
[17.06.2025 05:12] ********************************************************************************
[17.06.2025 05:12] Abstract 6. Ego-R1, a reinforcement learning-based framework, uses a structured tool-augmented chain-of-thought process to reason over ultra-long egocentric videos, achieving better performance than existing methods by extending time coverage to a week.  					AI-generated summary 				 We introduce Ego-R1, a nov...
[17.06.2025 05:12] ********************************************************************************
[17.06.2025 05:12] Abstract 7. The paper presents a method for generating diverse and complex instruction data for large language models using attributed grounding, achieving top performance on benchmarks with a large synthesized dataset.  					AI-generated summary 				 The pursuit of diverse, complex, and large-scale instruction...
[17.06.2025 05:12] ********************************************************************************
[17.06.2025 05:12] Abstract 8. BridgeVLA is a 3D vision-language-action model that projects 3D inputs to 2D images and uses 2D heatmaps for efficient and effective action prediction, outperforming baselines in various benchmarks.  					AI-generated summary 				 Recently, leveraging pre-trained vision-language models (VLMs) for bu...
[17.06.2025 05:12] ********************************************************************************
[17.06.2025 05:12] Abstract 9. Test3R, a test-time learning technique for 3D reconstruction, enhances geometric accuracy by optimizing network consistency using self-supervised learning on image triplets.  					AI-generated summary 				 Dense matching methods like DUSt3R regress pairwise pointmaps for 3D reconstruction. However, ...
[17.06.2025 05:12] ********************************************************************************
[17.06.2025 05:12] Abstract 10. Research confirms natural representation alignment in large language models and introduces Inference-Time Language Control to enhance cross-lingual performance.  					AI-generated summary 				 Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across tasks and lang...
[17.06.2025 05:12] ********************************************************************************
[17.06.2025 05:12] Abstract 11. The study evaluates various LLMs on diverse text tasks using a new dataset, revealing distinct personality traits and improving model interpretability.  					AI-generated summary 				 Large Language Models (LLMs) are increasingly integrated into everyday applications. As their influence grows, under...
[17.06.2025 05:12] ********************************************************************************
[17.06.2025 05:12] Abstract 12. A new field, AI Agent Behavioral Science, is proposed to systematically study the behaviors of AI agents in diverse contexts, emphasizing external factors and their interactions, and addressing responsible AI aspects.  					AI-generated summary 				 Recent advances in large language models (LLMs) ha...
[17.06.2025 05:12] ********************************************************************************
[17.06.2025 05:12] Abstract 13. PatchInstruct enhances LLM forecasting quality through specialized prompting methods that include time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) have demonstrated new pos...
[17.06.2025 05:12] ********************************************************************************
[17.06.2025 05:12] Abstract 14. A novel benchmark and dataset are proposed for multi-modal summarization of UI instructional videos, addressing the need for step-by-step executable instructions and key video frames.  					AI-generated summary 				 We study multi-modal summarization for instructional videos, whose goal is to provid...
[17.06.2025 05:12] ********************************************************************************
[17.06.2025 05:12] Abstract 15. Budget guidance is a method that steers LLM reasoning within a targeted budget without fine-tuning and achieves improved efficiency and performance on math benchmarks.  					AI-generated summary 				 Recent deep-thinking large language models often reason extensively to improve performance, but such...
[17.06.2025 05:12] Read previous papers.
[17.06.2025 05:12] Generating reviews via LLM API.
[17.06.2025 05:12] Using data from previous issue: {"categories": ["#rl", "#training", "#open_source", "#architecture", "#reasoning", "#long_context", "#optimization"], "emoji": "üß†", "ru": {"title": "MiniMax-M1: –ì–∏–±—Ä–∏–¥–Ω—ã–π –ò–ò –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "MiniMax-M1 - —ç—Ç–æ –≥–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Mixture-of-Exp
[17.06.2025 05:12] Using data from previous issue: {"categories": ["#reasoning", "#science", "#multimodal", "#benchmark"], "emoji": "üî¨", "ru": {"title": "–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞—É—á–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò", "desc": "–£—á—ë–Ω—ã–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Scientists' First Exam (SFE) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞—É—á–Ω—ã—Ö –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ
[17.06.2025 05:12] Using data from previous issue: {"categories": ["#math", "#training", "#diffusion", "#inference", "#multimodal", "#survey"], "emoji": "üß†", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —è–∑—ã–∫–æ–≤–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏: –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (dLLMs) –∏
[17.06.2025 05:12] Using data from previous issue: {"categories": ["#open_source", "#science", "#agents", "#alignment", "#benchmark"], "emoji": "üî¨", "ru": {"title": "–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –≥–ª—É–±–æ–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π", "desc": "DeepResearch Bench - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω—Ç–æ–≤ –≥–ª—É–±–æ–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏ —Ç
[17.06.2025 05:12] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#inference", "#multimodal"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ NoWait, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–¥–∞–≤–ª—è–µ—Ç —Ç–æ–∫–µ–Ω—ã —è–≤–Ω–æ–π —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å
[17.06.2025 05:12] Using data from previous issue: {"categories": ["#alignment", "#interpretability", "#multimodal", "#benchmark"], "emoji": "üé≠", "ru": {"title": "PersonaFeedback: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ PersonaFeedback –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤
[17.06.2025 05:12] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#benchmark", "#training", "#multimodal", "#long_context", "#rl"], "emoji": "üé•", "ru": {"title": "Ego-R1: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–Ω–∞–ª–∏–∑–µ –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ", "desc": "Ego-R1 - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–≤–µ—Ä—Ö–¥–ª–∏–Ω–Ω—ã—Ö —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è —Å—Ç
[17.06.2025 05:12] Using data from previous issue: {"categories": ["#dataset", "#synthetic", "#alignment", "#data", "#benchmark"], "emoji": "üß†", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∏ —Å–ª–æ–∂–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π 
[17.06.2025 05:12] Using data from previous issue: {"categories": ["#rl", "#3d", "#games", "#optimization", "#agents", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "BridgeVLA: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ —á–µ—Ä–µ–∑ –ø—Ä–æ–µ–∫—Ü–∏—é 3D –≤ 2D", "desc": "BridgeVLA - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è 3D-–∑—Ä–µ–Ω–∏–µ, —è–∑—ã–∫ –∏
[17.06.2025 05:12] Using data from previous issue: {"categories": ["#3d", "#training", "#optimization"], "emoji": "üèõÔ∏è", "ru": {"title": "Test3R: –ü–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —á–µ—Ä–µ–∑ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "Test3R - —ç—Ç–æ –Ω–æ–≤–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å.
[17.06.2025 05:12] Using data from previous issue: {"categories": ["#alignment", "#machine_translation", "#inference", "#multilingual"], "emoji": "üåê", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –∫—Ä–æ—Å—Å-—è–∑—ã–∫–æ–≤—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π LLM —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö 
[17.06.2025 05:12] Using data from previous issue: {"categories": ["#dataset", "#small_models", "#reasoning", "#long_context", "#interpretability", "#multimodal", "#benchmark"], "emoji": "üß†", "ru": {"title": "–†–∞—Å–∫—Ä—ã–≤–∞—è –ª–∏—á–Ω–æ—Å—Ç—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –±–æ–ª—å—à–∏
[17.06.2025 05:12] Using data from previous issue: {"categories": ["#agi", "#healthcare", "#ethics", "#multimodal", "#agents", "#interpretability"], "emoji": "üß†", "ru": {"title": "–û—Ç –º–æ–¥–µ–ª–∏ –∫ –ø–æ–≤–µ–¥–µ–Ω–∏—é: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –∏–∑—É—á–µ–Ω–∏–µ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è –æ–±–ª–∞—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π - –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∞—è –Ω–∞—É–∫–∞ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤. –û–Ω–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∏
[17.06.2025 05:12] Using data from previous issue: {"categories": ["#data", "#training", "#optimization"], "emoji": "üìà", "ru": {"title": "–¢–æ—á–Ω–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ PatchInstruct –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM).
[17.06.2025 05:12] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#video", "#dataset"], "emoji": "üé¨", "ru": {"title": "–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å—É–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –≤–∏–¥–µ–æ –ø–æ UI", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π —Å—É–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –≤–∏–¥–µ–æ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞
[17.06.2025 05:12] Using data from previous issue: {"categories": ["#optimization", "#inference", "#training", "#reasoning", "#math"], "emoji": "üí°", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ –ò–ò –≤ —Ä–∞–º–∫–∞—Ö –±—é–¥–∂–µ—Ç–∞", "desc": "–ú–µ—Ç–æ–¥ '–±—é–¥–∂–µ—Ç–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞' –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ø—Ä–∞–≤–ª—è—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–∞–º–∫–∞—Ö –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –±—é–¥–∂–µ—Ç–∞ –±–µ–∑ –¥–æ–ø–æ
[17.06.2025 05:12] Renaming data file.
[17.06.2025 05:12] Renaming previous data. hf_papers.json to ./d/2025-06-17.json
[17.06.2025 05:12] Saving new data file.
[17.06.2025 05:12] Generating page.
[17.06.2025 05:12] Renaming previous page.
[17.06.2025 05:12] Renaming previous data. index.html to ./d/2025-06-17.html
[17.06.2025 05:12] Writing result.
[17.06.2025 05:12] Renaming log file.
[17.06.2025 05:12] Renaming previous data. log.txt to ./logs/2025-06-17_last_log.txt
