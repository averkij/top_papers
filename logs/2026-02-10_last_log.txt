[10.02.2026 01:48] Read previous papers.
[10.02.2026 01:48] Generating top page (month).
[10.02.2026 01:48] Writing top page (month).
[10.02.2026 04:27] Read previous papers.
[10.02.2026 04:27] Get feed.
[10.02.2026 04:27] Extract page data from URL. URL: https://huggingface.co/papers/2602.07026
[10.02.2026 04:27] Extract page data from URL. URL: https://huggingface.co/papers/2602.06422
[10.02.2026 04:27] Extract page data from URL. URL: https://huggingface.co/papers/2602.07962
[10.02.2026 04:27] Extract page data from URL. URL: https://huggingface.co/papers/2602.06454
[10.02.2026 04:27] Extract page data from URL. URL: https://huggingface.co/papers/2602.07075
[10.02.2026 04:27] Extract page data from URL. URL: https://huggingface.co/papers/2602.06540
[10.02.2026 04:27] Extract page data from URL. URL: https://huggingface.co/papers/2601.21363
[10.02.2026 04:27] Extract page data from URL. URL: https://huggingface.co/papers/2602.06694
[10.02.2026 04:27] Extract page data from URL. URL: https://huggingface.co/papers/2602.08236
[10.02.2026 04:27] Extract page data from URL. URL: https://huggingface.co/papers/2602.06445
[10.02.2026 04:27] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.02.2026 04:27] Downloading and parsing papers (pdf, html). Total: 10.
[10.02.2026 04:27] Downloading and parsing paper https://huggingface.co/papers/2602.07026.
[10.02.2026 04:28] Downloading paper 2602.07026 from https://arxiv.org/pdf/2602.07026v1...
[10.02.2026 04:28] Extracting affiliations from text.
[10.02.2026 04:28] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Modality GapDriven Subspace Alignment Training Paradigm For Multimodal Large Language Models Xiaomin Yu1,2, Yi Xin3,4, Wenjie Zhang1, Chonghan Liu5, Hanzhen Zhao2 Xiaoxing Hu6, Xinlei Yu2, Ziyue Qiao7, Hao Tang8, Xue Yang6 Xiaobin Hu2, Chengwei Qin1, Hui Xiong1, Yu Qiao4, Shuicheng YAN2 1HKUST(GZ), 2NUS, 3sh AILab, 4SII, 5UCLA, 6SJTU, 7GBU, 8PKU Corresponding Authors "
[10.02.2026 04:28] Response: ```python
["HKUST(GZ)", "NUS", "sh AILab", "SII", "UCLA", "SJTU", "GBU", "PKU"]
```
[10.02.2026 04:28] Deleting PDF ./assets/pdf/2602.07026.pdf.
[10.02.2026 04:28] Success.
[10.02.2026 04:28] Downloading and parsing paper https://huggingface.co/papers/2602.06422.
[10.02.2026 04:29] Downloading paper 2602.06422 from https://arxiv.org/pdf/2602.06422v1...
[10.02.2026 04:29] Extracting affiliations from text.
[10.02.2026 04:29] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO Yunze Tong 1 * Mushui Liu 1 2 * Canyu Zhao 1 Wanggui He 2 Shiyi Zhang 3 Hongwei Zhang 1 Peng Zhang 2 Jinlong Liu 2 Ju Huang 2 Jiamang Wang 2 Hao Jiang 2 Pipei Huang 2 6 2 0 2 6 ] . [ 1 2 2 4 6 0 . 2 0 6 2 : r a "
[10.02.2026 04:29] Response: ```python
[]
```
[10.02.2026 04:29] Extracting affiliations from text.
[10.02.2026 04:29] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO Yunze Tong 1 * Mushui Liu 1 2 * Canyu Zhao 1 Wanggui He 2 Shiyi Zhang 3 Hongwei Zhang 1 Peng Zhang 2 Jinlong Liu 2 Ju Huang 2 Jiamang Wang 2 Hao Jiang 2 Pipei Huang 2 6 2 0 2 6 ] . [ 1 2 2 4 6 0 . 2 0 6 2 : r aDeploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TPGRPO), GRPO framework that alleviates stepwise reward sparsity and explicitly models longterm effects within the denoising trajectory. TPGRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing dense, step-aware learning signal that better isolates each denoising actions pure effect, and (ii) it identifies turning pointssteps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trendand assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameterfree. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github. com/YunzeTong/TurningPoint-GRPO. 1. Introduction Flow Matching (FM) (Lipman et al., 2022; Liu et al., 2022) models can transport simple prior to complex target dis- *Equal contribution 1Zhejiang University, Hangzhou, China 2Alibaba Group, Hangzhou, China 3Tsinghua University, Beijing, China. Correspondence to: Hao Jiang <aoshu.jh@taobao.com>. Preprint. February 9, 2026. tribution via the learned velocity field, and have been widely adopted for text-to-image generation. Motivated by recent progress in large language models, researchers have applied Group Relative Policy Optimization (GRPO) (Shao et al., 2024; Guo et al., 2025) to FM models. Representative methods such as Flow-GRPO (Liu et al., 2025) and DanceGRPO (Xue et al., 2025) evaluate reward on the final generated image and assign this same terminal reward to each preceding denoising step produced by Stochastic Differential Equation (SDE)-based sampler (Song et al., 2021). Advantages computed from these per-step rewards enable effective post-RL fine-tuning and can improve overall performance. However, this design does not accurately model step-level reward assignment. It leads to two issues: (1) The reward reflects the aggregate effect of the entire denoising trajectory and is identically assigned to every step, without distinguishing the contribution of different steps, which induces reward sparsity. (2) Existing methods mainly leverage trajectorylevel ranking across sampled trajectories, while neglecting within-trajectory interactions among steps, even though such dependencies are crucial for composing coherent sample. We illustrate the first issue by sampling several trajectories and visualizing their step-wise reward evolution in Figure 1. For latent at the intermediate step, we perform Ordinary Differential Equation (ODE) sampling for the remaining steps to obtain clean image on which the reward can be evaluated. This procedure is motivated by the fact that, compared to SDE sampling, ODE sampling removes stochasticity while preserving the same marginal distributions (Song et al., 2021); thus, the ODE completion can be interpreted as an average over possible SDE outcomes from the same intermediate state. With this estimator, Figure 1 shows that the reward can oscillate frequently during denoising. In contrast, Flow-GRPO assigns only the reward of clean images to all preceding steps, which captures the relative ordering of complete trajectories but may conflict with local progress. For example, from = 6 to = 5, the orange and green trajectories exhibit local reward decrease; yet because their full SDE-based samples achieve higher terminal rewards at = 0, they receive larger advantages for this step, incorrectly reinforcing an action that locally degrades reward. Overall, this outcome-based reward allocation cannot isolate Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO sampling at that step makes the subsequent reward evolution consistent with the overall trend. For these turning-point actions, we assign an aggregated long-term reward as feedback. This encourages directions that are likely to improve future reward trends and discourages those leading to overall reward drops. Crucially, turning points are identified solely by sign changes in incremental rewards, not their magnitudes, making our method efficient and hyperparameter-free. We summarize our contributions as follows: We identify reward sparsity and step-level misalignment caused by propagating an outcome-based reward to intermediate denoising steps. We address this by using step-wise reward differences to capture the incremental effect of each SDE update, yielding better estimate of single steps pure gain. Based on this fine-grained signal, we uncover turning pointssteps that flip the local reward trend to match the overall trajectory. We provide strict sign-based criterion to identify them and assign aggregated longterm rewards to model their delayed impact. To our knowledge, this is the first work to explicitly model such implicit interaction in Flow-based GRPO. Building on these insights, we propose TurningPointGRPO, which mitigates reward sparsity and improves delayed credit assignment. Extensive experiments further demonstrate the effectiveness of our method. 2. Related Work Common RL Techniques. Reinforcement learning (RL) has become central tool for aligning large language models (LLMs) with human preferences and downstream objectives (Guo et al., 2025; Jaech et al., 2024). standard pipeline first trains reward model from human preference data, and then optimizes the policy using proximal policy optimization (PPO) (Schulman et al., 2017). To better exploit multiple candidates per prompt, group-based ranking objectives have been"
[10.02.2026 04:29] Mistral response. {"id": "939f8eac8943404aa0406693de2efef4", "created": 1770697753, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1437, "total_tokens": 1475, "completion_tokens": 38, "num_cached_tokens": 1436}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Zhejiang University, Hangzhou, China\", \"Alibaba Group, Hangzhou, China\", \"Tsinghua University, Beijing, China\"]\n```"}}]}
[10.02.2026 04:29] Response: ```python
["Zhejiang University, Hangzhou, China", "Alibaba Group, Hangzhou, China", "Tsinghua University, Beijing, China"]
```
[10.02.2026 04:29] Deleting PDF ./assets/pdf/2602.06422.pdf.
[10.02.2026 04:29] Success.
[10.02.2026 04:29] Downloading and parsing paper https://huggingface.co/papers/2602.07962.
[10.02.2026 04:29] Downloading paper 2602.07962 from https://arxiv.org/pdf/2602.07962v1...
[10.02.2026 04:29] Extracting affiliations from text.
[10.02.2026 04:29] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth Weihao Zeng * 1 Yuzhen Huang * 1 Junxian He "
[10.02.2026 04:29] Response: ```python
[]
```
[10.02.2026 04:29] Extracting affiliations from text.
[10.02.2026 04:29] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth Weihao Zeng * 1 Yuzhen Huang * 1 Junxian HeLarge language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, phenomenon known as context rot. Existing long-context benchmarks primarily focus on single-step settings that evaluate models ability to retrieve information from long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agents context length. This design enables LOCA-bench to extend the context length potentially to infinity in controlled way while keeping the underlying task semantics fixed. LOCAbench evaluates language agents as combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCAbench to provide platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench. 6 2 0 2 8 ] . [ 1 2 6 9 7 0 . 2 0 6 2 : r 1. Introduction large language models (LLMs) (Anthropic, Frontier 2025g;h; OpenAI, 2025a; Google, 2025; Google DeepMind) are increasingly capable of handling real-world, long- *Equal contribution to: Weihao Zeng <wzengak@cse.ust.hk>, Yuzhen Huang <yhuanghj@cse.ust.hk>, Junxian He <junxianh@cse.ust.hk>. 1HKUST. Correspondence Preprint. February 10, 2026. 1 running tasks that would take humans significant time, such as software engineering (Jimenez et al., 2023; Lin, 2026), deep research (OpenAI, 2025; Google, 2025), and agentic workflows (Li et al., 2025; Team, 2025; Wu et al., 2025). As these tasks grow in complexity, the amount of text an LLM must keep track of within its context window is also expanding rapidly, from few thousand tokens to hundreds of thousands, millions, and potentially more (Lee, 2025). Although state-of-the-art models now offer context windows on the order of millions of tokens (Google, 2025; Google DeepMind), in practice they do not use every part of that context equally well (Lee, 2025). As more tokens are added, performance often becomes less consistent and more errorprone, an effect commonly referred to as context rot (Lee, 2025; Chroma, 2025; Anthropic, 2025e). Designing challenging benchmarks that track the longcontext difficulties models face in real-world applications is non-trivial. Existing long-context benchmarks still fall short of realistic scenarios. Most assume static setting: the model either receives all relevant information up front, or can obtain it with straightforward retrieval step (Zhou et al., 2025; Chen et al., 2025). The task then mainly reduces to locating few key snippets (e.g., needle in haystack (Kamradt, 2023)) or single-step aggregation of scattered facts (Hsieh et al., 2024; Vodrahalli et al., 2024; OpenAI, 2025b; Bertsch et al., 2025; Bai et al., 2025). Realworld use, especially in agentic settings, is often dynamic. An agent typically begins with limited knowledge about its environment. It must decide what to look for, explore during execution, and continually add newly discovered information to its context (Anthropic, 2025e). The core difficulty is not just finding the right evidence once, but remaining organized and reliable at every action as the context grows over time. In this work, we introduce LOCA-bench, benchmark for LOng-Context Agents under extreme and controllable context growth. LOCA-bench is built on tasks drawn from realworld scenarios, where models must actively explore an environment through tools that are grounded in real-world sources. Different from other agent benchmarks, LOCAbench specifically targets long-context modeling abilities in agentic scenarios, where the evaluation varies context length in an automated and controllable manner while keepLOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth Figure 1. Overview of results.Left: Accuracy changes across models as the environment description length increases. Right: Accuracy gains from different context engineering strategies for Gemini-3-Flash and GPT-5.2-Medium at 128K environment description length. ing the task semantics unchanged. Concretely, LOCA-bench varies the environment description length, which reflects the amount of information in the initial environment state, such as the size of an Excel sheet, PDF file, or other databases. The core intuition is that as the initial description length increases, agents are required to handle increasingly long contexts during environment exploration, while the underlying task prompts remain fixed. Rather than focusing solely on retrieving relevant facts for given question as in prior long context benchmarks, LOCAbench introduces combination of challenges that emerge as the context grows: (1) Complex retrieval and reasoning, where agents often need to retrieve multiple pieces of relevant information from tool outputs and reason over them jointly; (2) Instruction following, since the tasks are designed with multiple constraints that must be satisfied, and agents frequently forget earlier instructions; (3) Environment exploration, as our experiments show that agents tend to explore less and behave more conservatively when the context becomes long; and (4) Hallucination, where models are more prone to hallucinate under longer contexts, often subtly altering factual details during generation. As shown in Figure 1 Left, most models perform strongly when the context is short, with accuracy typically above 70%. As the context grows, performance drops sharply even though the underlying task does not change, and the gap between frontier models and open-source models becomes increasingly pronounced. Moreover, LOCA-bench treats language agents as combination of models and scaffolds, and aims to serve as platform for assessing wide range of models as well as scaffolds, including different context management strat"
[10.02.2026 04:29] Mistral response. {"id": "59eb6a7799144c83b6818e9d0d66c424", "created": 1770697795, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1508, "total_tokens": 1517, "completion_tokens": 9, "num_cached_tokens": 1507}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"HKUST\"]\n```"}}]}
[10.02.2026 04:29] Response: ```python
["HKUST"]
```
[10.02.2026 04:29] Deleting PDF ./assets/pdf/2602.07962.pdf.
[10.02.2026 04:29] Success.
[10.02.2026 04:29] Downloading and parsing paper https://huggingface.co/papers/2602.06454.
[10.02.2026 04:29] Downloading paper 2602.06454 from https://arxiv.org/pdf/2602.06454v1...
[10.02.2026 04:29] Extracting affiliations from text.
[10.02.2026 04:29] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"RelayGen: Intra-Generation Model Switching for Efficient Reasoning Jiwon Song1, Yoongon Kim1, Jae-Joon Kim1 1Seoul National University {jiwon.song,yoon_g_kim,kimjaejoon}@snu.ac.kr https://github.com/jiwonsong-dev/RelayGen 6 2 0 2 6 ] . [ 1 4 5 4 6 0 . 2 0 6 2 : r a "
[10.02.2026 04:29] Response: ```python
["Seoul National University"]
```
[10.02.2026 04:29] Deleting PDF ./assets/pdf/2602.06454.pdf.
[10.02.2026 04:29] Success.
[10.02.2026 04:29] Downloading and parsing paper https://huggingface.co/papers/2602.07075.
[10.02.2026 04:30] Downloading paper 2602.07075 from https://arxiv.org/pdf/2602.07075v1...
[10.02.2026 04:30] Extracting affiliations from text.
[10.02.2026 04:30] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 ] - c . s [ 1 5 7 0 7 0 . 2 0 6 2 : r LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning "
[10.02.2026 04:30] Response: ```python
[]
```
[10.02.2026 04:30] Extracting affiliations from text.
[10.02.2026 04:30] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 ] - c . s [ 1 5 7 0 7 0 . 2 0 6 2 : r LatentChem: From Textual CoT to Latent Thinking in Chemical ReasoningChemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves 59.88% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering 10.84 average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories. Date: February 10, 2026 Code: https://github.com/xinwuye/LatentChemLLMs have emerged as transformative tools for scientific discovery, facilitating tasks ranging from molecule generation to reaction synthesis [13, 44, 49]. In this context, chemical reasoning is typically mediated through explicit CoT [48, 50, 51]. In current chemical LLM paradigms, models are trained to linearize complex physicochemical intuitions, such as electron delocalization or steric hindrance, into discrete sequence of natural language tokens before arriving at solution. While this CoT approach has unified scientific tasks under generative framework [54], it relies on the implicit assumption that natural language is an adequate vehicle for the continuous physicochemical dynamics inherent to chemistry. However, it remains unclear whether such discrete symbolic reasoning is optimal, for complex chemical reasoning tasks [21, 49]. We hypothesize that forcing chemical logic into linguistic bottleneck results in fundamental continuitydiscretization gap in chemical reasoning (Figure 1). Much like an expert chemist who manipulates abstract 3D structures mentally before verbalizing the result, we posit that the core of chemical reasoning, such as navigating manifolds, optimizing properties, and identifying substructures, is more naturally performed in continuous latent space. In this view, natural language should serve merely as the input/output interface rather than the computational substrate for the reasoning process itself. 1 Figure 1Conceptual illustration of the continuitydiscretization gap in chemical reasoning. (a) The intrinsic chemical property landscape is continuous and high-dimensional. (b) We posit that continuous latent space can theoretically offer smoother optimization surface akin to the property landscape, avoiding the jagged trajectories of discrete tokens. (c) Linguistic tokenization fragments chemical state transitions into discrete symbolic steps, inducing staircase-like landscapes and inefficient reasoning paths. To study this hypothesis, we introduce LatentChem, latent reasoning interface [7, 14] designed to decouple chemical reasoning from explicit natural language generation. As illustrated in Figure 2, unlike standard models that rigidly bind reasoning to token output, LatentChem injects lightweight sequence of continuous thought vectors between perception and generation. Crucially, we equip this interface with ChemUpdater mechanism, allowing these latent thoughts to actively re-query molecular features in dynamic loop. This architecture serves as an experimental instrument, allowing us to observe whether the model prefers to reason via explicit text or through internal latent dynamics when optimized for task success. Our investigation reveals an intriguing emergent behavior: the model exhibits spontaneous internalization of chemical logic. Despite being initialized with explicit CoT data, when optimized via reinforcement learning with rewards only on format adherence, validity, and correctness, LatentChem voluntarily discards verbose textual reasoning chains in the majority of tasks. Instead, it predominantly collapses the reasoning process into the continuous latent space, outputting solutions directly after sequence of silent thought vectors. While this shift sacrifices the explicit readability of intermediate steps, it strongly suggests that the model perceives natural language as low-bandwidth constraint, identifying the continuous latent reasoning as more native and expressive mode for chemical logic. Crucially, this spontaneous shift does not imply that the model is merely taking shortcut to minimize generation effort, as such behavior would typically degrade task performance. On the contrary, our experimental results demonstrate that this internalization represents superior computational strategy. Quantitatively, by choosing to compress linguistic steps into compact latent states, LatentChem achieves dramatic efficiency gain, reducing the reasoning token overhead by an average of 10.84 across all benchmarks. In molecule optimization and reaction tasks, this reduction factor even exceeds 28. This efficiency is achieved alongside dominant performance, as evidenced by extensive evaluations across four diverse benchmarks, ChemCoTBench [20], Mol-Instructions [11], ChEBI-20 [10], and ChemLLMBench [13]. Notably, LatentChem achieves 59.88% non-tie win rate against the strong explicit CoT baseline on the reasoning-intensive ChemCoTBench. The simultaneous improvement in both speed and accuracy confirms that decoupling reasoning from language unlocks more native and effective reasoning paradigm for chemical LLM. In summary, our contributions are as follows: We propose reasoning mode to investigate chemical LLMs, challenging the necessity of natural language CoT for chemical tasks. We introduce LatentChem, system that empowers chemical LLMs to perform latent thinking via continuous thought vectors and dynamic perception loop. We report the phenomenon of spontaneous internalization, where the model autonomously absorbs explicit"
[10.02.2026 04:30] Mistral response. {"id": "d92864053f464f0cbe72c2879c6de4a5", "created": 1770697851, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1267, "total_tokens": 1273, "completion_tokens": 6, "num_cached_tokens": 1266}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}}]}
[10.02.2026 04:30] Response: ```python
[]
```
[10.02.2026 04:30] Deleting PDF ./assets/pdf/2602.07075.pdf.
[10.02.2026 04:30] Success.
[10.02.2026 04:30] Downloading and parsing paper https://huggingface.co/papers/2602.06540.
[10.02.2026 04:30] Downloading paper 2602.06540 from https://arxiv.org/pdf/2602.06540v1...
[10.02.2026 04:30] Extracting affiliations from text.
[10.02.2026 04:30] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 ] . [ 1 0 4 5 6 0 . 2 0 6 2 : r AgentCPM-Report AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research AgentCPM Team https://huggingface.co/openbmb/AgentCPM-Report https://huggingface.co/openbmb/AgentCPM-Report-GGUF https://github.com/OpenBMB/AgentCPM Abstract Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing significant challenge for current language models. Most existing approaches follow plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for userauthored data. In this work, we present AgentCPM-Report, lightweight yet highperforming local solution composed of framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce Multi-Stage Agentic Training strategy, consisting of coldstart, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight. Open-ended deep research requires artificial agents to navigate vast information landscapes and synthesize their findings into coherent, insightful reports (OpenAI, 2025; Google, 2025; x.AI, 2025;"
[10.02.2026 04:30] Response: ```python
[]
```
[10.02.2026 04:30] Extracting affiliations from text.
[10.02.2026 04:30] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 ] . [ 1 0 4 5 6 0 . 2 0 6 2 : r AgentCPM-Report AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research AgentCPM Team https://huggingface.co/openbmb/AgentCPM-Report https://huggingface.co/openbmb/AgentCPM-Report-GGUF https://github.com/OpenBMB/AgentCPM Abstract Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing significant challenge for current language models. Most existing approaches follow plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for userauthored data. In this work, we present AgentCPM-Report, lightweight yet highperforming local solution composed of framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce Multi-Stage Agentic Training strategy, consisting of coldstart, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.Open-ended deep research requires artificial agents to navigate vast information landscapes and synthesize their findings into coherent, insightful reports (OpenAI, 2025; Google, 2025; x.AI, 2025; Perplexity, 2025; Kimi, 2025; ByteDance, 2025). In the context of such complex inquiry, writing is far more than the mere transcription of retrieved data. Instead, it reflects the knowledge-transforming process described in cognitive psychology (Scardamalia & Bereiter, 1987): because the information landscape is initially opaque, researchers rarely execute rigid, end-to-end plan derived solely from pre-existing thoughts. Rather, writing itself functions as reasoning mechanism, progressively revealing what is not yet known. Indeed, researchers often identify gaps, contradictions, or novel directions only during the act of drafting, indicating that effective synthesis depends on tight and continual coupling between planning and writing. Despite this, existing deep research systems struggle to replicate such dynamics. Early approaches followed retrieval-then-write paradigm (Fig. 1a) (Hu et al., 2025), in which agents generated content sequentially based on retrieved evidence. While flexible, this loosely structured process frequently degenerates into incoherence over long horizons. To improve structural consistency, more recent frameworks (Fig. 1b) (Wang et al., 2024, 2025; Yan et al., 2025), such as WebWeaver (Li et al., 2025b), adopt plan-then-write paradigm. By freezing comprehensive outline prior to writing, these systems enforce global structure and stability. However, this paradigm rests on the assumption of initial information completenessan assumption that is often violated in open-ended research. By reducing the downstream writer to an executor of static blueprint, this rigid separation prevents agents from capturing emergent insights: subtle connections and refinements that surface 1 AgentCPM-Report Figure 1: Comparison of different writing paradigms. only when articulating concrete arguments. As result, such methods encounter an insight ceiling, producing reports that are structurally sound yet intellectually shallow. Another critical limitation of the plan-then-write paradigm lies in its heavy reliance on generating highquality, comprehensive outline before writing begins. This requirement places substantial demands on the models reasoning capacity and domain knowledge. Smaller models are generally weaker in these aspects than large-scale models, which has led most existing deep research systems to rely on closed-source or online large models as their backbone. This reliance introduces practical and often overlooked challenge: online deployment makes it difficult to support writing over users local or private data, as uploading such data inevitably raises security and privacy concerns. Consequently, there is growing need for fully local, on-device deep research and writing solution that does not depend on external large-scale models. These two limitations stem from the same root cause: the rigid separation between planning and writing. To address both the insight ceiling and the challenges of on-device deep research, we present AgentCPM-Report, lightweight yet high-performance local system built upon novel WARP (Writing As Reasoning Policy) framework and an 8B-parameter deep research agent. WARP is policy-level reformulation of deep research, motivated by the observation that any approach grounded in static planning inevitably incurs an insight ceiling. By modeling research as an iterative refinement loop, WARP enables planning decisions to emerge from, and adapt to, the writing process itself. Rather than adhering to fixed outline, the agent alternates between two macro-states: Evidence-based Drafting and Reasoning-driven Deepening. Crucially, WARP is formulated as dynamic policy instead of rule-based heuristic. In the Reasoning-driven Deepening state, the agent autonomously determines whether to terminate or continue deepening by evaluating the semantic density and logical coherence of the current draft. When further deepening is warranted, it decomposes high-level sections into more granular inquiries and updates the outline based on feedback from the writing process itselfclosely mirroring the human knowledge-transforming process. The dynamic nature of WARP introduces long-horizon credit assignment and vastly expanded action space, which standard training pipelines fail to handle. We design Multi-Stage Agentic Training strategy to ensure stable convergence under reasonable resource constraints.Specifically, we employ trajectory pruning mechanism to filter high-quality supervision signals, and design curr"
[10.02.2026 04:30] Mistral response. {"id": "f7cb1c77721b4ecf866f6bf83742026f", "created": 1770697856, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1333, "total_tokens": 1343, "completion_tokens": 10, "num_cached_tokens": 1332}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"OpenBMB\"]\n```"}}]}
[10.02.2026 04:30] Response: ```python
["OpenBMB"]
```
[10.02.2026 04:30] Deleting PDF ./assets/pdf/2602.06540.pdf.
[10.02.2026 04:30] Success.
[10.02.2026 04:30] Downloading and parsing paper https://huggingface.co/papers/2601.21363.
[10.02.2026 04:31] Downloading paper 2601.21363 from https://arxiv.org/pdf/2601.21363v1...
[10.02.2026 04:32] Extracting affiliations from text.
[10.02.2026 04:32] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 2 ] . [ 1 3 6 3 1 2 . 1 0 6 2 : r Published as conference paper at ICLR 2026 TOWARDS BRIDGING THE GAP BETWEEN LARGESCALE PRETRAINING AND EFFICIENT FINETUNING FOR HUMANOID CONTROL Weidong Huang1, Zhehan Li1,2, Hangxin Liu1, Biao Hou2, Yao Su1, Jingwen Zhang1 1State Key Laboratory of General Artificial Intelligence, BIGAI 2School of Artificial Intelligence, Xidian University "
[10.02.2026 04:32] Response: ```python
[
    "State Key Laboratory of General Artificial Intelligence, BIGAI",
    "School of Artificial Intelligence, Xidian University"
]
```
[10.02.2026 04:32] Deleting PDF ./assets/pdf/2601.21363.pdf.
[10.02.2026 04:32] Success.
[10.02.2026 04:32] Downloading and parsing paper https://huggingface.co/papers/2602.06694.
[10.02.2026 04:32] Downloading paper 2602.06694 from https://arxiv.org/pdf/2602.06694v1...
[10.02.2026 04:32] Extracting affiliations from text.
[10.02.2026 04:32] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 ] . [ 1 4 9 6 6 0 . 2 0 6 2 : r NANOQUANT: Efficient Sub-1-Bit Quantization of Large Language Models Hyochan Chong * 1 Dongkyu Kim * 1 Changdong Kim 1 Minseop Choi 1 Abstract Weight-only quantization has become standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NANOQUANT, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NANOQUANT formulates quantization as low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through block and model reconstruction process. Consequently, NANOQUANT establishes new Pareto frontier in low-memory post-training quantization, achieving state-of-theart accuracy even at sub-1-bit compression rates. NANOQUANT makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8 in just 13 hours on single H100, enabling 70B model to operate on consumer 8 GB GPU. 1. Introduction Large language models (LLMs) have demonstrated remarkable performance across wide variety of tasks. However, their extremely large size makes deployment costly. Weightonly quantization offers standard route to alleviate these bottlenecks (Frantar et al., 2022; Lin et al., 2024; Shao et al., 2023; Liu et al., 2024). This has led to its widespread adoption within production-grade inference engines, such as vLLM (Kwon et al., 2023) and SGLang (Zheng et al., 2024). Recent post-training quantization (PTQ) efforts have suc- *Equal contribution 1Samsung Research, Seoul, Korea. Correspondence to: Dongkyu Kim"
[10.02.2026 04:32] Response: ```python
["Samsung Research, Seoul, Korea"]
```
[10.02.2026 04:32] Deleting PDF ./assets/pdf/2602.06694.pdf.
[10.02.2026 04:32] Success.
[10.02.2026 04:32] Downloading and parsing paper https://huggingface.co/papers/2602.08236.
[10.02.2026 04:32] Downloading paper 2602.08236 from https://arxiv.org/pdf/2602.08236v1...
[10.02.2026 04:32] Extracting affiliations from text.
[10.02.2026 04:32] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 ] . [ 1 6 3 2 8 0 . 2 0 6 2 : r When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning Shoubin Yu * 1 Yue Zhang * 1 Zun Wang 1 Jaehong Yoon 2 Huaxiu Yao 1 Mingyu Ding 1 Mohit Bansal 1 https://adaptive-visual-tts.github.io "
[10.02.2026 04:32] Response: ```python
[]
```
[10.02.2026 04:32] Extracting affiliations from text.
[10.02.2026 04:32] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 ] . [ 1 6 3 2 8 0 . 2 0 6 2 : r When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning Shoubin Yu * 1 Yue Zhang * 1 Zun Wang 1 Jaehong Yoon 2 Huaxiu Yao 1 Mingyu Ding 1 Mohit Bansal 1 https://adaptive-visual-tts.github.io1. Introduction Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning. *Equal contribution 1Department of Computer Science, University of North Carolina, Chapel Hill 2Nanyang Technological University, Singapore. Correspondence to: Shoubin Yu <shoubin@cs.unc.edu>. Preprint. February 10, 2026. 1 Recent advances in multimodal large language models (MLLMs) (Li et al., 2024a; 2023) have led to impressive progress in visual understanding and reasoning across various tasks. These models can follow natural language instructions, perceive visual scenes, and reason over multimodal input to support decision making. Despite the progress, visual spatial reasoning remains persistent challenge (Yang et al., 2024; Cheng et al., 2024; Ray et al., 2024; Tong et al., 2024), particularly for questions whose answer depends on unseen regions, viewpoint changes, or transformations that cannot be reliably inferred from single static observation. natural way to address this challenge, mirroring how humans operate, is through visual imagination (Kosslyn et al., 2006): when the observed visual evidence is insufficient, people mentally simulate how scene would appear from alternative viewpoints or after potential movements, leveraging strong world priors learned from years of physical interaction and visual experience. Inspired by this intuition, recent work (Yang et al., 2025b; Cao et al., 2025; Qian et al., 2026) has begun to integrate MLLMs with visual world models that can generate controlled novel views conditioned on hypothetical action at inference time. However, existing approaches often invoke visual imagination using fixed and exhaustive strategies (see Figure 1), without first reasoning about whether additional imagination is necessary and helpful. This lack of deliberation can lead to problematic imagination, producing misleading (Figure 1 (b)) or redundant (Figure 1(c)) views that not only incur substantial computational overhead but can also distract downstream reasoning and result in worse performance than relying on the original observation alone. Through systematic analysis of always-on imagination (Section 3), we show that such strategies are both inefficient and unreliable, motivating the need for more adaptive use of world models. Based on these observations, we aim to answer two fundamental questions for visual spatial reasoning with world model imagination: when should model invoke visual imagination, and how much imagined visual evidence is When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning Figure 1. Different cases in always-on visual imagination. Imagined views are generated independently for different beam-searched actions (shown by multiple arrows). Case 1 (Helpful): Visual imagination reveals previously unseen viewpoints, enabling helpful spatial reasoning. Case 2 (Misleading): Imagination fails to preserve task-relevant objects (e.g., the white table in the red box), resulting in incorrect spatial inference and wrong answers. Case 3 (Unnecessary): The required information is already clearly observable in the original view (e.g., the bathtub in the blue box), making additional imagined views redundant. necessary if imagination is required. Rather than treating visual imagination as an always-on operation, we seek to make it controllable, self-adaptive component during inference time. In this paper, we introduce Adaptive Visual Imagination Control (AVIC). Given an observation and question, we first regulate visual world model invocation via policy model. Specifically, the policy model first reasons about the sufficiency of the available visual evidence and conditionally decides whether to invoke the world model. If it decides not to invoke the world model, it answers directly from the observed view. Otherwise, when additional visual information is expected to be beneficial, the policy generates dynamic length action plan that specifies how the imagination should move or reorient to acquire informative viewpoints, which are then rendered by the visual world model. This design enables instance-dependent visual test-time scaling, allowing us to move beyond fixed or exhaustive imagination strategies and to study different visual spatial reasoning systematically. We evaluate our approach on challenging spatial reasoning benchmarks (SAT (Ray et al., 2024), MMSI (Yang et al., 2025a)), and navigation benchmark (R2R (Anderson et al., 2018)). Across these settings, adaptive test-time scaling achieves SoTA or competitive performance while requiring substantially fewer extra language tokens and world-model calls compared to fixed imagination strategies. Overall, beyond impro"
[10.02.2026 04:32] Mistral response. {"id": "68de2ca4b81e42879bbe4250b5e171b3", "created": 1770697962, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1355, "total_tokens": 1387, "completion_tokens": 32, "num_cached_tokens": 1354}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Department of Computer Science, University of North Carolina, Chapel Hill\",\n    \"Nanyang Technological University, Singapore\"\n]\n```"}}]}
[10.02.2026 04:32] Response: ```python
[
    "Department of Computer Science, University of North Carolina, Chapel Hill",
    "Nanyang Technological University, Singapore"
]
```
[10.02.2026 04:32] Deleting PDF ./assets/pdf/2602.08236.pdf.
[10.02.2026 04:32] Success.
[10.02.2026 04:32] Downloading and parsing paper https://huggingface.co/papers/2602.06445.
[10.02.2026 04:32] Downloading paper 2602.06445 from https://arxiv.org/pdf/2602.06445v1...
[10.02.2026 04:33] Extracting affiliations from text.
[10.02.2026 04:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING. PREPRINT VERSION. ACCEPTED FEB, 2026 1 ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking Weidong Huang, Jingwen Zhang, Member, IEEE, Jiongye Li, Shibowen Zhang, Jiayang Wu, Jiayi Wang, Hangxin Liu, Member, IEEE, Yaodong Yang, Member, IEEE, Yao Su, Member, IEEE 6 2 0 2 ] . [ 1 5 4 4 6 0 . 2 0 6 2 : r AbstractAchieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in realworld applications. Existing model predictive control (MPC) and reinforcement learning (RL) approaches often rely on energyrelated metrics embedded within multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid. Note to PractitionersTraditional MPC and RL approaches often require exte"
[10.02.2026 04:33] Response: ```python
[]
```
[10.02.2026 04:33] Extracting affiliations from text.
[10.02.2026 04:33] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING. PREPRINT VERSION. ACCEPTED FEB, 2026 1 ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking Weidong Huang, Jingwen Zhang, Member, IEEE, Jiongye Li, Shibowen Zhang, Jiayang Wu, Jiayi Wang, Hangxin Liu, Member, IEEE, Yaodong Yang, Member, IEEE, Yao Su, Member, IEEE 6 2 0 2 ] . [ 1 5 4 4 6 0 . 2 0 6 2 : r AbstractAchieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in realworld applications. Existing model predictive control (MPC) and reinforcement learning (RL) approaches often rely on energyrelated metrics embedded within multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid. Note to PractitionersTraditional MPC and RL approaches often require extensive hyperparameter tuning and frequently result in suboptimal solutions for improving energy efficiency while maintaining stable walking performance. ECO is designed to address these challenges by reformulating energy consumption as explicit inequality constraints, providing physically interpretable and intuitive approach to optimizing energy efficiency. This framework is particularly well-suited for applications prioritizing energy conservation and operational stability, such as surveillance, disaster response, and long-duration autonomous operations. Additionally, ECO generates emergent behaviors such Fig. 1: Comparison between the proposed constrained RL framework, ECO, with MPC and normal RL (PPO) baselines. It creates synergy between energy consumption and walking stability of the humanoid robots without requiring an extensive parameter-turning process, outperforming both MPC and normal RL baselines. as lighter steps and reduced body shaking, which are especially advantageous for loco-manipulation tasks by minimizing disruptions to upper-body manipulation caused by locomotion. Comparative experiments empirically offer valuable insights into constraint selection and learning setups, which may also inspire relevant ongoing research in constrained RL. Index TermsHumanoid and bipedal locomotion, constrained reinforcement learning, legged robots I. INTRODUCTION This work was supported in part by the National Natural Science Foundation of China (No. 62403064, 62403063) and Shenzhen Science and Technology Program (No. ZDCY20250901094531003). (Weidong Huang, Jingwen Zhang contributed equally to this work.) (Corresponding authors: Jingwen Zhang and Yao Su.) Weidong Huang, Jingwen Zhang, Jiongye Li, Shibowen Zhang, Jiayang Wu, Jiayi Wang, Hangxin Liu, Yao Su are with State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI), Beijing 100080, China (e-mails: bigeasthuang@gmail.com; zhangjingwen@bigai.ai; lijiongye@bigai.ai; zhangshibowen@bigai.ai; wujiayang@bigai.ai; wangjiayi@bigai.ai; liuhx@bigai.ai; suyao@bigai.ai). Jiongye Li is also with Department of Automation, Tsinghua University, Beijing 100084, China. Shibowen Zhang is also with Department of Automation, University of Science and Technology of China, Hefei 230022, China. Jiayang Wu is also with Department of Computer Science, Harbin Institute of Technology, Harbin 150001, China. Yaodong Yang is with Institute for Artificial Intelligence and School of Artificial Intelligence, Peking University, Beijing 100871, China (e-mail: yaodong.yang@pku.edu.cn).these robots to perform complex motions, such as walking, running, jumping, and even loco-manipulation [14]. Despite these achievements, energy efficiency still remains significant bottleneck for humanoid robots. Compared to their biological counterparts [5], humanoid robots require substantially more energy to perform similar tasks, limiting their operational range, endurance, and maximum payload in real-world applications [5, 6]. To improve the energy efficiency of humanoid robots, one promising approach is to leverage reinforcement learning (RL) to optimize the control policy. For instance, Proximal Policy Optimization (PPO)-based frameworks incorporate energyrelated terms into reward functions, such as minimizing joint torques, joint accelerations, and contact forces [79]. However, 2 IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING. PREPRINT VERSION. ACCEPTED FEB, 2026 to achieve efficient and stable locomotion, these methods often require extensive tuning on the weights of the reward terms regarding energy cost, task completion, and stability. This tuning process is non-intuitive and time-consuming, as (i) the effects of weight configurations are not straightforward, (ii) the training periodsranging from hours to daysto evaluate single coefficient choice even with parallel computation [10, 11]. Furthermore, conflicting objectives within the reward terms can result in suboptimal solutions or even convergence failures [12, 13]. For example, the learned policy might prioritize minimizing energy consumption at the cost of stability, resulting in unstable walking, or emphasize stability at the expense of significantly increased energy usage. To simplify the reward tuning process for optimizing locomotion with RL, we propose energetically efficient ECO (Energy-Constrained Optimization). ECO leverages constrained RL formualtion to separate conflicting o"
[10.02.2026 04:33] Mistral response. {"id": "698492fbb8cf426b9e39e06211585472", "created": 1770698012, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1469, "total_tokens": 1552, "completion_tokens": 83, "num_cached_tokens": 1468}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)\",\n    \"Department of Automation, Tsinghua University\",\n    \"Department of Automation, University of Science and Technology of China\",\n    \"Department of Computer Science, Harbin Institute of Technology\",\n    \"Institute for Artificial Intelligence and School of Artificial Intelligence, Peking University\"\n]\n```"}}]}
[10.02.2026 04:33] Response: ```python
[
    "State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)",
    "Department of Automation, Tsinghua University",
    "Department of Automation, University of Science and Technology of China",
    "Department of Computer Science, Harbin Institute of Technology",
    "Institute for Artificial Intelligence and School of Artificial Intelligence, Peking University"
]
```
[10.02.2026 04:33] Deleting PDF ./assets/pdf/2602.06445.pdf.
[10.02.2026 04:33] Success.
[10.02.2026 04:33] Enriching papers with extra data.
[10.02.2026 04:33] ********************************************************************************
[10.02.2026 04:33] Abstract 0. Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data.  					AI-generated summary 				 Despite the success of multimodal contrastive learning in aligni...
[10.02.2026 04:33] ********************************************************************************
[10.02.2026 04:33] Abstract 1. TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories.  					AI-generated summary 				 Deploying GRPO on Flow Matching models has proven effective for text-to-image...
[10.02.2026 04:33] ********************************************************************************
[10.02.2026 04:33] Abstract 2. LOCA-bench is introduced as a benchmark for evaluating language agents in long-context, agentic scenarios with controlled environment state management.  					AI-generated summary 				 Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as th...
[10.02.2026 04:33] ********************************************************************************
[10.02.2026 04:33] Abstract 3. RelayGen is a training-free framework that dynamically switches between large and small models during reasoning by identifying difficulty transitions at the segment level, achieving faster inference with minimal accuracy loss.  					AI-generated summary 				 Large reasoning models (LRMs) achieve str...
[10.02.2026 04:33] ********************************************************************************
[10.02.2026 04:33] Abstract 4. LatentChem enables chemical reasoning through continuous latent space computations instead of discrete textual tokens, achieving superior performance and efficiency compared to traditional chain-of-thought approaches.  					AI-generated summary 				 Chemical large language models (LLMs) predominantl...
[10.02.2026 04:33] ********************************************************************************
[10.02.2026 04:33] Abstract 5. AgentCPM-Report presents a lightweight local solution for deep research report generation using a Writing As Reasoning Policy framework and multi-stage agentic training to enhance small models' reasoning and outline evolution capabilities.  					AI-generated summary 				 Generating deep research rep...
[10.02.2026 04:33] ********************************************************************************
[10.02.2026 04:33] Abstract 6. Off-policy Soft Actor-Critic with large-batch updates enables efficient humanoid locomotion policy pretraining, while model-based methods facilitate safe adaptation through deterministic data collection and stochastic exploration within physics-informed world models.  					AI-generated summary 				 ...
[10.02.2026 04:33] ********************************************************************************
[10.02.2026 04:33] Abstract 7. NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment.  					AI-generated summar...
[10.02.2026 04:33] ********************************************************************************
[10.02.2026 04:33] Abstract 8. Adaptive test-time framework with world models enables selective visual imagination for spatial reasoning, improving efficiency and reliability by determining when imagination is necessary.  					AI-generated summary 				 Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spa...
[10.02.2026 04:33] ********************************************************************************
[10.02.2026 04:33] Abstract 9. Energy-constrained optimization framework separates energy metrics from rewards using Lagrangian method to achieve stable, energy-efficient humanoid robot locomotion with reduced hyperparameter tuning.  					AI-generated summary 				 Achieving stable and energy-efficient locomotion is essential for ...
[10.02.2026 04:33] Read previous papers.
[10.02.2026 04:33] Generating reviews via LLM API.
[10.02.2026 04:33] Querying the API.
[10.02.2026 04:33] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data.  					AI-generated summary 				 Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.
[10.02.2026 04:33] Response: ```json
{
  "desc": "        ,    ,       .    ReAlign,        ,         ,    .   ReAlign   ReVision       ,         -.   ,               .",
  "emoji": "",
  "title": "        "
}
```
[10.02.2026 04:33] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data.  					AI-generated summary 				 Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs."

[10.02.2026 04:33] Response: ```python
["MULTIMODAL", "TRAINING", "ARCHITECTURE"]
```
[10.02.2026 04:33] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data.  					AI-generated summary 				 Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs."

[10.02.2026 04:33] Response: ```python
['TRANSFER_LEARNING', 'SYNTHETIC']
```

**Reasoning:**

- **TRANSFER_LEARNING**: The paper discusses aligning visual and linguistic representations across different modalities and enabling knowledge transfer between image and text domains through the ReAlign method. The approach leverages unpaired data to transfer visual representation distributions to text representations.

- **SYNTHETIC**: The paper explicitly addresses using unpaired/synthetic data as a substitute for expensive image-text pairs during pretraining. The core contribution is demonstrating that "statistically aligned unpaired data can effectively substitute for expensive image-text pairs," which is directly about leveraging artificial/synthetic data for training.
[10.02.2026 04:33] Error. Failed to parse JSON from LLM. ["TRANSFER_LEARNING", "SYNTHETIC"]


**Reasoning:**

- **TRANSFER_LEARNING**: The paper discusses aligning visual and linguistic representations across different modalities and enabling knowledge transfer between image and text domains through the ReAlign method. The approach leverages unpaired data to transfer visual representation distributions to text representations.

- **SYNTHETIC**: The paper explicitly addresses using unpaired/synthetic data as a substitute for expensive image-text pairs during pretraining. The core contribution is demonstrating that "statistically aligned unpaired data can effectively substitute for expensive image-text pairs," which is directly about leveraging artificial/synthetic data for training.
[10.02.2026 04:33] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper tackles the challenge of the Modality Gap in multimodal learning, which occurs when different types of data (like images and text) that mean the same thing are not aligned properly in their representation spaces. The authors introduce a new theory called Fixed-frame Modality Gap Theory, which helps to understand and decompose the geometric misalignment between these modalities. They propose a method called ReAlign that aligns text and image representations without needing extensive training, using unpaired data instead. Finally, they present ReVision, a scalable approach for training Multimodal Large Language Models that leverages this alignment to improve learning efficiency without relying on large sets of paired data.","title":"Bridging the Modality Gap for Efficient Multimodal Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper tackles the challenge of the Modality Gap in multimodal learning, which occurs when different types of data (like images and text) that mean the same thing are not aligned properly in their representation spaces. The authors introduce a new theory called Fixed-frame Modality Gap Theory, which helps to understand and decompose the geometric misalignment between these modalities. They propose a method called ReAlign that aligns text and image representations without needing extensive training, using unpaired data instead. Finally, they present ReVision, a scalable approach for training Multimodal Large Language Models that leverages this alignment to improve learning efficiency without relying on large sets of paired data.', title='Bridging the Modality Gap for Efficient Multimodal Learning'))
[10.02.2026 04:33] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ReAlignReAlignReVision-","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ReAlignReAlignReVision-', title=''))
[10.02.2026 04:33] Querying the API.
[10.02.2026 04:33] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories.  					AI-generated summary 				 Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's "pure" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.
[10.02.2026 04:33] Response: ```json
{
  "desc": "TP-GRPO       flow matching            .        ,                .       ,            .   ,  TP-GRPO           .",
  "emoji": "",
  "title": "         "
}
```
[10.02.2026 04:33] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories.  					AI-generated summary 				 Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's "pure" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO."

[10.02.2026 04:33] Response: ```python
["RLHF", "TRAINING", "MULTIMODAL"]
```

**Justification:**
- **RLHF**: The paper discusses GRPO (Group Relative Policy Optimization), which is a reinforcement learning from human feedback technique for optimizing model outputs based on reward signals.
- **TRAINING**: The paper proposes TP-GRPO as a framework to improve model training through better reward design and learning signals during the training process.
- **MULTIMODAL**: The application is text-to-image generation, which involves both text and image modalities as inputs and outputs.
[10.02.2026 04:33] Error. Failed to parse JSON from LLM. ["RLHF", "TRAINING", "MULTIMODAL"]


**Justification:**
- **RLHF**: The paper discusses GRPO (Group Relative Policy Optimization), which is a reinforcement learning from human feedback technique for optimizing model outputs based on reward signals.
- **TRAINING**: The paper proposes TP-GRPO as a framework to improve model training through better reward design and learning signals during the training process.
- **MULTIMODAL**: The application is text-to-image generation, which involves both text and image modalities as inputs and outputs.
[10.02.2026 04:33] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories.  					AI-generated summary 				 Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's "pure" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO."

[10.02.2026 04:33] Response: ```python
["OPTIMIZATION"]
```
[10.02.2026 04:33] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TP-GRPO is a novel framework designed to improve reward mechanisms in flow matching models, particularly for text-to-image generation tasks. It introduces step-level incremental rewards that provide a more detailed learning signal, allowing for better isolation of the effects of individual denoising actions. Additionally, TP-GRPO identifies turning points in the reward trend, which are critical steps that influence future rewards, thus capturing long-term dependencies in the denoising process. This approach enhances the efficiency of reward utilization and leads to consistent improvements in generation quality.","title":"Enhancing Reward Signals for Better Denoising in Flow Matching"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='TP-GRPO is a novel framework designed to improve reward mechanisms in flow matching models, particularly for text-to-image generation tasks. It introduces step-level incremental rewards that provide a more detailed learning signal, allowing for better isolation of the effects of individual denoising actions. Additionally, TP-GRPO identifies turning points in the reward trend, which are critical steps that influence future rewards, thus capturing long-term dependencies in the denoising process. This approach enhances the efficiency of reward utilization and leads to consistent improvements in generation quality.', title='Enhancing Reward Signals for Better Denoising in Flow Matching'))
[10.02.2026 04:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TP-GRPOTP-GRPOTP-GRPO","title":"TP-GRPO"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='TP-GRPOTP-GRPOTP-GRPO', title='TP-GRPO'))
[10.02.2026 04:34] Querying the API.
[10.02.2026 04:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LOCA-bench is introduced as a benchmark for evaluating language agents in long-context, agentic scenarios with controlled environment state management.  					AI-generated summary 				 Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as "context rot". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench
[10.02.2026 04:34] Response: ```json
{
  "desc": "  LOCA-bench             .  ,       LLM  (  ).     ,    , LOCA-bench   ,    ,         .  ,               .",
  "emoji": "",
  "title": "         "
}
```
[10.02.2026 04:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LOCA-bench is introduced as a benchmark for evaluating language agents in long-context, agentic scenarios with controlled environment state management.  					AI-generated summary 				 Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as "context rot". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench"

[10.02.2026 04:34] Response: ```python
["BENCHMARK", "AGENTS"]
```
[10.02.2026 04:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LOCA-bench is introduced as a benchmark for evaluating language agents in long-context, agentic scenarios with controlled environment state management.  					AI-generated summary 				 Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as "context rot". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench"

[10.02.2026 04:34] Response: ```python
['LONG_CONTEXT', 'OPEN_SOURCE']
```
[10.02.2026 04:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LOCA-bench is a new benchmark designed to evaluate language agents in complex, long-context scenarios where they must manage their environment effectively. It addresses the issue of \'context rot\', where the performance of large language models declines as the context length increases. Unlike existing benchmarks that focus on single-step tasks, LOCA-bench allows for dynamic context management, enabling agents to handle tasks that require exploration and decision-making over extended interactions. By using advanced context management strategies, LOCA-bench aims to improve the reliability and success rates of language agents in real-world applications.","title":"Evaluating Language Agents in Dynamic Long-Context Scenarios"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="LOCA-bench is a new benchmark designed to evaluate language agents in complex, long-context scenarios where they must manage their environment effectively. It addresses the issue of 'context rot', where the performance of large language models declines as the context length increases. Unlike existing benchmarks that focus on single-step tasks, LOCA-bench allows for dynamic context management, enabling agents to handle tasks that require exploration and decision-making over extended interactions. By using advanced context management strategies, LOCA-bench aims to improve the reliability and success rates of language agents in real-world applications.", title='Evaluating Language Agents in Dynamic Long-Context Scenarios'))
[10.02.2026 04:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LOCA-benchLOCA-bench","title":"LOCA-bench"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LOCA-benchLOCA-bench', title='LOCA-bench'))
[10.02.2026 04:34] Querying the API.
[10.02.2026 04:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

RelayGen is a training-free framework that dynamically switches between large and small models during reasoning by identifying difficulty transitions at the segment level, achieving faster inference with minimal accuracy loss.  					AI-generated summary 				 Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present RelayGen, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2times end-to-end speedup with less than 2\% accuracy degradation, without requiring additional training or learned routing components.
[10.02.2026 04:34] Response: ```json
{
  "desc": "RelayGen         ,           ,          .      ,    ,          .      ( 2.2 )       ,  -     . RelayGen ,                .",
  "emoji": "",
  "title": "      "
}
```
[10.02.2026 04:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RelayGen is a training-free framework that dynamically switches between large and small models during reasoning by identifying difficulty transitions at the segment level, achieving faster inference with minimal accuracy loss.  					AI-generated summary 				 Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present RelayGen, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2times end-to-end speedup with less than 2\% accuracy degradation, without requiring additional training or learned routing components."

[10.02.2026 04:34] Response: ```python
["INFERENCE", "SMALL_MODELS", "TRAINING"]
```
[10.02.2026 04:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RelayGen is a training-free framework that dynamically switches between large and small models during reasoning by identifying difficulty transitions at the segment level, achieving faster inference with minimal accuracy loss.  					AI-generated summary 				 Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present RelayGen, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2times end-to-end speedup with less than 2\% accuracy degradation, without requiring additional training or learned routing components."

[10.02.2026 04:34] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[10.02.2026 04:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RelayGen is a novel framework designed to enhance the efficiency of large reasoning models (LRMs) by dynamically switching between large and small models based on the difficulty of reasoning tasks. It identifies transitions in difficulty at the segment level, allowing for faster inference without significant loss in accuracy. Unlike traditional methods that either overlook difficulty variations or require complex supervised routing, RelayGen operates without additional training, making it simpler and more effective. By leveraging token probability margins to analyze generation uncertainty, RelayGen achieves substantial speed improvements while maintaining high performance across various reasoning benchmarks.","title":"Dynamic Model Switching for Efficient Reasoning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RelayGen is a novel framework designed to enhance the efficiency of large reasoning models (LRMs) by dynamically switching between large and small models based on the difficulty of reasoning tasks. It identifies transitions in difficulty at the segment level, allowing for faster inference without significant loss in accuracy. Unlike traditional methods that either overlook difficulty variations or require complex supervised routing, RelayGen operates without additional training, making it simpler and more effective. By leveraging token probability margins to analyze generation uncertainty, RelayGen achieves substantial speed improvements while maintaining high performance across various reasoning benchmarks.', title='Dynamic Model Switching for Efficient Reasoning'))
[10.02.2026 04:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RelayGen RelayGen ","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RelayGen RelayGen ', title=''))
[10.02.2026 04:34] Querying the API.
[10.02.2026 04:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LatentChem enables chemical reasoning through continuous latent space computations instead of discrete textual tokens, achieving superior performance and efficiency compared to traditional chain-of-thought approaches.  					AI-generated summary 				 Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering a 10.84times average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories.
[10.02.2026 04:34] Response: ```json
{
  "desc": "LatentChem      ,          .  ,   LLM         ,     .         ,      ,  59.88%        .    10.84-  , ,           .",
  "emoji": "",
  "title": "        "
}
```
[10.02.2026 04:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LatentChem enables chemical reasoning through continuous latent space computations instead of discrete textual tokens, achieving superior performance and efficiency compared to traditional chain-of-thought approaches.  					AI-generated summary 				 Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering a 10.84times average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories."

[10.02.2026 04:34] Response: ```python
["ARCHITECTURE", "TRAINING", "INFERENCE"]
```

**Justification:**

- **ARCHITECTURE**: The paper proposes LatentChem, a novel neural architecture component that introduces a "latent reasoning interface" decoupling chemical computation from textual generation, enabling multi-step reasoning in continuous latent space rather than discrete tokens.

- **TRAINING**: The paper discusses how models are optimized for task success and spontaneously internalize reasoning during training, progressively abandoning verbose textual derivations in favor of implicit latent computation.

- **INFERENCE**: The paper emphasizes inference efficiency, reporting a "10.84 average inference speedup" compared to baselines, which is a direct focus on optimizing model deployment and inference performance.
[10.02.2026 04:34] Error. Failed to parse JSON from LLM. ["ARCHITECTURE", "TRAINING", "INFERENCE"]


**Justification:**

- **ARCHITECTURE**: The paper proposes LatentChem, a novel neural architecture component that introduces a "latent reasoning interface" decoupling chemical computation from textual generation, enabling multi-step reasoning in continuous latent space rather than discrete tokens.

- **TRAINING**: The paper discusses how models are optimized for task success and spontaneously internalize reasoning during training, progressively abandoning verbose textual derivations in favor of implicit latent computation.

- **INFERENCE**: The paper emphasizes inference efficiency, reporting a "10.84 average inference speedup" compared to baselines, which is a direct focus on optimizing model deployment and inference performance.
[10.02.2026 04:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LatentChem enables chemical reasoning through continuous latent space computations instead of discrete textual tokens, achieving superior performance and efficiency compared to traditional chain-of-thought approaches.  					AI-generated summary 				 Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering a 10.84times average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories."

[10.02.2026 04:34] Response: ```python
["REASONING", "SCIENCE"]
```
[10.02.2026 04:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LatentChem is a novel approach that enhances chemical reasoning by utilizing continuous latent space computations instead of relying on discrete textual tokens. Traditional methods, which use Chain-of-Thought (CoT) reasoning, often struggle with the inherent continuous nature of chemical data, leading to inefficiencies. By allowing models to perform multi-step reasoning directly in latent space, LatentChem improves both performance and speed, achieving a significant win rate over existing CoT-based methods. This research demonstrates that chemical reasoning benefits from a continuous representation, resulting in faster and more effective computations.","title":"Revolutionizing Chemical Reasoning with Continuous Latent Dynamics"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LatentChem is a novel approach that enhances chemical reasoning by utilizing continuous latent space computations instead of relying on discrete textual tokens. Traditional methods, which use Chain-of-Thought (CoT) reasoning, often struggle with the inherent continuous nature of chemical data, leading to inefficiencies. By allowing models to perform multi-step reasoning directly in latent space, LatentChem improves both performance and speed, achieving a significant win rate over existing CoT-based methods. This research demonstrates that chemical reasoning benefits from a continuous representation, resulting in faster and more effective computations.', title='Revolutionizing Chemical Reasoning with Continuous Latent Dynamics'))
[10.02.2026 04:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LatentChem LatentChem ","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LatentChem LatentChem ', title=''))
[10.02.2026 04:34] Querying the API.
[10.02.2026 04:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

AgentCPM-Report presents a lightweight local solution for deep research report generation using a Writing As Reasoning Policy framework and multi-stage agentic training to enhance small models' reasoning and outline evolution capabilities.  					AI-generated summary 				 Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.
[10.02.2026 04:34] Response: ```json
{
  "desc": "AgentCPM-Report         ,    Writing As Reasoning Policy       .    8-         ,          .            RL,   ,       .   ,          .",
  "emoji": "",
  "title": "   :      "
}
```
[10.02.2026 04:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AgentCPM-Report presents a lightweight local solution for deep research report generation using a Writing As Reasoning Policy framework and multi-stage agentic training to enhance small models' reasoning and outline evolution capabilities.  					AI-generated summary 				 Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight."

[10.02.2026 04:34] Response: ```python
["AGENTS", "SMALL_MODELS", "TRAINING", "RL", "BENCHMARK"]
```
[10.02.2026 04:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AgentCPM-Report presents a lightweight local solution for deep research report generation using a Writing As Reasoning Policy framework and multi-stage agentic training to enhance small models' reasoning and outline evolution capabilities.  					AI-generated summary 				 Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight."

[10.02.2026 04:34] Response: ```python
['REASONING', 'OPEN_SOURCE']
```
[10.02.2026 04:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"AgentCPM-Report introduces a novel approach for generating deep research reports using a lightweight local model that enhances reasoning and outline development. It employs a Writing As Reasoning Policy (WARP) that allows the model to iteratively refine its outline while drafting the report. This method addresses the limitations of traditional plan-then-write strategies by integrating evidence-based drafting with reasoning-driven enhancements. The Multi-Stage Agentic Training strategy further empowers smaller models to achieve performance levels comparable to larger, closed-source systems, while ensuring user data privacy and safety.","title":"Empowering Small Models for Insightful Research Reports"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='AgentCPM-Report introduces a novel approach for generating deep research reports using a lightweight local model that enhances reasoning and outline development. It employs a Writing As Reasoning Policy (WARP) that allows the model to iteratively refine its outline while drafting the report. This method addresses the limitations of traditional plan-then-write strategies by integrating evidence-based drafting with reasoning-driven enhancements. The Multi-Stage Agentic Training strategy further empowers smaller models to achieve performance levels comparable to larger, closed-source systems, while ensuring user data privacy and safety.', title='Empowering Small Models for Insightful Research Reports'))
[10.02.2026 04:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"AgentCPM-Report AgentCPM-Report ","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='AgentCPM-Report AgentCPM-Report ', title=''))
[10.02.2026 04:34] Querying the API.
[10.02.2026 04:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Off-policy Soft Actor-Critic with large-batch updates enables efficient humanoid locomotion policy pretraining, while model-based methods facilitate safe adaptation through deterministic data collection and stochastic exploration within physics-informed world models.  					AI-generated summary 				 Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.
[10.02.2026 04:34] Response: ```json
{
  "desc": " ,   Soft Actor-Critic         (UTD)          .                ,     ,         .        ,         .         -            .",
  "emoji": "",
  "title": "        off-policy   "
}
```
[10.02.2026 04:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Off-policy Soft Actor-Critic with large-batch updates enables efficient humanoid locomotion policy pretraining, while model-based methods facilitate safe adaptation through deterministic data collection and stochastic exploration within physics-informed world models.  					AI-generated summary 				 Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning."

[10.02.2026 04:34] Response: ```python
["RL", "ROBOTICS", "TRAINING"]
```
[10.02.2026 04:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Off-policy Soft Actor-Critic with large-batch updates enables efficient humanoid locomotion policy pretraining, while model-based methods facilitate safe adaptation through deterministic data collection and stochastic exploration within physics-informed world models.  					AI-generated summary 				 Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning."

[10.02.2026 04:34] Response: ```python
["OPTIMIZATION"]
```

The paper focuses on improving training efficiency through optimization techniques (large-batch updates, high Update-To-Data ratios in SAC) and efficient learning methods (off-policy RL, model-based RL). While it discusses reinforcement learning for robotics, the core contribution is about optimizing the training process for better sample efficiency and computational efficiency.
[10.02.2026 04:34] Error. Failed to parse JSON from LLM. ["OPTIMIZATION"]


The paper focuses on improving training efficiency through optimization techniques (large-batch updates, high Update-To-Data ratios in SAC) and efficient learning methods (off-policy RL, model-based RL). While it discusses reinforcement learning for robotics, the core contribution is about optimizing the training process for better sample efficiency and computational efficiency.
[10.02.2026 04:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents an approach that combines off-policy Soft Actor-Critic (SAC) with large-batch updates to enhance the pretraining of humanoid locomotion policies. By leveraging high Update-To-Data (UTD) ratios, the method achieves efficient zero-shot deployment of these policies on real robots. For adapting to new environments, the authors utilize model-based techniques that allow for safe data collection through deterministic policies while maintaining stochastic exploration in a physics-informed world model. This strategy effectively bridges the gap between large-scale pretraining and efficient fine-tuning, ensuring both safety and performance in humanoid control tasks.","title":"Efficient Humanoid Control: Bridging Pretraining and Adaptation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents an approach that combines off-policy Soft Actor-Critic (SAC) with large-batch updates to enhance the pretraining of humanoid locomotion policies. By leveraging high Update-To-Data (UTD) ratios, the method achieves efficient zero-shot deployment of these policies on real robots. For adapting to new environments, the authors utilize model-based techniques that allow for safe data collection through deterministic policies while maintaining stochastic exploration in a physics-informed world model. This strategy effectively bridges the gap between large-scale pretraining and efficient fine-tuning, ensuring both safety and performance in humanoid control tasks.', title='Efficient Humanoid Control: Bridging Pretraining and Adaptation'))
[10.02.2026 04:35] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SACSAC","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SACSAC', title=''))
[10.02.2026 04:35] Querying the API.
[10.02.2026 04:35] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment.  					AI-generated summary 				 Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8times in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.
[10.02.2026 04:35] Response: ```json
{
  "desc": "NanoQuant      ,        (1-)   ,    .       ,        (ADMM).      ,         .                 .",
  "emoji": "",
  "title": "   :      "
}
```
[10.02.2026 04:35] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment.  					AI-generated summary 				 Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8times in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU."

[10.02.2026 04:35] Response: ```python
["INFERENCE", "TRAINING"]
```

**Justification:**

- **INFERENCE**: The paper focuses on post-training quantization (PTQ), which is a model deployment optimization technique. Quantization to binary and sub-1-bit levels is explicitly about optimizing models for efficient deployment on consumer hardware, which falls directly under inference optimization.

- **TRAINING**: The paper discusses post-training quantization methods and optimization techniques (ADMM, block and model reconstruction), which are training/fine-tuning methodologies applied after initial model training.
[10.02.2026 04:35] Error. Failed to parse JSON from LLM. ["INFERENCE", "TRAINING"]


**Justification:**

- **INFERENCE**: The paper focuses on post-training quantization (PTQ), which is a model deployment optimization technique. Quantization to binary and sub-1-bit levels is explicitly about optimizing models for efficient deployment on consumer hardware, which falls directly under inference optimization.

- **TRAINING**: The paper discusses post-training quantization methods and optimization techniques (ADMM, block and model reconstruction), which are training/fine-tuning methodologies applied after initial model training.
[10.02.2026 04:35] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment.  					AI-generated summary 				 Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8times in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU."

[10.02.2026 04:35] Response: ```python
["OPTIMIZATION"]
```
[10.02.2026 04:35] Response: ParsedChatCompletionMessage[Article](content='{"desc":"NanoQuant is a novel method for post-training quantization of large language models (LLMs) that allows them to be compressed to binary and sub-1-bit levels. It uses low-rank binary factorization to convert full-precision weights into low-rank binary matrices, significantly reducing memory usage. The method employs an efficient alternating direction method of multipliers (ADMM) for precise initialization and tuning of these binary matrices. As a result, NanoQuant achieves state-of-the-art accuracy while making it possible to deploy large models on consumer hardware, such as compressing Llama2-70B by 25.8 times.","title":"NanoQuant: Revolutionizing Model Compression for Consumer Hardware"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='NanoQuant is a novel method for post-training quantization of large language models (LLMs) that allows them to be compressed to binary and sub-1-bit levels. It uses low-rank binary factorization to convert full-precision weights into low-rank binary matrices, significantly reducing memory usage. The method employs an efficient alternating direction method of multipliers (ADMM) for precise initialization and tuning of these binary matrices. As a result, NanoQuant achieves state-of-the-art accuracy while making it possible to deploy large models on consumer hardware, such as compressing Llama2-70B by 25.8 times.', title='NanoQuant: Revolutionizing Model Compression for Consumer Hardware'))
[10.02.2026 04:35] Response: ParsedChatCompletionMessage[Article](content='{"desc":"NanoQuant1ADMMNanoQuant13Llama2-70B25.88GBGPU","title":"NanoQuant"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='NanoQuant1ADMMNanoQuant13Llama2-70B25.88GBGPU', title='NanoQuant'))
[10.02.2026 04:35] Querying the API.
[10.02.2026 04:35] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Adaptive test-time framework with world models enables selective visual imagination for spatial reasoning, improving efficiency and reliability by determining when imagination is necessary.  					AI-generated summary 				 Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.
[10.02.2026 04:35] Response: ```json
{
  "desc": "          ,           .   AVIC      world models,  ,        .     ,             ,    .   ,          ,      world models.",
  "emoji": "",
  "title": " :   ,   "
}
```
[10.02.2026 04:35] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Adaptive test-time framework with world models enables selective visual imagination for spatial reasoning, improving efficiency and reliability by determining when imagination is necessary.  					AI-generated summary 				 Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning."

[10.02.2026 04:35] Response: ```python
["MULTIMODAL", "CV", "BENCHMARK", "INFERENCE"]
```
[10.02.2026 04:35] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Adaptive test-time framework with world models enables selective visual imagination for spatial reasoning, improving efficiency and reliability by determining when imagination is necessary.  					AI-generated summary 				 Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning."

[10.02.2026 04:35] Response: ```python
["REASONING", "INTERPRETABILITY", "OPTIMIZATION"]
```

**Justification:**

1. **REASONING**: The paper explicitly focuses on enhancing spatial reasoning capabilities in multimodal models, particularly addressing how to improve visual spatial reasoning through selective visual imagination.

2. **INTERPRETABILITY**: The paper analyzes model behavior by investigating "when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful" - this is fundamentally about understanding and explaining when and why the model should use certain capabilities.

3. **OPTIMIZATION**: The paper addresses efficiency concerns, showing how "selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens" - this is about optimizing computational resources and performance during inference.
[10.02.2026 04:35] Error. Failed to parse JSON from LLM. ["REASONING", "INTERPRETABILITY", "OPTIMIZATION"]


**Justification:**

1. **REASONING**: The paper explicitly focuses on enhancing spatial reasoning capabilities in multimodal models, particularly addressing how to improve visual spatial reasoning through selective visual imagination.

2. **INTERPRETABILITY**: The paper analyzes model behavior by investigating "when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful" - this is fundamentally about understanding and explaining when and why the model should use certain capabilities.

3. **OPTIMIZATION**: The paper addresses efficiency concerns, showing how "selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens" - this is about optimizing computational resources and performance during inference.
[10.02.2026 04:35] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces an adaptive framework called AVIC that enhances spatial reasoning by selectively using visual imagination based on the sufficiency of current visual evidence. It addresses the challenges of indiscriminate imagination, which can lead to increased computation and degraded performance. The study analyzes when visual imagination is beneficial and when it can be harmful, providing insights into optimizing its use. The results demonstrate that controlled imagination can improve efficiency and accuracy in spatial reasoning tasks compared to fixed strategies.","title":"Selective Imagination for Smarter Spatial Reasoning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces an adaptive framework called AVIC that enhances spatial reasoning by selectively using visual imagination based on the sufficiency of current visual evidence. It addresses the challenges of indiscriminate imagination, which can lead to increased computation and degraded performance. The study analyzes when visual imagination is beneficial and when it can be harmful, providing insights into optimizing its use. The results demonstrate that controlled imagination can improve efficiency and accuracy in spatial reasoning tasks compared to fixed strategies.', title='Selective Imagination for Smarter Spatial Reasoning'))
[10.02.2026 04:35] Response: ParsedChatCompletionMessage[Article](content='{"desc":"AVIC","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='AVIC', title=''))
[10.02.2026 04:35] Querying the API.
[10.02.2026 04:35] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Energy-constrained optimization framework separates energy metrics from rewards using Lagrangian method to achieve stable, energy-efficient humanoid robot locomotion with reduced hyperparameter tuning.  					AI-generated summary 				 Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.
[10.02.2026 04:35] Response: ```json
{
  "desc": "   ECO (Energy-Constrained Optimization)     ,       ,  -.                .               .     BRUCE ,  ECO            .",
  "emoji": "",
  "title": "        "
}
```
[10.02.2026 04:35] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Energy-constrained optimization framework separates energy metrics from rewards using Lagrangian method to achieve stable, energy-efficient humanoid robot locomotion with reduced hyperparameter tuning.  					AI-generated summary 				 Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid."

[10.02.2026 04:35] Response: ```python
["RL", "ROBOTICS"]
```
[10.02.2026 04:35] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Energy-constrained optimization framework separates energy metrics from rewards using Lagrangian method to achieve stable, energy-efficient humanoid robot locomotion with reduced hyperparameter tuning.  					AI-generated summary 				 Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid."

[10.02.2026 04:35] Response: ```python
["OPTIMIZATION"]
```
[10.02.2026 04:35] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents a new framework called Energy-Constrained Optimization (ECO) for improving the locomotion of humanoid robots. It separates energy metrics from rewards using a Lagrangian method, allowing for clearer constraints on energy consumption. This approach reduces the need for extensive hyperparameter tuning, leading to more efficient and stable walking patterns. Experimental results show that ECO significantly lowers energy use while maintaining robust performance compared to existing methods.","title":"ECO: Revolutionizing Energy-Efficient Humanoid Robot Locomotion"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents a new framework called Energy-Constrained Optimization (ECO) for improving the locomotion of humanoid robots. It separates energy metrics from rewards using a Lagrangian method, allowing for clearer constraints on energy consumption. This approach reduces the need for extensive hyperparameter tuning, leading to more efficient and stable walking patterns. Experimental results show that ECO significantly lowers energy use while maintaining robust performance compared to existing methods.', title='ECO: Revolutionizing Energy-Efficient Humanoid Robot Locomotion'))
[10.02.2026 04:35] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ECOECOECO","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ECOECOECO', title=''))
[10.02.2026 04:35] Renaming data file.
[10.02.2026 04:35] Renaming previous data. hf_papers.json to ./d/2026-02-10.json
[10.02.2026 04:35] Saving new data file.
[10.02.2026 04:35] Generating page.
[10.02.2026 04:35] Renaming previous page.
[10.02.2026 04:35] Renaming previous data. index.html to ./d/2026-02-10.html
[10.02.2026 04:35] Writing result.
[10.02.2026 04:35] Renaming log file.
[10.02.2026 04:35] Renaming previous data. log.txt to ./logs/2026-02-10_last_log.txt
