[10.02.2026 16:01] Read previous papers.
[10.02.2026 16:01] Generating top page (month).
[10.02.2026 16:01] Writing top page (month).
[10.02.2026 17:06] Read previous papers.
[10.02.2026 17:06] Get feed.
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08794
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07026
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07085
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08222
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07845
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06855
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08676
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06422
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09007
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08439
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06025
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07962
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08543
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09022
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08990
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07055
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07075
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03784
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08658
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06540
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06454
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08808
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08236
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06694
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07796
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07775
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08145
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21363
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08961
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06445
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09003
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08829
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07803
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06942
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06600
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08818
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07970
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07491
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07150
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07090
[10.02.2026 17:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.07080
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05929
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05708
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07054
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07040
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08629
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08004
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07948
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05946
[10.02.2026 17:06] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02285
[10.02.2026 17:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.00169
[10.02.2026 17:06] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.02.2026 17:06] No deleted papers detected.
[10.02.2026 17:06] Downloading and parsing papers (pdf, html). Total: 51.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.08794.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.08794.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.08794.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07026.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.07026.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.07026.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07085.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.07085.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.07085.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.08222.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.08222.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.08222.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07845.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.07845.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.07845.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.06855.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.06855.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.06855.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.08676.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.08676.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.08676.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.06422.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.06422.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.06422.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.09007.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.09007.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.09007.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.08439.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.08439.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.08439.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.06025.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.06025.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.06025.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07962.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.07962.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.07962.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.08543.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.08543.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.08543.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.09022.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.09022.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.09022.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.08990.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.08990.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.08990.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07055.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.07055.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.07055.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07075.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.07075.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.07075.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.03784.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.03784.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.03784.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.08658.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.08658.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.08658.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.06540.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.06540.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.06540.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.06454.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.06454.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.06454.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.08808.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.08808.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.08808.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.08236.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.08236.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.08236.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.06694.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.06694.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.06694.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07796.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.07796.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.07796.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07775.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.07775.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.07775.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.08145.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.08145.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.08145.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2601.21363.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2601.21363.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2601.21363.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.08961.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.08961.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.08961.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.06445.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.06445.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.06445.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.09003.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.09003.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.09003.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.08829.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.08829.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.08829.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07803.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.07803.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.07803.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.06942.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.06942.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.06942.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.06600.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.06600.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.06600.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.08818.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.08818.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.08818.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07970.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.07970.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.07970.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07491.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.07491.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.07491.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07150.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.07150.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.07150.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07090.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.07090.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.07090.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07080.
[10.02.2026 17:06] Downloading paper 2602.07080 from https://arxiv.org/pdf/2602.07080v1...
[10.02.2026 17:06] Extracting affiliations from text.
[10.02.2026 17:06] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs Yicheng He * 1 Zheng Zhao * 2 Kaiyu Zhou 3 Bryan Dai 4 Jie Fu 4 Yonghui Yang "
[10.02.2026 17:06] Response: ```python
[]
```
[10.02.2026 17:06] Extracting affiliations from text.
[10.02.2026 17:06] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs Yicheng He * 1 Zheng Zhao * 2 Kaiyu Zhou 3 Bryan Dai 4 Jie Fu 4 Yonghui YangCurrent paradigms for code verification rely heavily on external mechanismssuch as executionbased unit tests or auxiliary LLM judgeswhich are often labor-intensive or limited by the judging models own capabilities. This raises fundamental, yet unexplored question: Can an LLMs functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the models neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as mechanistic diagnostic task, mapping the models explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the models internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit. 6 2 0 2 6 ] . [ 1 0 8 0 7 0 . 2 0 6 2 : r 1. Introduction Large language models (LLMs; Liu et al., 2024a; He et al., 2025b; Rozière et al., 2023; Guo et al., 2024) have transformed code generation from simple solvers to complex agents (Wang et al., 2024a; Shinn et al., 2023; Madaan et al., *Equal contribution 1University of Illinois Urbana-Champaign 2University of Edinburgh 3Nanyang Technological University 4IQuest Research 5National University of Singapore. Correspondence to: Yicheng He <yh84@uiuc.edu>. Preprint. February 10, 2026. 1 2023). As these models integrate into mission-critical workflows, the demand for functionally correct and maintainable code has intensified (Liu et al., 2023; 2024b). However, the stochastic nature of LLMs makes reliable assessment persistent bottleneck. While execution-based verification via unit tests remains the standard (Khan et al., 2024; Dong et al., 2025; Wang et al., 2023b; Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021), it is limited by labor-intensive test design and sparse edge-case coverage. Emergent LLM-as-a-Judge frameworks offer scalable automation (He et al., 2025a; Jiang et al., 2025), yet they introduce inference overhead and suffer from recursive dependencies where reliability is bounded by the evaluators own reasoning priors. Given these limitations of external mechanisms, natural question arises: Can code correctness be assessed intrinsically, by examining the LLMs internal computational structure? Prior work has probed neuron activations and representation dynamics during generation (Bui et al., 2025; Huang et al., 2025; Patel et al., 2025), yet these analyses have not been directly linked to the correctness of the generated code. In parallel, recent advances in mechanistic interpretability (Michaud et al., 2024; Sharkey et al., 2025; Rai et al., 2024a; Gao et al., 2025; Templeton et al., 2024), specifically the use of attribution graphs to trace reasoning pathways (Dunefsky et al., 2024; Ameisen et al., 2025; Zhao et al., 2025), have shown promise in reasoning domains. We believe that bridging this gap requires deeper look at how code-specific structures are manifested within the models latent space during the generation process. Unlike natural language or abstract math, code possesses rigid, objective topology, such as control flow and variable binding. Recent research (Zhang et al., 2025) demonstrates that LLMs could form internal finite automata rather than relying on superficial statistical shortcuts. Thus, we hypothesize that reliable code generation requires the model to instantiate specific neural circuits that mirror this topology, and that errors manifest as detectable structural collapses within these circuits. By decoding the intrinsic circuits that drive code generation, we gain window into the models internally consistent computational trajectoriesbypassing both the opacity of black-box methods and the incompleteCodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs ness of external verification frameworks, ultimately providing more robust foundation for assessing model reliability. To operationalize this, we introduce CodeCircuit, whitebox verification framework that analyzes the line-level attribution graph of the generated code. We adopt the instrumentation of sparse autoencoders (transcoders) to decompose complex residual flows into causal graph of interpretable features. Using CodeCircuit, we conduct an extensive analysis across diverse programming languages (Python, C++, Java). Although line-level attribution graphs are computationally demanding, this detail is crucial for exposing failure mechanisms. CodeCircuit thus serves as scientific instrument for precise auditing and debugging, providing mechanistic insights into code generation beyond what coarser methods can reveal. Our results reveal that, within fixed model, internal attribution structures correlate with code correctness across multiple programming languages. We find that correct and incorrect code exhibit systematic topological differences in their attribution graphs (Figure 1). Consequently, CodeCircuit consistently outperforms both black-box methods (e.g., Temperature Scaling; Shih et al., 2023) and gray-box methods (e.g., Chain-ofEmbedding; Wang et al., 2024b) in verification performance. Notably, we demonstrate that these signals are not merely correlational: by performing targeted interventions on the graphs nodes (Meng et al., 2022; 2023), we can causally correct erroneous code, proving that the attribution graph captures the functional mechanism of generation. Our contributions are summarized as follows: Assessing Generated Code via Internal Computational Structure. We investigate the feasibility of evaluating LLM-generated code by examining the models internal neural dynamics rather than its surface-level output. By extracting line-level attribution graphs, we demonstrate that code correctness appears to"
[10.02.2026 17:06] Mistral response. {"id": "db52ef296d6f447b9076a241d433952e", "created": 1770743175, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1471, "total_tokens": 1516, "completion_tokens": 45, "num_cached_tokens": 1470}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"University of Illinois Urbana-Champaign\",\n    \"University of Edinburgh\",\n    \"Nanyang Technological University\",\n    \"IQuest Research\",\n    \"National University of Singapore\"\n]\n```"}}]}
[10.02.2026 17:06] Response: ```python
[
    "University of Illinois Urbana-Champaign",
    "University of Edinburgh",
    "Nanyang Technological University",
    "IQuest Research",
    "National University of Singapore"
]
```
[10.02.2026 17:06] Deleting PDF ./assets/pdf/2602.07080.pdf.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.05929.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.05929.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.05929.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.05708.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.05708.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.05708.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07054.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.07054.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.07054.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07040.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.07040.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.07040.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.08629.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.08629.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.08629.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.08004.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.08004.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.08004.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.07948.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.07948.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.07948.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.05946.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.05946.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.05946.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.02285.
[10.02.2026 17:06] Extra JSON file exists (./assets/json/2602.02285.json), skip PDF parsing.
[10.02.2026 17:06] Paper image links file exists (./assets/img_data/2602.02285.json), skip HTML parsing.
[10.02.2026 17:06] Success.
[10.02.2026 17:06] Downloading and parsing paper https://huggingface.co/papers/2602.00169.
[10.02.2026 17:07] Downloading paper 2602.00169 from https://arxiv.org/pdf/2602.00169v2...
[10.02.2026 17:07] Extracting affiliations from text.
[10.02.2026 17:07] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Towards Agentic Intelligence for Materials Science Huan Zhang1,2, Yizhan Li1,2, Wenhao Huang1,2, Ziyu Hou4, Yu Song1,2, Xuye Liu4, Farshid Effaty1,2,5, Jinya Jiang8, Sifan Wu1,2, Qianggang Ding1,2, Izumi Takahara6, Leonard R. MacGillivray1,5, Teruyasu Mizoguchi6, Tianshu Yu7, Lizi Liao12, Yuyu Luo9, Yu Rong10, Jia Li9, Ying Diao3, Heng Ji3, and Bang Liu1,2,11,* 1DIRO & Institut Courtois, Universit de Montr eal 2Mila Quebec AI Institute 3University of Illinois Urbana-Champaign 4University of Waterloo 5Universit de Sherbrooke 6The University of Tokyo, Institute of Industrial Science 7The Chinese University of Hong Kong, Shenzhen 8University of California, San Diego 9The Hong Kong University of Science and Technology, Guangzhou 10Alibaba DAMO Academy 11Canada CIFAR AI Chair 12Singapore Management University *Correspondence: bang.liu@umontreal.ca Abstract The convergence of artificial intelligence and materials science presents transformative opportunity, but achieving true acceleration in discovery requires moving beyond taskisolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances unique pipeline-centric view that spans from corpus curation and pre-training, through domain adaptation and instruction tuning, to goalconditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choicessuch as data curation and training objectivescan be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the "
[10.02.2026 17:07] Response: ```python
[
    "DIRO & Institut Courtois, Université de Montréal",
    "Mila Quebec AI Institute",
    "University of Illinois Urbana-Champaign",
    "University of Waterloo",
    "Université de Sherbrooke",
    "The University of Tokyo, Institute of Industrial Science",
    "The Chinese University of Hong Kong, Shenzhen",
    "University of California, San Diego",
    "The Hong Kong University of Science and Technology, Guangzhou",
    "Alibaba DAMO Academy",
    "Canada CIFAR AI Chair",
    "Singapore Management University"
]
```
[10.02.2026 17:07] Deleting PDF ./assets/pdf/2602.00169.pdf.
[10.02.2026 17:07] Success.
[10.02.2026 17:07] Enriching papers with extra data.
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 0. MOVA is an open-source model that generates synchronized audio-visual content using a Mixture-of-Experts architecture with 32 billion parameters, supporting image-text to video-audio generation tasks.  					AI-generated summary 				 Audio is indispensable for real-world video, yet generation models ...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 1. Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data.  					AI-generated summary 				 Despite the success of multimodal contrastive learning in aligni...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 2. Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated exper...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 3. WMSS is a post-training paradigm that uses weak model checkpoints to identify and fill learning gaps, enabling continued improvement beyond conventional saturation points in large language models.  					AI-generated summary 				 As post-training optimization becomes central to improving large langua...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 4. RD-VLA introduces a recurrent architecture for vision-language-action models that adapts computational depth through latent iterative refinement, achieving constant memory usage and improved task success rates.  					AI-generated summary 				 Current Vision-Language-Action (VLA) models rely on fixed...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 5. AIRS-Bench presents a comprehensive benchmark suite for evaluating LLM agents across diverse scientific domains, demonstrating current limitations while providing open-source resources for advancing autonomous scientific research.  					AI-generated summary 				 LLM agents hold significant promise f...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 6. LLaDA2.1 introduces a novel token-to-token editing approach with speed and quality modes, enhanced through reinforcement learning for improved reasoning and instruction following in large language diffusion models.  					AI-generated summary 				 While LLaDA2.0 showcased the scaling potential of 100...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 7. TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories.  					AI-generated summary 				 Deploying GRPO on Flow Matching models has proven effective for text-to-image...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 8. A new benchmark and evaluation metric are introduced for assessing temporal coherence and dynamic interaction in GUI generation models, revealing significant challenges in maintaining consistency over extended interaction sequences.  					AI-generated summary 				 Recent advancements in image genera...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 9. Researchers introduce a new video understanding task and benchmark that evaluates models' ability to learn from few-shot demonstrations, along with a specialized MLLM architecture trained using a two-stage approach combining video supervision and preference optimization.  					AI-generated summary 	...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 10. BudgetMem is a runtime memory framework for LLM agents that uses modular components with three budget tiers and a neural policy router to optimize performance-cost trade-offs in memory usage.  					AI-generated summary 				 Memory is increasingly central to Large Language Model (LLM) agents operatin...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 11. LOCA-bench is introduced as a benchmark for evaluating language agents in long-context, agentic scenarios with controlled environment state management.  					AI-generated summary 				 Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as th...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 12. A new benchmark called GISA is introduced for evaluating information-seeking assistants, featuring human-crafted queries with structured answer formats and live updates to prevent memorization.  					AI-generated summary 				 The advancement of large language models (LLMs) has significantly accelera...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 13. WorldCompass enhances long-horizon video-based world models through reinforcement learning post-training with clip-level rollouts, complementary rewards, and efficient RL algorithms.  					AI-generated summary 				 This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training fr...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 14. InternAgent-1.5 is a unified system for autonomous scientific discovery that integrates computational modeling and experimental research through coordinated subsystems for generation, verification, and evolution.  					AI-generated summary 				 We introduce InternAgent-1.5, a unified system designed...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 15. Current multimodal foundation models show limitations in maintaining coherent spatial beliefs during active exploration, exhibiting gaps between active and passive performance, inefficient exploration strategies, and difficulties in updating outdated spatial knowledge.  					AI-generated summary 			...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 16. LatentChem enables chemical reasoning through continuous latent space computations instead of discrete textual tokens, achieving superior performance and efficiency compared to traditional chain-of-thought approaches.  					AI-generated summary 				 Chemical large language models (LLMs) predominantl...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 17. ComprExIT introduces a novel approach to long-context inference in LLMs by using explicit information transmission over frozen hidden states, improving compression efficiency through depth-wise and width-wise transmission mechanisms.  					AI-generated summary 				 Long-context inference with Large ...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 18. Research investigates how fundamental reasoning paradigms influence large language model generalization through targeted training approaches and evaluation on real-world tasks.  					AI-generated summary 				 Deduction, induction, and abduction are fundamental reasoning paradigms, core for human log...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 19. AgentCPM-Report presents a lightweight local solution for deep research report generation using a Writing As Reasoning Policy framework and multi-stage agentic training to enhance small models' reasoning and outline evolution capabilities.  					AI-generated summary 				 Generating deep research rep...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 20. RelayGen is a training-free framework that dynamically switches between large and small models during reasoning by identifying difficulty transitions at the segment level, achieving faster inference with minimal accuracy loss.  					AI-generated summary 				 Large reasoning models (LRMs) achieve str...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 21. A scalable framework for evaluating and improving goal-conditioned procedure generation using large-scale web mining, automated scoring, and reinforcement learning to enhance step-by-step instruction quality.  					AI-generated summary 				 Generating step-by-step "how-to" procedures is a key LLM ca...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 22. Adaptive test-time framework with world models enables selective visual imagination for spatial reasoning, improving efficiency and reliability by determining when imagination is necessary.  					AI-generated summary 				 Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spa...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 23. NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment.  					AI-generated summar...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 24. Explicit reasoning in LLM agents can degrade performance in user-engaged scenarios by reducing information disclosure and weakening agent-user communication, with transparency-aware prompting showing better results.  					AI-generated summary 				 Eliciting reasoning has emerged as a powerful techni...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 25. Autoregressive video diffusion models suffer from train-test gaps when generating long videos, but a training-free approach called Rolling Sink addresses this by maintaining AR cache and enabling ultra-long video synthesis.  					AI-generated summary 				 Recently, autoregressive (AR) video diffusio...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 26. Foundation models including LLMs, MLLMs, and generative models require reliable and responsible development addressing bias, security, explainability, and other critical issues for trustworthy deployment across multiple domains.  					AI-generated summary 				 Foundation models, including Large Lang...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 27. Off-policy Soft Actor-Critic with large-batch updates enables efficient humanoid locomotion policy pretraining, while model-based methods facilitate safe adaptation through deterministic data collection and stochastic exploration within physics-informed world models.  					AI-generated summary 				 ...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 28. MotionCrafter is a video diffusion framework that jointly reconstructs 4D geometry and estimates dense motion using a novel joint representation and 4D VAE architecture.  					AI-generated summary 				 We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometr...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 29. Energy-constrained optimization framework separates energy metrics from rewards using Lagrangian method to achieve stable, energy-efficient humanoid robot locomotion with reduced hyperparameter tuning.  					AI-generated summary 				 Achieving stable and energy-efficient locomotion is essential for ...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 30. Large language models are increasingly guiding data management processes through a tiered framework that optimizes data quality, cost, and training efficiency across different stages of model development.  					AI-generated summary 				 The development of artificial intelligence can be viewed as an ...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 31. WildReward demonstrates that reward models can be effectively trained from in-the-wild user interactions using ordinal regression, achieving performance comparable to traditional methods while benefiting from user diversity.  					AI-generated summary 				 Reward models (RMs) are crucial for the tra...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 32. A high-quality open-source singing voice synthesis system is presented with support for multiple languages and controllable generation, along with a dedicated benchmark for evaluating zero-shot performance.  					AI-generated summary 				 While recent years have witnessed rapid progress in speech sy...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 33. A comprehensive study of Turkish subword tokenization systematically investigates the relationship between vocabulary size, training corpus, and tokenizer performance across multiple linguistic tasks and diagnostics.  					AI-generated summary 				 Tokenization is a pivotal design choice for neural ...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 34. Large reasoning models exhibit spontaneous question repetition patterns that can be formalized and leveraged to improve computational efficiency and accuracy through echo-aware training and prompting techniques.  					AI-generated summary 				 Test-time compute allocation in large reasoning models (...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 35. FlexMoRE demonstrates that low-rank adapters can replace full-sized experts in mixture-of-experts architectures, achieving better performance with significantly fewer parameters.  					AI-generated summary 				 Recent advances in mixture-of-experts architectures have shown that individual experts mo...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 36. Research explores PDE solvers including neural frameworks for scientific simulations, examining forward solutions, inverse problems, and equation discovery across multi-variable and non-linear systems.  					AI-generated summary 				 Partial Differential Equations are precise in modelling the physic...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 37. A multi-agent framework guided by knowledge graphs addresses materials science challenges by integrating specialized agents for problem decomposition, evidence retrieval, and graph traversal to discover sustainable PFAS alternatives.  					AI-generated summary 				 Large Language Models (LLMs) promi...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 38. Analysis of agentic system evaluation reveals significant variance in single-run performance estimates, necessitating multiple runs and advanced metrics for reliable assessment.  					AI-generated summary 				 Agentic systems are evaluated on benchmarks where agents interact with environments to sol...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 39. SPARSE is a user-centric framework that protects text embeddings from privacy leaks by selectively perturbing sensitive dimensions using differentiable masking and Mahalanobis noise calibration.  					AI-generated summary 				 Text embeddings enable numerous NLP applications but face severe privacy ...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 40. LLM code verification can be achieved through internal neural dynamics analysis, identifying structural signatures that distinguish correct reasoning from logical failures in computational circuits.  					AI-generated summary 				 Current paradigms for code verification rely heavily on external mech...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 41. KV-CoRE method evaluates kv-cache compressibility through SVD-based low-rank approximation, revealing patterns linking compressibility to model architecture and training data across multiple languages and domains.  					AI-generated summary 				 Large language models rely on kv-caches to avoid redun...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 42. CE-RAG4EM reduces computational overhead in large-scale entity matching by implementing blocking-based batch retrieval and generation while maintaining competitive matching quality.  					AI-generated summary 				 Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tas...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 43. A benchmark and optimization technique are presented to improve multimodal large language models' emotion understanding by addressing spurious associations and hallucinations in audiovisual cues.  					AI-generated summary 				 Emotion understanding is essential for building socially intelligent age...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 44. Aster is an AI agent that accelerates scientific discovery by iteratively improving programs, achieving state-of-the-art results across multiple domains including mathematics, biology, and machine learning with significantly reduced computational requirements.  					AI-generated summary 				 We intr...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 45. CauScale is a neural architecture that enables efficient causal discovery on large graphs through compressed embeddings and tied attention weights, achieving high accuracy and significant speedups over previous methods.  					AI-generated summary 				 Causal discovery is essential for advancing data...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 46. Agent skills extend large language model (LLM) agents with reusable, program-like modules that define triggering conditions, procedural logic, and tool interactions. As these skills proliferate in public marketplaces, it is unclear what types are available, how users adopt them, and what risks they ...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 47. Collective motion in fish schools exemplifies emergent self-organization in active matter systems, yet computational tools for simulating and analyzing these dynamics remain fragmented across research groups. We present dewi-kadita, an open-source Python library implementing the three-dimensional Co...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 48. Preference alignment objectives are extended to general alignment settings using f-divergence variational representations, introducing novel on-policy and hybrid policy optimization methods for LLM alignment with theoretical and empirical validation.  					AI-generated summary 				 Recent research s...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 49. A comprehensive formalization of statistical learning theory in Lean 4 addresses gaps in mathematical libraries and demonstrates human-AI collaboration for verified machine learning theory foundations.  					AI-generated summary 				 We present the first comprehensive Lean 4 formalization of statist...
[10.02.2026 17:07] ********************************************************************************
[10.02.2026 17:07] Abstract 50. AI-driven materials science integrates large language models across discovery pipelines from data curation to agent-based experimentation, emphasizing system-level optimization and autonomous goal pursuit.  					AI-generated summary 				 The convergence of artificial intelligence and materials scien...
[10.02.2026 17:07] Read previous papers.
[10.02.2026 17:07] Generating reviews via LLM API.
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#inference", "#video", "#audio", "#dataset", "#architecture", "#open_source", "#multimodal"], "emoji": "🎬", "ru": {"title": "Совместная генерация видео и аудио с одной моделью", "desc": "MOVA — это открытая модель для синтеза видео и аудио с архитектурой Mixture-of-Experts, содержащ
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#training", "#multimodal", "#architecture"], "emoji": "🔗", "ru": {"title": "Выравнивание модальностей без обучения для эффективного масштабирования мультимодальных моделей", "desc": "Исследователи решили проблему несоответствия между модальностями в мультимодальном обучении, предлож
[10.02.2026 17:07] Using data from previous issue: {"categories": [], "emoji": "💹", "ru": {"title": "Эволюционный поиск торговых факторов через оптимизацию траекторий", "desc": "QuantaAlpha представляет эволюционный фреймворк для автоматизированного поиска альфа-факторов на финансовых рынках, который рассматривает каждый цикл майнинга как траекторию
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "📈", "ru": {"title": "Слабые контрольные точки как путь к сильным моделям", "desc": "В статье предлагается WMSS — новая парадигма постобучения для больших языковых моделей, которая преодолевает проблему насыщения, возникающую при стандартном о
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#inference", "#training", "#robotics", "#architecture", "#multimodal"], "emoji": "🤖", "ru": {"title": "Адаптивные вычисления вместо фиксированной глубины: скрытое уточнение для robotics", "desc": "RD-VLA представляет рекуррентную архитектуру для моделей vision-language-action, котор
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#agents", "#open_source", "#survey", "#science", "#benchmark", "#dataset"], "emoji": "🧪", "ru": {"title": "Мерило для автономных научных агентов", "desc": "Авторы представляют AIRS-Bench — комплексный набор бенчмарков для оценки способностей LLM агентов в научных исследованиях. Бенч
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization", "#alignment", "#benchmark", "#architecture", "#open_source", "#reasoning", "#diffusion", "#plp"], "emoji": "⚡", "ru": {"title": "Балансировка скорости и качества в диффузионных языковых моделях через T2T редактирование и обучение с подкреплением",
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#optimization"], "emoji": "🎯", "ru": {"title": "Точные вознаграждения на каждом шаге для лучшего обучения моделей генерации", "desc": "TP-GRPO решает проблему разреженности вознаграждений в моделях flow matching путём введения пошаговых инкрементальных вознаграждений вместо итоговог
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#dataset"], "emoji": "🖥️", "ru": {"title": "Оценка временной когерентности в генеративных моделях интерфейсов", "desc": "В статье представлен новый бенчмарк GEBench для оценки моделей генерации графических интерфейсов, содержащий 700 тщательно отобранных примеро
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#training", "#video", "#benchmark", "#dataset", "#multimodal", "#rlhf"], "emoji": "🎥", "ru": {"title": "Обучение видеопонимания на немногих примерах через контекстные демонстрации", "desc": "Исследователи представляют новую задачу понимания видео и бенчмарк для оценки способности мо
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#optimization", "#rl", "#agents", "#long_context", "#training"], "emoji": "⚖️", "ru": {"title": "Интеллектуальное распределение ресурсов памяти для агентов на основе больших языковых моделей", "desc": "BudgetMem — это фреймворк для управления памятью LLM-агентов во время выполнения,
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#agents", "#long_context", "#open_source", "#benchmark"], "emoji": "🔄", "ru": {"title": "Управление длинным контекстом в языковых агентах через контролируемое состояние окружения", "desc": "Введена бенчмарк LOCA-bench для оценки языковых агентов в сценариях с длинным контекстом и уп
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#dataset", "#survey", "#agents"], "emoji": "🔍", "ru": {"title": "GISA: бенчмарк для оценки поисковых агентов с естественными задачами и живыми данными", "desc": "Представлен новый бенчмарк GISA для оценки информационно-поисковых ассистентов, содержащий 37
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#training", "#video", "#rl"], "emoji": "🧭", "ru": {"title": "Направляем видео-модели мира: обучение с подкреплением для точного исследования", "desc": "WorldCompass представляет собой фреймворк для пост-обучения видео-моделей мира с использованием обучения с подкреплением. Авторы вв
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#science", "#reasoning", "#benchmark", "#open_source", "#agents"], "emoji": "🧬", "ru": {"title": "Автономная научная система для открытий через координацию вычислений и экспериментов", "desc": "InternAgent-1.5 — это единая система для автономного научного открытия, которая объединяе
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#multimodal", "#robotics"], "emoji": "🗺️", "ru": {"title": "Агенты не понимают пространство: диагностика слабостей активного исследования в foundation models", "desc": "Исследование показывает, что современные мультимодальные foundation models плохо справляю
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#science", "#reasoning"], "emoji": "⚗️", "ru": {"title": "Химические рассуждения в скрытом пространстве вместо текстовых цепочек мысли", "desc": "LatentChem предлагает новый подход к химическому рассуждению, используя непрерывные вычисления в скрытом пространстве вместо дискретных т
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#optimization", "#long_context", "#inference", "#benchmark"], "emoji": "📦", "ru": {"title": "Явная передача информации для эффективного сжатия контекста", "desc": "ComprExIT предлагает новый подход к сжатию контекста в больших языковых моделях, использующий явную передачу информации
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#benchmark", "#training", "#reasoning", "#synthetic", "#dataset"], "emoji": "🧠", "ru": {"title": "Три типа логики — один ключ к обобщению LLM", "desc": "В этом исследовании авторы изучают, как три фундаментальных типа логического мышления — дедукция, индукция и абдукция — влияют на 
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#training", "#small_models", "#open_source", "#rl", "#agents", "#reasoning", "#benchmark"], "emoji": "📝", "ru": {"title": "Рассуждение во время письма: малые модели для качественного исследовательского анализа", "desc": "AgentCPM-Report представляет лёгкое локальное решение для гене
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#inference", "#training", "#small_models", "#optimization", "#reasoning"], "emoji": "⚡", "ru": {"title": "Динамическое переключение между моделями для ускорения рассуждений", "desc": "RelayGen — это фреймворк для оптимизации инференса больших языковых моделей, который динамически пе
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization", "#synthetic", "#dataset", "#benchmark", "#open_source", "#reasoning"], "emoji": "📋", "ru": {"title": "Закрытый цикл оценки и улучшения инструкций через обучение с подкреплением", "desc": "В работе представлена How2Everything - масштабируемая систе
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#cv", "#benchmark"], "emoji": "🎯", "ru": {"title": "Избирательное воображение: когда видеть нужно, а когда нет", "desc": "В работе исследуется проблема пространственного рассуждения в многомодальных больших языковых моделях, которые часто ошибаются при н
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#optimization"], "emoji": "⚙️", "ru": {"title": "Экстремальное сжатие больших моделей: путь к развертыванию на потребительских устройствах", "desc": "NanoQuant — это новый метод постобучающего квантования, который сжимает большие языковые модели до бинарного (1-бит) и суббинарного у
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#alignment", "#agents", "#reasoning", "#benchmark", "#open_source", "#training"], "emoji": "🤐", "ru": {"title": "Молчание золото? Почему явное рассуждение ослабляет взаимодействие агента с пользователем", "desc": "В статье исследуется влияние явного рассуждения в LLM агентах, взаимо
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#inference", "#training", "#video", "#long_context", "#diffusion"], "emoji": "🎬", "ru": {"title": "Беспроблемное расширение автрегрессивных видеомоделей до сверхдлинных последовательностей без переобучения", "desc": "В статье рассматривается проблема несоответствия между обучением и
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#security", "#alignment", "#ethics", "#hallucinations", "#survey", "#interpretability"], "emoji": "🛡️", "ru": {"title": "Путь к надежным и ответственным фундаментальным моделям", "desc": "В этом обзоре рассматриваются критические аспекты надежной и ответственной разработки фундамент
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#training", "#robotics", "#rl"], "emoji": "🤖", "ru": {"title": "Эффективное предварительное обучение и безопасная адаптация гуманоидов через off-policy обучение с усилением", "desc": "Исследование показывает, что алгоритм Soft Actor-Critic с большим размером батча и высоким коэффици
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#diffusion", "#video", "#architecture", "#3d", "#training"], "emoji": "🎬", "ru": {"title": "Совместная реконструкция геометрии и движения видео через 4D диффузию", "desc": "MotionCrafter — это фреймворк на основе диффузионных моделей, который одновременно реконструирует 4D геометрию
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#rl"], "emoji": "🤖", "ru": {"title": "Оптимизация энергопотребления через явные ограничения вместо штрафов в награде", "desc": "В работе предложена ECO (Energy-Constrained Optimization) — фреймворк обучения с подкреплением, который отделяет энергетическ
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#training", "#open_source", "#alignment", "#agi", "#optimization", "#data"], "emoji": "📊", "ru": {"title": "Данные и модели растут вместе: многоуровневое управление для AGI", "desc": "Статья предлагает новую парадигму совместной эволюции моделей и данных, где языковые модели активно
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#dataset", "#training", "#rlhf", "#data"], "emoji": "🏆", "ru": {"title": "Обучение моделей вознаграждения на реальных взаимодействиях пользователей", "desc": "В работе представлен WildReward — модель вознаграждения, которая обучается на реальных взаимодействиях пользователей с языко
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#low_resource", "#audio", "#dataset", "#benchmark", "#open_source", "#multilingual"], "emoji": "🎤", "ru": {"title": "Открытая система синтеза певческого голоса с универсальной поддержкой языков и надёжной оценкой производительности", "desc": "В статье представлена SoulX-Singer — выс
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#open_source", "#interpretability", "#multilingual", "#low_resource", "#data", "#benchmark"], "emoji": "🔤", "ru": {"title": "Систематическое изучение субсловной токенизации для морфологически богатых языков", "desc": "В работе проведено систематическое исследование субсловной токени
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#math", "#training", "#reasoning", "#open_source", "#inference", "#interpretability", "#optimization"], "emoji": "🔄", "ru": {"title": "Превращаем интуитивное повторение в якорь для эффективного рассуждения", "desc": "В статье исследуется явление спонтанного повторения вопроса в боль
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#reasoning"], "emoji": "🎛️", "ru": {"title": "Низкоранговые адаптеры вместо полных экспертов в смешанной архитектуре", "desc": "В работе предложена архитектура FlexMoRE, которая использует низкоранговые адаптеры вместо полнораспределённых экспертов в
[10.02.2026 17:07] Using data from previous issue: {"categories": [], "emoji": "🧮", "ru": {"title": "Нейросетевые решатели уравнений в частных производных для научных вычислений", "desc": "Статья исследует различные методы решения уравнений в частных производных (УЧП) с использованием нейросетевых подходов для научных симуляций. Авторы анализируют п
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#hallucinations", "#science", "#reasoning"], "emoji": "🔬", "ru": {"title": "Многоагентное рассуждение на графах знаний для открытия материалов", "desc": "В работе представлена многоагентная система, управляемая графами знаний, для решения задач материаловедения. Система интегрирует 
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#agents", "#benchmark"], "emoji": "🎲", "ru": {"title": "Множественные прогоны вместо одного: путь к надёжной оценке агентных систем", "desc": "В этой работе исследователи анализируют надёжность оценки агентных систем, которые взаимодействуют с окружением для решения задач. Они обнар
[10.02.2026 17:07] Using data from previous issue: {"categories": ["#security", "#leakage"], "emoji": "🔐", "ru": {"title": "Умная защита эмбеддингов: избирательный шум вместо слепого", "desc": "SPARSE — это фреймворк, защищающий текстовые эмбеддинги от утечек приватной информации путём избирательного возмущения чувствительных измерений. Метод исполь
[10.02.2026 17:07] Querying the API.
[10.02.2026 17:07] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LLM code verification can be achieved through internal neural dynamics analysis, identifying structural signatures that distinguish correct reasoning from logical failures in computational circuits.  					AI-generated summary 				 Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit.
[10.02.2026 17:07] Response: ```json
{
  "desc": "В этой работе исследуется возможность проверки корректности кода, генерируемого языковыми моделями, через анализ их внутренней нейронной динамики вместо использования внешних механизмов, таких как юнит-тесты или судейские модели. Авторы применяют подход механистической интерпретируемости, отображая вычислительную траекторию модели в графы атрибуции на уровне строк кода, чтобы выявить структурные сигнатуры, которые различают правильные рассуждения от логических ошибок. Анализ остаточных потоков в нейронных сетях показывает, что признаки корректности кодируются во внутренних схемах модели и остаются устойчивыми к различиям синтаксиса языков программирования. Топологические характеристики этих внутренних графов позволяют предсказывать ошибки более надежно, чем поверхностные эвристики, и даже проводить целевые интервенции для исправления ошибочной логики.",
  "emoji": "🧠",
  "title": "Внутренние схемы моделей раскрывают ошибки кода"
}
```
[10.02.2026 17:07] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLM code verification can be achieved through internal neural dynamics analysis, identifying structural signatures that distinguish correct reasoning from logical failures in computational circuits.  					AI-generated summary 				 Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit."

[10.02.2026 17:07] Response: ```python
["PLP", "ARCHITECTURE"]
```

**Justification:**

1. **PLP**: The paper focuses on code verification for programming languages (Python, C++, Java), analyzing how LLMs generate and verify code correctness. This directly relates to Programming Language Processing.

2. **ARCHITECTURE**: The paper investigates the internal neural architecture and computational structure of LLMs, examining "neural dynamics," "internal circuits," "residual flows," and "mechanistic interpretability." It analyzes how the model's internal architectural components encode correctness signals, which is central to understanding and proposing insights about neural architectures.
[10.02.2026 17:07] Error. Failed to parse JSON from LLM. ["PLP", "ARCHITECTURE"]


**Justification:**

1. **PLP**: The paper focuses on code verification for programming languages (Python, C++, Java), analyzing how LLMs generate and verify code correctness. This directly relates to Programming Language Processing.

2. **ARCHITECTURE**: The paper investigates the internal neural architecture and computational structure of LLMs, examining "neural dynamics," "internal circuits," "residual flows," and "mechanistic interpretability." It analyzes how the model"s internal architectural components encode correctness signals, which is central to understanding and proposing insights about neural architectures.
[10.02.2026 17:07] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLM code verification can be achieved through internal neural dynamics analysis, identifying structural signatures that distinguish correct reasoning from logical failures in computational circuits.  					AI-generated summary 				 Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit."

[10.02.2026 17:07] Response: ```python
["INTERPRETABILITY", "REASONING", "OPEN_SOURCE"]
```

**Justification:**

1. **INTERPRETABILITY**: The paper explicitly focuses on analyzing internal neural dynamics and mechanistic interpretability of LLMs. It examines "structural signatures" within the model's "internal circuits" and uses "line-level attribution graphs" to understand how the model encodes correctness signals.

2. **REASONING**: The paper addresses logical reasoning capabilities, specifically distinguishing "correct reasoning from logical failures" and identifying how models perform computational reasoning during code generation.

3. **OPEN_SOURCE**: The paper mentions releasing code publicly at a GitHub repository ("Our code is at https://github.com/bruno686/CodeCircuit").
[10.02.2026 17:07] Error. Failed to parse JSON from LLM. ["INTERPRETABILITY", "REASONING", "OPEN_SOURCE"]


**Justification:**

1. **INTERPRETABILITY**: The paper explicitly focuses on analyzing internal neural dynamics and mechanistic interpretability of LLMs. It examines "structural signatures" within the model"s "internal circuits" and uses "line-level attribution graphs" to understand how the model encodes correctness signals.

2. **REASONING**: The paper addresses logical reasoning capabilities, specifically distinguishing "correct reasoning from logical failures" and identifying how models perform computational reasoning during code generation.

3. **OPEN_SOURCE**: The paper mentions releasing code publicly at a GitHub repository ("Our code is at https://github.com/bruno686/CodeCircuit").
[10.02.2026 17:08] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores a new method for verifying the correctness of code generated by large language models (LLMs) by analyzing their internal neural dynamics. Instead of relying on external tests or judges, the authors propose that the model\'s internal structure can reveal signals that indicate whether the reasoning is correct or flawed. They use mechanistic interpretability to create line-level attribution graphs that map the model\'s decision-making process, allowing them to identify patterns that correlate with logical validity. Their findings show that these internal signals are consistent across different programming languages and can be used to improve the accuracy of code generation.","title":"Unlocking Code Verification Through Neural Dynamics"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper explores a new method for verifying the correctness of code generated by large language models (LLMs) by analyzing their internal neural dynamics. Instead of relying on external tests or judges, the authors propose that the model's internal structure can reveal signals that indicate whether the reasoning is correct or flawed. They use mechanistic interpretability to create line-level attribution graphs that map the model's decision-making process, allowing them to identify patterns that correlate with logical validity. Their findings show that these internal signals are consistent across different programming languages and can be used to improve the accuracy of code generation.", title='Unlocking Code Verification Through Neural Dynamics'))
[10.02.2026 17:08] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文探讨了通过分析大型语言模型（LLM）内部神经动态来验证代码的正确性。我们提出，模型的内部计算结构中可能编码了可解码的信号，这些信号可以预测代码生成过程中的逻辑有效性。通过将代码验证视为一种机械诊断任务，我们能够识别出区分正确推理与逻辑失败的结构特征。研究表明，这些内部图的拓扑特征比表面启发式方法更可靠地预测正确性，并能够针对性地修复错误逻辑。","title":"通过内部动态分析实现代码验证"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文探讨了通过分析大型语言模型（LLM）内部神经动态来验证代码的正确性。我们提出，模型的内部计算结构中可能编码了可解码的信号，这些信号可以预测代码生成过程中的逻辑有效性。通过将代码验证视为一种机械诊断任务，我们能够识别出区分正确推理与逻辑失败的结构特征。研究表明，这些内部图的拓扑特征比表面启发式方法更可靠地预测正确性，并能够针对性地修复错误逻辑。', title='通过内部动态分析实现代码验证'))
[10.02.2026 17:08] Using data from previous issue: {"categories": ["#inference", "#optimization", "#long_context", "#multilingual", "#low_resource", "#benchmark"], "emoji": "📦", "ru": {"title": "Оценка низкорангового потенциала KV-кэша в больших языковых моделях", "desc": "Авторы представляют метод KV-CoRE для оценки сжимаемости KV-кэша в больших яз
[10.02.2026 17:08] Using data from previous issue: {"categories": [], "emoji": "⚡", "ru": {"title": "Эффективный RAG для сопоставления сущностей в больших данных", "desc": "Статья описывает CE-RAG4EM, архитектуру для экономичного поиска с аугментацией поколения (RAG), которая снижает вычислительные затраты при сопоставлении сущностей в больших масшт
[10.02.2026 17:08] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#video", "#hallucinations", "#audio", "#benchmark", "#rlhf", "#open_source", "#training"], "emoji": "😊", "ru": {"title": "Преодоление галлюцинаций: мультимодальное понимание эмоций без ложных ассоциаций", "desc": "В работе представлены бенчмарк EmoReA
[10.02.2026 17:08] Using data from previous issue: {"categories": ["#science", "#training", "#optimization", "#open_source", "#agents", "#plp"], "emoji": "🔬", "ru": {"title": "Автоматизированное научное открытие через итеративное улучшение программ", "desc": "Aster — это AI-агент для автономного научного открытия, который итеративно улучшает програм
[10.02.2026 17:08] Using data from previous issue: {"categories": ["#science", "#graphs", "#open_source"], "emoji": "🔗", "ru": {"title": "Масштабируемое обнаружение причинности в больших графах через компрессированные представления", "desc": "CauScale представляет нейросетевую архитектуру для эффективного обнаружения причинно-следственных связей в б
[10.02.2026 17:08] Using data from previous issue: {"categories": ["#ethics", "#security"], "emoji": "🛠️", "ru": {"title": "Маркетплейс навыков для AI-агентов: анализ экосистемы, рисков и тенденций спроса", "desc": "В статье проводится крупномасштабный анализ 40 285 публично доступных навыков (skills) для LLM-агентов из основного маркетплейса. Автор
[10.02.2026 17:08] Using data from previous issue: {"categories": [], "emoji": "🐟", "ru": {"title": "Энтропия как мост к пониманию коллективного порядка в природе", "desc": "Авторы представляют dewi-kadita — открытую библиотеку на Python для моделирования коллективного поведения рыб с использованием зоны-базированной модели Couzin в трёхмерном прост
[10.02.2026 17:08] Using data from previous issue: {"categories": ["#optimization", "#alignment", "#reasoning"], "emoji": "🎯", "ru": {"title": "Единая рамка выравнивания языковых моделей через f-дивергенции", "desc": "В работе предложен единый подход к выравниванию LLM, основанный на представлении целевых функций как дивергенций между распределениям
[10.02.2026 17:08] Using data from previous issue: {"categories": ["#open_source", "#science"], "emoji": "✓", "ru": {"title": "Машинная математика: верифицированная теория обучения через человеко-AI сотрудничество", "desc": "В работе представлена первая полная формализация статистической теории обучения в Lean 4, основанная на эмпирической теории пр
[10.02.2026 17:08] Querying the API.
[10.02.2026 17:08] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

AI-driven materials science integrates large language models across discovery pipelines from data curation to agent-based experimentation, emphasizing system-level optimization and autonomous goal pursuit.  					AI-generated summary 				 The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment.   To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials.
[10.02.2026 17:08] Response: ```json
{
  "desc": "В статье рассматривается интеграция больших языковых моделей (LLM) в полный цикл открытия новых материалов, от подготовки данных до автономного проведения экспериментов. Авторы предлагают системный подход, который оптимизирует весь конвейер от предварительного обучения моделей через адаптацию к конкретным задачам до создания целеориентированных агентов, способных взаимодействовать с симуляциями и экспериментальными платформами. В отличие от предыдущих обзоров, исследование рассматривает процесс как единую интегрированную систему, ориентированную на реальные результаты в открытии материалов, а не только на промежуточные метрики. Статья объединяет перспективы как AI, так и материаловедения, описывая применение языковых моделей для анализа литературы, прогнозирования свойств и дизайна материалов с использованием автономных систем, обладающих памятью и инструментарием.",
  "emoji": "🧪",
  "title": "От изолированных моделей к автономным агентам: LLM для ускорения открытия новых материалов"
}
```
[10.02.2026 17:08] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AI-driven materials science integrates large language models across discovery pipelines from data curation to agent-based experimentation, emphasizing system-level optimization and autonomous goal pursuit.  					AI-generated summary 				 The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment.   To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."

[10.02.2026 17:08] Response: ```python
["AGENTS", "TRAINING", "DATA", "BENCHMARK"]
```
[10.02.2026 17:08] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AI-driven materials science integrates large language models across discovery pipelines from data curation to agent-based experimentation, emphasizing system-level optimization and autonomous goal pursuit.  					AI-generated summary 				 The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment.   To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."

[10.02.2026 17:08] Response: ```python
['SURVEY', 'SCIENCE', 'OPTIMIZATION']
```
[10.02.2026 17:08] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses how artificial intelligence, particularly large language models (LLMs), can enhance materials science by creating a comprehensive discovery pipeline. It emphasizes the need for systems that can autonomously plan, act, and learn throughout the entire discovery process, rather than just focusing on isolated tasks. The authors propose a pipeline-centric approach that connects data curation, model training, and experimental execution to optimize outcomes in materials discovery. By aligning AI capabilities with materials science applications, the paper aims to foster the development of autonomous agents that can effectively discover new materials.","title":"Empowering Materials Discovery with Autonomous AI Agents"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses how artificial intelligence, particularly large language models (LLMs), can enhance materials science by creating a comprehensive discovery pipeline. It emphasizes the need for systems that can autonomously plan, act, and learn throughout the entire discovery process, rather than just focusing on isolated tasks. The authors propose a pipeline-centric approach that connects data curation, model training, and experimental execution to optimize outcomes in materials discovery. By aligning AI capabilities with materials science applications, the paper aims to foster the development of autonomous agents that can effectively discover new materials.', title='Empowering Materials Discovery with Autonomous AI Agents'))
[10.02.2026 17:08] Response: ParsedChatCompletionMessage[Article](content='{"desc":"这篇论文探讨了人工智能与材料科学的结合，强调了在材料发现过程中需要系统级的优化和自主目标追求。作者提出了一种独特的管道中心视角，涵盖了从数据整理到实验平台的整个过程，旨在优化实际的发现结果。与以往的研究不同，这篇论文将整个过程视为一个端到端的系统，而不是孤立的任务模型。最后，论文还分析了主动设计与被动反应方法的对比，推动了自主、智能的材料发现代理系统的发展。","title":"智能材料发现的自主系统优化"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='这篇论文探讨了人工智能与材料科学的结合，强调了在材料发现过程中需要系统级的优化和自主目标追求。作者提出了一种独特的管道中心视角，涵盖了从数据整理到实验平台的整个过程，旨在优化实际的发现结果。与以往的研究不同，这篇论文将整个过程视为一个端到端的系统，而不是孤立的任务模型。最后，论文还分析了主动设计与被动反应方法的对比，推动了自主、智能的材料发现代理系统的发展。', title='智能材料发现的自主系统优化'))
[10.02.2026 17:08] Renaming data file.
[10.02.2026 17:08] Renaming previous data. hf_papers.json to ./d/2026-02-10.json
[10.02.2026 17:08] Saving new data file.
[10.02.2026 17:08] Generating page.
[10.02.2026 17:08] Renaming previous page.
[10.02.2026 17:08] Renaming previous data. index.html to ./d/2026-02-10.html
[10.02.2026 17:08] Writing result.
[10.02.2026 17:08] Renaming log file.
[10.02.2026 17:08] Renaming previous data. log.txt to ./logs/2026-02-10_last_log.txt
