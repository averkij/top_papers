[10.02.2026 12:56] Read previous papers.
[10.02.2026 12:56] Generating top page (month).
[10.02.2026 12:56] Writing top page (month).
[10.02.2026 14:20] Read previous papers.
[10.02.2026 14:20] Get feed.
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08794
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07026
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07085
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07845
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08222
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08676
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06422
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09007
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08439
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06025
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07962
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08543
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06855
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07055
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09022
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08990
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07075
[10.02.2026 14:20] Extract page data from URL. URL: https://huggingface.co/papers/2602.03784
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08658
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06540
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06454
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08236
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08808
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06694
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21363
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08961
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06445
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08145
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09003
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08829
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07803
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07796
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07775
[10.02.2026 14:20] Extract page data from URL. URL: https://huggingface.co/papers/2602.06942
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08818
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07970
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07491
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07150
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06600
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07090
[10.02.2026 14:20] Extract page data from URL. URL: https://huggingface.co/papers/2602.05929
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07054
[10.02.2026 14:20] Extract page data from URL. URL: https://huggingface.co/papers/2602.08629
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07948
[10.02.2026 14:20] Extract page data from URL. URL: https://huggingface.co/papers/2602.05946
[10.02.2026 14:20] Extract page data from URL. URL: https://huggingface.co/papers/2602.05708
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07040
[10.02.2026 14:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02285
[10.02.2026 14:20] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.02.2026 14:20] No deleted papers detected.
[10.02.2026 14:20] Downloading and parsing papers (pdf, html). Total: 48.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.08794.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.08794.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.08794.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.07026.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.07026.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.07026.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.07085.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.07085.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.07085.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.07845.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.07845.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.07845.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.08222.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.08222.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.08222.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.08676.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.08676.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.08676.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.06422.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.06422.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.06422.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.09007.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.09007.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.09007.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.08439.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.08439.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.08439.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.06025.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.06025.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.06025.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.07962.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.07962.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.07962.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.08543.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.08543.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.08543.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.06855.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.06855.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.06855.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.07055.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.07055.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.07055.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.09022.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.09022.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.09022.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.08990.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.08990.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.08990.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.07075.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.07075.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.07075.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.03784.
[10.02.2026 14:20] Downloading paper 2602.03784 from https://arxiv.org/pdf/2602.03784v1...
[10.02.2026 14:20] Extracting affiliations from text.
[10.02.2026 14:20] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Jiangnan Ye 1 Hanqi Yan 1 Zhenyi Shen 1 Heng Chang 2 Ye Mao 3 Yulan He 1 4 Abstract Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing keyvalue caches, motivating In this work, we study context compression. soft context compression, where long context is condensed into small set of continuous representations. Existing methods typically repurpose the LLM itself as trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), lightweight framework that formulates soft compression into new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the models internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into small number of slots via globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only 1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust longcontext compression. 6 2 0 2 3 ] . [ 1 4 8 7 3 0 . 2 0 6 2 : r 1. Introduction As Large Language Models (LLMs) continue to scale, the computational and memory costs associated with long1Kings College London, UK 2Tsinghua University, China 3Imperial College London, UK 4The Alan Turing Institute, UK. Correspondence to: Yulan He <yulan.he@kcl.ac.uk>. Preprint. February 4, 2026. 1 Figur"
[10.02.2026 14:20] Response: ```python
[
    "Kings College London, UK",
    "Tsinghua University, China",
    "Imperial College London, UK",
    "The Alan Turing Institute, UK"
]
```
[10.02.2026 14:20] Deleting PDF ./assets/pdf/2602.03784.pdf.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.08658.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.08658.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.08658.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.06540.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.06540.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.06540.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.06454.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.06454.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.06454.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.08236.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.08236.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.08236.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.08808.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.08808.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.08808.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.06694.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.06694.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.06694.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2601.21363.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2601.21363.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2601.21363.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.08961.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.08961.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.08961.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.06445.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.06445.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.06445.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.08145.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.08145.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.08145.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.09003.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.09003.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.09003.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.08829.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.08829.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.08829.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.07803.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.07803.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.07803.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.07796.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.07796.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.07796.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.07775.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.07775.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.07775.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.06942.
[10.02.2026 14:20] Downloading paper 2602.06942 from https://arxiv.org/pdf/2602.06942v1...
[10.02.2026 14:20] Extracting affiliations from text.
[10.02.2026 14:20] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 ] . [ 1 2 4 9 6 0 . 2 0 6 2 : r Cambridge Default Journal (), , 146 Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of DataVocabularyMorphology Interplay Duygu Altinok* Independent Researcher, Berlin, Germany *Corresponding author. Email: duygu@turkish-nlp-suite.com Abstract Tokenization is pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes, but they typically (i) vary vocabulary without systematically controlling the tokenizers training corpus, (ii) provide limited intrinsic diagnostics of segmentation quality, and (iii) evaluate narrow slice of downstream tasks. We present the first and only comprehensive, principled study of Turkish subword tokenization subwords manifest that jointly varies vocabulary size and tokenizer training corpus size (datavocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology-level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro-F1 over gold morpheme boundaries, decoupled lemma atomicity vssurface boundary hits, over-/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) systematic investigation of the vocabularycorpussuccess triad for Turkish, including larger data regimes than prior work; (ii) unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) extensive, controlled comparisons that identify "
[10.02.2026 14:20] Response: ```python
["Independent Researcher, Berlin, Germany"]
```
[10.02.2026 14:20] Deleting PDF ./assets/pdf/2602.06942.pdf.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.08818.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.08818.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.08818.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.07970.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.07970.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.07970.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.07491.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.07491.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.07491.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.07150.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.07150.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.07150.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.06600.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.06600.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.06600.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.07090.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.07090.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.07090.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.05929.
[10.02.2026 14:20] Downloading paper 2602.05929 from https://arxiv.org/pdf/2602.05929v2...
[10.02.2026 14:20] Extracting affiliations from text.
[10.02.2026 14:20] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 7 ] . [ 2 9 2 9 5 0 . 2 0 6 2 : r KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs Jian Chen1,3*, Zhuoran Wang2,4, Jiayu Qin1, Ming Li5, Meng Wang6 Changyou Chen1, Yin Chen2, Qizhen Weng2, Yirui Liu2 1University at Buffalo 2 Institute of Artificial Intelligence (TeleAI), China Telecom 3Dolby Laboratories 4Delft University of Technology 5University of Maryland 6ByteDance Abstract Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE (KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient datasetlevel, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development. Large language models (LLMs) adopt the Transformer architecture [1] and generate text autoregressively under causal mask, which ensures that past key and value vectors can be cached (KV-cache) [2]. These caches are stored in high-bandwidth memory (HBM) on GPU"
[10.02.2026 14:20] Response: ```python
[
    "University at Buffalo",
    "Institute of Artificial Intelligence (TeleAI), China Telecom",
    "Dolby Laboratories",
    "Delft University of Technology",
    "University of Maryland",
    "ByteDance"
]
```
[10.02.2026 14:20] Deleting PDF ./assets/pdf/2602.05929.pdf.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.07054.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.07054.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.07054.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.08629.
[10.02.2026 14:20] Downloading paper 2602.08629 from https://arxiv.org/pdf/2602.08629v1...
[10.02.2026 14:20] Extracting affiliations from text.
[10.02.2026 14:20] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CauScale: Neural Causal Discovery at Scale Bo Peng 1 2 3 Sirui Chen 1 4 Jiaguo Tian 3 Yu Qiao 1 2 Chaochao Lu 1 6 2 0 2 9 ] . [ 1 9 2 6 8 0 . 2 0 6 2 : r a "
[10.02.2026 14:20] Response: ```python
[]
```
[10.02.2026 14:20] Extracting affiliations from text.
[10.02.2026 14:20] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CauScale: Neural Causal Discovery at Scale Bo Peng 1 2 3 Sirui Chen 1 4 Jiaguo Tian 3 Yu Qiao 1 2 Chaochao Lu 1 6 2 0 2 9 ] . [ 1 9 2 6 8 0 . 2 0 6 2 : r aCausal discovery is essential for advancing datadriven fields such as scientific AI and data analysis, yet existing approaches face significant timeand space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts two-stream design: data stream extracts relational evidence from high-dimensional observations, while graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on indistribution data and 84.4% on out-of-distribution data, while delivering 413,000 inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale. 1. Introduction Causal discovery aims at uncovering causal relationships and mechanisms from observational data (Spirtes et al., 2000; Pearl, 2009; Glymour et al., 2019). central component of causal discovery is causal structure learning, which identifies the underlying structural causal models (SCMs) and learns directed acyclic graphs (DAGs) where edges represent direct causal relationships between variables (Peters et al., 2017). The inference of causal relationships is an 1Shanghai Artificial Intelligence Laboratory 2Shanghai Innovation Institute 3Shanghai Jiao Tong University 4Tongji University. Correspondence to: Chaochao Lu <luchaochao@pjlab.org.cn>. 1 important problem across many fields including bioinformatics (Sachs et al., 2005; Zhang et al., 2013), epidemiology (Vandenbroucke et al., 2016), and economics (Hicks et al., 1980). As data grow increasingly complex, discovering causal relationships from massive datasets has become an urgent challenge. However, existing causal discovery algorithms face major timeand space-efficiency bottlenecks, particularly when scaling to large graphs. Constraint-based algorithms (e.g., PC and FCI (Spirtes et al., 2000)) can become time-prohibitive because they rely on large numbers of conditional-independence tests, whose count grows exponentially in the worst case. In contrast, score-based methods such as NOTEARS (Zheng et al., 2018) and RL-BIC (Zhu et al., 2019) avoid explicit combinatorial search but typically require solving fresh continuous optimization problem for each dataset, which remains computationally expensive at scale. To reduce runtime, AVICI (Lorch et al., 2022) amortizes causal discovery by pretraining supervised model on simulated data and performing zero-shot graph prediction at test time. However, its attention mechanism scales unfavorably with the number of variables, often leading to substantial memory pressure on large graphs. To overcome these timeand space-efficiency bottlenecks, we propose CauScale, an efficient neural architecture for causal discovery. Overall, CauScale adopts two-stream design with data stream and graph stream. For time efficiency, we introduce reduction unit that compresses the data embeddings during network processing. For space efficiency, we adopt tied attention weights (Rao et al., 2021) in both streams: sharing attention weights across axis avoids maintaining axis-specific attention maps and substantially reduces the memory footprint of attention. To improve efficiency without sacrificing discovery quality, we further design data-graph block that (i) injects graph-prior information and (ii) mitigates information loss from data reduction. Specifically, it distills relational evidence from high-dimensional data into graph message to guide representation learning in the graph stream. Moreover, it fuses the two streams by injecting the data stream into the graph embedding before reduction, so that the model preserves key relational signals and alleviates information loss. We conduct extensive experiments on synthetic and singleCauScale: Neural Causal Discovery at Scale cell expression datasets with varying sizes and causal structures. The results demonstrate that CauScale achieves superior accuracy with markedly improved efficiency. Specifically, CauScale achieves an mAP of 99.6% on indistribution data and 84.4% on out-of-distribution (OOD) It stand out as the fastest method, outperforming data. previous approaches by 4 to 13,000. Furthermore, during training, CauScale scales successfully to 500-node graphs, setting where AVICI fails due to limited memory and excessive space costs. In summary, our contributions are: We present one of the first studies on pre-training neural networks for efficient causal discovery at scale, offering scalable step toward uncovering causal relations from increasingly complex data. We introduce CauScale, neural architecture that jointly improves time and memory efficiency with high causal discovery accuracy. We conduct comprehensive experiments to validate the effectiveness of CauScale across varying graph scales and causal mechanisms, demonstrating that CauScale improves both efficiency and causal discovery performance. 2. Related Work Existing causal structure learning methods can be broadly categorized into non-amortized and amortized (zero-shot) approaches, relying on whether inference on new dataset requires solving dataset-specific optimization problem. Non-amortized causal discovery. Non-amortized methods perform causal discovery by solving an optimization or search problem independently for each dataset, leading to high computational cost and limited scalability. 1) Constraint-based algorithms, such as PC and FCI (Spirtes et al., 2000), infer graph structures via conditional independence tests but suffer from exponential complexity as the number of variables grows. 2) Score-based algorithms optimize predefined score over the space of graph structures (Tsamardinos et al., 2006; Goudet et al., 2017). Classical approaches rely on greedy combinatorial search, including GES (Chickering, 2002) and GIES (Hauser"
[10.02.2026 14:20] Mistral response. {"id": "152c0b3dff26489188b383cb092d317c", "created": 1770733245, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1434, "total_tokens": 1469, "completion_tokens": 35, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Shanghai Artificial Intelligence Laboratory\",\n    \"Shanghai Innovation Institute\",\n    \"Shanghai Jiao Tong University\",\n    \"Tongji University\"\n]\n```"}}]}
[10.02.2026 14:20] Response: ```python
[
    "Shanghai Artificial Intelligence Laboratory",
    "Shanghai Innovation Institute",
    "Shanghai Jiao Tong University",
    "Tongji University"
]
```
[10.02.2026 14:20] Deleting PDF ./assets/pdf/2602.08629.pdf.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.07948.
[10.02.2026 14:20] Extra JSON file exists (./assets/json/2602.07948.json), skip PDF parsing.
[10.02.2026 14:20] Paper image links file exists (./assets/img_data/2602.07948.json), skip HTML parsing.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.05946.
[10.02.2026 14:20] Downloading paper 2602.05946 from https://arxiv.org/pdf/2602.05946v2...
[10.02.2026 14:20] Extracting affiliations from text.
[10.02.2026 14:20] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"f -GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment 6 2 0 2 9 ] . [ 2 6 4 9 5 0 . 2 0 6 2 : r Rajdeep Haldar 1 Lantao Mei Guang Lin 1 Yue Xing 2 Qifan Song "
[10.02.2026 14:20] Response: ```python
[]
```
[10.02.2026 14:20] Extracting affiliations from text.
[10.02.2026 14:20] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"f -GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment 6 2 0 2 9 ] . [ 2 6 4 9 5 0 . 2 0 6 2 : r Rajdeep Haldar 1 Lantao Mei Guang Lin 1 Yue Xing 2 Qifan SongRecent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose -Group Relative Policy Optimization (f -GRPO), class of on-policy reinforcement learning, and -Hybrid Alignment Loss (f -HAL), hybrid on/off policy objectives, for general LLM alignment based on variational representation of -divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods. 1. Introduction Alignment is the final stage in the training pipeline of large language models (LLMs), aimed at instilling capabilities beyond language comprehension and instruction-following that are learned during pretraining and instruction finetuning (Ouyang et al., 2022). These capabilities range from complex reasoning, such as coding or math problem-solving (Shao et al., 2024), to the adherence to safety and ethical constraints based on human preferences (Bai et al., 2022b). In practice, alignment is achieved via reinforcement learning (RL) using reward signal that encodes the target behavior. While various RL-based alignment strategies have been developed, they differ mainly in the source of the reward signal. In general, they can be categorized into two regimes: 1Department of Statistics, Purdue University 2Department of Statistics, Michigan State University. Correspondence to: Rajdeep Haldar <rhaldar@purdue.edu>. Preprint. February 10, 2026. 1 (i) In the verifiable reward regime (RLVR), one has access to an explicit reward function that quantitatively evaluates the correctness or quality of response, as in tasks such as mathematical reasoning (Guo et al., 2025), code generation (Le et al., 2022), tool use (Jiang et al., 2025), etc. Learning in this regime is naturally formulated using an on-policy reinforcement learning algorithm that samples from the current policy, evaluates responses based on the reward, and updates the model accordingly. Among on-policy algorithms, GRPO (Shao et al., 2024) stands out for its computational efficiency and superior performance. (ii) In the preference alignment regime (PA), reward signal is provided implicitly through relative comparisons or binary feedback between responses (e.g., safe vs. unsafe, preferred vs. rejected). Alignment can then be performed either by training reward model followed by on-policy learning, commonly referred to as reinforcement learning with human feedback (RLHF), or by using direct alignment methods that optimize the policy directly via supervised objective on preference data (Off-Policy). Further details for these two regimes are provided in 2.2 and 2.3. Divergence Perspective for Preference Alignment Haldar et al. (2025) proposed unifying framework for preference alignment that interprets popular PA methods as instances of divergence estimators between an aligned (D+, chosen) and unaligned (D, rejected) distributions, differentiated by the choice of divergences. In principle, one can construct new preference alignment methods given choice of divergence, resulting in class of direct, off-policy alignment objectives for any general -divergence; refer to 2.3.1 for details. natural question then arises: Can this divergence-based viewpoint for PA be extended beyond preference alignment to the verifiable reward regime (RLVR)? In other words, can divergence estimation serve as unifying foundation for both off-policy preference alignment and on-policy reinforcement learning with verifiable rewards? This work addresses this question by extending the divergence framework to construct general on-policy and hybrid RL objectives (Figure 1 summarizes our framework). In particular, our contribution is three-fold: Divergence-Based RL for General LLM Alignment vs. below-average reward Figure 1. Divergence Estimation Framework. RLVR (left): verifiable reward signal r(x, y) induces reward-aligned/unaligned distributions (above D+ under the old policy), and -GRPO performs on-policy alignment by estimating an -divergence between these distributions. Preference alignment (right): preference data samples chosen and rejected prompt-response pairs from aligned (D+) and unaligned (D) distributions, and direct aligners (e.g., FDO) optimize the variational -divergence objective between (D+, D). Hybrid (bottom): -HAL combines both information sources, interpolating between off-policy preference alignment and on-policy RLVR. Divergence-based RL objectives. We introduce -GRPO (Equation (15)), class of on-policy alignment objectives, and -HAL (Equation (16)), class of hybrid objectives that combine on-policy RL with preference-based supervision, both derived from general -divergences. Theoretical characterization and guarantees. We show that -GRPO and -HAL are divergence estimators: - GRPO estimates divergence between above-average and below-average reward distributions, while -HAL estimates divergence between mixtures of reward-based and preference-based aligned/unaligned distributions. We further establish Alignment Consistency and guarantee an improvement in the expected reward after alignment (Theorem 4.3). Furthermore, we justify the expected improvement of -GRPO over GRPO (4.1). Empirical validation across alignment regimes. We empirically validate our theoretical framework in 5. In particular, we examine the performance of -GRPO in both regimes and validate -HAL in the PA regime given the potential reward hacking in this regime. Experiment results demonstrate enhancement. 2. Preliminary 2.1. Notation Our notation follows (Haldar et al., 2025). Let denote the prompt and represent response. We use yw and yl to indicate aligned and unaligned responses, with flexible contexts depending on the regime (high reward, preferred) or (low reward, unpreferred). The triplet (x, yw, yl) characterized the joint distribution of"
[10.02.2026 14:20] Mistral response. {"id": "32522066be37460b8f215cd532876e1f", "created": 1770733255, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1446, "total_tokens": 1472, "completion_tokens": 26, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Department of Statistics, Purdue University\",\n    \"Department of Statistics, Michigan State University\"\n]\n```"}}]}
[10.02.2026 14:20] Response: ```python
[
    "Department of Statistics, Purdue University",
    "Department of Statistics, Michigan State University"
]
```
[10.02.2026 14:20] Deleting PDF ./assets/pdf/2602.05946.pdf.
[10.02.2026 14:20] Success.
[10.02.2026 14:20] Downloading and parsing paper https://huggingface.co/papers/2602.05708.
[10.02.2026 14:20] Downloading paper 2602.05708 from https://arxiv.org/pdf/2602.05708v1...
[10.02.2026 14:21] Extracting affiliations from text.
[10.02.2026 14:21] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 5 ] . [ 1 8 0 7 5 0 . 2 0 6 2 : r Cost-Efficient RAG for Entity Matching with LLMs: Blocking-based Exploration Chuangtao Ma Aalborg University Aalborg, Denmark chuma@cs.aau.dk Zeyu Zhang University of Amsterdam Amsterdam, the Netherlands z.zhang2@uva.nl Arijit Khan Bowling Green State University, USA Aalborg University, Denmark arijitk@bgsu.edu Sebastian Schelter BIFOLD & TU Berlin Berlin, Germany schelter@tu-berlin.de Paul Groth University of Amsterdam Amsterdam, the Netherlands p.t.groth@uva.nl "
[10.02.2026 14:21] Response: ```python
[
    "Aalborg University",
    "University of Amsterdam",
    "Bowling Green State University",
    "BIFOLD & TU Berlin",
    "TU Berlin"
]
```
[10.02.2026 14:21] Deleting PDF ./assets/pdf/2602.05708.pdf.
[10.02.2026 14:21] Success.
[10.02.2026 14:21] Downloading and parsing paper https://huggingface.co/papers/2602.07040.
[10.02.2026 14:21] Extra JSON file exists (./assets/json/2602.07040.json), skip PDF parsing.
[10.02.2026 14:21] Paper image links file exists (./assets/img_data/2602.07040.json), skip HTML parsing.
[10.02.2026 14:21] Success.
[10.02.2026 14:21] Downloading and parsing paper https://huggingface.co/papers/2602.02285.
[10.02.2026 14:21] Extra JSON file exists (./assets/json/2602.02285.json), skip PDF parsing.
[10.02.2026 14:21] Paper image links file exists (./assets/img_data/2602.02285.json), skip HTML parsing.
[10.02.2026 14:21] Success.
[10.02.2026 14:21] Enriching papers with extra data.
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 0. MOVA is an open-source model that generates synchronized audio-visual content using a Mixture-of-Experts architecture with 32 billion parameters, supporting image-text to video-audio generation tasks.  					AI-generated summary 				 Audio is indispensable for real-world video, yet generation models ...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 1. Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data.  					AI-generated summary 				 Despite the success of multimodal contrastive learning in aligni...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 2. Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated exper...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 3. RD-VLA introduces a recurrent architecture for vision-language-action models that adapts computational depth through latent iterative refinement, achieving constant memory usage and improved task success rates.  					AI-generated summary 				 Current Vision-Language-Action (VLA) models rely on fixed...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 4. WMSS is a post-training paradigm that uses weak model checkpoints to identify and fill learning gaps, enabling continued improvement beyond conventional saturation points in large language models.  					AI-generated summary 				 As post-training optimization becomes central to improving large langua...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 5. LLaDA2.1 introduces a novel token-to-token editing approach with speed and quality modes, enhanced through reinforcement learning for improved reasoning and instruction following in large language diffusion models.  					AI-generated summary 				 While LLaDA2.0 showcased the scaling potential of 100...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 6. TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories.  					AI-generated summary 				 Deploying GRPO on Flow Matching models has proven effective for text-to-image...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 7. A new benchmark and evaluation metric are introduced for assessing temporal coherence and dynamic interaction in GUI generation models, revealing significant challenges in maintaining consistency over extended interaction sequences.  					AI-generated summary 				 Recent advancements in image genera...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 8. Researchers introduce a new video understanding task and benchmark that evaluates models' ability to learn from few-shot demonstrations, along with a specialized MLLM architecture trained using a two-stage approach combining video supervision and preference optimization.  					AI-generated summary 	...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 9. BudgetMem is a runtime memory framework for LLM agents that uses modular components with three budget tiers and a neural policy router to optimize performance-cost trade-offs in memory usage.  					AI-generated summary 				 Memory is increasingly central to Large Language Model (LLM) agents operatin...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 10. LOCA-bench is introduced as a benchmark for evaluating language agents in long-context, agentic scenarios with controlled environment state management.  					AI-generated summary 				 Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as th...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 11. A new benchmark called GISA is introduced for evaluating information-seeking assistants, featuring human-crafted queries with structured answer formats and live updates to prevent memorization.  					AI-generated summary 				 The advancement of large language models (LLMs) has significantly accelera...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 12. AIRS-Bench presents a comprehensive benchmark suite for evaluating LLM agents across diverse scientific domains, demonstrating current limitations while providing open-source resources for advancing autonomous scientific research.  					AI-generated summary 				 LLM agents hold significant promise f...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 13. Current multimodal foundation models show limitations in maintaining coherent spatial beliefs during active exploration, exhibiting gaps between active and passive performance, inefficient exploration strategies, and difficulties in updating outdated spatial knowledge.  					AI-generated summary 			...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 14. WorldCompass enhances long-horizon video-based world models through reinforcement learning post-training with clip-level rollouts, complementary rewards, and efficient RL algorithms.  					AI-generated summary 				 This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training fr...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 15. InternAgent-1.5 is a unified system for autonomous scientific discovery that integrates computational modeling and experimental research through coordinated subsystems for generation, verification, and evolution.  					AI-generated summary 				 We introduce InternAgent-1.5, a unified system designed...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 16. LatentChem enables chemical reasoning through continuous latent space computations instead of discrete textual tokens, achieving superior performance and efficiency compared to traditional chain-of-thought approaches.  					AI-generated summary 				 Chemical large language models (LLMs) predominantl...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 17. ComprExIT introduces a novel approach to long-context inference in LLMs by using explicit information transmission over frozen hidden states, improving compression efficiency through depth-wise and width-wise transmission mechanisms.  					AI-generated summary 				 Long-context inference with Large ...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 18. Research investigates how fundamental reasoning paradigms influence large language model generalization through targeted training approaches and evaluation on real-world tasks.  					AI-generated summary 				 Deduction, induction, and abduction are fundamental reasoning paradigms, core for human log...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 19. AgentCPM-Report presents a lightweight local solution for deep research report generation using a Writing As Reasoning Policy framework and multi-stage agentic training to enhance small models' reasoning and outline evolution capabilities.  					AI-generated summary 				 Generating deep research rep...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 20. RelayGen is a training-free framework that dynamically switches between large and small models during reasoning by identifying difficulty transitions at the segment level, achieving faster inference with minimal accuracy loss.  					AI-generated summary 				 Large reasoning models (LRMs) achieve str...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 21. Adaptive test-time framework with world models enables selective visual imagination for spatial reasoning, improving efficiency and reliability by determining when imagination is necessary.  					AI-generated summary 				 Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spa...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 22. A scalable framework for evaluating and improving goal-conditioned procedure generation using large-scale web mining, automated scoring, and reinforcement learning to enhance step-by-step instruction quality.  					AI-generated summary 				 Generating step-by-step "how-to" procedures is a key LLM ca...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 23. NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment.  					AI-generated summar...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 24. Off-policy Soft Actor-Critic with large-batch updates enables efficient humanoid locomotion policy pretraining, while model-based methods facilitate safe adaptation through deterministic data collection and stochastic exploration within physics-informed world models.  					AI-generated summary 				 ...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 25. MotionCrafter is a video diffusion framework that jointly reconstructs 4D geometry and estimates dense motion using a novel joint representation and 4D VAE architecture.  					AI-generated summary 				 We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometr...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 26. Energy-constrained optimization framework separates energy metrics from rewards using Lagrangian method to achieve stable, energy-efficient humanoid robot locomotion with reduced hyperparameter tuning.  					AI-generated summary 				 Achieving stable and energy-efficient locomotion is essential for ...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 27. Foundation models including LLMs, MLLMs, and generative models require reliable and responsible development addressing bias, security, explainability, and other critical issues for trustworthy deployment across multiple domains.  					AI-generated summary 				 Foundation models, including Large Lang...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 28. Large language models are increasingly guiding data management processes through a tiered framework that optimizes data quality, cost, and training efficiency across different stages of model development.  					AI-generated summary 				 The development of artificial intelligence can be viewed as an ...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 29. WildReward demonstrates that reward models can be effectively trained from in-the-wild user interactions using ordinal regression, achieving performance comparable to traditional methods while benefiting from user diversity.  					AI-generated summary 				 Reward models (RMs) are crucial for the tra...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 30. A high-quality open-source singing voice synthesis system is presented with support for multiple languages and controllable generation, along with a dedicated benchmark for evaluating zero-shot performance.  					AI-generated summary 				 While recent years have witnessed rapid progress in speech sy...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 31. Explicit reasoning in LLM agents can degrade performance in user-engaged scenarios by reducing information disclosure and weakening agent-user communication, with transparency-aware prompting showing better results.  					AI-generated summary 				 Eliciting reasoning has emerged as a powerful techni...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 32. Autoregressive video diffusion models suffer from train-test gaps when generating long videos, but a training-free approach called Rolling Sink addresses this by maintaining AR cache and enabling ultra-long video synthesis.  					AI-generated summary 				 Recently, autoregressive (AR) video diffusio...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 33. A comprehensive study of Turkish subword tokenization systematically investigates the relationship between vocabulary size, training corpus, and tokenizer performance across multiple linguistic tasks and diagnostics.  					AI-generated summary 				 Tokenization is a pivotal design choice for neural ...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 34. FlexMoRE demonstrates that low-rank adapters can replace full-sized experts in mixture-of-experts architectures, achieving better performance with significantly fewer parameters.  					AI-generated summary 				 Recent advances in mixture-of-experts architectures have shown that individual experts mo...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 35. Research explores PDE solvers including neural frameworks for scientific simulations, examining forward solutions, inverse problems, and equation discovery across multi-variable and non-linear systems.  					AI-generated summary 				 Partial Differential Equations are precise in modelling the physic...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 36. A multi-agent framework guided by knowledge graphs addresses materials science challenges by integrating specialized agents for problem decomposition, evidence retrieval, and graph traversal to discover sustainable PFAS alternatives.  					AI-generated summary 				 Large Language Models (LLMs) promi...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 37. Analysis of agentic system evaluation reveals significant variance in single-run performance estimates, necessitating multiple runs and advanced metrics for reliable assessment.  					AI-generated summary 				 Agentic systems are evaluated on benchmarks where agents interact with environments to sol...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 38. Large reasoning models exhibit spontaneous question repetition patterns that can be formalized and leveraged to improve computational efficiency and accuracy through echo-aware training and prompting techniques.  					AI-generated summary 				 Test-time compute allocation in large reasoning models (...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 39. SPARSE is a user-centric framework that protects text embeddings from privacy leaks by selectively perturbing sensitive dimensions using differentiable masking and Mahalanobis noise calibration.  					AI-generated summary 				 Text embeddings enable numerous NLP applications but face severe privacy ...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 40. KV-CoRE method evaluates kv-cache compressibility through SVD-based low-rank approximation, revealing patterns linking compressibility to model architecture and training data across multiple languages and domains.  					AI-generated summary 				 Large language models rely on kv-caches to avoid redun...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 41. A benchmark and optimization technique are presented to improve multimodal large language models' emotion understanding by addressing spurious associations and hallucinations in audiovisual cues.  					AI-generated summary 				 Emotion understanding is essential for building socially intelligent age...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 42. CauScale is a neural architecture that enables efficient causal discovery on large graphs through compressed embeddings and tied attention weights, achieving high accuracy and significant speedups over previous methods.  					AI-generated summary 				 Causal discovery is essential for advancing data...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 43. Collective motion in fish schools exemplifies emergent self-organization in active matter systems, yet computational tools for simulating and analyzing these dynamics remain fragmented across research groups. We present dewi-kadita, an open-source Python library implementing the three-dimensional Co...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 44. Preference alignment objectives are extended to general alignment settings using f-divergence variational representations, introducing novel on-policy and hybrid policy optimization methods for LLM alignment with theoretical and empirical validation.  					AI-generated summary 				 Recent research s...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 45. CE-RAG4EM reduces computational overhead in large-scale entity matching by implementing blocking-based batch retrieval and generation while maintaining competitive matching quality.  					AI-generated summary 				 Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tas...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 46. Aster is an AI agent that accelerates scientific discovery by iteratively improving programs, achieving state-of-the-art results across multiple domains including mathematics, biology, and machine learning with significantly reduced computational requirements.  					AI-generated summary 				 We intr...
[10.02.2026 14:21] ********************************************************************************
[10.02.2026 14:21] Abstract 47. A comprehensive formalization of statistical learning theory in Lean 4 addresses gaps in mathematical libraries and demonstrates human-AI collaboration for verified machine learning theory foundations.  					AI-generated summary 				 We present the first comprehensive Lean 4 formalization of statist...
[10.02.2026 14:21] Read previous papers.
[10.02.2026 14:21] Generating reviews via LLM API.
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#inference", "#video", "#audio", "#dataset", "#architecture", "#open_source", "#multimodal"], "emoji": "", "ru": {"title": "       ", "desc": "MOVA            Mixture-of-Experts, 
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#training", "#multimodal", "#architecture"], "emoji": "", "ru": {"title": "        ", "desc": "        , 
[10.02.2026 14:21] Using data from previous issue: {"categories": [], "emoji": "", "ru": {"title": "      ", "desc": "QuantaAlpha       -   ,       
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#inference", "#training", "#robotics", "#architecture", "#multimodal"], "emoji": "", "ru": {"title": "    :    robotics", "desc": "RD-VLA      vision-language-action, 
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "", "ru": {"title": "       ", "desc": "   WMSS        ,    ,    
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization", "#alignment", "#benchmark", "#architecture", "#open_source", "#reasoning", "#diffusion", "#plp"], "emoji": "", "ru": {"title": "         T2T     ",
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#optimization"], "emoji": "", "ru": {"title": "         ", "desc": "TP-GRPO       flow matching       
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#dataset"], "emoji": "", "ru": {"title": "      ", "desc": "     GEBench      ,  700   
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#training", "#video", "#benchmark", "#dataset", "#multimodal", "#rlhf"], "emoji": "", "ru": {"title": "       ", "desc": "           
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#optimization", "#rl", "#agents", "#long_context", "#training"], "emoji": "", "ru": {"title": "          ", "desc": "BudgetMem       LLM-   ,
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#agents", "#long_context", "#open_source", "#benchmark"], "emoji": "", "ru": {"title": "         ", "desc": "  LOCA-bench           
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#dataset", "#survey", "#agents"], "emoji": "", "ru": {"title": "GISA:           ", "desc": "   GISA   - ,  37
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#agents", "#open_source", "#survey", "#science", "#benchmark", "#dataset"], "emoji": "", "ru": {"title": "    ", "desc": "  AIRS-Bench        LLM    . 
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#multimodal", "#robotics"], "emoji": "", "ru": {"title": "   :      foundation models", "desc": " ,    foundation models  
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#training", "#video", "#rl"], "emoji": "", "ru": {"title": " - :      ", "desc": "WorldCompass     - -      .  
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#science", "#reasoning", "#benchmark", "#open_source", "#agents"], "emoji": "", "ru": {"title": "         ", "desc": "InternAgent-1.5        ,  
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#science", "#reasoning"], "emoji": "", "ru": {"title": "        ", "desc": "LatentChem      ,         
[10.02.2026 14:21] Querying the API.
[10.02.2026 14:21] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ComprExIT introduces a novel approach to long-context inference in LLMs by using explicit information transmission over frozen hidden states, improving compression efficiency through depth-wise and width-wise transmission mechanisms.  					AI-generated summary 				 Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key-value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression.
[10.02.2026 14:21] Response: ```json
{
  "desc": "ComprExIT          ,        .           -     .     ,       , ComprExIT    :  ( )       ( )    .  ,        -     1%  .",
  "emoji": "",
  "title": "      "
}
```
[10.02.2026 14:21] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ComprExIT introduces a novel approach to long-context inference in LLMs by using explicit information transmission over frozen hidden states, improving compression efficiency through depth-wise and width-wise transmission mechanisms.  					AI-generated summary 				 Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key-value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression."

[10.02.2026 14:21] Response: ```python
['INFERENCE', 'BENCHMARK']
```
[10.02.2026 14:21] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ComprExIT introduces a novel approach to long-context inference in LLMs by using explicit information transmission over frozen hidden states, improving compression efficiency through depth-wise and width-wise transmission mechanisms.  					AI-generated summary 				 Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key-value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression."

[10.02.2026 14:21] Response: ```python
["LONG_CONTEXT", "OPTIMIZATION"]
```
[10.02.2026 14:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ComprExIT presents a new method for improving long-context inference in Large Language Models (LLMs) by using explicit information transmission over fixed hidden states. This approach enhances compression efficiency through two mechanisms: depth-wise transmission, which prevents overwriting of information across layers, and width-wise transmission, which optimally aggregates information into fewer slots. Unlike traditional methods that rely on the model\'s self-attention, ComprExIT decouples compression from these dynamics, allowing for better coordination of information allocation. The results show that ComprExIT outperforms existing context compression techniques while adding minimal parameters, making it a promising solution for efficient long-context processing.","title":"Efficient Long-Context Inference with ComprExIT"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="ComprExIT presents a new method for improving long-context inference in Large Language Models (LLMs) by using explicit information transmission over fixed hidden states. This approach enhances compression efficiency through two mechanisms: depth-wise transmission, which prevents overwriting of information across layers, and width-wise transmission, which optimally aggregates information into fewer slots. Unlike traditional methods that rely on the model's self-attention, ComprExIT decouples compression from these dynamics, allowing for better coordination of information allocation. The results show that ComprExIT outperforms existing context compression techniques while adding minimal parameters, making it a promising solution for efficient long-context processing.", title='Efficient Long-Context Inference with ComprExIT'))
[10.02.2026 14:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ComprExITComprExITComprExIT1%","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ComprExITComprExITComprExIT1%', title=''))
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#benchmark", "#training", "#reasoning", "#synthetic", "#dataset"], "emoji": "", "ru": {"title": "        LLM", "desc": "    ,        ,       
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#training", "#small_models", "#open_source", "#rl", "#agents", "#reasoning", "#benchmark"], "emoji": "", "ru": {"title": "   :      ", "desc": "AgentCPM-Report      
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#inference", "#training", "#small_models", "#optimization", "#reasoning"], "emoji": "", "ru": {"title": "      ", "desc": "RelayGen         ,   
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#cv", "#benchmark"], "emoji": "", "ru": {"title": " :   ,   ", "desc": "          ,     
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization", "#synthetic", "#dataset", "#benchmark", "#open_source", "#reasoning"], "emoji": "", "ru": {"title": "         ", "desc": "   How2Everything -  
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#optimization"], "emoji": "", "ru": {"title": "   :      ", "desc": "NanoQuant      ,        (1-)   
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#training", "#robotics", "#rl"], "emoji": "", "ru": {"title": "        off-policy   ", "desc": " ,   Soft Actor-Critic       
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#diffusion", "#video", "#architecture", "#3d", "#training"], "emoji": "", "ru": {"title": "       4D ", "desc": "MotionCrafter       ,    4D 
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#rl"], "emoji": "", "ru": {"title": "        ", "desc": "   ECO (Energy-Constrained Optimization)     ,   
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#security", "#alignment", "#ethics", "#hallucinations", "#survey", "#interpretability"], "emoji": "", "ru": {"title": "      ", "desc": "          
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#training", "#open_source", "#alignment", "#agi", "#optimization", "#data"], "emoji": "", "ru": {"title": "    :    AGI", "desc": "        ,    
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#dataset", "#training", "#rlhf", "#data"], "emoji": "", "ru": {"title": "      ", "desc": "   WildReward   ,        
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#low_resource", "#audio", "#dataset", "#benchmark", "#open_source", "#multilingual"], "emoji": "", "ru": {"title": "            ", "desc": "   SoulX-Singer  
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#alignment", "#agents", "#reasoning", "#benchmark", "#open_source", "#training"], "emoji": "", "ru": {"title": " ?        ", "desc": "       LLM , 
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#inference", "#training", "#video", "#long_context", "#diffusion"], "emoji": "", "ru": {"title": "        ", "desc": "       
[10.02.2026 14:21] Querying the API.
[10.02.2026 14:21] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A comprehensive study of Turkish subword tokenization systematically investigates the relationship between vocabulary size, training corpus, and tokenizer performance across multiple linguistic tasks and diagnostics.  					AI-generated summary 				 Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a "subwords manifest", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this "subwords manifest" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.
[10.02.2026 14:21] Response: ```json
{
  "desc": "         ,         .         ,     (WordPiece,  ,  )      :  (NLI, STS,  , NER),  (POS,  )   .                / F1,  -  -   .                 .",
  "emoji": "",
  "title": "       "
}
```
[10.02.2026 14:21] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A comprehensive study of Turkish subword tokenization systematically investigates the relationship between vocabulary size, training corpus, and tokenizer performance across multiple linguistic tasks and diagnostics.  					AI-generated summary 				 Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a "subwords manifest", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this "subwords manifest" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research."

[10.02.2026 14:21] Response: ```python
["MULTILINGUAL", "DATA", "BENCHMARK"]
```
[10.02.2026 14:21] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A comprehensive study of Turkish subword tokenization systematically investigates the relationship between vocabulary size, training corpus, and tokenizer performance across multiple linguistic tasks and diagnostics.  					AI-generated summary 				 Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a "subwords manifest", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this "subwords manifest" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research."

[10.02.2026 14:21] Response: ```python
['LOW_RESOURCE', 'OPEN_SOURCE', 'INTERPRETABILITY']
```
[10.02.2026 14:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a detailed study on subword tokenization for Turkish, a morphologically rich language. It explores how vocabulary size and the training corpus affect the performance of different tokenizers across various linguistic tasks. The authors introduce a new diagnostic toolkit that evaluates tokenizers at a granular level, providing insights into their strengths and weaknesses. This research aims to improve tokenizer design in Turkish and offers a framework for future studies in similar languages.","title":"Unlocking Effective Tokenization for Turkish Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a detailed study on subword tokenization for Turkish, a morphologically rich language. It explores how vocabulary size and the training corpus affect the performance of different tokenizers across various linguistic tasks. The authors introduce a new diagnostic toolkit that evaluates tokenizers at a granular level, providing insights into their strengths and weaknesses. This research aims to improve tokenizer design in Turkish and offers a framework for future studies in similar languages.', title='Unlocking Effective Tokenization for Turkish Language Models'))
[10.02.2026 14:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='', title=''))
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#reasoning"], "emoji": "", "ru": {"title": "       ", "desc": "    FlexMoRE,        
[10.02.2026 14:21] Using data from previous issue: {"categories": [], "emoji": "", "ru": {"title": "        ", "desc": "         ()       .   
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#hallucinations", "#science", "#reasoning"], "emoji": "", "ru": {"title": "       ", "desc": "    ,   ,    .   
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#agents", "#benchmark"], "emoji": "", "ru": {"title": "   :      ", "desc": "        ,       .  
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#math", "#training", "#reasoning", "#open_source", "#inference", "#interpretability", "#optimization"], "emoji": "", "ru": {"title": "       ", "desc": "        
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#security", "#leakage"], "emoji": "", "ru": {"title": "  :    ", "desc": "SPARSE   ,            .  
[10.02.2026 14:21] Querying the API.
[10.02.2026 14:21] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

KV-CoRE method evaluates kv-cache compressibility through SVD-based low-rank approximation, revealing patterns linking compressibility to model architecture and training data across multiple languages and domains.  					AI-generated summary 				 Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.
[10.02.2026 14:21] Response: ```json
{
  "desc": "   KV-CoRE    KV-      SVD-  .   -  ,            .              ,        .     ,         ,         .",
  "emoji": "",
  "title": "   KV-    "
}
```
[10.02.2026 14:21] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"KV-CoRE method evaluates kv-cache compressibility through SVD-based low-rank approximation, revealing patterns linking compressibility to model architecture and training data across multiple languages and domains.  					AI-generated summary 				 Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development."

[10.02.2026 14:21] Response: ```python
['INFERENCE', 'BENCHMARK', 'MULTILINGUAL']
```
[10.02.2026 14:21] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"KV-CoRE method evaluates kv-cache compressibility through SVD-based low-rank approximation, revealing patterns linking compressibility to model architecture and training data across multiple languages and domains.  					AI-generated summary 				 Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development."

[10.02.2026 14:21] Response: ```python
['LONG_CONTEXT', 'LOW_RESOURCE', 'OPTIMIZATION']
```
[10.02.2026 14:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The KV-CoRE method introduces a new way to evaluate the compressibility of kv-caches in large language models using singular value decomposition (SVD) for low-rank approximation. This approach highlights how compressibility varies depending on the model architecture and the training data used, which is often overlooked in previous studies. By analyzing multiple models across different languages and domains, KV-CoRE reveals systematic patterns that connect compressibility to these factors. The method also provides a framework for understanding how compressibility impacts model performance, paving the way for more efficient and data-aware compression techniques.","title":"Unlocking KV-Cache Efficiency with KV-CoRE"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The KV-CoRE method introduces a new way to evaluate the compressibility of kv-caches in large language models using singular value decomposition (SVD) for low-rank approximation. This approach highlights how compressibility varies depending on the model architecture and the training data used, which is often overlooked in previous studies. By analyzing multiple models across different languages and domains, KV-CoRE reveals systematic patterns that connect compressibility to these factors. The method also provides a framework for understanding how compressibility impacts model performance, paving the way for more efficient and data-aware compression techniques.', title='Unlocking KV-Cache Efficiency with KV-CoRE'))
[10.02.2026 14:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"KV-CoREkv-cacheSVDKV-CoREkv-cache","title":"KV-CoREkv-cache"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='KV-CoREkv-cacheSVDKV-CoREkv-cache', title='KV-CoREkv-cache'))
[10.02.2026 14:21] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#video", "#hallucinations", "#audio", "#benchmark", "#rlhf", "#open_source", "#training"], "emoji": "", "ru": {"title": " :      ", "desc": "    EmoReA
[10.02.2026 14:21] Querying the API.
[10.02.2026 14:21] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

CauScale is a neural architecture that enables efficient causal discovery on large graphs through compressed embeddings and tied attention weights, achieving high accuracy and significant speedups over previous methods.  					AI-generated summary 				 Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.
[10.02.2026 14:21] Response: ```json
{
  "desc": "CauScale       -    .    embeddings           .    :        ,       .     (99.6%  in-distribution )      4-13,000      .",
  "emoji": "",
  "title": "        "
}
```
[10.02.2026 14:21] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CauScale is a neural architecture that enables efficient causal discovery on large graphs through compressed embeddings and tied attention weights, achieving high accuracy and significant speedups over previous methods.  					AI-generated summary 				 Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale."

[10.02.2026 14:21] Response: ```python
["ARCHITECTURE", "TRAINING"]
```

**Justification:**

- **ARCHITECTURE**: The paper proposes CauScale, a novel neural architecture specifically designed for efficient causal discovery. It introduces architectural innovations including a reduction unit for compressing embeddings, tied attention weights, and a two-stream design (data stream and graph stream).

- **TRAINING**: The paper addresses training efficiency and scalability, demonstrating that CauScale can scale to 500-node graphs during training where prior methods fail due to space limitations. It discusses improvements in both time and space efficiency during the training process.
[10.02.2026 14:21] Error. Failed to parse JSON from LLM. ["ARCHITECTURE", "TRAINING"]


**Justification:**

- **ARCHITECTURE**: The paper proposes CauScale, a novel neural architecture specifically designed for efficient causal discovery. It introduces architectural innovations including a reduction unit for compressing embeddings, tied attention weights, and a two-stream design (data stream and graph stream).

- **TRAINING**: The paper addresses training efficiency and scalability, demonstrating that CauScale can scale to 500-node graphs during training where prior methods fail due to space limitations. It discusses improvements in both time and space efficiency during the training process.
[10.02.2026 14:21] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CauScale is a neural architecture that enables efficient causal discovery on large graphs through compressed embeddings and tied attention weights, achieving high accuracy and significant speedups over previous methods.  					AI-generated summary 				 Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale."

[10.02.2026 14:21] Response: ```python
['GRAPHS', 'SCIENCE', 'OPEN_SOURCE']
```
[10.02.2026 14:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CauScale is a novel neural architecture designed for efficient causal discovery in large graphs, addressing the limitations of existing methods in terms of time and space efficiency. It utilizes compressed embeddings to reduce data size and tied attention weights to streamline the attention mechanism, allowing it to handle graphs with up to 1000 nodes. The architecture features a two-stream design that combines a data stream for extracting relational evidence and a graph stream for integrating statistical priors, ensuring high accuracy in causal inference. With impressive performance metrics, CauScale achieves significant speed improvements, making it a powerful tool for data-driven fields requiring causal analysis.","title":"CauScale: Fast and Accurate Causal Discovery for Large Graphs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CauScale is a novel neural architecture designed for efficient causal discovery in large graphs, addressing the limitations of existing methods in terms of time and space efficiency. It utilizes compressed embeddings to reduce data size and tied attention weights to streamline the attention mechanism, allowing it to handle graphs with up to 1000 nodes. The architecture features a two-stream design that combines a data stream for extracting relational evidence and a graph stream for integrating statistical priors, ensuring high accuracy in causal inference. With impressive performance metrics, CauScale achieves significant speed improvements, making it a powerful tool for data-driven fields requiring causal analysis.', title='CauScale: Fast and Accurate Causal Discovery for Large Graphs'))
[10.02.2026 14:22] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CauScale1000CauScale500","title":"CauScale"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CauScale1000CauScale500', title='CauScale'))
[10.02.2026 14:22] Using data from previous issue: {"categories": [], "emoji": "", "ru": {"title": "        ", "desc": "  dewi-kadita     Python        -  Couzin   
[10.02.2026 14:22] Querying the API.
[10.02.2026 14:22] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Preference alignment objectives are extended to general alignment settings using f-divergence variational representations, introducing novel on-policy and hybrid policy optimization methods for LLM alignment with theoretical and empirical validation.  					AI-generated summary 				 Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose f-Group Relative Policy Optimization (f-GRPO), a class of on-policy reinforcement learning, and f-Hybrid Alignment Loss (f-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of f-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.
[10.02.2026 14:22] Response: ```json
{
  "desc": "       LLM,             .    f-,     : f-GRPO  on-policy     f-HAL   on/off-policy  .  ,         .               .",
  "emoji": "",
  "title": "      f-"
}
```
[10.02.2026 14:22] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Preference alignment objectives are extended to general alignment settings using f-divergence variational representations, introducing novel on-policy and hybrid policy optimization methods for LLM alignment with theoretical and empirical validation.  					AI-generated summary 				 Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose f-Group Relative Policy Optimization (f-GRPO), a class of on-policy reinforcement learning, and f-Hybrid Alignment Loss (f-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of f-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods."

[10.02.2026 14:22] Response: ```python
["RLHF", "RL", "TRAINING"]
```

**Justification:**
- **RLHF**: The paper discusses preference alignment objectives and human feedback mechanisms (chosen vs. rejected responses), which is central to RLHF methods.
- **RL**: The paper explicitly proposes reinforcement learning methods (f-GRPO, f-HAL) for LLM alignment and discusses reinforcement learning with verifiable rewards (RLVR).
- **TRAINING**: The paper focuses on improving model training and fine-tuning methods through novel alignment objectives and policy optimization techniques for LLMs.
[10.02.2026 14:22] Error. Failed to parse JSON from LLM. ["RLHF", "RL", "TRAINING"]


**Justification:**
- **RLHF**: The paper discusses preference alignment objectives and human feedback mechanisms (chosen vs. rejected responses), which is central to RLHF methods.
- **RL**: The paper explicitly proposes reinforcement learning methods (f-GRPO, f-HAL) for LLM alignment and discusses reinforcement learning with verifiable rewards (RLVR).
- **TRAINING**: The paper focuses on improving model training and fine-tuning methods through novel alignment objectives and policy optimization techniques for LLMs.
[10.02.2026 14:22] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Preference alignment objectives are extended to general alignment settings using f-divergence variational representations, introducing novel on-policy and hybrid policy optimization methods for LLM alignment with theoretical and empirical validation.  					AI-generated summary 				 Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose f-Group Relative Policy Optimization (f-GRPO), a class of on-policy reinforcement learning, and f-Hybrid Alignment Loss (f-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of f-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods."

[10.02.2026 14:22] Response: ```python
['ALIGNMENT', 'REASONING', 'OPTIMIZATION']
```
[10.02.2026 14:22] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper extends Preference Alignment (PA) objectives to broader alignment scenarios using f-divergence variational representations. It introduces new methods for policy optimization, specifically f-Group Relative Policy Optimization (f-GRPO) for on-policy reinforcement learning and f-Hybrid Alignment Loss (f-HAL) for hybrid objectives. The authors provide theoretical guarantees that these methods enhance average rewards after alignment. Empirical results show that the proposed framework outperforms existing techniques in both reinforcement learning with verifiable rewards and safety alignment tasks.","title":"Enhancing LLM Alignment with Divergence-Based Optimization"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper extends Preference Alignment (PA) objectives to broader alignment scenarios using f-divergence variational representations. It introduces new methods for policy optimization, specifically f-Group Relative Policy Optimization (f-GRPO) for on-policy reinforcement learning and f-Hybrid Alignment Loss (f-HAL) for hybrid objectives. The authors provide theoretical guarantees that these methods enhance average rewards after alignment. Empirical results show that the proposed framework outperforms existing techniques in both reinforcement learning with verifiable rewards and safety alignment tasks.', title='Enhancing LLM Alignment with Divergence-Based Optimization'))
[10.02.2026 14:22] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PAf-f-f-GRPOf-f-HALLLMRLVR","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PAf-f-f-GRPOf-f-HALLLMRLVR', title=''))
[10.02.2026 14:22] Querying the API.
[10.02.2026 14:22] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

CE-RAG4EM reduces computational overhead in large-scale entity matching by implementing blocking-based batch retrieval and generation while maintaining competitive matching quality.  					AI-generated summary 				 Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration.
[10.02.2026 14:22] Response: ```json
{
  "desc": "  CE-RAG4EM,        (RAG),          .             LLM  ,  .        RAG-,         .  ,  CE-RAG4EM           ,       .",
  "emoji": "",
  "title": " RAG      "
}
```
[10.02.2026 14:22] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CE-RAG4EM reduces computational overhead in large-scale entity matching by implementing blocking-based batch retrieval and generation while maintaining competitive matching quality.  					AI-generated summary 				 Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration."

[10.02.2026 14:22] Response: ```python
["RAG", "BENCHMARK", "INFERENCE"]
```

**Justification:**

1. **RAG**: The paper explicitly focuses on retrieval-augmented generation (RAG), proposing CE-RAG4EM, a cost-efficient RAG architecture for entity matching.

2. **BENCHMARK**: The paper presents "a unified framework for analyzing and evaluating RAG systems for entity matching," which constitutes a benchmarking contribution for evaluating RAG systems.

3. **INFERENCE**: The paper addresses computational efficiency and optimization of RAG systems, focusing on reducing "retrieval and generation overhead" and "end-to-end runtime," which are core inference optimization concerns.
[10.02.2026 14:22] Error. Failed to parse JSON from LLM. ["RAG", "BENCHMARK", "INFERENCE"]


**Justification:**

1. **RAG**: The paper explicitly focuses on retrieval-augmented generation (RAG), proposing CE-RAG4EM, a cost-efficient RAG architecture for entity matching.

2. **BENCHMARK**: The paper presents "a unified framework for analyzing and evaluating RAG systems for entity matching," which constitutes a benchmarking contribution for evaluating RAG systems.

3. **INFERENCE**: The paper addresses computational efficiency and optimization of RAG systems, focusing on reducing "retrieval and generation overhead" and "end-to-end runtime," which are core inference optimization concerns.
[10.02.2026 14:22] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CE-RAG4EM reduces computational overhead in large-scale entity matching by implementing blocking-based batch retrieval and generation while maintaining competitive matching quality.  					AI-generated summary 				 Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration."

[10.02.2026 14:22] Response: ```python
["REASONING", "OPTIMIZATION"]
```

**Justification:**

- **REASONING**: The paper discusses retrieval-augmented generation (RAG) which enhances LLM reasoning capabilities in knowledge-intensive tasks like entity matching.

- **OPTIMIZATION**: The paper focuses on reducing computational overhead and improving efficiency through blocking-based batch retrieval and generation, and analyzes trade-offs between performance and computational cost, which are core optimization concerns.
[10.02.2026 14:22] Error. Failed to parse JSON from LLM. ["REASONING", "OPTIMIZATION"]


**Justification:**

- **REASONING**: The paper discusses retrieval-augmented generation (RAG) which enhances LLM reasoning capabilities in knowledge-intensive tasks like entity matching.

- **OPTIMIZATION**: The paper focuses on reducing computational overhead and improving efficiency through blocking-based batch retrieval and generation, and analyzes trade-offs between performance and computational cost, which are core optimization concerns.
[10.02.2026 14:22] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CE-RAG4EM is a novel architecture designed to improve the efficiency of entity matching tasks by minimizing computational costs. It employs a blocking-based approach for batch retrieval and generation, which helps in managing large datasets effectively. The paper also introduces a comprehensive framework for evaluating retrieval-augmented generation (RAG) systems, emphasizing optimizations that consider blocking and retrieval granularity. Experimental results demonstrate that CE-RAG4EM maintains or enhances matching quality while significantly reducing runtime compared to existing methods.","title":"Efficient Entity Matching with CE-RAG4EM"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CE-RAG4EM is a novel architecture designed to improve the efficiency of entity matching tasks by minimizing computational costs. It employs a blocking-based approach for batch retrieval and generation, which helps in managing large datasets effectively. The paper also introduces a comprehensive framework for evaluating retrieval-augmented generation (RAG) systems, emphasizing optimizations that consider blocking and retrieval granularity. Experimental results demonstrate that CE-RAG4EM maintains or enhances matching quality while significantly reducing runtime compared to existing methods.', title='Efficient Entity Matching with CE-RAG4EM'))
[10.02.2026 14:22] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CE-RAG4EMRAGRAGCE-RAG4EM","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CE-RAG4EMRAGRAGCE-RAG4EM', title=''))
[10.02.2026 14:22] Using data from previous issue: {"categories": ["#science", "#training", "#optimization", "#open_source", "#agents", "#plp"], "emoji": "", "ru": {"title": "      ", "desc": "Aster   AI-    ,    
[10.02.2026 14:22] Using data from previous issue: {"categories": ["#open_source", "#science"], "emoji": "", "ru": {"title": " :     -AI ", "desc": "          Lean 4,     
[10.02.2026 14:22] Renaming data file.
[10.02.2026 14:22] Renaming previous data. hf_papers.json to ./d/2026-02-10.json
[10.02.2026 14:22] Saving new data file.
[10.02.2026 14:22] Generating page.
[10.02.2026 14:22] Renaming previous page.
[10.02.2026 14:22] Renaming previous data. index.html to ./d/2026-02-10.html
[10.02.2026 14:22] Writing result.
[10.02.2026 14:22] Renaming log file.
[10.02.2026 14:22] Renaming previous data. log.txt to ./logs/2026-02-10_last_log.txt
