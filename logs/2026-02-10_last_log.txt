[09.02.2026 23:32] Read previous papers.
[09.02.2026 23:32] Generating top page (month).
[09.02.2026 23:32] Writing top page (month).
[10.02.2026 01:46] Read previous papers.
[10.02.2026 01:46] Get feed.
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06717
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06570
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05843
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05027
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03392
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01734
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18415
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06949
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06130
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06291
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05940
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06391
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05281
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06075
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06079
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06139
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05367
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06960
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06869
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06669
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06663
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04837
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02581
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06854
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06176
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05847
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05711
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06554
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06471
[10.02.2026 01:46] Extract page data from URL. URL: https://huggingface.co/papers/2602.04649
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03075
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06883
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03548
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06724
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06566
[10.02.2026 01:46] Extract page data from URL. URL: https://huggingface.co/papers/2602.06181
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06129
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04454
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03998
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01064
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23039
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06964
[10.02.2026 01:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04811
[10.02.2026 01:46] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.02.2026 01:46] No deleted papers detected.
[10.02.2026 01:46] Downloading and parsing papers (pdf, html). Total: 43.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06717.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06717.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06717.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06570.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06570.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06570.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.05843.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.05843.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.05843.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.05027.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.05027.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.05027.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.03392.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.03392.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.03392.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.01734.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.01734.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.01734.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2601.18415.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2601.18415.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2601.18415.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06949.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06949.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06949.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06130.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06130.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06130.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06291.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06291.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06291.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.05940.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.05940.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.05940.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06391.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06391.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06391.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.05281.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.05281.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.05281.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06075.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06075.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06075.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06079.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06079.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06079.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06139.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06139.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06139.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.05367.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.05367.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.05367.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06960.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06960.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06960.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06869.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06869.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06869.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06669.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06669.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06669.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06663.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06663.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06663.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.04837.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.04837.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.04837.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.02581.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.02581.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.02581.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06854.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06854.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06854.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06176.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06176.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06176.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.05847.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.05847.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.05847.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.05711.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.05711.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.05711.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06554.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06554.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06554.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06471.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06471.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06471.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.04649.
[10.02.2026 01:46] Downloading paper 2602.04649 from https://arxiv.org/pdf/2602.04649v1...
[10.02.2026 01:46] Extracting affiliations from text.
[10.02.2026 01:46] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2026-02-05 Outcome Accuracy is Not Enough: Aligning the Reasoning Process of Reward Models Binghai Wang1,2, Yantao Liu1, Yuxuan Liu1, Tianyi Tang1, Shenzhi Wang1,3, Chang Gao1, Chujie Zheng1, Yichang Zhang1, Le Yu1, Shixuan Liu1, Tao Gui2, Qi Zhang2, Xuanjing Huang2, Bowen Yu1, Fei Huang1, Junyang Lin1 1Qwen Team, Alibaba Group 2Fudan University 3Tsinghua University https://github.com/QwenLM/RationaleRM "
[10.02.2026 01:46] Response: ```python
[
    "Alibaba Group",
    "Fudan University",
    "Tsinghua University"
]
```
[10.02.2026 01:46] Deleting PDF ./assets/pdf/2602.04649.pdf.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.03075.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.03075.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.03075.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06883.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06883.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06883.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.03548.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.03548.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.03548.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06724.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06724.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06724.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06566.
[10.02.2026 01:46] Extra JSON file exists (./assets/json/2602.06566.json), skip PDF parsing.
[10.02.2026 01:46] Paper image links file exists (./assets/img_data/2602.06566.json), skip HTML parsing.
[10.02.2026 01:46] Success.
[10.02.2026 01:46] Downloading and parsing paper https://huggingface.co/papers/2602.06181.
[10.02.2026 01:46] Downloading paper 2602.06181 from https://arxiv.org/pdf/2602.06181v1...
[10.02.2026 01:47] Extracting affiliations from text.
[10.02.2026 01:47] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Stanley Z. Hua1 Sanae Lotfi2 Irene Y. Chen1 1UC Berkeley & UCSF 2Meta Superintelligence Labs 6 2 0 F 5 ] . [ 1 1 8 1 6 0 . 2 0 6 2 : r Abstract Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on PostTrainingBiasBench, unified benchmark of 13 closedand open-ended bias datasets. We identify phenomenon we term quantization-induced masked bias flipping, in which up to 21% of responses flip between biased and unbiased states after quantization, despite showing no change in aggregate bias scores. These flips are strongly driven by model uncertainty, where the responses with high uncertainty are 311 more likely to change than the confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 46 more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups, where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes. Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, requiring crucial post-quantization evaluation and interventions to ensure reliability in practice. Post-training quantization (PTQ) is widely applied to make large language models (LLMs) more practical in resource-constrained settings, yet we know surprisingly little about its impact on social bias. While PTQ methods optimize for computational efficiency at the sub-module level, they operate without awareness of downstream behavioral changes; disconnect that requires urgent attention as quantized models become increasingly deployed in healthcare, law, and other high-stakes domains. Rec"
[10.02.2026 01:47] Response: ```python
["UC Berkeley", "UCSF", "Meta Superintelligence Labs"]
```
[10.02.2026 01:47] Deleting PDF ./assets/pdf/2602.06181.pdf.
[10.02.2026 01:47] Success.
[10.02.2026 01:47] Downloading and parsing paper https://huggingface.co/papers/2602.06129.
[10.02.2026 01:47] Extra JSON file exists (./assets/json/2602.06129.json), skip PDF parsing.
[10.02.2026 01:47] Paper image links file exists (./assets/img_data/2602.06129.json), skip HTML parsing.
[10.02.2026 01:47] Success.
[10.02.2026 01:47] Downloading and parsing paper https://huggingface.co/papers/2602.04454.
[10.02.2026 01:47] Extra JSON file exists (./assets/json/2602.04454.json), skip PDF parsing.
[10.02.2026 01:47] Paper image links file exists (./assets/img_data/2602.04454.json), skip HTML parsing.
[10.02.2026 01:47] Success.
[10.02.2026 01:47] Downloading and parsing paper https://huggingface.co/papers/2602.03998.
[10.02.2026 01:47] Extra JSON file exists (./assets/json/2602.03998.json), skip PDF parsing.
[10.02.2026 01:47] Paper image links file exists (./assets/img_data/2602.03998.json), skip HTML parsing.
[10.02.2026 01:47] Success.
[10.02.2026 01:47] Downloading and parsing paper https://huggingface.co/papers/2602.01064.
[10.02.2026 01:47] Extra JSON file exists (./assets/json/2602.01064.json), skip PDF parsing.
[10.02.2026 01:47] Paper image links file exists (./assets/img_data/2602.01064.json), skip HTML parsing.
[10.02.2026 01:47] Success.
[10.02.2026 01:47] Downloading and parsing paper https://huggingface.co/papers/2601.23039.
[10.02.2026 01:47] Extra JSON file exists (./assets/json/2601.23039.json), skip PDF parsing.
[10.02.2026 01:47] Paper image links file exists (./assets/img_data/2601.23039.json), skip HTML parsing.
[10.02.2026 01:47] Success.
[10.02.2026 01:47] Downloading and parsing paper https://huggingface.co/papers/2602.06964.
[10.02.2026 01:47] Extra JSON file exists (./assets/json/2602.06964.json), skip PDF parsing.
[10.02.2026 01:47] Paper image links file exists (./assets/img_data/2602.06964.json), skip HTML parsing.
[10.02.2026 01:47] Success.
[10.02.2026 01:47] Downloading and parsing paper https://huggingface.co/papers/2602.04811.
[10.02.2026 01:47] Extra JSON file exists (./assets/json/2602.04811.json), skip PDF parsing.
[10.02.2026 01:47] Paper image links file exists (./assets/img_data/2602.04811.json), skip HTML parsing.
[10.02.2026 01:47] Success.
[10.02.2026 01:47] Enriching papers with extra data.
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 0. RLVR methods using group sampling suffer from bias toward likely trajectories and missed rare-correct ones; a difficulty-aware advantage scaling technique improves performance on benchmarks without increasing computational cost.  					AI-generated summary 				 Reinforcement Learning with Verifiable ...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 1. Baichuan-M3 is a medical-enhanced large language model designed for clinical decision support with capabilities in proactive information gathering, long-horizon reasoning, and hallucination suppression.  					AI-generated summary 				 We introduce Baichuan-M3, a medical-enhanced large language model...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 2. OdysseyArena presents a new framework for evaluating large language models on long-horizon, inductive agent tasks that emphasize autonomous discovery of environmental transition laws.  					AI-generated summary 				 The rapid advancement of Large Language Models (LLMs) has catalyzed the development ...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 3. Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.  					AI-generated summary 				 Sparse Aut...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 4. The paper establishes a theoretical framework for analyzing entropy dynamics in reinforcement fine-tuning of large language models, deriving expressions for entropy change and proposing entropy control methods based on discriminant analysis.  					AI-generated summary 				 Entropy serves as a critic...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 5. Training instability in large language models is linked to weight matrix stable rank decline and Jacobian alignment, which MSign addresses through matrix sign operations to prevent gradient explosions.  					AI-generated summary 				 Training instability remains a critical challenge in large languag...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 6. A three-component speech-to-text system combines Wav2Vec2, AST, and Whisper models with curriculum learning and uncertainty modeling to improve transcription accuracy and reduce hallucinations in Russian-language speech.  					AI-generated summary 				 This work presents a speech-to-text system "Pis...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 7. DreamDojo is a foundation world model trained on 44k hours of egocentric human videos that enables efficient simulation of dexterous robotic tasks through continuous latent actions and real-time distillation.  					AI-generated summary 				 Being able to simulate the outcomes of actions in varied en...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 8. SWIRL is a self-improvement framework that learns world models from state-only sequences by alternating between forward and inverse dynamics modeling with variational information maximization and ELBO maximization, achieving improved performance on various reasoning and planning benchmarks.  					AI...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 9. Consequence-Based Utility evaluates mathematical solutions by testing their effectiveness as exemplars for related problems, outperforming reward models and LLM judges in ranking quality and correct-wrong separation.  					AI-generated summary 				 Recent progress in reasoning models suggests that g...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 10. TRIT framework improves multilingual reasoning by jointly training translation and reasoning components, enhancing question understanding and response generation across languages.  					AI-generated summary 				 Long reasoning models often struggle in multilingual settings: they tend to reason in En...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 11. GUI agents for automated digital tasks rely on vision-language models with enhanced grounding capabilities, achieved through refined data engineering, improved training strategies, and reinforcement learning with verifiable rewards.  					AI-generated summary 				 The rapid advancement of vision-lan...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 12. A novel reinforcement learning approach called ARM addresses entropy collapse in LLM reasoning by equilibrating confidence levels across correct responses through dynamic reward shaping.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispens...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 13. A comprehensive memory-focused benchmark for mobile GUI agents reveals significant memory capability gaps and provides systematic evaluation methods and design insights.  					AI-generated summary 				 Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 14. Canzona presents a unified asynchronous framework that addresses the conflict between matrix-based optimizers and distributed tensor fragmentation in LLM training, improving efficiency and reducing latency.  					AI-generated summary 				 The scaling of Large Language Models (LLMs) drives interest i...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 15. Multi-modal large language models struggle to jointly understand audio and visual signals in egocentric videos, but a new scalable data engine and dataset significantly improve their performance through targeted fine-tuning.  					AI-generated summary 				 Understanding egocentric videos plays a vit...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 16. Residual binarization framework RaBiT addresses feature co-adaptation in quantized LLMs through hierarchical path derivation and robust initialization, achieving superior accuracy-efficiency trade-offs.  					AI-generated summary 				 Efficient deployment of large language models (LLMs) requires ext...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 17. InftyThink+ uses reinforcement learning to optimize iterative reasoning processes, improving accuracy and efficiency in large language models.  					AI-generated summary 				 Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from ...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 18. Multi-objective alignment in LLMs suffers from cross-objective interference where improving performance on some objectives degrades others, with a covariance-based analysis and a proposed method to maintain positive correlations between rewards and training signals.  					AI-generated summary 				 W...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 19. Compar:IA is an open-source platform that collects large-scale human preference data for multilingual language model training and evaluation, featuring a blind pairwise comparison interface and releasing three datasets under open licenses.  					AI-generated summary 				 Large Language Models (LLMs)...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 20. PlanViz benchmark evaluates unified multimodal models' capabilities in computer-use planning tasks through route planning, work diagramming, and web&UI displaying sub-tasks with a task-adaptive scoring system.  					AI-generated summary 				 Unified multimodal models (UMMs) have shown impressive cap...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 21. Group-Evolving Agents enable open-ended self-improvement by treating groups of agents as evolutionary units, allowing efficient experience sharing and reuse to enhance coding performance and robustness.  					AI-generated summary 				 Open-ended self-improving agents can autonomously modify their ow...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 22. QuantLRM uses weight update magnitude signals from fine-tuning to improve quantization of Large Reasoning Models, achieving better performance than traditional methods through channel importance estimation.  					AI-generated summary 				 Weight-only quantization is important for compressing Large L...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 23. A novel framework called SEMA is introduced that effectively trains multi-turn attackers for large language models without relying on existing strategies or external data, achieving state-of-the-art attack success rates while being compact, reproducible, and transferable across different models and ...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 24. Large language models exhibit significant reasoning failures that can be categorized into embodied and non-embodied types, with fundamental, application-specific, and robustness-related subtypes, requiring systematic analysis and mitigation strategies.  					AI-generated summary 				 Large Language ...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 25. OmniVideo-R1 enhances audio-visual understanding through reinforced frameworks that integrate self-supervised and contrastive learning for multimodal reasoning.  					AI-generated summary 				 While humans perceive the world through diverse modalities that operate synergistically to support a holist...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 26. OmniMoE presents a system-algorithm co-designed framework that achieves fine-grained expert specialization in Mixture-of-Experts architectures through vector-level atomic experts and optimized routing and scheduling mechanisms.  					AI-generated summary 				 Mixture-of-Experts (MoE) architectures a...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 27. SeeUPO is a critic-free reinforcement learning method that ensures convergence guarantees in multi-turn agent interactions by modeling sequential decision-making as multi-agent bandit problems and using backward induction for policy updates.  					AI-generated summary 				 Reinforcement learning (RL...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 28. Replacing conventional feed-forward networks with hourglass-shaped MLPs in Transformers improves model efficiency and performance by enabling better parameter utilization and competitive scaling.  					AI-generated summary 				 Dense Transformer language models have largely adhered to one consistent...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 29. Generative Reward Models suffer from deceptive alignment due to outcome accuracy prioritization, but rationale consistency metrics and hybrid training signals improve performance and generalization in RLHF.  					AI-generated summary 				 Generative Reward Models (GenRMs) and LLM-as-a-Judge exhibit ...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 30. ReMiT introduces a bidirectional training approach where reinforcement learning-guided mid-training token reweighting improves large language model pre-training and post-training performance through an iterative feedback loop.  					AI-generated summary 				 Standard training pipelines for large lan...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 31. Vision transformer components exhibit varying plasticity levels that correlate with finetuning performance, challenging the assumption that smoothness is always beneficial.  					AI-generated summary 				 The smoothness of the transformer architecture has been extensively studied in the context of g...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 32. SEAD framework enables service dialogue agents to learn effective strategies through self-evolving user modeling components, achieving superior task completion and dialogue efficiency compared to existing foundation and commercial models.  					AI-generated summary 				 Large Language Models have de...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 33. Table-as-Search framework reformulates information seeking tasks as table completion problems, improving long-horizon search robustness through structured state management.  					AI-generated summary 				 Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence durin...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 34. SPARC is a modular framework that decouples visual perception from reasoning in vision-language models, enabling efficient test-time scaling through targeted compute allocation and improved performance on visual reasoning tasks.  					AI-generated summary 				 Despite recent successes, test-time sca...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 35. Post-training quantization of large language models causes significant changes in social biases that aggregate metrics fail to detect, with quantization-induced masked bias flipping occurring more frequently in uncertain responses and stronger quantization levels.  					AI-generated summary 				 Pos...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 36. A diffusion-transformer framework integrates spatio-temporal urban data to predict building-level climate risks while incorporating transportation network structures for emergency response applications.  					AI-generated summary 				 Climate hazards increasingly disrupt urban transportation and eme...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 37. Seg-ReSearch introduces a novel segmentation approach that combines interleaved reasoning with external search to overcome limitations of frozen MLLM knowledge, using hierarchical reward design for training and demonstrating superior performance on video object segmentation benchmarks.  					AI-gene...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 38. AtlasPatch is an efficient and scalable whole-slide image preprocessing framework that uses fine-tuned Segment-Anything model for accurate tissue detection and high-throughput patch extraction with reduced computational overhead.  					AI-generated summary 				 Whole-slide image (WSI) preprocessing,...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 39. Knowledge purification techniques consolidate rationales from multiple teacher LLMs to reduce conflicts and improve efficiency in distillation processes.  					AI-generated summary 				 Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language ...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 40. Researchers identify and address premature mode collapse in optimal transport-based structural prediction models through an adaptive stability control algorithm that prevents gradient explosions during large-scale training.  					AI-generated summary 				 Differentiable matching layers and residual ...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 41. Training diffusion models on neural network activations creates meta-models that learn internal state distributions and improve intervention fidelity without restrictive structural assumptions.  					AI-generated summary 				 Existing approaches for analyzing neural network activations, such as PCA ...
[10.02.2026 01:47] ********************************************************************************
[10.02.2026 01:47] Abstract 42. SE-Bench presents a diagnostic environment that obscures NumPy's API to evaluate agents' ability to internally store and utilize novel knowledge without external documentation, revealing challenges in knowledge retention and internalization through different training approaches.  					AI-generated s...
[10.02.2026 01:47] Read previous papers.
[10.02.2026 01:47] Generating reviews via LLM API.
[10.02.2026 01:47] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –∫–∞–∫ –Ω–∞–π—Ç–∏ —Ä–µ–¥–∫–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —Å–º–µ—â–µ–Ω–∏—è –≤ –º–µ—Ç–æ–¥–∞—Ö –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏ (RLVR), –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≥—Ä—É–ø–ø–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤. –ê
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#open_source", "#science", "#hallucinations", "#reasoning"], "emoji": "‚öïÔ∏è", "ru": {"title": "–û—Ç –ø–∞—Å—Å–∏–≤–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∫ –∞–∫—Ç–∏–≤–Ω–æ–π –≤—Ä–∞—á–µ–±–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–µ", "desc": "Baichuan-M3 ‚Äî —ç—Ç–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è LLM, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –ø–∞—Ä–∞–¥–∏–≥–º—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ø–∞—Ü–∏–µ–Ω—Ç–∞–º–∏ –æ—Ç –ø–∞—Å—Å–∏–≤–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∫ –∞–∫
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#reasoning", "#long_context", "#open_source", "#dataset"], "emoji": "üß≠", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è –∑–∞–∫–æ–Ω–æ–≤ –ø—Ä–∏—Ä–æ–¥—ã –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω OdysseyArena ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#open_source", "#interpretability"], "emoji": "üëÇ", "ru": {"title": "–†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–≤—É–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ (SAE) –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤ –∞—É–¥–∏–æ–º–æ–¥–µ–ª—è—Ö Whisper –∏ HuBERT. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, 
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#rlhf", "#math", "#training", "#rl"], "emoji": "‚öñÔ∏è", "ru": {"title": "–¢–µ–æ—Ä–∏—è —ç–Ω—Ç—Ä–æ–ø–∏–∏ –≤ —É—Å–∏–ª–µ–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–Ω–∞–º–∏–∫–∏ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –ø—Ä–∏ —É—Å–∏–ª–µ–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –≤—ã–≤–æ–¥—è—Ç –≤—ã—Ä–∞–∂–µ–Ω–∏—è 
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization"], "emoji": "‚ö°", "ru": {"title": "MSign: —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–∞–Ω–≥–∞ –º–∞—Ç—Ä–∏—Ü", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–æ—è–≤–ª—è–µ—Ç—Å—è –≤ –≤–∏–¥–µ –≤–∑—Ä—ã–≤–Ω–æ–≥
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#multilingual", "#low_resource", "#architecture", "#open_source", "#training", "#audio", "#hallucinations"], "emoji": "üé§", "ru": {"title": "–ê–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä—É—Å—Å–∫–æ–π —Ä–µ—á–∏ –±–µ–∑ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ ¬´Pisets¬ª
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#inference", "#open_source", "#synthetic", "#transfer_learning", "#video", "#training", "#robotics", "#dataset"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –ª–æ–≤–∫–æ–≥–æ —Ä–æ–±–æ—É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–∑ –≤–∏–¥–µ–æ —á–µ–ª–æ–≤–µ–∫–∞", "desc": "DreamDojo ‚Äî —ç—Ç–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#benchmark", "#multimodal", "#optimization", "#training"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞ —á–µ—Ä–µ–∑ —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä—è–º–æ–π –∏ –æ–±—Ä–∞—Ç–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏", "desc": "SWIRL ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏–∑—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞ –∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ç–æ–ª
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#science", "#math", "#dataset"], "emoji": "üßÆ", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∏—Ö –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å –Ω–∞ —Å–æ—Å–µ–¥–Ω–∏—Ö –∑–∞–¥–∞—á–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Consequence-Based Utility –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π 
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#machine_translation", "#transfer_learning", "#multilingual", "#training", "#reasoning"], "emoji": "üåç", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ TRIT, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#multimodal", "#rl", "#benchmark", "#agents", "#training", "#cv", "#data"], "emoji": "üñ±Ô∏è", "ru": {"title": "–¢–æ—á–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ GUI —á–µ—Ä–µ–∑ –∏–Ω–∂–µ–Ω–µ—Ä–∏—é –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å POINTS-GUI-G-8B –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–¥–∞—á —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#rl"], "emoji": "‚öñÔ∏è", "ru": {"title": "–£—Ä–∞–≤–Ω–æ–≤–µ—à–∏–≤–∞–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ ARM (Advantage Re-weighting Mechanism) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –∫–æ–ª–ª–∞–ø—Å–∞ —ç–Ω—Ç
[10.02.2026 01:47] Using data from previous issue: {"categories": [], "emoji": "üß†", "ru": {"title": "–ü–∞–º—è—Ç—å –∫–∞–∫ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –º–æ–±–∏–ª—å–Ω—ã—Ö GUI-–∞–≥–µ–Ω—Ç–æ–≤: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∏ —Ä–µ—à–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ MemGUI-Bench ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–±–∏–ª—å–Ω—ã—Ö GUI-–∞–≥–µ–Ω—Ç–æ–≤ –∫ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –æ–±—É—á–µ–Ω–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "‚ö°", "ru": {"title": "–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–∞—Ä–º–æ–Ω–∏—è: –º–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã –≤—Å—Ç—Ä–µ—á–∞—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã", "desc": "Canzona –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç –º–µ–∂–¥—É –º–∞—Ç—Ä–∏—á–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞–º–∏ 
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#benchmark", "#video", "#training", "#robotics", "#dataset"], "emoji": "üëÅÔ∏è‚Äçüó®Ô∏è", "ru": {"title": "–ì–∞—Ä–º–æ–Ω–∏—á–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –∑–≤—É–∫–∞ –∏ –≤–∏–¥–µ–Ω–∏—è –≤ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ö", "desc": "–†–∞–±–æ—Ç–∞ –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞—É–¥–∏–æ –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#inference", "#architecture", "#training", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ LLM", "desc": "RaBiT ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#small_models", "#optimization", "#rl", "#training", "#reasoning", "#long_context"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ –º—ã—Å–ª–µ–π —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ InftyThink+, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#training", "#rlhf", "#optimization", "#architecture", "#alignment"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–∞–ª–∞–Ω—Å —Ü–µ–ª–µ–π: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –ø—Ä–∏ –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–º –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏–∑—É—á–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –∫—Ä–æ—Å—Å-–æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –ø—Ä–∏ –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏ 
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#multilingual", "#data", "#low_resource", "#rlhf", "#alignment", "#dataset", "#open_source", "#benchmark"], "emoji": "üåç", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö —á–µ–ª–æ–≤–µ–∫–∞ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "Compar:IA ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#survey", "#multimodal", "#benchmark", "#cv", "#dataset"], "emoji": "üìã", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ PlanViz –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#benchmark", "#plp", "#training", "#agents"], "emoji": "üß¨", "ru": {"title": "–ì—Ä—É–ø–ø–æ–≤–∞—è —ç–≤–æ–ª—é—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ Group-Evolving Agents (GEA) –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#optimization", "#inference", "#benchmark", "#training", "#reasoning"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –æ–±–æ–∏—Ö –∫–æ–Ω—Ü–æ–≤: –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Å–∏–≥–Ω–∞–ª—ã fine-tuning", "desc": "QuantLRM ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ 
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#training", "#rl", "#alignment", "#security", "#open_source"], "emoji": "üéØ", "ru": {"title": "–ú–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã–µ –∞—Ç–∞–∫–∏ –Ω–∞ LLM —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ SEMA –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –∞—Ç–∞–∫ –Ω–∞ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#survey", "#reasoning", "#open_source"], "emoji": "üß†", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—à–∏–±–æ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø—Ä–µ–¥–ª–æ–∂–∏–≤ –Ω–æ–≤—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#training", "#video", "#rl", "#multimodal", "#audio"], "emoji": "üé¨", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–≤—É–∫–∞ –∏ –≤–∏–¥–µ–æ", "desc": "OmniVideo-R1 ‚Äî —ç—Ç–æ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–≤—É–∫–æ–≤—ã–µ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#inference", "#training", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–ê—Ç–æ–º–∞—Ä–Ω—ã–µ —ç–∫—Å–ø–µ—Ä—Ç—ã: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏", "desc": "OmniMoE –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º—É, –∫–æ—Ç–æ—Ä–∞—è –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Mixture-of-Experts –±–ª–∞–≥–æ–¥–∞—Ä
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#agents", "#training", "#rl", "#alignment", "#optimization", "#reasoning"], "emoji": "üéØ", "ru": {"title": "–ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–µ–∑ –∫—Ä–∏—Ç–∏–∫–∞ —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω—É—é –∏–Ω–¥—É–∫—Ü–∏—é", "desc": "SeeUPO ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–µ–∑ –∫—Ä–∏—Ç–∏–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç 
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#optimization"], "emoji": "‚è≥", "ru": {"title": "–ü–µ—Å–æ—á–Ω—ã–µ —á–∞—Å—ã –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–∏: –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∑–∞–º–µ–Ω–∏—Ç—å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–µ—Ç–∏ (FFN) –≤ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö –Ω–∞ –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏ –≤ —Ñ–æ—Ä–º–µ –ø–µ—Å–æ—á–Ω—ã—Ö —á–∞—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ª—É—á—à–µ –∏—Å–ø–æ–ª
[10.02.2026 01:47] Querying the API.
[10.02.2026 01:47] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Generative Reward Models suffer from deceptive alignment due to outcome accuracy prioritization, but rationale consistency metrics and hybrid training signals improve performance and generalization in RLHF.  					AI-generated summary 				 Generative Reward Models (GenRMs) and LLM-as-a-Judge exhibit deceptive alignment by producing correct judgments for incorrect reasons, as they are trained and evaluated to prioritize Outcome Accuracy, which undermines their ability to generalize during RLHF. We introduce Rationale Consistency, a fine-grained metric that quantifies the alignment between the model's reasoning process and human judgment. Our evaluation of frontier models reveals that rationale consistency effectively discriminates among state-of-the-art models and detects deceptive alignment, while outcome accuracy falls short in both respects. To mitigate this gap, we introduce a hybrid signal that combines rationale consistency with outcome accuracy for GenRM training. Our training method achieves state-of-the-art performance on RM-Bench (87.1%) and JudgeBench (82%), surpassing outcome-only baselines by an average of 5%. Using RM during RLHF, our method effectively improves performance as demonstrated on Arena Hard v2, notably yielding a 7% improvement in creative writing tasks. Further analysis confirms that our method escapes the deceptive alignment trap, effectively reversing the decline in rationale consistency observed in outcome-only training.
[10.02.2026 01:47] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –æ–±–º–∞–Ω—á–∏–≤–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –¥–∞—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –ø–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø—Ä–∏—á–∏–Ω–∞–º –∏–∑-–∑–∞ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç—Ä–∏–∫—É —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –∏–∑–º–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –ª–æ–≥–∏–∫–æ–π –º–æ–¥–µ–ª–∏ –∏ —Å—É–∂–¥–µ–Ω–∏—è–º–∏ —á–µ–ª–æ–≤–µ–∫–∞. –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –∫–æ–º–±–∏–Ω–∏—Ä—É—é—â–∏–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–µ–Ω–∏–µ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –æ—Ç –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ —á–µ–ª–æ–≤–µ–∫–∞. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –≤–∫–ª—é—á–∞—è 7% —É–ª—É—á—à–µ–Ω–∏–µ –≤ –∑–∞–¥–∞—á–∞—Ö —Ç–≤–æ—Ä—á–µ—Å–∫–æ–≥–æ –ø–∏—Å—å–º–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤ RLHF.",
  "emoji": "üéØ",
  "title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–º–µ—Å—Ç–æ —Ç–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –º–æ–¥–µ–ª—è—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è"
}
```
[10.02.2026 01:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generative Reward Models suffer from deceptive alignment due to outcome accuracy prioritization, but rationale consistency metrics and hybrid training signals improve performance and generalization in RLHF.  					AI-generated summary 				 Generative Reward Models (GenRMs) and LLM-as-a-Judge exhibit deceptive alignment by producing correct judgments for incorrect reasons, as they are trained and evaluated to prioritize Outcome Accuracy, which undermines their ability to generalize during RLHF. We introduce Rationale Consistency, a fine-grained metric that quantifies the alignment between the model's reasoning process and human judgment. Our evaluation of frontier models reveals that rationale consistency effectively discriminates among state-of-the-art models and detects deceptive alignment, while outcome accuracy falls short in both respects. To mitigate this gap, we introduce a hybrid signal that combines rationale consistency with outcome accuracy for GenRM training. Our training method achieves state-of-the-art performance on RM-Bench (87.1%) and JudgeBench (82%), surpassing outcome-only baselines by an average of 5%. Using RM during RLHF, our method effectively improves performance as demonstrated on Arena Hard v2, notably yielding a 7% improvement in creative writing tasks. Further analysis confirms that our method escapes the deceptive alignment trap, effectively reversing the decline in rationale consistency observed in outcome-only training."

[10.02.2026 01:47] Response: ```python
["RLHF", "BENCHMARK", "TRAINING"]
```
[10.02.2026 01:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generative Reward Models suffer from deceptive alignment due to outcome accuracy prioritization, but rationale consistency metrics and hybrid training signals improve performance and generalization in RLHF.  					AI-generated summary 				 Generative Reward Models (GenRMs) and LLM-as-a-Judge exhibit deceptive alignment by producing correct judgments for incorrect reasons, as they are trained and evaluated to prioritize Outcome Accuracy, which undermines their ability to generalize during RLHF. We introduce Rationale Consistency, a fine-grained metric that quantifies the alignment between the model's reasoning process and human judgment. Our evaluation of frontier models reveals that rationale consistency effectively discriminates among state-of-the-art models and detects deceptive alignment, while outcome accuracy falls short in both respects. To mitigate this gap, we introduce a hybrid signal that combines rationale consistency with outcome accuracy for GenRM training. Our training method achieves state-of-the-art performance on RM-Bench (87.1%) and JudgeBench (82%), surpassing outcome-only baselines by an average of 5%. Using RM during RLHF, our method effectively improves performance as demonstrated on Arena Hard v2, notably yielding a 7% improvement in creative writing tasks. Further analysis confirms that our method escapes the deceptive alignment trap, effectively reversing the decline in rationale consistency observed in outcome-only training."

[10.02.2026 01:47] Response: ```python
['ALIGNMENT', 'INTERPRETABILITY', 'REASONING']
```
[10.02.2026 01:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the challenges faced by Generative Reward Models (GenRMs) in Reinforcement Learning from Human Feedback (RLHF), particularly the issue of deceptive alignment where models produce correct outcomes for the wrong reasons. The authors propose a new metric called Rationale Consistency, which measures how well a model\'s reasoning aligns with human judgment, providing a more nuanced evaluation than just outcome accuracy. They introduce a hybrid training signal that combines rationale consistency with outcome accuracy, leading to improved performance on benchmark tasks. The results show that this approach not only enhances model performance but also helps avoid the pitfalls of deceptive alignment, ensuring more reliable reasoning in AI-generated outputs.","title":"Enhancing AI Reasoning with Rationale Consistency"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper discusses the challenges faced by Generative Reward Models (GenRMs) in Reinforcement Learning from Human Feedback (RLHF), particularly the issue of deceptive alignment where models produce correct outcomes for the wrong reasons. The authors propose a new metric called Rationale Consistency, which measures how well a model's reasoning aligns with human judgment, providing a more nuanced evaluation than just outcome accuracy. They introduce a hybrid training signal that combines rationale consistency with outcome accuracy, leading to improved performance on benchmark tasks. The results show that this approach not only enhances model performance but also helps avoid the pitfalls of deceptive alignment, ensuring more reliable reasoning in AI-generated outputs.", title='Enhancing AI Reasoning with Rationale Consistency'))
[10.02.2026 01:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ÁîüÊàêÂ•ñÂä±Ê®°ÂûãÔºàGenRMsÔºâÂú®ÁªìÊûúÂáÜÁ°ÆÊÄß‰ºòÂÖàÁöÑÊÉÖÂÜµ‰∏ãÔºåÂÆπÊòìÂá∫Áé∞ËØØÂØºÊÄßÂØπÈΩêÁöÑÈóÆÈ¢òÔºåËøôÂΩ±Âìç‰∫ÜÂÆÉ‰ª¨Âú®Âº∫ÂåñÂ≠¶‰π†‰∫∫Á±ªÂèçÈ¶àÔºàRLHFÔºâ‰∏≠ÁöÑË°®Áé∞ÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÊåáÊ†á‚Äî‚ÄîÊé®ÁêÜ‰∏ÄËá¥ÊÄßÔºåÁî®‰∫éÈáèÂåñÊ®°ÂûãÊé®ÁêÜËøáÁ®ã‰∏é‰∫∫Á±ªÂà§Êñ≠‰πãÈó¥ÁöÑÂØπÈΩêÁ®ãÂ∫¶„ÄÇÈÄöËøáÂØπÂâçÊ≤øÊ®°ÂûãÁöÑËØÑ‰º∞ÔºåÊàë‰ª¨ÂèëÁé∞Êé®ÁêÜ‰∏ÄËá¥ÊÄßËÉΩÂ§üÊúâÊïàÂå∫ÂàÜÊúÄÂÖàËøõÁöÑÊ®°ÂûãÔºåÂπ∂Ê£ÄÊµãÂá∫ËØØÂØºÊÄßÂØπÈΩêÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊ∑∑Âêà‰ø°Âè∑ÔºåÂ∞ÜÊé®ÁêÜ‰∏ÄËá¥ÊÄß‰∏éÁªìÊûúÂáÜÁ°ÆÊÄßÁªìÂêàËµ∑Êù•ËøõË°åËÆ≠ÁªÉÔºå‰ªéËÄåÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ","title":"ÊèêÂçáÁîüÊàêÂ•ñÂä±Ê®°ÂûãÁöÑÊé®ÁêÜ‰∏ÄËá¥ÊÄß"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ÁîüÊàêÂ•ñÂä±Ê®°ÂûãÔºàGenRMsÔºâÂú®ÁªìÊûúÂáÜÁ°ÆÊÄß‰ºòÂÖàÁöÑÊÉÖÂÜµ‰∏ãÔºåÂÆπÊòìÂá∫Áé∞ËØØÂØºÊÄßÂØπÈΩêÁöÑÈóÆÈ¢òÔºåËøôÂΩ±Âìç‰∫ÜÂÆÉ‰ª¨Âú®Âº∫ÂåñÂ≠¶‰π†‰∫∫Á±ªÂèçÈ¶àÔºàRLHFÔºâ‰∏≠ÁöÑË°®Áé∞ÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÊåáÊ†á‚Äî‚ÄîÊé®ÁêÜ‰∏ÄËá¥ÊÄßÔºåÁî®‰∫éÈáèÂåñÊ®°ÂûãÊé®ÁêÜËøáÁ®ã‰∏é‰∫∫Á±ªÂà§Êñ≠‰πãÈó¥ÁöÑÂØπÈΩêÁ®ãÂ∫¶„ÄÇÈÄöËøáÂØπÂâçÊ≤øÊ®°ÂûãÁöÑËØÑ‰º∞ÔºåÊàë‰ª¨ÂèëÁé∞Êé®ÁêÜ‰∏ÄËá¥ÊÄßËÉΩÂ§üÊúâÊïàÂå∫ÂàÜÊúÄÂÖàËøõÁöÑÊ®°ÂûãÔºåÂπ∂Ê£ÄÊµãÂá∫ËØØÂØºÊÄßÂØπÈΩêÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊ∑∑Âêà‰ø°Âè∑ÔºåÂ∞ÜÊé®ÁêÜ‰∏ÄËá¥ÊÄß‰∏éÁªìÊûúÂáÜÁ°ÆÊÄßÁªìÂêàËµ∑Êù•ËøõË°åËÆ≠ÁªÉÔºå‰ªéËÄåÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ', title='ÊèêÂçáÁîüÊàêÂ•ñÂä±Ê®°ÂûãÁöÑÊé®ÁêÜ‰∏ÄËá¥ÊÄß'))
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#optimization", "#training", "#rl", "#reasoning"], "emoji": "üîÑ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –≤ –¥–≤–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è: –∫–∞–∫ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–∞–µ—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç –º–æ–¥–µ–ª–∏", "desc": "ReMiT –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –≥–¥–µ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–µ—Ä–µwe
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#open_source", "#transfer_learning", "#interpretability"], "emoji": "üîÑ", "ru": {"title": "–ü–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ –≥–ª–∞–¥–∫–æ—Å—Ç–∏: –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –¥–æ–æ–±—É—á–µ–Ω–∏—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Vision Transformer ‚Äî –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Ö–æ–¥—ã
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#multimodal", "#training", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–°–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∏–π –∞–≥–µ–Ω—Ç –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è", "desc": "SEAD ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –±–µ–∑ –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#dataset"], "emoji": "üóÇÔ∏è", "ru": {"title": "–¢–∞–±–ª–∏—Ü–∞ –∫–∞–∫ –Ω–∞–≤–∏–≥–∞—Ç–æ—Ä: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –¥–æ–ª–≥–∏—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Table-as-Search (TaS) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–∏—Å–∫–∞ –≤ –∑–∞–¥
[10.02.2026 01:47] Using data from previous issue: {"categories": ["#long_context", "#optimization", "#multimodal", "#inference", "#reasoning", "#architecture", "#cv"], "emoji": "üß†", "ru": {"title": "–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "SPARC ‚Äî —ç—Ç–æ –º–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –≤–∏–¥–µ–æ-—è–∑—ã–∫
[10.02.2026 01:47] Querying the API.
[10.02.2026 01:47] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Post-training quantization of large language models causes significant changes in social biases that aggregate metrics fail to detect, with quantization-induced masked bias flipping occurring more frequently in uncertain responses and stronger quantization levels.  					AI-generated summary 				 Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on PostTrainingBiasBench, a unified benchmark of 13 closed- and open-ended bias datasets. We identify a phenomenon we term quantization-induced masked bias flipping, in which up to 21% of responses flip between biased and unbiased states after quantization, despite showing no change in aggregate bias scores. These flips are strongly driven by model uncertainty, where the responses with high uncertainty are 3-11x more likely to change than the confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 4-6x more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups, where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes. Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, requiring crucial post-quantization evaluation and interventions to ensure reliability in practice.
[10.02.2026 01:47] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ —Ñ–µ–Ω–æ–º–µ–Ω ¬´–∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ—Ä–æ—Ç–∞ —Å–º–µ—â–µ–Ω–∏—è¬ª, –∫–æ–≥–¥–∞ –¥–æ 21% –æ—Ç–≤–µ—Ç–æ–≤ –º–µ–Ω—è—é—Ç —Å–≤–æ—é –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –ø–æ—Å–ª–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏, —Ö–æ—Ç—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π. –≠—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –æ—Å–æ–±–µ–Ω–Ω–æ —á–∞—Å—Ç–æ –≤ –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–∞—Ö –º–æ–¥–µ–ª–∏, –≥–¥–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ 3-11 —Ä–∞–∑ –≤—ã—à–µ. –ë–æ–ª–µ–µ —Å–∏–ª—å–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (4-–±–∏—Ç –≤–º–µ—Å—Ç–æ 8-–±–∏—Ç) —É—Å—É–≥—É–±–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç, —Å–æ–∑–¥–∞–≤–∞—è –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–∞–∑–Ω—ã–µ –¥–µ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –≥—Ä—É–ø–ø—ã, –≥–¥–µ —Å–º–µ—â–µ–Ω–∏–µ –º–æ–∂–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —É–ª—É—á—à–∏—Ç—å—Å—è –¥–ª—è –æ–¥–Ω–∏—Ö –∏ —É—Ö—É–¥—à–∏—Ç—å—Å—è –¥–ª—è –¥—Ä—É–≥–∏—Ö –≥—Ä—É–ø–ø.",
  "emoji": "‚öñÔ∏è",
  "title": "–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è —Å–∫—Ä—ã–≤–∞–µ—Ç —Å–º–µ—â–µ–Ω–∏—è: –Ω–µ–≤–∏–¥–∏–º–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏–π –≤ —Å–∂–∞—Ç—ã—Ö –º–æ–¥–µ–ª—è—Ö"
}
```
[10.02.2026 01:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Post-training quantization of large language models causes significant changes in social biases that aggregate metrics fail to detect, with quantization-induced masked bias flipping occurring more frequently in uncertain responses and stronger quantization levels.  					AI-generated summary 				 Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on PostTrainingBiasBench, a unified benchmark of 13 closed- and open-ended bias datasets. We identify a phenomenon we term quantization-induced masked bias flipping, in which up to 21% of responses flip between biased and unbiased states after quantization, despite showing no change in aggregate bias scores. These flips are strongly driven by model uncertainty, where the responses with high uncertainty are 3-11x more likely to change than the confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 4-6x more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups, where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes. Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, requiring crucial post-quantization evaluation and interventions to ensure reliability in practice."

[10.02.2026 01:47] Response: ```python
["INFERENCE", "BENCHMARK"]
```
[10.02.2026 01:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Post-training quantization of large language models causes significant changes in social biases that aggregate metrics fail to detect, with quantization-induced masked bias flipping occurring more frequently in uncertain responses and stronger quantization levels.  					AI-generated summary 				 Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on PostTrainingBiasBench, a unified benchmark of 13 closed- and open-ended bias datasets. We identify a phenomenon we term quantization-induced masked bias flipping, in which up to 21% of responses flip between biased and unbiased states after quantization, despite showing no change in aggregate bias scores. These flips are strongly driven by model uncertainty, where the responses with high uncertainty are 3-11x more likely to change than the confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 4-6x more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups, where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes. Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, requiring crucial post-quantization evaluation and interventions to ensure reliability in practice."

[10.02.2026 01:47] Response: ```python
['ETHICS', 'SECURITY']
```

**Justification:**

- **ETHICS**: The paper directly addresses social biases in language models, fairness across demographic groups, and the ethical implications of quantization-induced bias flipping. It discusses how quantization creates asymmetric impacts across different demographic groups and examines bias patterns systematically.

- **SECURITY**: The paper relates to model robustness and reliability concerns. Quantization-induced masked bias flipping represents a form of model behavior degradation that affects the trustworthiness and safe deployment of quantized models, which falls under security and robustness considerations.
[10.02.2026 01:47] Error. Failed to parse JSON from LLM. ["ETHICS", "SECURITY"]


**Justification:**

- **ETHICS**: The paper directly addresses social biases in language models, fairness across demographic groups, and the ethical implications of quantization-induced bias flipping. It discusses how quantization creates asymmetric impacts across different demographic groups and examines bias patterns systematically.

- **SECURITY**: The paper relates to model robustness and reliability concerns. Quantization-induced masked bias flipping represents a form of model behavior degradation that affects the trustworthiness and safe deployment of quantized models, which falls under security and robustness considerations.
[10.02.2026 01:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates how post-training quantization of large language models affects their social biases. It reveals a phenomenon called quantization-induced masked bias flipping, where responses can switch between biased and unbiased after quantization, even if overall bias metrics remain unchanged. The study shows that uncertain responses are more likely to experience these flips, and stronger quantization levels exacerbate the issue. The findings highlight the need for careful evaluation of biases post-quantization, as the changes can disproportionately impact different demographic groups.","title":"Quantization: Unmasking Hidden Biases in Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates how post-training quantization of large language models affects their social biases. It reveals a phenomenon called quantization-induced masked bias flipping, where responses can switch between biased and unbiased after quantization, even if overall bias metrics remain unchanged. The study shows that uncertain responses are more likely to experience these flips, and stronger quantization levels exacerbate the issue. The findings highlight the need for careful evaluation of biases post-quantization, as the changes can disproportionately impact different demographic groups.', title='Quantization: Unmasking Hidden Biases in Language Models'))
[10.02.2026 01:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ËøôÁØáËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÂêéÊúüÈáèÂåñËøáÁ®ã‰∏≠Á§æ‰ºöÂÅèËßÅÁöÑÂèòÂåñ„ÄÇÈáèÂåñËôΩÁÑ∂Èôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨Ôºå‰ΩÜ‰ºöÂØºËá¥Ê®°ÂûãÁöÑÂÅèËßÅÊ®°ÂºèÂèëÁîüÊ†πÊú¨ÊÄßÊîπÂèòÔºå‰º†ÁªüÁöÑËÅöÂêàÊåáÊ†áÊó†Ê≥ïÊçïÊçâÂà∞Ëøô‰∫õÂèòÂåñ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÈáèÂåñÂºïËµ∑ÁöÑÂÅèËßÅÁøªËΩ¨Áé∞Ë±°Âú®‰∏çÁ°ÆÂÆöÁöÑÂõûÁ≠î‰∏≠Êõ¥‰∏∫È¢ëÁπÅÔºå‰∏îÈáèÂåñÂº∫Â∫¶Ë∂äÂ§ßÔºåË°å‰∏∫ÂèòÂåñË∂äÊòéÊòæ„ÄÇ‰∏çÂêå‰∫∫Áæ§ÁöÑÂÅèËßÅÂèòÂåñ‰∏çÂØπÁß∞ÔºåÊúâ‰∫õÁæ§‰ΩìÁöÑÂÅèËßÅÂèØËÉΩÂä†ÈáçÔºåËÄåÂè¶‰∏Ä‰∫õÁæ§‰ΩìÂàôÂèØËÉΩÊîπÂñÑÔºåÂõ†Ê≠§ÈúÄË¶ÅÂú®ÈáèÂåñÂêéËøõË°åËØÑ‰º∞ÂíåÂπ≤È¢ÑÔºå‰ª•Á°Æ‰øùÊ®°ÂûãÁöÑÂèØÈù†ÊÄß„ÄÇ","title":"ÈáèÂåñÂêéÂÅèËßÅÂèòÂåñÔºöÊ®°ÂûãÂèØÈù†ÊÄßÁöÑÊñ∞ÊåëÊàò"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ËøôÁØáËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÂêéÊúüÈáèÂåñËøáÁ®ã‰∏≠Á§æ‰ºöÂÅèËßÅÁöÑÂèòÂåñ„ÄÇÈáèÂåñËôΩÁÑ∂Èôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨Ôºå‰ΩÜ‰ºöÂØºËá¥Ê®°ÂûãÁöÑÂÅèËßÅÊ®°ÂºèÂèëÁîüÊ†πÊú¨ÊÄßÊîπÂèòÔºå‰º†ÁªüÁöÑËÅöÂêàÊåáÊ†áÊó†Ê≥ïÊçïÊçâÂà∞Ëøô‰∫õÂèòÂåñ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÈáèÂåñÂºïËµ∑ÁöÑÂÅèËßÅÁøªËΩ¨Áé∞Ë±°Âú®‰∏çÁ°ÆÂÆöÁöÑÂõûÁ≠î‰∏≠Êõ¥‰∏∫È¢ëÁπÅÔºå‰∏îÈáèÂåñÂº∫Â∫¶Ë∂äÂ§ßÔºåË°å‰∏∫ÂèòÂåñË∂äÊòéÊòæ„ÄÇ‰∏çÂêå‰∫∫Áæ§ÁöÑÂÅèËßÅÂèòÂåñ‰∏çÂØπÁß∞ÔºåÊúâ‰∫õÁæ§‰ΩìÁöÑÂÅèËßÅÂèØËÉΩÂä†ÈáçÔºåËÄåÂè¶‰∏Ä‰∫õÁæ§‰ΩìÂàôÂèØËÉΩÊîπÂñÑÔºåÂõ†Ê≠§ÈúÄË¶ÅÂú®ÈáèÂåñÂêéËøõË°åËØÑ‰º∞ÂíåÂπ≤È¢ÑÔºå‰ª•Á°Æ‰øùÊ®°ÂûãÁöÑÂèØÈù†ÊÄß„ÄÇ', title='ÈáèÂåñÂêéÂÅèËßÅÂèòÂåñÔºöÊ®°ÂûãÂèØÈù†ÊÄßÁöÑÊñ∞ÊåëÊàò'))
[10.02.2026 01:48] Using data from previous issue: {"categories": ["#graphs", "#open_source", "#benchmark", "#diffusion", "#dataset", "#science", "#transfer_learning", "#architecture", "#multimodal"], "emoji": "üè¢", "ru": {"title": "–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∏—Å–∫–æ–≤ –∏ –º–∞—Ä—à—Ä—É—Ç–æ–≤ —ç–≤–∞–∫—É–∞—Ü–∏–∏ –≤ –≥–æ—Ä–æ–¥–∞—Ö —Å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø
[10.02.2026 01:48] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#dataset", "#training", "#multimodal", "#rag", "#cv", "#benchmark", "#video"], "emoji": "üîç", "ru": {"title": "–í—ã—Ö–æ–¥ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π: —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å –ø–æ–∏—Å–∫–æ–º –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º", "desc": "Seg-ReSearch –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, –∫–æ—Ç
[10.02.2026 01:48] Using data from previous issue: {"categories": ["#healthcare", "#cv", "#dataset", "#open_source", "#data", "#inference", "#science"], "emoji": "üî¨", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Å–Ω–∏–º–∫–æ–≤ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Segment-Anything", "desc": "AtlasPatch ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
[10.02.2026 01:48] Using data from previous issue: {"categories": ["#transfer_learning", "#small_models", "#training"], "emoji": "üßπ", "ru": {"title": "–û—á–∏—Å—Ç–∫–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∏–∑ –º–Ω–æ–≥–∏—Ö —É—á–∏—Ç–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –æ—á–∏—Å—Ç–∫–∏ –∑–Ω–∞–Ω–∏–π –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–µ–∂–¥—É –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –±–æ–ª–µ–µ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–º–∏ –º–æ–¥–µ
[10.02.2026 01:48] Using data from previous issue: {"categories": ["#optimization"], "emoji": "‚ö°", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∂–∏–º–æ–≤ –≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–µ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤—ã—è–≤–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –ø—Ä–µ–∂–¥–µ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–ª–∞–ø—Å–∞ –º–æ–¥ –≤ –º–æ–¥–µ–ª—è—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–ø—Ç–∏
[10.02.2026 01:48] Using data from previous issue: {"categories": ["#training", "#interpretability", "#diffusion", "#architecture"], "emoji": "üß†", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏—è—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, —Å–æ–∑–¥–∞–≤–∞—è
[10.02.2026 01:48] Using data from previous issue: {"categories": ["#plp", "#benchmark", "#training", "#agents", "#rlhf", "#optimization", "#rl", "#open_source", "#reasoning", "#dataset"], "emoji": "üß†", "ru": {"title": "–ö–∞–∫ –∑–∞—Å—Ç–∞–≤–∏—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —É—á–∏—Ç—å—Å—è –Ω–∞ –≤—Å—é –∂–∏–∑–Ω—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SE-Bench ‚Äî –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫—É—é —Å—Ä–µ–¥—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏
[10.02.2026 01:48] Renaming data file.
[10.02.2026 01:48] Renaming previous data. hf_papers.json to ./d/2026-02-10.json
[10.02.2026 01:48] Saving new data file.
[10.02.2026 01:48] Generating page.
[10.02.2026 01:48] Renaming previous page.
[10.02.2026 01:48] Renaming previous data. index.html to ./d/2026-02-10.html
[10.02.2026 01:48] Writing result.
[10.02.2026 01:48] Renaming log file.
[10.02.2026 01:48] Renaming previous data. log.txt to ./logs/2026-02-10_last_log.txt
