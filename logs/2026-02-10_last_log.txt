[10.02.2026 17:08] Read previous papers.
[10.02.2026 17:08] Generating top page (month).
[10.02.2026 17:08] Writing top page (month).
[10.02.2026 18:57] Read previous papers.
[10.02.2026 18:57] Get feed.
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07085
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08794
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07026
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08222
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06855
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07845
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08676
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06422
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09007
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08439
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06025
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07962
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08543
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09022
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08990
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07055
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07075
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03784
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08658
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06540
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06454
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08808
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08236
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07775
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06694
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00169
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07796
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08145
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21363
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08961
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06445
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09003
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08829
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07803
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06942
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06600
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08818
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07970
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07491
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07150
[10.02.2026 18:57] Extract page data from URL. URL: https://huggingface.co/papers/2602.07120
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07090
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07080
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05929
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05708
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07054
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07040
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08629
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08004
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07948
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05946
[10.02.2026 18:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02285
[10.02.2026 18:57] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.02.2026 18:57] No deleted papers detected.
[10.02.2026 18:57] Downloading and parsing papers (pdf, html). Total: 52.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07085.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07085.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07085.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.08794.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.08794.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.08794.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07026.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07026.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07026.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.08222.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.08222.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.08222.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.06855.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.06855.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.06855.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07845.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07845.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07845.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.08676.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.08676.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.08676.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.06422.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.06422.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.06422.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.09007.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.09007.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.09007.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.08439.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.08439.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.08439.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.06025.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.06025.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.06025.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07962.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07962.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07962.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.08543.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.08543.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.08543.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.09022.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.09022.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.09022.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.08990.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.08990.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.08990.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07055.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07055.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07055.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07075.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07075.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07075.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.03784.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.03784.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.03784.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.08658.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.08658.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.08658.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.06540.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.06540.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.06540.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.06454.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.06454.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.06454.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.08808.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.08808.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.08808.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.08236.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.08236.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.08236.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07775.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07775.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07775.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.06694.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.06694.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.06694.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.00169.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.00169.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.00169.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07796.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07796.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07796.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.08145.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.08145.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.08145.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2601.21363.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2601.21363.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2601.21363.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.08961.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.08961.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.08961.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.06445.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.06445.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.06445.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.09003.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.09003.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.09003.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.08829.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.08829.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.08829.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07803.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07803.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07803.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.06942.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.06942.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.06942.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.06600.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.06600.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.06600.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.08818.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.08818.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.08818.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07970.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07970.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07970.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07491.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07491.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07491.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07150.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07150.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07150.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07120.
[10.02.2026 18:57] Downloading paper 2602.07120 from https://arxiv.org/pdf/2602.07120v1...
[10.02.2026 18:57] Extracting affiliations from text.
[10.02.2026 18:57] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Jacqueline He 1 Jonathan Hayase 1 Wen-tau Yih 1 Sewoong Oh 1 Luke Zettlemoyer 1 Pang Wei Koh 1 2 6 2 0 2 6 ] . [ 1 0 2 1 7 0 . 2 0 6 2 : r Abstract Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose ANCHORED DECODING, plugand-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to permissively trained safe LM. ANCHORED DECODING adaptively allocates user-chosen information budget over the generation trajectory and enforces per-step constraints that yield sequencelevel guarantee, enabling tunable riskutility trade-off. To make ANCHORED DECODING practically useful, we introduce new permissively trained safe model (TinyComma 1.8B), as well as ANCHOREDByte DECODING, bytelevel variant of our method that enables crossvocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. ANCHORED and ANCHOREDByte DECODING define new Pareto frontier, preserving nearoriginal fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and safe reference, at modest inference overhead. 1. Introduction The remarkable capabilities of modern language models (LMs) are fundamentally tied to the scale and diversity of 1University of Washington 2Allen Institute for ArtiJacqueline He Correspondence to: Intelligence. ficial <jyyh@cs.washington.edu>. Preprint. 1 their pre-training data. These corpora are often harvested from the open web with minimal filtering, and may contain sen"
[10.02.2026 18:57] Response: ```python
[
    "University of Washington",
    "Allen Institute for Artificial Intelligence"
]
```
[10.02.2026 18:57] Deleting PDF ./assets/pdf/2602.07120.pdf.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07090.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07090.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07090.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07080.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07080.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07080.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.05929.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.05929.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.05929.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.05708.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.05708.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.05708.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07054.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07054.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07054.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07040.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07040.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07040.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.08629.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.08629.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.08629.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.08004.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.08004.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.08004.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.07948.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.07948.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.07948.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.05946.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.05946.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.05946.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Downloading and parsing paper https://huggingface.co/papers/2602.02285.
[10.02.2026 18:57] Extra JSON file exists (./assets/json/2602.02285.json), skip PDF parsing.
[10.02.2026 18:57] Paper image links file exists (./assets/img_data/2602.02285.json), skip HTML parsing.
[10.02.2026 18:57] Success.
[10.02.2026 18:57] Enriching papers with extra data.
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 0. Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated exper...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 1. MOVA is an open-source model that generates synchronized audio-visual content using a Mixture-of-Experts architecture with 32 billion parameters, supporting image-text to video-audio generation tasks.  					AI-generated summary 				 Audio is indispensable for real-world video, yet generation models ...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 2. Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data.  					AI-generated summary 				 Despite the success of multimodal contrastive learning in aligni...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 3. WMSS is a post-training paradigm that uses weak model checkpoints to identify and fill learning gaps, enabling continued improvement beyond conventional saturation points in large language models.  					AI-generated summary 				 As post-training optimization becomes central to improving large langua...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 4. AIRS-Bench presents a comprehensive benchmark suite for evaluating LLM agents across diverse scientific domains, demonstrating current limitations while providing open-source resources for advancing autonomous scientific research.  					AI-generated summary 				 LLM agents hold significant promise f...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 5. RD-VLA introduces a recurrent architecture for vision-language-action models that adapts computational depth through latent iterative refinement, achieving constant memory usage and improved task success rates.  					AI-generated summary 				 Current Vision-Language-Action (VLA) models rely on fixed...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 6. LLaDA2.1 introduces a novel token-to-token editing approach with speed and quality modes, enhanced through reinforcement learning for improved reasoning and instruction following in large language diffusion models.  					AI-generated summary 				 While LLaDA2.0 showcased the scaling potential of 100...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 7. TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories.  					AI-generated summary 				 Deploying GRPO on Flow Matching models has proven effective for text-to-image...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 8. A new benchmark and evaluation metric are introduced for assessing temporal coherence and dynamic interaction in GUI generation models, revealing significant challenges in maintaining consistency over extended interaction sequences.  					AI-generated summary 				 Recent advancements in image genera...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 9. Researchers introduce a new video understanding task and benchmark that evaluates models' ability to learn from few-shot demonstrations, along with a specialized MLLM architecture trained using a two-stage approach combining video supervision and preference optimization.  					AI-generated summary 	...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 10. BudgetMem is a runtime memory framework for LLM agents that uses modular components with three budget tiers and a neural policy router to optimize performance-cost trade-offs in memory usage.  					AI-generated summary 				 Memory is increasingly central to Large Language Model (LLM) agents operatin...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 11. LOCA-bench is introduced as a benchmark for evaluating language agents in long-context, agentic scenarios with controlled environment state management.  					AI-generated summary 				 Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as th...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 12. A new benchmark called GISA is introduced for evaluating information-seeking assistants, featuring human-crafted queries with structured answer formats and live updates to prevent memorization.  					AI-generated summary 				 The advancement of large language models (LLMs) has significantly accelera...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 13. WorldCompass enhances long-horizon video-based world models through reinforcement learning post-training with clip-level rollouts, complementary rewards, and efficient RL algorithms.  					AI-generated summary 				 This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training fr...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 14. InternAgent-1.5 is a unified system for autonomous scientific discovery that integrates computational modeling and experimental research through coordinated subsystems for generation, verification, and evolution.  					AI-generated summary 				 We introduce InternAgent-1.5, a unified system designed...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 15. Current multimodal foundation models show limitations in maintaining coherent spatial beliefs during active exploration, exhibiting gaps between active and passive performance, inefficient exploration strategies, and difficulties in updating outdated spatial knowledge.  					AI-generated summary 			...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 16. LatentChem enables chemical reasoning through continuous latent space computations instead of discrete textual tokens, achieving superior performance and efficiency compared to traditional chain-of-thought approaches.  					AI-generated summary 				 Chemical large language models (LLMs) predominantl...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 17. ComprExIT introduces a novel approach to long-context inference in LLMs by using explicit information transmission over frozen hidden states, improving compression efficiency through depth-wise and width-wise transmission mechanisms.  					AI-generated summary 				 Long-context inference with Large ...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 18. Research investigates how fundamental reasoning paradigms influence large language model generalization through targeted training approaches and evaluation on real-world tasks.  					AI-generated summary 				 Deduction, induction, and abduction are fundamental reasoning paradigms, core for human log...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 19. AgentCPM-Report presents a lightweight local solution for deep research report generation using a Writing As Reasoning Policy framework and multi-stage agentic training to enhance small models' reasoning and outline evolution capabilities.  					AI-generated summary 				 Generating deep research rep...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 20. RelayGen is a training-free framework that dynamically switches between large and small models during reasoning by identifying difficulty transitions at the segment level, achieving faster inference with minimal accuracy loss.  					AI-generated summary 				 Large reasoning models (LRMs) achieve str...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 21. A scalable framework for evaluating and improving goal-conditioned procedure generation using large-scale web mining, automated scoring, and reinforcement learning to enhance step-by-step instruction quality.  					AI-generated summary 				 Generating step-by-step "how-to" procedures is a key LLM ca...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 22. Adaptive test-time framework with world models enables selective visual imagination for spatial reasoning, improving efficiency and reliability by determining when imagination is necessary.  					AI-generated summary 				 Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spa...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 23. Autoregressive video diffusion models suffer from train-test gaps when generating long videos, but a training-free approach called Rolling Sink addresses this by maintaining AR cache and enabling ultra-long video synthesis.  					AI-generated summary 				 Recently, autoregressive (AR) video diffusio...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 24. NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment.  					AI-generated summar...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 25. AI-driven materials science integrates large language models across discovery pipelines from data curation to agent-based experimentation, emphasizing system-level optimization and autonomous goal pursuit.  					AI-generated summary 				 The convergence of artificial intelligence and materials scien...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 26. Explicit reasoning in LLM agents can degrade performance in user-engaged scenarios by reducing information disclosure and weakening agent-user communication, with transparency-aware prompting showing better results.  					AI-generated summary 				 Eliciting reasoning has emerged as a powerful techni...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 27. Foundation models including LLMs, MLLMs, and generative models require reliable and responsible development addressing bias, security, explainability, and other critical issues for trustworthy deployment across multiple domains.  					AI-generated summary 				 Foundation models, including Large Lang...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 28. Off-policy Soft Actor-Critic with large-batch updates enables efficient humanoid locomotion policy pretraining, while model-based methods facilitate safe adaptation through deterministic data collection and stochastic exploration within physics-informed world models.  					AI-generated summary 				 ...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 29. MotionCrafter is a video diffusion framework that jointly reconstructs 4D geometry and estimates dense motion using a novel joint representation and 4D VAE architecture.  					AI-generated summary 				 We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometr...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 30. Energy-constrained optimization framework separates energy metrics from rewards using Lagrangian method to achieve stable, energy-efficient humanoid robot locomotion with reduced hyperparameter tuning.  					AI-generated summary 				 Achieving stable and energy-efficient locomotion is essential for ...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 31. Large language models are increasingly guiding data management processes through a tiered framework that optimizes data quality, cost, and training efficiency across different stages of model development.  					AI-generated summary 				 The development of artificial intelligence can be viewed as an ...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 32. WildReward demonstrates that reward models can be effectively trained from in-the-wild user interactions using ordinal regression, achieving performance comparable to traditional methods while benefiting from user diversity.  					AI-generated summary 				 Reward models (RMs) are crucial for the tra...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 33. A high-quality open-source singing voice synthesis system is presented with support for multiple languages and controllable generation, along with a dedicated benchmark for evaluating zero-shot performance.  					AI-generated summary 				 While recent years have witnessed rapid progress in speech sy...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 34. A comprehensive study of Turkish subword tokenization systematically investigates the relationship between vocabulary size, training corpus, and tokenizer performance across multiple linguistic tasks and diagnostics.  					AI-generated summary 				 Tokenization is a pivotal design choice for neural ...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 35. Large reasoning models exhibit spontaneous question repetition patterns that can be formalized and leveraged to improve computational efficiency and accuracy through echo-aware training and prompting techniques.  					AI-generated summary 				 Test-time compute allocation in large reasoning models (...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 36. FlexMoRE demonstrates that low-rank adapters can replace full-sized experts in mixture-of-experts architectures, achieving better performance with significantly fewer parameters.  					AI-generated summary 				 Recent advances in mixture-of-experts architectures have shown that individual experts mo...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 37. Research explores PDE solvers including neural frameworks for scientific simulations, examining forward solutions, inverse problems, and equation discovery across multi-variable and non-linear systems.  					AI-generated summary 				 Partial Differential Equations are precise in modelling the physic...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 38. A multi-agent framework guided by knowledge graphs addresses materials science challenges by integrating specialized agents for problem decomposition, evidence retrieval, and graph traversal to discover sustainable PFAS alternatives.  					AI-generated summary 				 Large Language Models (LLMs) promi...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 39. Analysis of agentic system evaluation reveals significant variance in single-run performance estimates, necessitating multiple runs and advanced metrics for reliable assessment.  					AI-generated summary 				 Agentic systems are evaluated on benchmarks where agents interact with environments to sol...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 40. Anchor decoding suppresses verbatim copying in language models while maintaining fluency and factual accuracy through constrained generation that balances risk and utility.  					AI-generated summary 				 Modern language models (LMs) tend to memorize portions of their training data and emit verbatim...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 41. SPARSE is a user-centric framework that protects text embeddings from privacy leaks by selectively perturbing sensitive dimensions using differentiable masking and Mahalanobis noise calibration.  					AI-generated summary 				 Text embeddings enable numerous NLP applications but face severe privacy ...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 42. LLM code verification can be achieved through internal neural dynamics analysis, identifying structural signatures that distinguish correct reasoning from logical failures in computational circuits.  					AI-generated summary 				 Current paradigms for code verification rely heavily on external mech...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 43. KV-CoRE method evaluates kv-cache compressibility through SVD-based low-rank approximation, revealing patterns linking compressibility to model architecture and training data across multiple languages and domains.  					AI-generated summary 				 Large language models rely on kv-caches to avoid redun...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 44. CE-RAG4EM reduces computational overhead in large-scale entity matching by implementing blocking-based batch retrieval and generation while maintaining competitive matching quality.  					AI-generated summary 				 Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tas...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 45. A benchmark and optimization technique are presented to improve multimodal large language models' emotion understanding by addressing spurious associations and hallucinations in audiovisual cues.  					AI-generated summary 				 Emotion understanding is essential for building socially intelligent age...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 46. Aster is an AI agent that accelerates scientific discovery by iteratively improving programs, achieving state-of-the-art results across multiple domains including mathematics, biology, and machine learning with significantly reduced computational requirements.  					AI-generated summary 				 We intr...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 47. CauScale is a neural architecture that enables efficient causal discovery on large graphs through compressed embeddings and tied attention weights, achieving high accuracy and significant speedups over previous methods.  					AI-generated summary 				 Causal discovery is essential for advancing data...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 48. Agent skills extend large language model (LLM) agents with reusable, program-like modules that define triggering conditions, procedural logic, and tool interactions. As these skills proliferate in public marketplaces, it is unclear what types are available, how users adopt them, and what risks they ...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 49. Collective motion in fish schools exemplifies emergent self-organization in active matter systems, yet computational tools for simulating and analyzing these dynamics remain fragmented across research groups. We present dewi-kadita, an open-source Python library implementing the three-dimensional Co...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 50. Preference alignment objectives are extended to general alignment settings using f-divergence variational representations, introducing novel on-policy and hybrid policy optimization methods for LLM alignment with theoretical and empirical validation.  					AI-generated summary 				 Recent research s...
[10.02.2026 18:57] ********************************************************************************
[10.02.2026 18:57] Abstract 51. A comprehensive formalization of statistical learning theory in Lean 4 addresses gaps in mathematical libraries and demonstrates human-AI collaboration for verified machine learning theory foundations.  					AI-generated summary 				 We present the first comprehensive Lean 4 formalization of statist...
[10.02.2026 18:57] Read previous papers.
[10.02.2026 18:57] Generating reviews via LLM API.
[10.02.2026 18:57] Using data from previous issue: {"categories": [], "emoji": "üíπ", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π", "desc": "QuantaAlpha –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∞–ª—å—Ñ–∞-—Ñ–∞–∫—Ç–æ—Ä–æ–≤ –Ω–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä—ã–Ω–∫–∞—Ö, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –∫–∞–∂–¥—ã–π —Ü–∏–∫–ª –º–∞–π–Ω–∏–Ω–≥–∞ –∫–∞–∫ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#inference", "#video", "#audio", "#dataset", "#architecture", "#open_source", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–°–æ–≤–º–µ—Å—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ —Å –æ–¥–Ω–æ–π –º–æ–¥–µ–ª—å—é", "desc": "MOVA ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Mixture-of-Experts, —Å–æ–¥–µ—Ä–∂–∞—â
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#training", "#multimodal", "#architecture"], "emoji": "üîó", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–µ—à–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏, –ø—Ä–µ–¥–ª–æ–∂
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "üìà", "ru": {"title": "–°–ª–∞–±—ã–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –∫–∞–∫ –ø—É—Ç—å –∫ —Å–∏–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è WMSS ‚Äî –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞—Å—ã—â–µ–Ω–∏—è, –≤–æ–∑–Ω–∏–∫–∞—é—â—É—é –ø—Ä–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º –æ
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#agents", "#open_source", "#survey", "#science", "#benchmark", "#dataset"], "emoji": "üß™", "ru": {"title": "–ú–µ—Ä–∏–ª–æ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç AIRS-Bench ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM –∞–≥–µ–Ω—Ç–æ–≤ –≤ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö. –ë–µ–Ω—á
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#inference", "#training", "#robotics", "#architecture", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–ª—É–±–∏–Ω—ã: —Å–∫—Ä—ã—Ç–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –¥–ª—è robotics", "desc": "RD-VLA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –º–æ–¥–µ–ª–µ–π vision-language-action, –∫–æ—Ç–æ—Ä
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization", "#alignment", "#benchmark", "#architecture", "#open_source", "#reasoning", "#diffusion", "#plp"], "emoji": "‚ö°", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ T2T —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º",
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#optimization"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω—ã–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "TP-GRPO —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤ –º–æ–¥–µ–ª—è—Ö flow matching –ø—É—Ç—ë–º –≤–≤–µ–¥–µ–Ω–∏—è –ø–æ—à–∞–≥–æ–≤—ã—Ö –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤–º–µ—Å—Ç–æ –∏—Ç–æ–≥–æ–≤–æ–≥
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#dataset"], "emoji": "üñ•Ô∏è", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ GEBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 700 —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#training", "#video", "#benchmark", "#dataset", "#multimodal", "#rlhf"], "emoji": "üé•", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ–ø–æ–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –Ω–µ–º–Ω–æ–≥–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ –∏ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#optimization", "#rl", "#agents", "#long_context", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø–∞–º—è—Ç–∏ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "BudgetMem ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è,
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#agents", "#long_context", "#open_source", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤ —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–∞—Ö —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è", "desc": "–í–≤–µ–¥–µ–Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫ LOCA-bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ —É–ø
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#dataset", "#survey", "#agents"], "emoji": "üîç", "ru": {"title": "GISA: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –∏ –∂–∏–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ GISA –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-–ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 37
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#training", "#video", "#rl"], "emoji": "üß≠", "ru": {"title": "–ù–∞–ø—Ä–∞–≤–ª—è–µ–º –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–∏ –º–∏—Ä–∞: –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "desc": "WorldCompass –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã –≤–≤
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#science", "#reasoning", "#benchmark", "#open_source", "#agents"], "emoji": "üß¨", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è –Ω–∞—É—á–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–∏–π —á–µ—Ä–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—é –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤", "desc": "InternAgent-1.5 ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#multimodal", "#robotics"], "emoji": "üó∫Ô∏è", "ru": {"title": "–ê–≥–µ–Ω—Ç—ã –Ω–µ –ø–æ–Ω–∏–º–∞—é—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ: –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å–ª–∞–±–æ—Å—Ç–µ–π –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ foundation models", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ foundation models –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è—é
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#science", "#reasoning"], "emoji": "‚öóÔ∏è", "ru": {"title": "–•–∏–º–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –≤–º–µ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ü–µ–ø–æ—á–µ–∫ –º—ã—Å–ª–∏", "desc": "LatentChem –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ö–∏–º–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –≤–º–µ—Å—Ç–æ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö —Ç
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#optimization", "#long_context", "#inference", "#benchmark"], "emoji": "üì¶", "ru": {"title": "–Ø–≤–Ω–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞", "desc": "ComprExIT –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∂–∞—Ç–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —è–≤–Ω—É—é –ø–µ—Ä–µ–¥–∞—á—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#benchmark", "#training", "#reasoning", "#synthetic", "#dataset"], "emoji": "üß†", "ru": {"title": "–¢—Ä–∏ —Ç–∏–ø–∞ –ª–æ–≥–∏–∫–∏ ‚Äî –æ–¥–∏–Ω –∫–ª—é—á –∫ –æ–±–æ–±—â–µ–Ω–∏—é LLM", "desc": "–í —ç—Ç–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∞–≤—Ç–æ—Ä—ã –∏–∑—É—á–∞—é—Ç, –∫–∞–∫ —Ç—Ä–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–∞ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è ‚Äî –¥–µ–¥—É–∫—Ü–∏—è, –∏–Ω–¥—É–∫—Ü–∏—è –∏ –∞–±–¥—É–∫—Ü–∏—è ‚Äî –≤–ª–∏—è—é—Ç –Ω–∞ 
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#training", "#small_models", "#open_source", "#rl", "#agents", "#reasoning", "#benchmark"], "emoji": "üìù", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è –ø–∏—Å—å–º–∞: –º–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "AgentCPM-Report –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ª—ë–≥–∫–æ–µ –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#inference", "#training", "#small_models", "#optimization", "#reasoning"], "emoji": "‚ö°", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "RelayGen ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization", "#synthetic", "#dataset", "#benchmark", "#open_source", "#reasoning"], "emoji": "üìã", "ru": {"title": "–ó–∞–∫—Ä—ã—Ç—ã–π —Ü–∏–∫–ª –æ—Ü–µ–Ω–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ How2Everything - –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#cv", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–ò–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ: –∫–æ–≥–¥–∞ –≤–∏–¥–µ—Ç—å –Ω—É–∂–Ω–æ, –∞ –∫–æ–≥–¥–∞ –Ω–µ—Ç", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –æ—à–∏–±–∞—é—Ç—Å—è –ø—Ä–∏ –Ω
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#inference", "#training", "#video", "#long_context", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–ë–µ—Å–ø—Ä–æ–±–ª–µ–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∞–≤—Ç—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π –¥–æ —Å–≤–µ—Ä—Ö–¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –æ–±—É—á–µ–Ω–∏–µ–º –∏
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#optimization"], "emoji": "‚öôÔ∏è", "ru": {"title": "–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π: –ø—É—Ç—å –∫ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é –Ω–∞ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö", "desc": "NanoQuant ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ—Å—Ç–æ–±—É—á–∞—é—â–µ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Å–∂–∏–º–∞–µ—Ç –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–æ –±–∏–Ω–∞—Ä–Ω–æ–≥–æ (1-–±–∏—Ç) –∏ —Å—É–±–±–∏–Ω–∞—Ä–Ω–æ–≥–æ —É
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#benchmark", "#training", "#survey", "#science", "#optimization", "#agents", "#data"], "emoji": "üß™", "ru": {"title": "–û—Ç –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–º –∞–≥–µ–Ω—Ç–∞–º: LLM –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ—Ç–∫—Ä—ã—Ç–∏—è –Ω–æ–≤—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (L
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#alignment", "#agents", "#reasoning", "#benchmark", "#open_source", "#training"], "emoji": "ü§ê", "ru": {"title": "–ú–æ–ª—á–∞–Ω–∏–µ –∑–æ–ª–æ—Ç–æ? –ü–æ—á–µ–º—É —è–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –æ—Å–ª–∞–±–ª—è–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∞–≥–µ–Ω—Ç–∞ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ —è–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ LLM –∞–≥–µ–Ω—Ç–∞—Ö, –≤–∑–∞–∏–º–æ
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#security", "#alignment", "#ethics", "#hallucinations", "#survey", "#interpretability"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ü—É—Ç—å –∫ –Ω–∞–¥–µ–∂–Ω—ã–º –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–º —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–í —ç—Ç–æ–º –æ–±–∑–æ—Ä–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã –Ω–∞–¥–µ–∂–Ω–æ–π –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#training", "#robotics", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≥—É–º–∞–Ω–æ–∏–¥–æ–≤ —á–µ—Ä–µ–∑ off-policy –æ–±—É—á–µ–Ω–∏–µ —Å —É—Å–∏–ª–µ–Ω–∏–µ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º Soft Actor-Critic —Å –±–æ–ª—å—à–∏–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞ –∏ –≤—ã—Å–æ–∫–∏–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#diffusion", "#video", "#architecture", "#3d", "#training"], "emoji": "üé¨", "ru": {"title": "–°–æ–≤–º–µ—Å—Ç–Ω–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –∏ –¥–≤–∏–∂–µ–Ω–∏—è –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ 4D –¥–∏—Ñ—Ñ—É–∑–∏—é", "desc": "MotionCrafter ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ—Ç 4D –≥–µ–æ–º–µ—Ç—Ä–∏—é
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —ç–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ —è–≤–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —à—Ç—Ä–∞—Ñ–æ–≤ –≤ –Ω–∞–≥—Ä–∞–¥–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ ECO (Energy-Constrained Optimization) ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–¥–µ–ª—è–µ—Ç —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#training", "#open_source", "#alignment", "#agi", "#optimization", "#data"], "emoji": "üìä", "ru": {"title": "–î–∞–Ω–Ω—ã–µ –∏ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Ç—É—Ç –≤–º–µ—Å—Ç–µ: –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è AGI", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π —ç–≤–æ–ª—é—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –∏ –¥–∞–Ω–Ω—ã—Ö, –≥–¥–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∞–∫—Ç–∏–≤–Ω–æ
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#dataset", "#training", "#rlhf", "#data"], "emoji": "üèÜ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω WildReward ‚Äî –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å —è–∑—ã–∫–æ
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#low_resource", "#audio", "#dataset", "#benchmark", "#open_source", "#multilingual"], "emoji": "üé§", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∏–Ω—Ç–µ–∑–∞ –ø–µ–≤—á–µ—Å–∫–æ–≥–æ –≥–æ–ª–æ—Å–∞ —Å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —è–∑—ã–∫–æ–≤ –∏ –Ω–∞–¥—ë–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ SoulX-Singer ‚Äî –≤—ã—Å
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#open_source", "#interpretability", "#multilingual", "#low_resource", "#data", "#benchmark"], "emoji": "üî§", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑—É—á–µ–Ω–∏–µ —Å—É–±—Å–ª–æ–≤–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –±–æ–≥–∞—Ç—ã—Ö —è–∑—ã–∫–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—É–±—Å–ª–æ–≤–Ω–æ–π —Ç–æ–∫–µ–Ω–∏
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#math", "#training", "#reasoning", "#open_source", "#inference", "#interpretability", "#optimization"], "emoji": "üîÑ", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –≤ —è–∫–æ—Ä—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —è–≤–ª–µ–Ω–∏–µ —Å–ø–æ–Ω—Ç–∞–Ω–Ω–æ–≥–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–∞ –≤ –±–æ–ª—å
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#reasoning"], "emoji": "üéõÔ∏è", "ru": {"title": "–ù–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—ã–µ –∞–¥–∞–ø—Ç–µ—Ä—ã –≤–º–µ—Å—Ç–æ –ø–æ–ª–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ —Å–º–µ—à–∞–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ FlexMoRE, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—ã–µ –∞–¥–∞–ø—Ç–µ—Ä—ã –≤–º–µ—Å—Ç–æ –ø–æ–ª–Ω–æ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤
[10.02.2026 18:57] Using data from previous issue: {"categories": [], "emoji": "üßÆ", "ru": {"title": "–ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ —Ä–µ—à–∞—Ç–µ–ª–∏ —É—Ä–∞–≤–Ω–µ–Ω–∏–π –≤ —á–∞—Å—Ç–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ä–µ—à–µ–Ω–∏—è —É—Ä–∞–≤–Ω–µ–Ω–∏–π –≤ —á–∞—Å—Ç–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö (–£–ß–ü) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö —Å–∏–º—É–ª—è—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#hallucinations", "#science", "#reasoning"], "emoji": "üî¨", "ru": {"title": "–ú–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ –≥—Ä–∞—Ñ–∞—Ö –∑–Ω–∞–Ω–∏–π –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, —É–ø—Ä–∞–≤–ª—è–µ–º–∞—è –≥—Ä–∞—Ñ–∞–º–∏ –∑–Ω–∞–Ω–∏–π, –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤–µ–¥–µ–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç 
[10.02.2026 18:57] Using data from previous issue: {"categories": ["#agents", "#benchmark"], "emoji": "üé≤", "ru": {"title": "–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–≥–æ–Ω—ã –≤–º–µ—Å—Ç–æ –æ–¥–Ω–æ–≥–æ: –ø—É—Ç—å –∫ –Ω–∞–¥—ë–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–µ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, –∫–æ—Ç–æ—Ä—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á. –û–Ω–∏ –æ–±–Ω–∞—Ä
[10.02.2026 18:57] Querying the API.
[10.02.2026 18:57] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Anchor decoding suppresses verbatim copying in language models while maintaining fluency and factual accuracy through constrained generation that balances risk and utility.  					AI-generated summary 				 Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored_{Byte} Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored_{Byte} Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead.
[10.02.2026 18:58] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Anchored Decoding, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–¥–∞–≤–ª—è–µ—Ç –¥–æ—Å–ª–æ–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ú–µ—Ç–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —ç—Ç–∞–ø–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é, —É–¥–µ—Ä–∂–∏–≤–∞—è –µ—ë –±–ª–∏–∑–∫–æ –∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω–æ–π –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑—Ä–µ—à–∞—é—â–µ–π –ª–∏—Ü–µ–Ω–∑–∏–µ–π. –ê–ª–≥–æ—Ä–∏—Ç–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –±—é–¥–∂–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –≤—Å–µ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Å–Ω–∏–∂–µ–Ω–∏–µ–º —Ä–∏—Å–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ –º–µ—Ç–æ–¥ –º–æ–∂–µ—Ç —É—Å—Ç—Ä–∞–Ω–∏—Ç—å –¥–æ 75% –ø—Ä–æ–±–ª–µ–º—ã –¥–æ—Å–ª–æ–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∏ —Ñ–∞–∫—É–ª—å—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.",
  "emoji": "üîí",
  "title": "–Ø–∫–æ—Ä–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ: –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –±–µ–∑ –Ω–∞—Ä—É—à–µ–Ω–∏—è –∞–≤—Ç–æ—Ä—Å–∫–∏—Ö –ø—Ä–∞–≤"
}
```
[10.02.2026 18:58] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Anchor decoding suppresses verbatim copying in language models while maintaining fluency and factual accuracy through constrained generation that balances risk and utility.  					AI-generated summary 				 Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored_{Byte} Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored_{Byte} Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead."

[10.02.2026 18:58] Response: ```python
["INFERENCE", "TRAINING", "SMALL_MODELS"]
```

**Justification:**

1. **INFERENCE**: The paper is explicitly about "Anchored Decoding, a plug-and-play inference-time method" that optimizes model deployment and generation behavior at inference time.

2. **TRAINING**: The paper discusses training considerations, including "permissively trained safe LM" and introduces "a new permissively trained safe model (TinyComma 1.8B)", indicating focus on training methodologies.

3. **SMALL_MODELS**: The paper introduces TinyComma 1.8B, which at 1.8 billion parameters falls within the small models category (below or around 1-2B parameters).
[10.02.2026 18:58] Error. Failed to parse JSON from LLM. ["INFERENCE", "TRAINING", "SMALL_MODELS"]


**Justification:**

1. **INFERENCE**: The paper is explicitly about "Anchored Decoding, a plug-and-play inference-time method" that optimizes model deployment and generation behavior at inference time.

2. **TRAINING**: The paper discusses training considerations, including "permissively trained safe LM" and introduces "a new permissively trained safe model (TinyComma 1.8B)", indicating focus on training methodologies.

3. **SMALL_MODELS**: The paper introduces TinyComma 1.8B, which at 1.8 billion parameters falls within the small models category (below or around 1-2B parameters).
[10.02.2026 18:58] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Anchor decoding suppresses verbatim copying in language models while maintaining fluency and factual accuracy through constrained generation that balances risk and utility.  					AI-generated summary 				 Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored_{Byte} Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored_{Byte} Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead."

[10.02.2026 18:58] Response: ```python
["SECURITY", "LEAKAGE", "OPEN_SOURCE"]
```
[10.02.2026 18:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Anchored Decoding, a method designed to reduce verbatim copying in language models while ensuring fluency and factual accuracy. It works by constraining the generation process to stay close to a safely trained model, allowing for a balance between risk and utility. The method allows users to set an information budget, which helps manage how much sensitive content can be generated. The authors also present a new safe model, TinyComma 1.8B, and a byte-level variant, Anchored_{Byte} Decoding, which together significantly decrease the risk of copyright infringement while maintaining high-quality output.","title":"Anchored Decoding: Balancing Safety and Fluency in Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Anchored Decoding, a method designed to reduce verbatim copying in language models while ensuring fluency and factual accuracy. It works by constraining the generation process to stay close to a safely trained model, allowing for a balance between risk and utility. The method allows users to set an information budget, which helps manage how much sensitive content can be generated. The authors also present a new safe model, TinyComma 1.8B, and a byte-level variant, Anchored_{Byte} Decoding, which together significantly decrease the risk of copyright infringement while maintaining high-quality output.', title='Anchored Decoding: Balancing Safety and Fluency in Language Models'))
[10.02.2026 18:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÈîöÂÆöËß£Á†ÅÔºàAnchored DecodingÔºâÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÊäëÂà∂ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÈÄêÂ≠óÂ§çÂà∂ÔºåÂêåÊó∂‰øùÊåÅÁîüÊàêÊñáÊú¨ÁöÑÊµÅÁïÖÊÄßÂíå‰∫ãÂÆûÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂú®ÁîüÊàêËøáÁ®ã‰∏≠‰øùÊåÅ‰∏éÂÆâÂÖ®Ê®°ÂûãÁöÑÊé•ËøëÔºåÊù•Âπ≥Ë°°È£éÈô©ÂíåÊïàÁî®ÔºåÈÄÇÁî®‰∫éÊ∑∑ÂêàËÆ∏ÂèØÊï∞ÊçÆËÆ≠ÁªÉÁöÑËØ≠Ë®ÄÊ®°Âûã„ÄÇÈîöÂÆöËß£Á†ÅÂÖÅËÆ∏Áî®Êà∑ÈÄâÊã©‰ø°ÊÅØÈ¢ÑÁÆóÔºåÂπ∂Âú®ÁîüÊàêËøáÁ®ã‰∏≠ÊñΩÂä†ÈÄêÊ≠•Á∫¶ÊùüÔºå‰ªéËÄåÂÆûÁé∞ÂèØË∞ÉÁöÑÈ£éÈô©-ÊïàÁî®ÊùÉË°°„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂÆâÂÖ®Ê®°ÂûãÔºàTinyComma 1.8BÔºâÂíåÂ≠óËäÇÁ∫ßÂèò‰ΩìÔºàAnchored_{Byte} DecodingÔºâÔºåÂπ∂Âú®Â§ö‰∏™Ê®°ÂûãÂØπ‰∏äËØÑ‰º∞‰∫ÜÂÖ∂Âú®ÁâàÊùÉÈ£éÈô©ÂíåÊïàÁî®ÊñπÈù¢ÁöÑË°®Áé∞„ÄÇ","title":"ÈîöÂÆöËß£Á†ÅÔºöÂπ≥Ë°°È£éÈô©‰∏éÊïàÁî®ÁöÑÂàõÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÈîöÂÆöËß£Á†ÅÔºàAnchored DecodingÔºâÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÊäëÂà∂ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÈÄêÂ≠óÂ§çÂà∂ÔºåÂêåÊó∂‰øùÊåÅÁîüÊàêÊñáÊú¨ÁöÑÊµÅÁïÖÊÄßÂíå‰∫ãÂÆûÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂú®ÁîüÊàêËøáÁ®ã‰∏≠‰øùÊåÅ‰∏éÂÆâÂÖ®Ê®°ÂûãÁöÑÊé•ËøëÔºåÊù•Âπ≥Ë°°È£éÈô©ÂíåÊïàÁî®ÔºåÈÄÇÁî®‰∫éÊ∑∑ÂêàËÆ∏ÂèØÊï∞ÊçÆËÆ≠ÁªÉÁöÑËØ≠Ë®ÄÊ®°Âûã„ÄÇÈîöÂÆöËß£Á†ÅÂÖÅËÆ∏Áî®Êà∑ÈÄâÊã©‰ø°ÊÅØÈ¢ÑÁÆóÔºåÂπ∂Âú®ÁîüÊàêËøáÁ®ã‰∏≠ÊñΩÂä†ÈÄêÊ≠•Á∫¶ÊùüÔºå‰ªéËÄåÂÆûÁé∞ÂèØË∞ÉÁöÑÈ£éÈô©-ÊïàÁî®ÊùÉË°°„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂÆâÂÖ®Ê®°ÂûãÔºàTinyComma 1.8BÔºâÂíåÂ≠óËäÇÁ∫ßÂèò‰ΩìÔºàAnchored_{Byte} DecodingÔºâÔºåÂπ∂Âú®Â§ö‰∏™Ê®°ÂûãÂØπ‰∏äËØÑ‰º∞‰∫ÜÂÖ∂Âú®ÁâàÊùÉÈ£éÈô©ÂíåÊïàÁî®ÊñπÈù¢ÁöÑË°®Áé∞„ÄÇ', title='ÈîöÂÆöËß£Á†ÅÔºöÂπ≥Ë°°È£éÈô©‰∏éÊïàÁî®ÁöÑÂàõÊñ∞ÊñπÊ≥ï'))
[10.02.2026 18:58] Using data from previous issue: {"categories": ["#security", "#leakage"], "emoji": "üîê", "ru": {"title": "–£–º–Ω–∞—è –∑–∞—â–∏—Ç–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω—ã–π —à—É–º –≤–º–µ—Å—Ç–æ —Å–ª–µ–ø–æ–≥–æ", "desc": "SPARSE ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∑–∞—â–∏—â–∞—é—â–∏–π —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ—Ç —É—Ç–µ—á–µ–∫ –ø—Ä–∏–≤–∞—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—É—Ç—ë–º –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–≥–æ –≤–æ–∑–º—É—â–µ–Ω–∏—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å
[10.02.2026 18:58] Using data from previous issue: {"categories": [], "emoji": "üß†", "ru": {"title": "–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Ö–µ–º—ã –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç –æ—à–∏–±–∫–∏ –∫–æ–¥–∞", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –∏—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏ –≤–º–µ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–Ω–µ—à–Ω–∏—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤, —Ç–∞–∫–∏
[10.02.2026 18:58] Using data from previous issue: {"categories": ["#inference", "#optimization", "#long_context", "#multilingual", "#low_resource", "#benchmark"], "emoji": "üì¶", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ KV-–∫—ç—à–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –º–µ—Ç–æ–¥ KV-CoRE –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∂–∏–º–∞–µ–º–æ—Å—Ç–∏ KV-–∫—ç—à–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑
[10.02.2026 18:58] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π RAG –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç CE-RAG4EM, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è —ç–∫–æ–Ω–æ–º–∏—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π –ø–æ–∫–æ–ª–µ–Ω–∏—è (RAG), –∫–æ—Ç–æ—Ä–∞—è —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –ø—Ä–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ –±–æ–ª—å—à–∏—Ö –º–∞—Å—à—Ç
[10.02.2026 18:58] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#video", "#hallucinations", "#audio", "#benchmark", "#rlhf", "#open_source", "#training"], "emoji": "üòä", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π –±–µ–∑ –ª–æ–∂–Ω—ã—Ö –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –±–µ–Ω—á–º–∞—Ä–∫ EmoReA
[10.02.2026 18:58] Using data from previous issue: {"categories": ["#science", "#training", "#optimization", "#open_source", "#agents", "#plp"], "emoji": "üî¨", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞—É—á–Ω–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ —á–µ—Ä–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º", "desc": "Aster ‚Äî —ç—Ç–æ AI-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–≥—Ä–∞–º
[10.02.2026 18:58] Using data from previous issue: {"categories": ["#science", "#graphs", "#open_source"], "emoji": "üîó", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø—Ä–∏—á–∏–Ω–Ω–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö –≥—Ä–∞—Ñ–∞—Ö —á–µ—Ä–µ–∑ –∫–æ–º–ø—Ä–µ—Å—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è", "desc": "CauScale –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π –≤ –±
[10.02.2026 18:58] Using data from previous issue: {"categories": ["#ethics", "#security"], "emoji": "üõ†Ô∏è", "ru": {"title": "–ú–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å –Ω–∞–≤—ã–∫–æ–≤ –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤: –∞–Ω–∞–ª–∏–∑ —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã, —Ä–∏—Å–∫–æ–≤ –∏ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–π —Å–ø—Ä–æ—Å–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ 40 285 –ø—É–±–ª–∏—á–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ (skills) –¥–ª—è LLM-–∞–≥–µ–Ω—Ç–æ–≤ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–∞. –ê–≤—Ç–æ—Ä
[10.02.2026 18:58] Using data from previous issue: {"categories": [], "emoji": "üêü", "ru": {"title": "–≠–Ω—Ç—Ä–æ–ø–∏—è –∫–∞–∫ –º–æ—Å—Ç –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –≤ –ø—Ä–∏—Ä–æ–¥–µ", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç dewi-kadita ‚Äî –æ—Ç–∫—Ä—ã—Ç—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É –Ω–∞ Python –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è —Ä—ã–± —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∑–æ–Ω—ã-–±–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Couzin –≤ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç
[10.02.2026 18:58] Using data from previous issue: {"categories": ["#optimization", "#alignment", "#reasoning"], "emoji": "üéØ", "ru": {"title": "–ï–¥–∏–Ω–∞—è —Ä–∞–º–∫–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ f-–¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –µ–¥–∏–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—é LLM, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ —Ü–µ–ª–µ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –∫–∞–∫ –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–π –º–µ–∂–¥—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º
[10.02.2026 18:58] Using data from previous issue: {"categories": ["#open_source", "#science"], "emoji": "‚úì", "ru": {"title": "–ú–∞—à–∏–Ω–Ω–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞: –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–µ–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ —á–µ–ª–æ–≤–µ–∫–æ-AI —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–µ—Ä–≤–∞—è –ø–æ–ª–Ω–∞—è —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ç–µ–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è –≤ Lean 4, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–π —Ç–µ–æ—Ä–∏–∏ –ø—Ä
[10.02.2026 18:58] Renaming data file.
[10.02.2026 18:58] Renaming previous data. hf_papers.json to ./d/2026-02-10.json
[10.02.2026 18:58] Saving new data file.
[10.02.2026 18:58] Generating page.
[10.02.2026 18:58] Renaming previous page.
[10.02.2026 18:58] Renaming previous data. index.html to ./d/2026-02-10.html
[10.02.2026 18:58] Writing result.
[10.02.2026 18:58] Renaming log file.
[10.02.2026 18:58] Renaming previous data. log.txt to ./logs/2026-02-10_last_log.txt
