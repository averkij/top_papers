[10.02.2026 07:59] Read previous papers.
[10.02.2026 07:59] Generating top page (month).
[10.02.2026 07:59] Writing top page (month).
[10.02.2026 08:42] Read previous papers.
[10.02.2026 08:42] Get feed.
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07026
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08794
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07085
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06422
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07845
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06025
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08676
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08222
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08439
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07962
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09007
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08543
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07075
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09022
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07055
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08990
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06454
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06540
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06694
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08236
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08145
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21363
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08961
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08808
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08829
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07970
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07803
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07796
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07090
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06445
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07054
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07775
[10.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07040
[10.02.2026 08:42] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.02.2026 08:42] No deleted papers detected.
[10.02.2026 08:42] Downloading and parsing papers (pdf, html). Total: 33.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.07026.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.07026.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.07026.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.08794.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.08794.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.08794.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.07085.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.07085.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.07085.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06422.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06422.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06422.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.07845.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.07845.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.07845.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06025.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06025.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06025.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.08676.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.08676.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.08676.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.08222.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.08222.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.08222.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.08439.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.08439.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.08439.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.07962.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.07962.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.07962.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.09007.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.09007.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.09007.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.08543.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.08543.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.08543.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.07075.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.07075.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.07075.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.09022.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.09022.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.09022.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.07055.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.07055.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.07055.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.08990.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.08990.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.08990.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06454.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06454.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06454.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06540.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06540.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06540.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06694.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06694.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06694.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.08236.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.08236.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.08236.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.08145.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.08145.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.08145.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.21363.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2601.21363.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2601.21363.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.08961.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.08961.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.08961.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.08808.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.08808.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.08808.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.08829.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.08829.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.08829.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.07970.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.07970.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.07970.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.07803.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.07803.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.07803.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.07796.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.07796.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.07796.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.07090.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.07090.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.07090.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06445.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06445.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06445.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.07054.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.07054.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.07054.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.07775.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.07775.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.07775.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.07040.
[10.02.2026 08:42] Extra JSON file exists (./assets/json/2602.07040.json), skip PDF parsing.
[10.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.07040.json), skip HTML parsing.
[10.02.2026 08:42] Success.
[10.02.2026 08:42] Enriching papers with extra data.
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 0. Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data.  					AI-generated summary 				 Despite the success of multimodal contrastive learning in aligni...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 1. MOVA is an open-source model that generates synchronized audio-visual content using a Mixture-of-Experts architecture with 32 billion parameters, supporting image-text to video-audio generation tasks.  					AI-generated summary 				 Audio is indispensable for real-world video, yet generation models ...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 2. Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated exper...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 3. TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories.  					AI-generated summary 				 Deploying GRPO on Flow Matching models has proven effective for text-to-image...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 4. RD-VLA introduces a recurrent architecture for vision-language-action models that adapts computational depth through latent iterative refinement, achieving constant memory usage and improved task success rates.  					AI-generated summary 				 Current Vision-Language-Action (VLA) models rely on fixed...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 5. BudgetMem is a runtime memory framework for LLM agents that uses modular components with three budget tiers and a neural policy router to optimize performance-cost trade-offs in memory usage.  					AI-generated summary 				 Memory is increasingly central to Large Language Model (LLM) agents operatin...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 6. LLaDA2.1 introduces a novel token-to-token editing approach with speed and quality modes, enhanced through reinforcement learning for improved reasoning and instruction following in large language diffusion models.  					AI-generated summary 				 While LLaDA2.0 showcased the scaling potential of 100...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 7. WMSS is a post-training paradigm that uses weak model checkpoints to identify and fill learning gaps, enabling continued improvement beyond conventional saturation points in large language models.  					AI-generated summary 				 As post-training optimization becomes central to improving large langua...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 8. Researchers introduce a new video understanding task and benchmark that evaluates models' ability to learn from few-shot demonstrations, along with a specialized MLLM architecture trained using a two-stage approach combining video supervision and preference optimization.  					AI-generated summary 	...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 9. LOCA-bench is introduced as a benchmark for evaluating language agents in long-context, agentic scenarios with controlled environment state management.  					AI-generated summary 				 Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as th...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 10. A new benchmark and evaluation metric are introduced for assessing temporal coherence and dynamic interaction in GUI generation models, revealing significant challenges in maintaining consistency over extended interaction sequences.  					AI-generated summary 				 Recent advancements in image genera...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 11. A new benchmark called GISA is introduced for evaluating information-seeking assistants, featuring human-crafted queries with structured answer formats and live updates to prevent memorization.  					AI-generated summary 				 The advancement of large language models (LLMs) has significantly accelera...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 12. LatentChem enables chemical reasoning through continuous latent space computations instead of discrete textual tokens, achieving superior performance and efficiency compared to traditional chain-of-thought approaches.  					AI-generated summary 				 Chemical large language models (LLMs) predominantl...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 13. WorldCompass enhances long-horizon video-based world models through reinforcement learning post-training with clip-level rollouts, complementary rewards, and efficient RL algorithms.  					AI-generated summary 				 This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training fr...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 14. Current multimodal foundation models show limitations in maintaining coherent spatial beliefs during active exploration, exhibiting gaps between active and passive performance, inefficient exploration strategies, and difficulties in updating outdated spatial knowledge.  					AI-generated summary 			...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 15. InternAgent-1.5 is a unified system for autonomous scientific discovery that integrates computational modeling and experimental research through coordinated subsystems for generation, verification, and evolution.  					AI-generated summary 				 We introduce InternAgent-1.5, a unified system designed...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 16. RelayGen is a training-free framework that dynamically switches between large and small models during reasoning by identifying difficulty transitions at the segment level, achieving faster inference with minimal accuracy loss.  					AI-generated summary 				 Large reasoning models (LRMs) achieve str...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 17. AgentCPM-Report presents a lightweight local solution for deep research report generation using a Writing As Reasoning Policy framework and multi-stage agentic training to enhance small models' reasoning and outline evolution capabilities.  					AI-generated summary 				 Generating deep research rep...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 18. NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment.  					AI-generated summar...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 19. Adaptive test-time framework with world models enables selective visual imagination for spatial reasoning, improving efficiency and reliability by determining when imagination is necessary.  					AI-generated summary 				 Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spa...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 20. Foundation models including LLMs, MLLMs, and generative models require reliable and responsible development addressing bias, security, explainability, and other critical issues for trustworthy deployment across multiple domains.  					AI-generated summary 				 Foundation models, including Large Lang...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 21. Off-policy Soft Actor-Critic with large-batch updates enables efficient humanoid locomotion policy pretraining, while model-based methods facilitate safe adaptation through deterministic data collection and stochastic exploration within physics-informed world models.  					AI-generated summary 				 ...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 22. MotionCrafter is a video diffusion framework that jointly reconstructs 4D geometry and estimates dense motion using a novel joint representation and 4D VAE architecture.  					AI-generated summary 				 We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometr...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 23. A scalable framework for evaluating and improving goal-conditioned procedure generation using large-scale web mining, automated scoring, and reinforcement learning to enhance step-by-step instruction quality.  					AI-generated summary 				 Generating step-by-step "how-to" procedures is a key LLM ca...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 24. WildReward demonstrates that reward models can be effectively trained from in-the-wild user interactions using ordinal regression, achieving performance comparable to traditional methods while benefiting from user diversity.  					AI-generated summary 				 Reward models (RMs) are crucial for the tra...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 25. Research explores PDE solvers including neural frameworks for scientific simulations, examining forward solutions, inverse problems, and equation discovery across multi-variable and non-linear systems.  					AI-generated summary 				 Partial Differential Equations are precise in modelling the physic...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 26. A high-quality open-source singing voice synthesis system is presented with support for multiple languages and controllable generation, along with a dedicated benchmark for evaluating zero-shot performance.  					AI-generated summary 				 While recent years have witnessed rapid progress in speech sy...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 27. Explicit reasoning in LLM agents can degrade performance in user-engaged scenarios by reducing information disclosure and weakening agent-user communication, with transparency-aware prompting showing better results.  					AI-generated summary 				 Eliciting reasoning has emerged as a powerful techni...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 28. SPARSE is a user-centric framework that protects text embeddings from privacy leaks by selectively perturbing sensitive dimensions using differentiable masking and Mahalanobis noise calibration.  					AI-generated summary 				 Text embeddings enable numerous NLP applications but face severe privacy ...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 29. Energy-constrained optimization framework separates energy metrics from rewards using Lagrangian method to achieve stable, energy-efficient humanoid robot locomotion with reduced hyperparameter tuning.  					AI-generated summary 				 Achieving stable and energy-efficient locomotion is essential for ...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 30. A benchmark and optimization technique are presented to improve multimodal large language models' emotion understanding by addressing spurious associations and hallucinations in audiovisual cues.  					AI-generated summary 				 Emotion understanding is essential for building socially intelligent age...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 31. Autoregressive video diffusion models suffer from train-test gaps when generating long videos, but a training-free approach called Rolling Sink addresses this by maintaining AR cache and enabling ultra-long video synthesis.  					AI-generated summary 				 Recently, autoregressive (AR) video diffusio...
[10.02.2026 08:42] ********************************************************************************
[10.02.2026 08:42] Abstract 32. Aster is an AI agent that accelerates scientific discovery by iteratively improving programs, achieving state-of-the-art results across multiple domains including mathematics, biology, and machine learning with significantly reduced computational requirements.  					AI-generated summary 				 We intr...
[10.02.2026 08:42] Read previous papers.
[10.02.2026 08:42] Generating reviews via LLM API.
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#training", "#multimodal", "#architecture"], "emoji": "üîó", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–µ—à–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏, –ø—Ä–µ–¥–ª–æ–∂
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#inference", "#video", "#audio", "#dataset", "#architecture", "#open_source", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–°–æ–≤–º–µ—Å—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ —Å –æ–¥–Ω–æ–π –º–æ–¥–µ–ª—å—é", "desc": "MOVA ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Mixture-of-Experts, —Å–æ–¥–µ—Ä–∂–∞—â
[10.02.2026 08:42] Using data from previous issue: {"categories": [], "emoji": "üíπ", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π", "desc": "QuantaAlpha –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∞–ª—å—Ñ–∞-—Ñ–∞–∫—Ç–æ—Ä–æ–≤ –Ω–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä—ã–Ω–∫–∞—Ö, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –∫–∞–∂–¥—ã–π —Ü–∏–∫–ª –º–∞–π–Ω–∏–Ω–≥–∞ –∫–∞–∫ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#optimization"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω—ã–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "TP-GRPO —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤ –º–æ–¥–µ–ª—è—Ö flow matching –ø—É—Ç—ë–º –≤–≤–µ–¥–µ–Ω–∏—è –ø–æ—à–∞–≥–æ–≤—ã—Ö –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤–º–µ—Å—Ç–æ –∏—Ç–æ–≥–æ–≤–æ–≥
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#inference", "#training", "#robotics", "#architecture", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–ª—É–±–∏–Ω—ã: —Å–∫—Ä—ã—Ç–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –¥–ª—è robotics", "desc": "RD-VLA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –º–æ–¥–µ–ª–µ–π vision-language-action, –∫–æ—Ç–æ—Ä
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#optimization", "#rl", "#agents", "#long_context", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø–∞–º—è—Ç–∏ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "BudgetMem ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è,
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization", "#alignment", "#benchmark", "#architecture", "#open_source", "#reasoning", "#diffusion", "#plp"], "emoji": "‚ö°", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ T2T —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º",
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "üìà", "ru": {"title": "–°–ª–∞–±—ã–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –∫–∞–∫ –ø—É—Ç—å –∫ —Å–∏–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è WMSS ‚Äî –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞—Å—ã—â–µ–Ω–∏—è, –≤–æ–∑–Ω–∏–∫–∞—é—â—É—é –ø—Ä–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º –æ
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#training", "#video", "#benchmark", "#dataset", "#multimodal", "#rlhf"], "emoji": "üé•", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ–ø–æ–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –Ω–µ–º–Ω–æ–≥–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ –∏ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#agents", "#long_context", "#open_source", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤ —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–∞—Ö —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è", "desc": "–í–≤–µ–¥–µ–Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫ LOCA-bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ —É–ø
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#dataset"], "emoji": "üñ•Ô∏è", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ GEBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 700 —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#dataset", "#survey", "#agents"], "emoji": "üîç", "ru": {"title": "GISA: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –∏ –∂–∏–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ GISA –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-–ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 37
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#science", "#reasoning"], "emoji": "‚öóÔ∏è", "ru": {"title": "–•–∏–º–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –≤–º–µ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ü–µ–ø–æ—á–µ–∫ –º—ã—Å–ª–∏", "desc": "LatentChem –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ö–∏–º–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –≤–º–µ—Å—Ç–æ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö —Ç
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#training", "#video", "#rl"], "emoji": "üß≠", "ru": {"title": "–ù–∞–ø—Ä–∞–≤–ª—è–µ–º –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–∏ –º–∏—Ä–∞: –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "desc": "WorldCompass –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã –≤–≤
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#multimodal", "#robotics"], "emoji": "üó∫Ô∏è", "ru": {"title": "–ê–≥–µ–Ω—Ç—ã –Ω–µ –ø–æ–Ω–∏–º–∞—é—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ: –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å–ª–∞–±–æ—Å—Ç–µ–π –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ foundation models", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ foundation models –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è—é
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#science", "#reasoning", "#benchmark", "#open_source", "#agents"], "emoji": "üß¨", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è –Ω–∞—É—á–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–∏–π —á–µ—Ä–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—é –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤", "desc": "InternAgent-1.5 ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#inference", "#training", "#small_models", "#optimization", "#reasoning"], "emoji": "‚ö°", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "RelayGen ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#training", "#small_models", "#open_source", "#rl", "#agents", "#reasoning", "#benchmark"], "emoji": "üìù", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è –ø–∏—Å—å–º–∞: –º–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "AgentCPM-Report –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ª—ë–≥–∫–æ–µ –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#optimization"], "emoji": "‚öôÔ∏è", "ru": {"title": "–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π: –ø—É—Ç—å –∫ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é –Ω–∞ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö", "desc": "NanoQuant ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ—Å—Ç–æ–±—É—á–∞—é—â–µ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Å–∂–∏–º–∞–µ—Ç –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–æ –±–∏–Ω–∞—Ä–Ω–æ–≥–æ (1-–±–∏—Ç) –∏ —Å—É–±–±–∏–Ω–∞—Ä–Ω–æ–≥–æ —É
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#cv", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–ò–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ: –∫–æ–≥–¥–∞ –≤–∏–¥–µ—Ç—å –Ω—É–∂–Ω–æ, –∞ –∫–æ–≥–¥–∞ –Ω–µ—Ç", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –æ—à–∏–±–∞—é—Ç—Å—è –ø—Ä–∏ –Ω
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#security", "#alignment", "#ethics", "#hallucinations", "#survey", "#interpretability"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ü—É—Ç—å –∫ –Ω–∞–¥–µ–∂–Ω—ã–º –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–º —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–í —ç—Ç–æ–º –æ–±–∑–æ—Ä–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã –Ω–∞–¥–µ–∂–Ω–æ–π –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#training", "#robotics", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≥—É–º–∞–Ω–æ–∏–¥–æ–≤ —á–µ—Ä–µ–∑ off-policy –æ–±—É—á–µ–Ω–∏–µ —Å —É—Å–∏–ª–µ–Ω–∏–µ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º Soft Actor-Critic —Å –±–æ–ª—å—à–∏–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞ –∏ –≤—ã—Å–æ–∫–∏–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#diffusion", "#video", "#architecture", "#3d", "#training"], "emoji": "üé¨", "ru": {"title": "–°–æ–≤–º–µ—Å—Ç–Ω–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –∏ –¥–≤–∏–∂–µ–Ω–∏—è –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ 4D –¥–∏—Ñ—Ñ—É–∑–∏—é", "desc": "MotionCrafter ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ—Ç 4D –≥–µ–æ–º–µ—Ç—Ä–∏—é
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization", "#synthetic", "#dataset", "#benchmark", "#open_source", "#reasoning"], "emoji": "üìã", "ru": {"title": "–ó–∞–∫—Ä—ã—Ç—ã–π —Ü–∏–∫–ª –æ—Ü–µ–Ω–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ How2Everything - –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#dataset", "#training", "#rlhf", "#data"], "emoji": "üèÜ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω WildReward ‚Äî –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å —è–∑—ã–∫–æ
[10.02.2026 08:42] Using data from previous issue: {"categories": [], "emoji": "üßÆ", "ru": {"title": "–ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ —Ä–µ—à–∞—Ç–µ–ª–∏ —É—Ä–∞–≤–Ω–µ–Ω–∏–π –≤ —á–∞—Å—Ç–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ä–µ—à–µ–Ω–∏—è —É—Ä–∞–≤–Ω–µ–Ω–∏–π –≤ —á–∞—Å—Ç–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö (–£–ß–ü) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö —Å–∏–º—É–ª—è—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#low_resource", "#audio", "#dataset", "#benchmark", "#open_source", "#multilingual"], "emoji": "üé§", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∏–Ω—Ç–µ–∑–∞ –ø–µ–≤—á–µ—Å–∫–æ–≥–æ –≥–æ–ª–æ—Å–∞ —Å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —è–∑—ã–∫–æ–≤ –∏ –Ω–∞–¥—ë–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ SoulX-Singer ‚Äî –≤—ã—Å
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#alignment", "#agents", "#reasoning", "#benchmark", "#open_source", "#training"], "emoji": "ü§ê", "ru": {"title": "–ú–æ–ª—á–∞–Ω–∏–µ –∑–æ–ª–æ—Ç–æ? –ü–æ—á–µ–º—É —è–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –æ—Å–ª–∞–±–ª—è–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∞–≥–µ–Ω—Ç–∞ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ —è–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ LLM –∞–≥–µ–Ω—Ç–∞—Ö, –≤–∑–∞–∏–º–æ
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#security", "#leakage"], "emoji": "üîê", "ru": {"title": "–£–º–Ω–∞—è –∑–∞—â–∏—Ç–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω—ã–π —à—É–º –≤–º–µ—Å—Ç–æ —Å–ª–µ–ø–æ–≥–æ", "desc": "SPARSE ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∑–∞—â–∏—â–∞—é—â–∏–π —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ—Ç —É—Ç–µ—á–µ–∫ –ø—Ä–∏–≤–∞—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—É—Ç—ë–º –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–≥–æ –≤–æ–∑–º—É—â–µ–Ω–∏—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —ç–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ —è–≤–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —à—Ç—Ä–∞—Ñ–æ–≤ –≤ –Ω–∞–≥—Ä–∞–¥–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ ECO (Energy-Constrained Optimization) ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–¥–µ–ª—è–µ—Ç —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#video", "#hallucinations", "#audio", "#benchmark", "#rlhf", "#open_source", "#training"], "emoji": "üòä", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π –±–µ–∑ –ª–æ–∂–Ω—ã—Ö –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –±–µ–Ω—á–º–∞—Ä–∫ EmoReA
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#inference", "#training", "#video", "#long_context", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–ë–µ—Å–ø—Ä–æ–±–ª–µ–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∞–≤—Ç—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π –¥–æ —Å–≤–µ—Ä—Ö–¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –æ–±—É—á–µ–Ω–∏–µ–º –∏
[10.02.2026 08:42] Using data from previous issue: {"categories": ["#science", "#training", "#optimization", "#open_source", "#agents", "#plp"], "emoji": "üî¨", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞—É—á–Ω–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ —á–µ—Ä–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º", "desc": "Aster ‚Äî —ç—Ç–æ AI-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–≥—Ä–∞–º
[10.02.2026 08:42] Renaming data file.
[10.02.2026 08:42] Renaming previous data. hf_papers.json to ./d/2026-02-10.json
[10.02.2026 08:42] Saving new data file.
[10.02.2026 08:42] Generating page.
[10.02.2026 08:42] Renaming previous page.
[10.02.2026 08:42] Renaming previous data. index.html to ./d/2026-02-10.html
[10.02.2026 08:42] Writing result.
[10.02.2026 08:42] Renaming log file.
[10.02.2026 08:42] Renaming previous data. log.txt to ./logs/2026-02-10_last_log.txt
