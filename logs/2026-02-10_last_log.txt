[10.02.2026 09:55] Read previous papers.
[10.02.2026 09:55] Generating top page (month).
[10.02.2026 09:55] Writing top page (month).
[10.02.2026 10:47] Read previous papers.
[10.02.2026 10:47] Get feed.
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08794
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07026
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07085
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07845
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06422
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08676
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09007
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06025
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08222
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08439
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07962
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08543
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07075
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07055
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09022
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08990
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06540
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06454
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08236
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06694
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08808
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21363
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08961
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06445
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08145
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08829
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07775
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07970
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07803
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07796
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07491
[10.02.2026 10:47] Extract page data from URL. URL: https://huggingface.co/papers/2602.07150
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07090
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07054
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08818
[10.02.2026 10:47] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07040
[10.02.2026 10:47] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.02.2026 10:47] No deleted papers detected.
[10.02.2026 10:47] Downloading and parsing papers (pdf, html). Total: 36.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.08794.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.08794.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.08794.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.07026.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.07026.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.07026.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.07085.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.07085.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.07085.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.07845.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.07845.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.07845.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.06422.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.06422.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.06422.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.08676.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.08676.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.08676.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.09007.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.09007.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.09007.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.06025.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.06025.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.06025.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.08222.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.08222.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.08222.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.08439.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.08439.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.08439.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.07962.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.07962.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.07962.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.08543.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.08543.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.08543.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.07075.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.07075.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.07075.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.07055.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.07055.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.07055.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.09022.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.09022.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.09022.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.08990.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.08990.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.08990.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.06540.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.06540.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.06540.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.06454.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.06454.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.06454.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.08236.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.08236.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.08236.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.06694.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.06694.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.06694.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.08808.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.08808.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.08808.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2601.21363.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2601.21363.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2601.21363.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.08961.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.08961.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.08961.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.06445.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.06445.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.06445.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.08145.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.08145.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.08145.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.08829.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.08829.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.08829.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.07775.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.07775.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.07775.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.07970.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.07970.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.07970.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.07803.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.07803.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.07803.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.07796.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.07796.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.07796.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.07491.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.07491.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.07491.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.07150.
[10.02.2026 10:47] Downloading paper 2602.07150 from https://arxiv.org/pdf/2602.07150v1...
[10.02.2026 10:47] Extracting affiliations from text.
[10.02.2026 10:47] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 ] . [ 1 0 5 1 7 0 . 2 0 6 2 : r Pre-print under review Bjarni Haukur Bjarnason, Andre Silva, Martin Monperrus KTH Royal Institute of Technology Stockholm, Sweden {bhbj, andreans, monperrus}@kth.se "
[10.02.2026 10:47] Response: ```python
["KTH Royal Institute of Technology"]
```
[10.02.2026 10:47] Deleting PDF ./assets/pdf/2602.07150.pdf.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.07090.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.07090.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.07090.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.07054.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.07054.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.07054.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.08818.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.08818.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.08818.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Downloading and parsing paper https://huggingface.co/papers/2602.07040.
[10.02.2026 10:47] Extra JSON file exists (./assets/json/2602.07040.json), skip PDF parsing.
[10.02.2026 10:47] Paper image links file exists (./assets/img_data/2602.07040.json), skip HTML parsing.
[10.02.2026 10:47] Success.
[10.02.2026 10:47] Enriching papers with extra data.
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 0. MOVA is an open-source model that generates synchronized audio-visual content using a Mixture-of-Experts architecture with 32 billion parameters, supporting image-text to video-audio generation tasks.  					AI-generated summary 				 Audio is indispensable for real-world video, yet generation models ...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 1. Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data.  					AI-generated summary 				 Despite the success of multimodal contrastive learning in aligni...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 2. Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated exper...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 3. RD-VLA introduces a recurrent architecture for vision-language-action models that adapts computational depth through latent iterative refinement, achieving constant memory usage and improved task success rates.  					AI-generated summary 				 Current Vision-Language-Action (VLA) models rely on fixed...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 4. TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories.  					AI-generated summary 				 Deploying GRPO on Flow Matching models has proven effective for text-to-image...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 5. LLaDA2.1 introduces a novel token-to-token editing approach with speed and quality modes, enhanced through reinforcement learning for improved reasoning and instruction following in large language diffusion models.  					AI-generated summary 				 While LLaDA2.0 showcased the scaling potential of 100...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 6. A new benchmark and evaluation metric are introduced for assessing temporal coherence and dynamic interaction in GUI generation models, revealing significant challenges in maintaining consistency over extended interaction sequences.  					AI-generated summary 				 Recent advancements in image genera...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 7. BudgetMem is a runtime memory framework for LLM agents that uses modular components with three budget tiers and a neural policy router to optimize performance-cost trade-offs in memory usage.  					AI-generated summary 				 Memory is increasingly central to Large Language Model (LLM) agents operatin...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 8. WMSS is a post-training paradigm that uses weak model checkpoints to identify and fill learning gaps, enabling continued improvement beyond conventional saturation points in large language models.  					AI-generated summary 				 As post-training optimization becomes central to improving large langua...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 9. Researchers introduce a new video understanding task and benchmark that evaluates models' ability to learn from few-shot demonstrations, along with a specialized MLLM architecture trained using a two-stage approach combining video supervision and preference optimization.  					AI-generated summary 	...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 10. LOCA-bench is introduced as a benchmark for evaluating language agents in long-context, agentic scenarios with controlled environment state management.  					AI-generated summary 				 Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as th...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 11. A new benchmark called GISA is introduced for evaluating information-seeking assistants, featuring human-crafted queries with structured answer formats and live updates to prevent memorization.  					AI-generated summary 				 The advancement of large language models (LLMs) has significantly accelera...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 12. LatentChem enables chemical reasoning through continuous latent space computations instead of discrete textual tokens, achieving superior performance and efficiency compared to traditional chain-of-thought approaches.  					AI-generated summary 				 Chemical large language models (LLMs) predominantl...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 13. Current multimodal foundation models show limitations in maintaining coherent spatial beliefs during active exploration, exhibiting gaps between active and passive performance, inefficient exploration strategies, and difficulties in updating outdated spatial knowledge.  					AI-generated summary 			...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 14. WorldCompass enhances long-horizon video-based world models through reinforcement learning post-training with clip-level rollouts, complementary rewards, and efficient RL algorithms.  					AI-generated summary 				 This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training fr...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 15. InternAgent-1.5 is a unified system for autonomous scientific discovery that integrates computational modeling and experimental research through coordinated subsystems for generation, verification, and evolution.  					AI-generated summary 				 We introduce InternAgent-1.5, a unified system designed...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 16. AgentCPM-Report presents a lightweight local solution for deep research report generation using a Writing As Reasoning Policy framework and multi-stage agentic training to enhance small models' reasoning and outline evolution capabilities.  					AI-generated summary 				 Generating deep research rep...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 17. RelayGen is a training-free framework that dynamically switches between large and small models during reasoning by identifying difficulty transitions at the segment level, achieving faster inference with minimal accuracy loss.  					AI-generated summary 				 Large reasoning models (LRMs) achieve str...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 18. Adaptive test-time framework with world models enables selective visual imagination for spatial reasoning, improving efficiency and reliability by determining when imagination is necessary.  					AI-generated summary 				 Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spa...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 19. NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment.  					AI-generated summar...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 20. A scalable framework for evaluating and improving goal-conditioned procedure generation using large-scale web mining, automated scoring, and reinforcement learning to enhance step-by-step instruction quality.  					AI-generated summary 				 Generating step-by-step "how-to" procedures is a key LLM ca...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 21. Off-policy Soft Actor-Critic with large-batch updates enables efficient humanoid locomotion policy pretraining, while model-based methods facilitate safe adaptation through deterministic data collection and stochastic exploration within physics-informed world models.  					AI-generated summary 				 ...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 22. MotionCrafter is a video diffusion framework that jointly reconstructs 4D geometry and estimates dense motion using a novel joint representation and 4D VAE architecture.  					AI-generated summary 				 We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometr...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 23. Energy-constrained optimization framework separates energy metrics from rewards using Lagrangian method to achieve stable, energy-efficient humanoid robot locomotion with reduced hyperparameter tuning.  					AI-generated summary 				 Achieving stable and energy-efficient locomotion is essential for ...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 24. Foundation models including LLMs, MLLMs, and generative models require reliable and responsible development addressing bias, security, explainability, and other critical issues for trustworthy deployment across multiple domains.  					AI-generated summary 				 Foundation models, including Large Lang...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 25. WildReward demonstrates that reward models can be effectively trained from in-the-wild user interactions using ordinal regression, achieving performance comparable to traditional methods while benefiting from user diversity.  					AI-generated summary 				 Reward models (RMs) are crucial for the tra...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 26. Autoregressive video diffusion models suffer from train-test gaps when generating long videos, but a training-free approach called Rolling Sink addresses this by maintaining AR cache and enabling ultra-long video synthesis.  					AI-generated summary 				 Recently, autoregressive (AR) video diffusio...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 27. Research explores PDE solvers including neural frameworks for scientific simulations, examining forward solutions, inverse problems, and equation discovery across multi-variable and non-linear systems.  					AI-generated summary 				 Partial Differential Equations are precise in modelling the physic...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 28. A high-quality open-source singing voice synthesis system is presented with support for multiple languages and controllable generation, along with a dedicated benchmark for evaluating zero-shot performance.  					AI-generated summary 				 While recent years have witnessed rapid progress in speech sy...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 29. Explicit reasoning in LLM agents can degrade performance in user-engaged scenarios by reducing information disclosure and weakening agent-user communication, with transparency-aware prompting showing better results.  					AI-generated summary 				 Eliciting reasoning has emerged as a powerful techni...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 30. A multi-agent framework guided by knowledge graphs addresses materials science challenges by integrating specialized agents for problem decomposition, evidence retrieval, and graph traversal to discover sustainable PFAS alternatives.  					AI-generated summary 				 Large Language Models (LLMs) promi...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 31. Analysis of agentic system evaluation reveals significant variance in single-run performance estimates, necessitating multiple runs and advanced metrics for reliable assessment.  					AI-generated summary 				 Agentic systems are evaluated on benchmarks where agents interact with environments to sol...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 32. SPARSE is a user-centric framework that protects text embeddings from privacy leaks by selectively perturbing sensitive dimensions using differentiable masking and Mahalanobis noise calibration.  					AI-generated summary 				 Text embeddings enable numerous NLP applications but face severe privacy ...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 33. A benchmark and optimization technique are presented to improve multimodal large language models' emotion understanding by addressing spurious associations and hallucinations in audiovisual cues.  					AI-generated summary 				 Emotion understanding is essential for building socially intelligent age...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 34. FlexMoRE demonstrates that low-rank adapters can replace full-sized experts in mixture-of-experts architectures, achieving better performance with significantly fewer parameters.  					AI-generated summary 				 Recent advances in mixture-of-experts architectures have shown that individual experts mo...
[10.02.2026 10:47] ********************************************************************************
[10.02.2026 10:47] Abstract 35. Aster is an AI agent that accelerates scientific discovery by iteratively improving programs, achieving state-of-the-art results across multiple domains including mathematics, biology, and machine learning with significantly reduced computational requirements.  					AI-generated summary 				 We intr...
[10.02.2026 10:47] Read previous papers.
[10.02.2026 10:47] Generating reviews via LLM API.
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#inference", "#video", "#audio", "#dataset", "#architecture", "#open_source", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–°–æ–≤–º–µ—Å—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ —Å –æ–¥–Ω–æ–π –º–æ–¥–µ–ª—å—é", "desc": "MOVA ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Mixture-of-Experts, —Å–æ–¥–µ—Ä–∂–∞—â
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#training", "#multimodal", "#architecture"], "emoji": "üîó", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–µ—à–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏, –ø—Ä–µ–¥–ª–æ–∂
[10.02.2026 10:47] Using data from previous issue: {"categories": [], "emoji": "üíπ", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π", "desc": "QuantaAlpha –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∞–ª—å—Ñ–∞-—Ñ–∞–∫—Ç–æ—Ä–æ–≤ –Ω–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä—ã–Ω–∫–∞—Ö, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –∫–∞–∂–¥—ã–π —Ü–∏–∫–ª –º–∞–π–Ω–∏–Ω–≥–∞ –∫–∞–∫ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#inference", "#training", "#robotics", "#architecture", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–ª—É–±–∏–Ω—ã: —Å–∫—Ä—ã—Ç–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –¥–ª—è robotics", "desc": "RD-VLA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –º–æ–¥–µ–ª–µ–π vision-language-action, –∫–æ—Ç–æ—Ä
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#optimization"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω—ã–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "TP-GRPO —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤ –º–æ–¥–µ–ª—è—Ö flow matching –ø—É—Ç—ë–º –≤–≤–µ–¥–µ–Ω–∏—è –ø–æ—à–∞–≥–æ–≤—ã—Ö –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤–º–µ—Å—Ç–æ –∏—Ç–æ–≥–æ–≤–æ–≥
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization", "#alignment", "#benchmark", "#architecture", "#open_source", "#reasoning", "#diffusion", "#plp"], "emoji": "‚ö°", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ T2T —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º",
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#dataset"], "emoji": "üñ•Ô∏è", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ GEBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 700 —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#optimization", "#rl", "#agents", "#long_context", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø–∞–º—è—Ç–∏ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "BudgetMem ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è,
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "üìà", "ru": {"title": "–°–ª–∞–±—ã–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –∫–∞–∫ –ø—É—Ç—å –∫ —Å–∏–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è WMSS ‚Äî –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞—Å—ã—â–µ–Ω–∏—è, –≤–æ–∑–Ω–∏–∫–∞—é—â—É—é –ø—Ä–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º –æ
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#training", "#video", "#benchmark", "#dataset", "#multimodal", "#rlhf"], "emoji": "üé•", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ–ø–æ–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –Ω–µ–º–Ω–æ–≥–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ –∏ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#agents", "#long_context", "#open_source", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤ —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–∞—Ö —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è", "desc": "–í–≤–µ–¥–µ–Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫ LOCA-bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ —É–ø
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#dataset", "#survey", "#agents"], "emoji": "üîç", "ru": {"title": "GISA: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –∏ –∂–∏–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ GISA –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-–ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 37
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#science", "#reasoning"], "emoji": "‚öóÔ∏è", "ru": {"title": "–•–∏–º–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –≤–º–µ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ü–µ–ø–æ—á–µ–∫ –º—ã—Å–ª–∏", "desc": "LatentChem –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ö–∏–º–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –≤–º–µ—Å—Ç–æ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö —Ç
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#multimodal", "#robotics"], "emoji": "üó∫Ô∏è", "ru": {"title": "–ê–≥–µ–Ω—Ç—ã –Ω–µ –ø–æ–Ω–∏–º–∞—é—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ: –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å–ª–∞–±–æ—Å—Ç–µ–π –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ foundation models", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ foundation models –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è—é
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#training", "#video", "#rl"], "emoji": "üß≠", "ru": {"title": "–ù–∞–ø—Ä–∞–≤–ª—è–µ–º –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–∏ –º–∏—Ä–∞: –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "desc": "WorldCompass –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã –≤–≤
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#science", "#reasoning", "#benchmark", "#open_source", "#agents"], "emoji": "üß¨", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è –Ω–∞—É—á–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–∏–π —á–µ—Ä–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—é –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤", "desc": "InternAgent-1.5 ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#training", "#small_models", "#open_source", "#rl", "#agents", "#reasoning", "#benchmark"], "emoji": "üìù", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è –ø–∏—Å—å–º–∞: –º–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "AgentCPM-Report –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ª—ë–≥–∫–æ–µ –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#inference", "#training", "#small_models", "#optimization", "#reasoning"], "emoji": "‚ö°", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "RelayGen ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#cv", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–ò–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ: –∫–æ–≥–¥–∞ –≤–∏–¥–µ—Ç—å –Ω—É–∂–Ω–æ, –∞ –∫–æ–≥–¥–∞ –Ω–µ—Ç", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –æ—à–∏–±–∞—é—Ç—Å—è –ø—Ä–∏ –Ω
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#optimization"], "emoji": "‚öôÔ∏è", "ru": {"title": "–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π: –ø—É—Ç—å –∫ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é –Ω–∞ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö", "desc": "NanoQuant ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ—Å—Ç–æ–±—É—á–∞—é—â–µ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Å–∂–∏–º–∞–µ—Ç –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–æ –±–∏–Ω–∞—Ä–Ω–æ–≥–æ (1-–±–∏—Ç) –∏ —Å—É–±–±–∏–Ω–∞—Ä–Ω–æ–≥–æ —É
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization", "#synthetic", "#dataset", "#benchmark", "#open_source", "#reasoning"], "emoji": "üìã", "ru": {"title": "–ó–∞–∫—Ä—ã—Ç—ã–π —Ü–∏–∫–ª –æ—Ü–µ–Ω–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ How2Everything - –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#training", "#robotics", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≥—É–º–∞–Ω–æ–∏–¥–æ–≤ —á–µ—Ä–µ–∑ off-policy –æ–±—É—á–µ–Ω–∏–µ —Å —É—Å–∏–ª–µ–Ω–∏–µ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º Soft Actor-Critic —Å –±–æ–ª—å—à–∏–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞ –∏ –≤—ã—Å–æ–∫–∏–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#diffusion", "#video", "#architecture", "#3d", "#training"], "emoji": "üé¨", "ru": {"title": "–°–æ–≤–º–µ—Å—Ç–Ω–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –∏ –¥–≤–∏–∂–µ–Ω–∏—è –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ 4D –¥–∏—Ñ—Ñ—É–∑–∏—é", "desc": "MotionCrafter ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ—Ç 4D –≥–µ–æ–º–µ—Ç—Ä–∏—é
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —ç–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ —è–≤–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —à—Ç—Ä–∞—Ñ–æ–≤ –≤ –Ω–∞–≥—Ä–∞–¥–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ ECO (Energy-Constrained Optimization) ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–¥–µ–ª—è–µ—Ç —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#security", "#alignment", "#ethics", "#hallucinations", "#survey", "#interpretability"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ü—É—Ç—å –∫ –Ω–∞–¥–µ–∂–Ω—ã–º –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–º —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–í —ç—Ç–æ–º –æ–±–∑–æ—Ä–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã –Ω–∞–¥–µ–∂–Ω–æ–π –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#dataset", "#training", "#rlhf", "#data"], "emoji": "üèÜ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω WildReward ‚Äî –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å —è–∑—ã–∫–æ
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#inference", "#training", "#video", "#long_context", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–ë–µ—Å–ø—Ä–æ–±–ª–µ–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∞–≤—Ç—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π –¥–æ —Å–≤–µ—Ä—Ö–¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –æ–±—É—á–µ–Ω–∏–µ–º –∏
[10.02.2026 10:47] Using data from previous issue: {"categories": [], "emoji": "üßÆ", "ru": {"title": "–ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ —Ä–µ—à–∞—Ç–µ–ª–∏ —É—Ä–∞–≤–Ω–µ–Ω–∏–π –≤ —á–∞—Å—Ç–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ä–µ—à–µ–Ω–∏—è —É—Ä–∞–≤–Ω–µ–Ω–∏–π –≤ —á–∞—Å—Ç–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö (–£–ß–ü) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö —Å–∏–º—É–ª—è—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#low_resource", "#audio", "#dataset", "#benchmark", "#open_source", "#multilingual"], "emoji": "üé§", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∏–Ω—Ç–µ–∑–∞ –ø–µ–≤—á–µ—Å–∫–æ–≥–æ –≥–æ–ª–æ—Å–∞ —Å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —è–∑—ã–∫–æ–≤ –∏ –Ω–∞–¥—ë–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ SoulX-Singer ‚Äî –≤—ã—Å
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#alignment", "#agents", "#reasoning", "#benchmark", "#open_source", "#training"], "emoji": "ü§ê", "ru": {"title": "–ú–æ–ª—á–∞–Ω–∏–µ –∑–æ–ª–æ—Ç–æ? –ü–æ—á–µ–º—É —è–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –æ—Å–ª–∞–±–ª—è–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∞–≥–µ–Ω—Ç–∞ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ —è–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ LLM –∞–≥–µ–Ω—Ç–∞—Ö, –≤–∑–∞–∏–º–æ
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#hallucinations", "#science", "#reasoning"], "emoji": "üî¨", "ru": {"title": "–ú–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ –≥—Ä–∞—Ñ–∞—Ö –∑–Ω–∞–Ω–∏–π –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, —É–ø—Ä–∞–≤–ª—è–µ–º–∞—è –≥—Ä–∞—Ñ–∞–º–∏ –∑–Ω–∞–Ω–∏–π, –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤–µ–¥–µ–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç 
[10.02.2026 10:47] Querying the API.
[10.02.2026 10:47] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Analysis of agentic system evaluation reveals significant variance in single-run performance estimates, necessitating multiple runs and advanced metrics for reliable assessment.  					AI-generated summary 				 Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.
[10.02.2026 10:47] Response: ```json
{
  "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, –∫–æ—Ç–æ—Ä—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á. –û–Ω–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é –¥–∏—Å–ø–µ—Ä—Å–∏—é –≤ –º–µ—Ç—Ä–∏–∫–µ pass@1, –≤—ã—á–∏—Å–ª–µ–Ω–Ω–æ–π –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–æ–≥–æ–Ω—É: –æ—Ü–µ–Ω–∫–∏ –≤–∞—Ä—å–∏—Ä—É—é—Ç—Å—è –Ω–∞ 2,2-6,0 –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –ø—É–Ω–∫—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ —Ä–∞—Å—Ö–æ–¥—è—Ç—Å—è –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö, –∏ –º–∞–ª—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ —Ç–æ–∫–µ–Ω–∞—Ö –ø—Ä–∏–≤–æ–¥—è—Ç –∫ —Ä–∞–∑–Ω—ã–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º —Ä–µ—à–µ–Ω–∏—è. –û–Ω–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –ø—Ä–æ–≥–æ–Ω–æ–≤, —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –º–æ—â–Ω–æ—Å—Ç–∏ –∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤—Ä–æ–¥–µ pass@k –∏ pass^k –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º.",
  "emoji": "üé≤",
  "title": "–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–≥–æ–Ω—ã –≤–º–µ—Å—Ç–æ –æ–¥–Ω–æ–≥–æ: –ø—É—Ç—å –∫ –Ω–∞–¥—ë–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–µ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º"
}
```
[10.02.2026 10:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Analysis of agentic system evaluation reveals significant variance in single-run performance estimates, necessitating multiple runs and advanced metrics for reliable assessment.  					AI-generated summary 				 Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise."

[10.02.2026 10:47] Response: ```python
["AGENTS", "BENCHMARK"]
```
[10.02.2026 10:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Analysis of agentic system evaluation reveals significant variance in single-run performance estimates, necessitating multiple runs and advanced metrics for reliable assessment.  					AI-generated summary 				 Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise."

[10.02.2026 10:47] Response: ```python
["OPTIMIZATION"]
```

The paper focuses on evaluation methodology and statistical reliability of agentic system performance assessment, which relates to optimization of evaluation practices and understanding performance metrics. However, while the paper discusses agentic systems, it does not substantively address AGI concepts, alignment, reasoning enhancement, or other listed topics. The core contribution is about improving evaluation methodology through multiple runs and statistical analysis rather than advancing the capabilities or alignment of these systems.
[10.02.2026 10:47] Error. Failed to parse JSON from LLM. ["OPTIMIZATION"]


The paper focuses on evaluation methodology and statistical reliability of agentic system performance assessment, which relates to optimization of evaluation practices and understanding performance metrics. However, while the paper discusses agentic systems, it does not substantively address AGI concepts, alignment, reasoning enhancement, or other listed topics. The core contribution is about improving evaluation methodology through multiple runs and statistical analysis rather than advancing the capabilities or alignment of these systems.
[10.02.2026 10:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper analyzes the evaluation methods used for agentic systems, which are AI agents that interact with environments to complete tasks. It highlights that relying on a single run to report performance scores can lead to misleading results due to significant variance in outcomes. The authors conducted extensive testing, revealing that single-run performance estimates can vary by several percentage points, indicating that reported improvements may not reflect true advancements. To improve evaluation reliability, they recommend conducting multiple runs, using statistical analysis to determine necessary run counts, and employing alternative metrics to capture a broader performance range.","title":"Enhancing Evaluation Reliability in Agentic Systems"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper analyzes the evaluation methods used for agentic systems, which are AI agents that interact with environments to complete tasks. It highlights that relying on a single run to report performance scores can lead to misleading results due to significant variance in outcomes. The authors conducted extensive testing, revealing that single-run performance estimates can vary by several percentage points, indicating that reported improvements may not reflect true advancements. To improve evaluation reliability, they recommend conducting multiple runs, using statistical analysis to determine necessary run counts, and employing alternative metrics to capture a broader performance range.', title='Enhancing Evaluation Reliability in Agentic Systems'))
[10.02.2026 10:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫ÊñáÂàÜÊûê‰∫Ü‰ª£ÁêÜÁ≥ªÁªüËØÑ‰º∞‰∏≠ÁöÑÊòæËëóÊñπÂ∑ÆÔºåÊåáÂá∫ÂçïÊ¨°ËøêË°åÁöÑÊÄßËÉΩ‰º∞ËÆ°‰∏çÂèØÈù†ÔºåÂõ†Ê≠§ÈúÄË¶ÅÂ§öÊ¨°ËøêË°åÂíåÂÖàËøõÁöÑËØÑ‰º∞ÊåáÊ†á„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂçïÊ¨°ËøêË°åÁöÑÈÄöËøáÁéáÔºàpass@1ÔºâÂú®‰∏çÂêåËøêË°å‰πãÈó¥ÁöÑÂ∑ÆÂºÇÂèØËææ2.2Âà∞6.0‰∏™ÁôæÂàÜÁÇπÔºåÊ†áÂáÜÂ∑ÆË∂ÖËøá1.5‰∏™ÁôæÂàÜÁÇπÔºåËøôË°®ÊòéÊä•ÂëäÁöÑÊîπËøõÂèØËÉΩÂè™ÊòØËØÑ‰º∞Âô™Â£∞ÔºåËÄåÈùûÁúüÊ≠£ÁöÑÁÆóÊ≥ïËøõÊ≠•„ÄÇÈÄöËøáÂØπ‰ª£ÁêÜËΩ®ËøπÁöÑÂàÜÊûêÔºåÂèëÁé∞Ëøô‰∫õËΩ®ËøπÂú®ÊúÄÂàùÁöÑÂá†‰∏™token‰∏≠Â∞±ÂºÄÂßãÂàÜÊ≠ßÔºåËøô‰∫õÂæÆÂ∞èÁöÑÂ∑ÆÂºÇ‰ºöÂØºËá¥‰∏çÂêåÁöÑËß£ÂÜ≥Á≠ñÁï•„ÄÇ‰∏∫Á°Æ‰øù‰ª£ÁêÜÁ≥ªÁªüÁöÑÂèØÈù†ËØÑ‰º∞ÔºåÂª∫ËÆÆÈááÁî®Â§öÊ¨°Áã¨Á´ãËøêË°å„ÄÅÁªüËÆ°ÂäüÊïàÂàÜÊûê‰ª•Âèä‰ΩøÁî®Êõ¥ÂÖ®Èù¢ÁöÑËØÑ‰º∞ÊåáÊ†á„ÄÇ","title":"ÊèêÈ´ò‰ª£ÁêÜÁ≥ªÁªüËØÑ‰º∞ÁöÑÂèØÈù†ÊÄß"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫ÊñáÂàÜÊûê‰∫Ü‰ª£ÁêÜÁ≥ªÁªüËØÑ‰º∞‰∏≠ÁöÑÊòæËëóÊñπÂ∑ÆÔºåÊåáÂá∫ÂçïÊ¨°ËøêË°åÁöÑÊÄßËÉΩ‰º∞ËÆ°‰∏çÂèØÈù†ÔºåÂõ†Ê≠§ÈúÄË¶ÅÂ§öÊ¨°ËøêË°åÂíåÂÖàËøõÁöÑËØÑ‰º∞ÊåáÊ†á„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂçïÊ¨°ËøêË°åÁöÑÈÄöËøáÁéáÔºàpass@1ÔºâÂú®‰∏çÂêåËøêË°å‰πãÈó¥ÁöÑÂ∑ÆÂºÇÂèØËææ2.2Âà∞6.0‰∏™ÁôæÂàÜÁÇπÔºåÊ†áÂáÜÂ∑ÆË∂ÖËøá1.5‰∏™ÁôæÂàÜÁÇπÔºåËøôË°®ÊòéÊä•ÂëäÁöÑÊîπËøõÂèØËÉΩÂè™ÊòØËØÑ‰º∞Âô™Â£∞ÔºåËÄåÈùûÁúüÊ≠£ÁöÑÁÆóÊ≥ïËøõÊ≠•„ÄÇÈÄöËøáÂØπ‰ª£ÁêÜËΩ®ËøπÁöÑÂàÜÊûêÔºåÂèëÁé∞Ëøô‰∫õËΩ®ËøπÂú®ÊúÄÂàùÁöÑÂá†‰∏™token‰∏≠Â∞±ÂºÄÂßãÂàÜÊ≠ßÔºåËøô‰∫õÂæÆÂ∞èÁöÑÂ∑ÆÂºÇ‰ºöÂØºËá¥‰∏çÂêåÁöÑËß£ÂÜ≥Á≠ñÁï•„ÄÇ‰∏∫Á°Æ‰øù‰ª£ÁêÜÁ≥ªÁªüÁöÑÂèØÈù†ËØÑ‰º∞ÔºåÂª∫ËÆÆÈááÁî®Â§öÊ¨°Áã¨Á´ãËøêË°å„ÄÅÁªüËÆ°ÂäüÊïàÂàÜÊûê‰ª•Âèä‰ΩøÁî®Êõ¥ÂÖ®Èù¢ÁöÑËØÑ‰º∞ÊåáÊ†á„ÄÇ', title='ÊèêÈ´ò‰ª£ÁêÜÁ≥ªÁªüËØÑ‰º∞ÁöÑÂèØÈù†ÊÄß'))
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#security", "#leakage"], "emoji": "üîê", "ru": {"title": "–£–º–Ω–∞—è –∑–∞—â–∏—Ç–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω—ã–π —à—É–º –≤–º–µ—Å—Ç–æ —Å–ª–µ–ø–æ–≥–æ", "desc": "SPARSE ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∑–∞—â–∏—â–∞—é—â–∏–π —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ—Ç —É—Ç–µ—á–µ–∫ –ø—Ä–∏–≤–∞—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—É—Ç—ë–º –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–≥–æ –≤–æ–∑–º—É—â–µ–Ω–∏—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#video", "#hallucinations", "#audio", "#benchmark", "#rlhf", "#open_source", "#training"], "emoji": "üòä", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π –±–µ–∑ –ª–æ–∂–Ω—ã—Ö –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –±–µ–Ω—á–º–∞—Ä–∫ EmoReA
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#reasoning"], "emoji": "üéõÔ∏è", "ru": {"title": "–ù–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—ã–µ –∞–¥–∞–ø—Ç–µ—Ä—ã –≤–º–µ—Å—Ç–æ –ø–æ–ª–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ —Å–º–µ—à–∞–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ FlexMoRE, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—ã–µ –∞–¥–∞–ø—Ç–µ—Ä—ã –≤–º–µ—Å—Ç–æ –ø–æ–ª–Ω–æ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤
[10.02.2026 10:47] Using data from previous issue: {"categories": ["#science", "#training", "#optimization", "#open_source", "#agents", "#plp"], "emoji": "üî¨", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞—É—á–Ω–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ —á–µ—Ä–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º", "desc": "Aster ‚Äî —ç—Ç–æ AI-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–≥—Ä–∞–º
[10.02.2026 10:47] Renaming data file.
[10.02.2026 10:47] Renaming previous data. hf_papers.json to ./d/2026-02-10.json
[10.02.2026 10:47] Saving new data file.
[10.02.2026 10:47] Generating page.
[10.02.2026 10:47] Renaming previous page.
[10.02.2026 10:47] Renaming previous data. index.html to ./d/2026-02-10.html
[10.02.2026 10:47] Writing result.
[10.02.2026 10:47] Renaming log file.
[10.02.2026 10:47] Renaming previous data. log.txt to ./logs/2026-02-10_last_log.txt
