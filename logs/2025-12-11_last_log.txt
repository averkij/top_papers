[11.12.2025 14:24] Read previous papers.
[11.12.2025 14:24] Generating top page (month).
[11.12.2025 14:24] Writing top page (month).
[11.12.2025 15:27] Read previous papers.
[11.12.2025 15:27] Get feed.
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09363
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.08560
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09824
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09247
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09928
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.08829
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.02892
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.04753
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09864
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09164
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09663
[11.12.2025 15:27] Extract page data from URL. URL: https://huggingface.co/papers/2512.09616
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09106
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.05446
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.08006
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.04519
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.07222
[11.12.2025 15:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.01453
[11.12.2025 15:27] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.12.2025 15:27] No deleted papers detected.
[11.12.2025 15:27] Downloading and parsing papers (pdf, html). Total: 18.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.09363.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.09363.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.09363.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.08560.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.08560.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.08560.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.09824.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.09824.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.09824.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.09247.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.09247.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.09247.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.09928.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.09928.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.09928.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.08829.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.08829.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.08829.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.02892.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.02892.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.02892.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.04753.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.04753.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.04753.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.09864.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.09864.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.09864.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.09164.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.09164.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.09164.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.09663.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.09663.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.09663.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.09616.
[11.12.2025 15:27] Downloading paper 2512.09616 from https://arxiv.org/pdf/2512.09616v1...
[11.12.2025 15:27] Extracting affiliations from text.
[11.12.2025 15:27] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Rethinking Chain-of-Thought Reasoning for Videos Yiwu Zhong1, Zi-Yuan Hu1, Yin Li2, Liwei Wang1* 1The Chinese University of Hong Kong, 2University of Wisconsin-Madison 5 2 0 2 0 1 ] . [ 1 6 1 6 9 0 . 2 1 5 2 : r a "
[11.12.2025 15:27] Response: ```python
[
    "The Chinese University of Hong Kong",
    "University of Wisconsin-Madison"
]
```
[11.12.2025 15:27] Deleting PDF ./assets/pdf/2512.09616.pdf.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.09106.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.09106.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.09106.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.05446.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.05446.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.05446.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.08006.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.08006.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.08006.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.04519.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.04519.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.04519.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.07222.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.07222.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.07222.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Downloading and parsing paper https://huggingface.co/papers/2512.01453.
[11.12.2025 15:27] Extra JSON file exists (./assets/json/2512.01453.json), skip PDF parsing.
[11.12.2025 15:27] Paper image links file exists (./assets/img_data/2512.01453.json), skip HTML parsing.
[11.12.2025 15:27] Success.
[11.12.2025 15:27] Enriching papers with extra data.
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 0. StereoWorld generates high-quality stereo video from monocular input using a pretrained video generator with geometry-aware regularization and spatio-temporal tiling.  					AI-generated summary 				 The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its pr...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 1. An automated framework uses unsupervised decomposition and natural language descriptions to identify and explain visual representations in human brain fMRI data.  					AI-generated summary 				 Understanding how the human brain represents visual concepts, and in which brain regions these representat...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 2. Bind & Compose uses Diffusion Transformers with hierarchical binders and temporal strategies to accurately compose complex visual concepts from images and videos.  					AI-generated summary 				 Visual concept composition, which aims to integrate different elements from images and videos into a sing...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 3. OmniPSD, a diffusion framework within the Flux ecosystem, enables text-to-PSD generation and image-to-PSD decomposition, achieving high-fidelity results with transparency awareness.  					AI-generated summary 				 Recent advances in diffusion models have greatly improved image generation and editing...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 4. HiF-VLA integrates motion for bidirectional temporal reasoning in VLA models, improving long-horizon manipulation performance with minimal additional latency.  					AI-generated summary 				 Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and lingui...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 5. InfiniteVL, a linear-complexity VLM architecture combining sliding window attention and Gated DeltaNet, achieves competitive performance with less data and faster inference than leading Transformer-based models.  					AI-generated summary 				 Window attention and linear attention represent two prin...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 6. SchED, a training-free early-exit algorithm, accelerates diffusion large language model decoding with minimal performance loss across various tasks.  					AI-generated summary 				 Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical uti...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 7. A novel knowledge editing framework, Edit-then-Consolidate, addresses overfitting and lack of knowledge integration in large language models through targeted fine-tuning and policy optimization, enhancing reliability and generalization.  					AI-generated summary 				 Knowledge editing aims to updat...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 8. A unified framework combines vision-language models and video generation to improve autonomous driving in complex scenarios by enhancing reasoning, trajectory planning, and video generation.  					AI-generated summary 				 Autonomous driving (AD) systems struggle in long-tail scenarios due to limite...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 9. WonderZoom generates multi-scale 3D scenes from a single image using scale-adaptive Gaussian surfels and a progressive detail synthesizer, outperforming existing models in quality and alignment.  					AI-generated summary 				 We present WonderZoom, a novel approach to generating 3D scenes with cont...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 10. The introduction of IF-Bench evaluates multimodal large language models' performance on infrared images using diverse assessment strategies and proposes a training-free method to improve comprehension.  					AI-generated summary 				 Recent advances in multimodal large language models (MLLMs) have l...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 11. Efficient video reasoning can be achieved using concise chains of thought and reduced visual tokens without manual annotations or supervised fine-tuning.  					AI-generated summary 				 Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processin...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 12. Reinforcement learning is used to train sampling procedures for masked discrete diffusion language models, improving token throughput and quality compared to heuristic strategies.  					AI-generated summary 				 Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their ...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 13. TED-4DGS combines sparse anchor-based 3D Gaussian Splatting with temporal activation and embedding for efficient and high-quality compression of dynamic 3D scenes.  					AI-generated summary 				 Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 14. A framework is proposed to improve phonemization quality in TTS systems without sacrificing real-time performance through lightweight context-aware phonemization and a service-oriented architecture.  					AI-generated summary 				 Lightweight, real-time text-to-speech systems are crucial for accessi...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 15. VideoSSM, a hybrid state-space memory model combining AR diffusion, achieves state-of-the-art temporal consistency and motion stability in long-video generation by coordinating short- and long-term context.  					AI-generated summary 				 Autoregressive (AR) diffusion enables streaming, interactive ...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 16. Function-word De-Attention (FDA) mitigates adversarial attacks on robust VLMs by differentially subtracting function-word cross-attention, improving robustness with minimal performance trade-offs.  					AI-generated summary 				 To address the trade-off between robustness and performance for robust ...
[11.12.2025 15:27] ********************************************************************************
[11.12.2025 15:27] Abstract 17. The survey analyzes the cognitive architecture of medical AI systems, focusing on the shift from generative text prediction to agentic autonomy, and categorizes methods into four archetypes based on knowledge source and agency objective.  					AI-generated summary 				 Clinical dialogue represents a...
[11.12.2025 15:27] Read previous papers.
[11.12.2025 15:27] Generating reviews via LLM API.
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#video", "#dataset", "#3d", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–û—Ç –æ–¥–Ω–æ–≥–æ –≥–ª–∞–∑–∞ –∫ –¥–≤—É–º: —Å–∏–Ω—Ç–µ–∑ —Å—Ç–µ—Ä–µ–æ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é", "desc": "StereoWorld ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –æ–¥–Ω–æ–≥–ª–∞–∑–Ω–æ–≥–æ –≤–∏–¥–µ–æ –≤ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å—Ç–µ—Ä–µ–æ –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–≤–∞—Ä
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#interpretability"], "emoji": "üß†", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–∞ –º–æ–∑–≥–∞ —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –∏ —è–∑—ã–∫", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö fMRI —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –º–æ–∑–≥–∞. –ú
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#video", "#multimodal", "#diffusion", "#architecture", "#cv"], "emoji": "üé®", "ru": {"title": "–ö–æ–º–ø–æ–∑–∏—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π —á–µ—Ä–µ–∑ –ø—Ä–∏–≤—è–∑–∫—É —Ç–æ–∫–µ–Ω–æ–≤ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Bind & Compose, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Diffusion Transformers —Å –∏–µ—Ä–∞—Ä—Ö–∏
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#diffusion", "#dataset", "#cv"], "emoji": "üé®", "ru": {"title": "–û—Ç —Ç–µ–∫—Å—Ç–∞ –∫ —Å–ª–æ—è–º: –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ PSD —Å –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å—é", "desc": "OmniPSD ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –±–∞–∑–µ Flux, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –∑–∞–¥–∞—á–∏:
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#benchmark", "#robotics", "#architecture", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–í–æ—Å–ø—Ä–∏—è—Ç–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤ —Ä–æ–±–æ—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç HiF-VLA ‚Äî –º–æ–¥–µ–ª—å –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ-–¥–µ–π—Å—Ç–≤–∏–π–Ω–æ–≥–æ —Ç–∏–ø–∞ (VLA), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –º
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#open_source", "#inference", "#architecture", "#long_context", "#optimization", "#video", "#multimodal"], "emoji": "‚ö°", "ru": {"title": "–ë–µ—Å–∫–æ–Ω–µ—á–Ω–∞—è –ø–∞–º—è—Ç—å –ø—Ä–∏ –ª–∏–Ω–µ–π–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: –Ω–æ–≤–∞—è —ç—Ä–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è InfiniteVL ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#training", "#inference", "#diffusion", "#machine_translation", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ —É–º–Ω—É—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É", "desc": "SchED ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏ —Ä–∞–±–æ—Ç–∞
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#optimization", "#training"], "emoji": "‚úèÔ∏è", "ru": {"title": "–û—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏: –Ω–∞–¥–µ–∂–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –≤ LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π Edit-then-Consolidate, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏ 
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#dataset", "#training", "#video", "#robotics", "#multimodal"], "emoji": "üöó", "ru": {"title": "–ï–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∞–º–æ–≤–æ–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç UniUGP ‚Äî —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#3d", "#cv"], "emoji": "üîç", "ru": {"title": "–û—Ç –º–∏–∫—Ä–æ –∫ –º–∞–∫—Ä–æ: –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ 3D-–º–∏—Ä–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ —Å–Ω–∏–º–∫–∞", "desc": "WonderZoom –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω —Å –¥–µ—Ç–∞–ª—è–º–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ö–ª—é—á–µ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–Ω–æ-–∞–¥–∞–ø—Ç–∏–≤–Ω—ã
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#dataset", "#training", "#benchmark", "#open_source", "#survey", "#multimodal"], "emoji": "üå°Ô∏è", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–Ω—Ñ—Ä–∞–∫—Ä–∞—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –±–µ–Ω—á–º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –ø–æ–¥—Å–∫–∞–∑—ã–≤–∞–Ω–∏–µ", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç IF-Bench ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä
[11.12.2025 15:27] Querying the API.
[11.12.2025 15:27] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Efficient video reasoning can be achieved using concise chains of thought and reduced visual tokens without manual annotations or supervised fine-tuning.  					AI-generated summary 				 Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.
[11.12.2025 15:27] Response: ```json
{
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–∏–¥–µ–æ—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—Ä–∞—Ç–∫–∏–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —Å–∂–∞—Ç—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Å–æ–∫—Ä–∞—â—ë–Ω–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Ü–µ–ø–æ—á–∫–∏ –º—ã—à–ª–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º. –ü–æ–¥—Ö–æ–¥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏–ª–∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –¥–ª–∏–Ω–Ω—ã–µ, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–µ —è–≤–ª—è—é—Ç—Å—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∞ –∫—Ä–∞—Ç–∫–∏–µ —Ü–µ–ø–æ—á–∫–∏ –º—ã—Å–ª–µ–π –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –∫–∞–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, —Ç–∞–∫ –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.",
  "emoji": "üé¨",
  "title": "–ö—Ä–∞—Ç–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –¥–ª–∏–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–∏–¥–µ–æ–∞–Ω–∞–ª–∏–∑–∞"
}
```
[11.12.2025 15:27] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Efficient video reasoning can be achieved using concise chains of thought and reduced visual tokens without manual annotations or supervised fine-tuning.  					AI-generated summary 				 Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video."

[11.12.2025 15:27] Response: ```python
['VIDEO', 'MULTIMODAL', 'INFERENCE', 'TRAINING']
```
[11.12.2025 15:27] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Efficient video reasoning can be achieved using concise chains of thought and reduced visual tokens without manual annotations or supervised fine-tuning.  					AI-generated summary 				 Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video."

[11.12.2025 15:27] Response: ```python
['REASONING', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[11.12.2025 15:27] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new approach to video reasoning that emphasizes the use of concise chains of thought and fewer visual tokens. The authors argue that traditional methods, which rely on lengthy reasoning and numerous visual inputs, can be improved by simplifying these processes. They introduce a framework that allows video models to work with compressed visual tokens and generate shorter reasoning paths, enhancing efficiency without the need for manual annotations or extensive fine-tuning. The results show that this streamlined approach maintains competitive performance while significantly improving inference speed.","title":"Streamlining Video Reasoning with Concise Thought Chains"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new approach to video reasoning that emphasizes the use of concise chains of thought and fewer visual tokens. The authors argue that traditional methods, which rely on lengthy reasoning and numerous visual inputs, can be improved by simplifying these processes. They introduce a framework that allows video models to work with compressed visual tokens and generate shorter reasoning paths, enhancing efficiency without the need for manual annotations or extensive fine-tuning. The results show that this streamlined approach maintains competitive performance while significantly improving inference speed.', title='Streamlining Video Reasoning with Concise Thought Chains'))
[11.12.2025 15:27] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÈ´òÊïàÁöÑËßÜÈ¢ëÊé®ÁêÜÊñπÊ≥ïÔºåÂà©Áî®ÁÆÄÊ¥ÅÁöÑÊé®ÁêÜÈìæÂíåÂáèÂ∞ëÁöÑËßÜËßâÊ†áËÆ∞ÔºåËÄåÊó†ÈúÄÊâãÂä®Ê≥®ÈáäÊàñÁõëÁù£ÂæÆË∞É„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁÆÄÊ¥ÅÁöÑÊé®ÁêÜÁªìÂêàËæÉÂ∞ëÁöÑËßÜËßâËæìÂÖ•ÂèØ‰ª•ÊúâÊïàÂú∞ËøõË°åËßÜÈ¢ëÊé®ÁêÜ„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÈ´òÊïàÁöÑÂêéËÆ≠ÁªÉÂíåÊé®ÁêÜÊ°ÜÊû∂ÔºåÊèêÂçá‰∫ÜËßÜÈ¢ëÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËØ•ÊñπÊ≥ïÂú®Êé®ÁêÜÊïàÁéá‰∏äÊòæËëóÊèêÈ´òÔºåÂπ∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Á´û‰∫âÂäõ„ÄÇ","title":"ÁÆÄÊ¥ÅÊé®ÁêÜÔºåÊèêÂçáËßÜÈ¢ëÁêÜËß£ÊïàÁéá"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÈ´òÊïàÁöÑËßÜÈ¢ëÊé®ÁêÜÊñπÊ≥ïÔºåÂà©Áî®ÁÆÄÊ¥ÅÁöÑÊé®ÁêÜÈìæÂíåÂáèÂ∞ëÁöÑËßÜËßâÊ†áËÆ∞ÔºåËÄåÊó†ÈúÄÊâãÂä®Ê≥®ÈáäÊàñÁõëÁù£ÂæÆË∞É„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁÆÄÊ¥ÅÁöÑÊé®ÁêÜÁªìÂêàËæÉÂ∞ëÁöÑËßÜËßâËæìÂÖ•ÂèØ‰ª•ÊúâÊïàÂú∞ËøõË°åËßÜÈ¢ëÊé®ÁêÜ„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÈ´òÊïàÁöÑÂêéËÆ≠ÁªÉÂíåÊé®ÁêÜÊ°ÜÊû∂ÔºåÊèêÂçá‰∫ÜËßÜÈ¢ëÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËØ•ÊñπÊ≥ïÂú®Êé®ÁêÜÊïàÁéá‰∏äÊòæËëóÊèêÈ´òÔºåÂπ∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Á´û‰∫âÂäõ„ÄÇ', title='ÁÆÄÊ¥ÅÊé®ÁêÜÔºåÊèêÂçáËßÜÈ¢ëÁêÜËß£ÊïàÁéá'))
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#optimization", "#training", "#diffusion", "#architecture", "#rl", "#transfer_learning"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#optimization", "#video", "#3d"], "emoji": "üé¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D —Å—Ü–µ–Ω —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—É—é –∞–∫—Ç–∏–≤–∞—Ü–∏—é –∏ –¥–µ—Ñ–æ—Ä–º–∞—Ü–∏–∏ —è–∫–æ—Ä–µ–π", "desc": "TED-4DGS –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D —Å—Ü–µ–Ω, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —è–∫–æ—Ä–µ–π 3D Gaussian 
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#small_models", "#architecture", "#audio", "#inference"], "emoji": "üé§", "ru": {"title": "–í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ñ–æ–Ω–µ–º–∏–∑–∞—Ü–∏–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —Å–∏—Å—Ç–µ–º —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞ –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º —Ñ–æ–Ω–µ–º–∏–∑–∞—Ü–∏–∏ –∏ —Å
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#video", "#diffusion", "#architecture", "#long_context", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –±–µ–∑ –∏—Å–∫–∞–∂–µ–Ω–∏–π", "desc": "VideoSSM ‚Äî —ç—Ç–æ –≥–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ state-space memory, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é –¥–ª—è
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#security", "#open_source"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–ª–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Function-word De-Attention (FDA) –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π vision-language –∫ adve
[11.12.2025 15:27] Using data from previous issue: {"categories": ["#healthcare", "#reasoning", "#agents", "#architecture", "#science", "#survey"], "emoji": "üè•", "ru": {"title": "–û—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–º –∞–≥–µ–Ω—Ç–∞–º: –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–∞–¥—ë–∂–Ω–æ–≥–æ AI –≤ –∫–ª–∏–Ω–∏–∫–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö AI —Å–∏—Å—Ç–µ–º –∏
[11.12.2025 15:27] Renaming data file.
[11.12.2025 15:27] Renaming previous data. hf_papers.json to ./d/2025-12-11.json
[11.12.2025 15:27] Saving new data file.
[11.12.2025 15:27] Generating page.
[11.12.2025 15:27] Renaming previous page.
[11.12.2025 15:27] Renaming previous data. index.html to ./d/2025-12-11.html
[11.12.2025 15:27] Writing result.
[11.12.2025 15:27] Renaming log file.
[11.12.2025 15:27] Renaming previous data. log.txt to ./logs/2025-12-11_last_log.txt
