[11.12.2025 13:41] Read previous papers.
[11.12.2025 13:41] Generating top page (month).
[11.12.2025 13:41] Writing top page (month).
[11.12.2025 14:24] Read previous papers.
[11.12.2025 14:24] Get feed.
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09363
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.08560
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09824
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09928
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09247
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.08829
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.02892
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09864
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.04753
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09663
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09164
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09106
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.08006
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.05446
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.04519
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.07222
[11.12.2025 14:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.01453
[11.12.2025 14:24] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.12.2025 14:24] No deleted papers detected.
[11.12.2025 14:24] Downloading and parsing papers (pdf, html). Total: 17.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.09363.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.09363.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.09363.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.08560.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.08560.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.08560.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.09824.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.09824.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.09824.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.09928.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.09928.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.09928.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.09247.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.09247.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.09247.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.08829.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.08829.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.08829.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.02892.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.02892.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.02892.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.09864.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.09864.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.09864.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.04753.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.04753.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.04753.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.09663.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.09663.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.09663.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.09164.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.09164.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.09164.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.09106.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.09106.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.09106.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.08006.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.08006.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.08006.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.05446.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.05446.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.05446.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.04519.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.04519.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.04519.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.07222.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.07222.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.07222.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Downloading and parsing paper https://huggingface.co/papers/2512.01453.
[11.12.2025 14:24] Extra JSON file exists (./assets/json/2512.01453.json), skip PDF parsing.
[11.12.2025 14:24] Paper image links file exists (./assets/img_data/2512.01453.json), skip HTML parsing.
[11.12.2025 14:24] Success.
[11.12.2025 14:24] Enriching papers with extra data.
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 0. StereoWorld generates high-quality stereo video from monocular input using a pretrained video generator with geometry-aware regularization and spatio-temporal tiling.  					AI-generated summary 				 The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its pr...
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 1. An automated framework uses unsupervised decomposition and natural language descriptions to identify and explain visual representations in human brain fMRI data.  					AI-generated summary 				 Understanding how the human brain represents visual concepts, and in which brain regions these representat...
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 2. Bind & Compose uses Diffusion Transformers with hierarchical binders and temporal strategies to accurately compose complex visual concepts from images and videos.  					AI-generated summary 				 Visual concept composition, which aims to integrate different elements from images and videos into a sing...
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 3. HiF-VLA integrates motion for bidirectional temporal reasoning in VLA models, improving long-horizon manipulation performance with minimal additional latency.  					AI-generated summary 				 Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and lingui...
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 4. OmniPSD, a diffusion framework within the Flux ecosystem, enables text-to-PSD generation and image-to-PSD decomposition, achieving high-fidelity results with transparency awareness.  					AI-generated summary 				 Recent advances in diffusion models have greatly improved image generation and editing...
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 5. InfiniteVL, a linear-complexity VLM architecture combining sliding window attention and Gated DeltaNet, achieves competitive performance with less data and faster inference than leading Transformer-based models.  					AI-generated summary 				 Window attention and linear attention represent two prin...
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 6. SchED, a training-free early-exit algorithm, accelerates diffusion large language model decoding with minimal performance loss across various tasks.  					AI-generated summary 				 Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical uti...
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 7. A unified framework combines vision-language models and video generation to improve autonomous driving in complex scenarios by enhancing reasoning, trajectory planning, and video generation.  					AI-generated summary 				 Autonomous driving (AD) systems struggle in long-tail scenarios due to limite...
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 8. A novel knowledge editing framework, Edit-then-Consolidate, addresses overfitting and lack of knowledge integration in large language models through targeted fine-tuning and policy optimization, enhancing reliability and generalization.  					AI-generated summary 				 Knowledge editing aims to updat...
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 9. The introduction of IF-Bench evaluates multimodal large language models' performance on infrared images using diverse assessment strategies and proposes a training-free method to improve comprehension.  					AI-generated summary 				 Recent advances in multimodal large language models (MLLMs) have l...
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 10. WonderZoom generates multi-scale 3D scenes from a single image using scale-adaptive Gaussian surfels and a progressive detail synthesizer, outperforming existing models in quality and alignment.  					AI-generated summary 				 We present WonderZoom, a novel approach to generating 3D scenes with cont...
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 11. Reinforcement learning is used to train sampling procedures for masked discrete diffusion language models, improving token throughput and quality compared to heuristic strategies.  					AI-generated summary 				 Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their ...
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 12. A framework is proposed to improve phonemization quality in TTS systems without sacrificing real-time performance through lightweight context-aware phonemization and a service-oriented architecture.  					AI-generated summary 				 Lightweight, real-time text-to-speech systems are crucial for accessi...
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 13. TED-4DGS combines sparse anchor-based 3D Gaussian Splatting with temporal activation and embedding for efficient and high-quality compression of dynamic 3D scenes.  					AI-generated summary 				 Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension...
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 14. VideoSSM, a hybrid state-space memory model combining AR diffusion, achieves state-of-the-art temporal consistency and motion stability in long-video generation by coordinating short- and long-term context.  					AI-generated summary 				 Autoregressive (AR) diffusion enables streaming, interactive ...
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 15. Function-word De-Attention (FDA) mitigates adversarial attacks on robust VLMs by differentially subtracting function-word cross-attention, improving robustness with minimal performance trade-offs.  					AI-generated summary 				 To address the trade-off between robustness and performance for robust ...
[11.12.2025 14:24] ********************************************************************************
[11.12.2025 14:24] Abstract 16. The survey analyzes the cognitive architecture of medical AI systems, focusing on the shift from generative text prediction to agentic autonomy, and categorizes methods into four archetypes based on knowledge source and agency objective.  					AI-generated summary 				 Clinical dialogue represents a...
[11.12.2025 14:24] Read previous papers.
[11.12.2025 14:24] Generating reviews via LLM API.
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#video", "#dataset", "#3d", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–û—Ç –æ–¥–Ω–æ–≥–æ –≥–ª–∞–∑–∞ –∫ –¥–≤—É–º: —Å–∏–Ω—Ç–µ–∑ —Å—Ç–µ—Ä–µ–æ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é", "desc": "StereoWorld ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –æ–¥–Ω–æ–≥–ª–∞–∑–Ω–æ–≥–æ –≤–∏–¥–µ–æ –≤ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å—Ç–µ—Ä–µ–æ –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–≤–∞—Ä
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#interpretability"], "emoji": "üß†", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–∞ –º–æ–∑–≥–∞ —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –∏ —è–∑—ã–∫", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö fMRI —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –º–æ–∑–≥–∞. –ú
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#video", "#multimodal", "#diffusion", "#architecture", "#cv"], "emoji": "üé®", "ru": {"title": "–ö–æ–º–ø–æ–∑–∏—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π —á–µ—Ä–µ–∑ –ø—Ä–∏–≤—è–∑–∫—É —Ç–æ–∫–µ–Ω–æ–≤ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Bind & Compose, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Diffusion Transformers —Å –∏–µ—Ä–∞—Ä—Ö–∏
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#benchmark", "#robotics", "#architecture", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–í–æ—Å–ø—Ä–∏—è—Ç–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤ —Ä–æ–±–æ—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç HiF-VLA ‚Äî –º–æ–¥–µ–ª—å –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ-–¥–µ–π—Å—Ç–≤–∏–π–Ω–æ–≥–æ —Ç–∏–ø–∞ (VLA), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –º
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#diffusion", "#dataset", "#cv"], "emoji": "üé®", "ru": {"title": "–û—Ç —Ç–µ–∫—Å—Ç–∞ –∫ —Å–ª–æ—è–º: –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ PSD —Å –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å—é", "desc": "OmniPSD ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –±–∞–∑–µ Flux, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –∑–∞–¥–∞—á–∏:
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#open_source", "#inference", "#architecture", "#long_context", "#optimization", "#video", "#multimodal"], "emoji": "‚ö°", "ru": {"title": "–ë–µ—Å–∫–æ–Ω–µ—á–Ω–∞—è –ø–∞–º—è—Ç—å –ø—Ä–∏ –ª–∏–Ω–µ–π–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: –Ω–æ–≤–∞—è —ç—Ä–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è InfiniteVL ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#training", "#inference", "#diffusion", "#machine_translation", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ —É–º–Ω—É—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É", "desc": "SchED ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏ —Ä–∞–±–æ—Ç–∞
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#dataset", "#training", "#video", "#robotics", "#multimodal"], "emoji": "üöó", "ru": {"title": "–ï–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∞–º–æ–≤–æ–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç UniUGP ‚Äî —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#optimization", "#training"], "emoji": "‚úèÔ∏è", "ru": {"title": "–û—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏: –Ω–∞–¥–µ–∂–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –≤ LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π Edit-then-Consolidate, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏ 
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#dataset", "#training", "#benchmark", "#open_source", "#survey", "#multimodal"], "emoji": "üå°Ô∏è", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–Ω—Ñ—Ä–∞–∫—Ä–∞—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –±–µ–Ω—á–º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –ø–æ–¥—Å–∫–∞–∑—ã–≤–∞–Ω–∏–µ", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç IF-Bench ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#3d", "#cv"], "emoji": "üîç", "ru": {"title": "–û—Ç –º–∏–∫—Ä–æ –∫ –º–∞–∫—Ä–æ: –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ 3D-–º–∏—Ä–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ —Å–Ω–∏–º–∫–∞", "desc": "WonderZoom –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω —Å –¥–µ—Ç–∞–ª—è–º–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ö–ª—é—á–µ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–Ω–æ-–∞–¥–∞–ø—Ç–∏–≤–Ω—ã
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#optimization", "#training", "#diffusion", "#architecture", "#rl", "#transfer_learning"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#small_models", "#architecture", "#audio", "#inference"], "emoji": "üé§", "ru": {"title": "–í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ñ–æ–Ω–µ–º–∏–∑–∞—Ü–∏–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —Å–∏—Å—Ç–µ–º —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞ –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º —Ñ–æ–Ω–µ–º–∏–∑–∞—Ü–∏–∏ –∏ —Å
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#optimization", "#video", "#3d"], "emoji": "üé¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D —Å—Ü–µ–Ω —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—É—é –∞–∫—Ç–∏–≤–∞—Ü–∏—é –∏ –¥–µ—Ñ–æ—Ä–º–∞—Ü–∏–∏ —è–∫–æ—Ä–µ–π", "desc": "TED-4DGS –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D —Å—Ü–µ–Ω, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —è–∫–æ—Ä–µ–π 3D Gaussian 
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#video", "#diffusion", "#architecture", "#long_context", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –±–µ–∑ –∏—Å–∫–∞–∂–µ–Ω–∏–π", "desc": "VideoSSM ‚Äî —ç—Ç–æ –≥–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ state-space memory, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é –¥–ª—è
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#security", "#open_source"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–ª–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Function-word De-Attention (FDA) –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π vision-language –∫ adve
[11.12.2025 14:24] Using data from previous issue: {"categories": ["#healthcare", "#reasoning", "#agents", "#architecture", "#science", "#survey"], "emoji": "üè•", "ru": {"title": "–û—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–º –∞–≥–µ–Ω—Ç–∞–º: –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–∞–¥—ë–∂–Ω–æ–≥–æ AI –≤ –∫–ª–∏–Ω–∏–∫–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö AI —Å–∏—Å—Ç–µ–º –∏
[11.12.2025 14:24] Renaming data file.
[11.12.2025 14:24] Renaming previous data. hf_papers.json to ./d/2025-12-11.json
[11.12.2025 14:24] Saving new data file.
[11.12.2025 14:24] Generating page.
[11.12.2025 14:24] Renaming previous page.
[11.12.2025 14:24] Renaming previous data. index.html to ./d/2025-12-11.html
[11.12.2025 14:24] Writing result.
[11.12.2025 14:24] Renaming log file.
[11.12.2025 14:24] Renaming previous data. log.txt to ./logs/2025-12-11_last_log.txt
