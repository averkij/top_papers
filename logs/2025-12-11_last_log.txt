[11.12.2025 07:46] Read previous papers.
[11.12.2025 07:46] Generating top page (month).
[11.12.2025 07:46] Writing top page (month).
[11.12.2025 08:32] Read previous papers.
[11.12.2025 08:32] Get feed.
[11.12.2025 08:32] Extract page data from URL. URL: https://huggingface.co/papers/2512.09363
[11.12.2025 08:32] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09824
[11.12.2025 08:32] Extract page data from URL. URL: https://huggingface.co/papers/2512.08829
[11.12.2025 08:32] Extract page data from URL. URL: https://huggingface.co/papers/2512.09864
[11.12.2025 08:32] Extract page data from URL. URL: https://huggingface.co/papers/2512.09928
[11.12.2025 08:32] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09247
[11.12.2025 08:32] Get page data from previous paper. URL: https://huggingface.co/papers/2512.04753
[11.12.2025 08:32] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09106
[11.12.2025 08:32] Get page data from previous paper. URL: https://huggingface.co/papers/2512.02892
[11.12.2025 08:32] Extract page data from URL. URL: https://huggingface.co/papers/2512.09164
[11.12.2025 08:32] Extract page data from URL. URL: https://huggingface.co/papers/2512.08560
[11.12.2025 08:32] Extract page data from URL. URL: https://huggingface.co/papers/2512.08006
[11.12.2025 08:32] Get page data from previous paper. URL: https://huggingface.co/papers/2512.07222
[11.12.2025 08:32] Extract page data from URL. URL: https://huggingface.co/papers/2512.05446
[11.12.2025 08:32] Get page data from previous paper. URL: https://huggingface.co/papers/2512.04519
[11.12.2025 08:32] Get page data from previous paper. URL: https://huggingface.co/papers/2512.01453
[11.12.2025 08:32] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.12.2025 08:32] No deleted papers detected.
[11.12.2025 08:32] Downloading and parsing papers (pdf, html). Total: 16.
[11.12.2025 08:32] Downloading and parsing paper https://huggingface.co/papers/2512.09363.
[11.12.2025 08:32] Downloading paper 2512.09363 from https://arxiv.org/pdf/2512.09363v1...
[11.12.2025 08:32] Extracting affiliations from text.
[11.12.2025 08:32] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation Ke Xing1,2,4 Longfei Li1,4 Yuyang Yin1,4 Hanwen Liang3 Guixun Luo1,4 Chen Fang2 Jue Wang2 Konstantinos N. Plataniotis3 Xiaojie Jin1,4 Yao Zhao1,4 Yunchao Wei1,4, 1Beijing Jiaotong University 2Dzine AI 3University of Toronto 4Visual Intelligence + International Joint Laboratory Corresponding Author 5 2 0 D 0 1 ] . [ 1 3 6 3 9 0 . 2 1 5 2 : r Figure 1. Examples generated by StereoWorld. Our method directly generates stereo videos from arbitrary monocular videos without requiring additional information. The results can be displayed on 3D glasses, XR headsets, and other stereoscopic devices. "
[11.12.2025 08:32] Response: ```python
[
    "Beijing Jiaotong University",
    "Dzine AI",
    "University of Toronto",
    "Visual Intelligence + International Joint Laboratory"
]
```
[11.12.2025 08:32] Deleting PDF ./assets/pdf/2512.09363.pdf.
[11.12.2025 08:32] Success.
[11.12.2025 08:32] Downloading and parsing paper https://huggingface.co/papers/2512.09824.
[11.12.2025 08:32] Downloading paper 2512.09824 from https://arxiv.org/pdf/2512.09824v1...
[11.12.2025 08:32] Extracting affiliations from text.
[11.12.2025 08:32] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Composing Concepts from Images and Videos via Concept-prompt Binding Xianghao Kong1 , Zeyu Zhang1 , Yuwei Guo2 , Zhuoran Zhao1,3 , Songchun Zhang1 , Anyi Rao1 1 HKUST 2 CUHK 3 HKUST(GZ) https://refkxh.github.io/BiCo_Webpage 5 2 0 2 0 1 ] . [ 1 4 2 8 9 0 . 2 1 5 2 : r Figure 1. Illustration of BiCo, one-shot method that enables flexible visual concept composition by binding visual concepts with the corresponding prompt tokens and composing the target prompt with bound tokens from various sources (1). "
[11.12.2025 08:32] Response: ```python
['HKUST', 'CUHK', 'HKUST(GZ)']
```
[11.12.2025 08:32] Deleting PDF ./assets/pdf/2512.09824.pdf.
[11.12.2025 08:32] Success.
[11.12.2025 08:32] Downloading and parsing paper https://huggingface.co/papers/2512.08829.
[11.12.2025 08:32] Downloading paper 2512.08829 from https://arxiv.org/pdf/2512.08829v1...
[11.12.2025 08:32] Extracting affiliations from text.
[11.12.2025 08:32] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 ] . [ 1 9 2 8 8 0 . 2 1 5 2 : r InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models Hongyuan Tao1, Bencheng Liao1 Shaoyu Chen2 Haoran Yin2 Qian Zhang2 Wenyu Liu1 Xinggang Wang1 1Huazhong University of Science and Technology 2Horizon Robotics Code & Model & Demo: hustvl/InfiniteVL Figure 1. Efficiency and performance of InfiniteVL. Left: Under comparable performance, InfiniteVL significantly improves single-GPU training throughput per day, streaming FPS, inference cache usage, and per-token latency over Qwen2.5VL-3B. Right: Speedperformance trade-off among VLMs, where InfiniteVL achieves real-time 24 streaming FPS with competitive performance at similar model scale. All inference results are measured on single NVIDIA RTX 4090. "
[11.12.2025 08:32] Response: ```python
[
    "Huazhong University of Science and Technology",
    "Horizon Robotics"
]
```
[11.12.2025 08:32] Deleting PDF ./assets/pdf/2512.08829.pdf.
[11.12.2025 08:32] Success.
[11.12.2025 08:32] Downloading and parsing paper https://huggingface.co/papers/2512.09864.
[11.12.2025 08:32] Downloading paper 2512.09864 from https://arxiv.org/pdf/2512.09864v1...
[11.12.2025 08:32] Extracting affiliations from text.
[11.12.2025 08:32] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 1 ] . [ 1 4 6 8 9 0 . 2 1 5 2 : r UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving See Contributions section for full author list. "
[11.12.2025 08:32] Response: ```python
[]
```
[11.12.2025 08:32] Extracting affiliations from text.
[11.12.2025 08:32] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 1 ] . [ 1 4 6 8 9 0 . 2 1 5 2 : r UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous DrivingSee Contributions section for full author list.Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations. Project Page: https://seed-uniugp.github.io/ Date: December 11,Autonomous driving (AD) has recently made remarkable progress, especially in areas such as birds-eye view perception [27, 38, 42, 43, 46], end-to-end [8, 25, 30, 40], scene reconstruction [45, 82, 93, 100], and video generation [15, 44, 70, 75]. Recently, given the superior capabilities of multimodal large language models (MLLMs) in world knowledge, reasoning ability, and interpretability, they have been widely applied in AD [26, 34, 48, 90]. One promising direction is the end-to-end vision-language-action (VLA) model [37, 72, 102], which leverages pre-trained vision-language model (VLM) to directly extract scene features from visual observations and language instructions, subsequently generating vehicle control commands (e.g., speed and trajectory). This paradigm not only simplifies system architecture and minimizes information loss, but also enables the utilization of the models world knowledge to analyze driving environments and reason about safe decisions in complex scenarios [6, 31, 32, 68]. However, they were unable to fully utilize the large number of un-labeled driving videos, which limited their ability to learn visual causal reasoning from large-scale datasets. In addition to the advanced VLA technology, the world model can learn visual causal reasoning by predicting the next frame of the video [9, 23, 84, 91, 95]. The world model can learn visual causal reasoning by 1 Table 1 Comparison of Different Methods. "VLA" refers to the use of additional models to predict more accurate trajectories, which is different from VLMs text-based prediction. "Reason." refers to whether the model can generate chain of thoughts. "Inter." refers to whether the model can change its trajectory based on human instructions. "Cont." refers to whether the token is continuous or discrete value. "World Model" only lists the methods that can simultaneously generate future images and trajectories. "FM." means flow matching. Category Method Model Reason. Inter. Modality Cont. Method Cont. Understanding Generation Action World Model World Model OccWorld [95] Epona [91] - - VLM VLM VLM VLA VLA VLA VLA VLA DriveLM [58] Impr. VLA [10] OmniDrive [67] ReCogDrive [37] ORION [14] AutoVLA [102] DriveMoE [86] Alpamayo-R1 [72] Llama2 Qwen2.5-VL-3B LLaMA2-7B Qwen2.5-VL-3B Vicuna v1.5 InternVL3-8B π0 Cosmos-Reason Unified Model Doe-1 [97] Unified Model Occ-LLM [81] Unified Model OccLlama [74] Unified Model HERMES [101] Unified Model FSDrive [88] Lumina-mGPT Llava-7B Llama-7B InternVL2-2B Qwen2-VL-2B Unified Model UniUGP Qwen2.5-VL-3B Occ Video - - - - - - - - Video Occ Occ LiDAR Video Video - - - - - - - - Codebook FM. Text Text Text Diffusion VAE Codebook FM. FM. Text Text Text Text Text FM. predicting the next frame of the video, which has been proven to be helpful in achieving the final end-to-end AD [84, 88, 91]. But the world model is unable to match the world knowledge, reasoning ability, and interaction capability from large language models. Summaries of different methods are presented in Tab. 1. Unified models, which aim to bridge perception, reasoning, and action, can simultaneously combine the advantages of the world model and the VLA model. However, there are several additional issues here: 1) How to efficiently establish unified model to fully utilize the pre-trained VLM and world model. 2) How to effectively and repeatedly utilize various driving data (such as VQA pairs, video trajectory pairs, etc.) to fully exploit the potential of the unified model. 3) How to evaluate the capabilities of the unified model, especially in terms of understanding, reasoning, and planning in complex scenarios. To address these limitations, we propose UniUGP, unified UnderstandingGeneratePlanning framework for end-to-end AD that jointly models complex scene reasoning, future video generation, and trajectory planning. Built on hybrid expert architecture, UniUGP fully exploits the causal reasoning capabilities of pre-trained MLLMs and video generation models, while further enhancing cross-modal causal alignment through large-scale multimodal data training. Specifically, UniUGP takes natural language instructions and continuous image sequences as inputs, and outputs three complementary results: chain-of-thought (CoT) reasoning process for interpretability, physically consistent trajectory for safe driving, and coherent future video for visual causal validation. To ensure output consistency and accuracy, we design multi-term loss function that enforces CoT logical consistency, trajectory temporal smoothness, and video visual coherence. Moreover, we propose four-stage training framework that sequentially builds foundational scene understanding, visual dynamic modeling, text-based reasoning, and multi-capability fusion, leveraging over 10 diverse AD datasets to cover common scenarios and long-tailed cases. Our main contributions are summarized as follows: 1) We construct multiple s"
[11.12.2025 08:32] Mistral response. {"id": "6ca4335e64ad4ad5bcf363e0e49a3d35", "created": 1765441973, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1534, "total_tokens": 1540, "completion_tokens": 6}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}}]}
[11.12.2025 08:32] Response: ```python
[]
```
[11.12.2025 08:32] Deleting PDF ./assets/pdf/2512.09864.pdf.
[11.12.2025 08:32] Success.
[11.12.2025 08:32] Downloading and parsing paper https://huggingface.co/papers/2512.09928.
[11.12.2025 08:32] Downloading paper 2512.09928 from https://arxiv.org/pdf/2512.09928v1...
[11.12.2025 08:33] Extracting affiliations from text.
[11.12.2025 08:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 1 ] . [ 1 8 2 9 9 0 . 2 1 5 2 : r HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models Minghui Lin1, Pengxiang Ding1,2, Shu Wang1, Zifeng Zhuang1,2, Yang Liu1,2, Xinyang Tong1, Wenxuan Song3, Shangke Lyu4, Siteng Huang2, Donglin Wang1,5 1Westlake University 2Zhejiang University 3HKUST(GZ) 4Nanjing University 5Westlake Robotics linminghui@westlake.edu.cn, siteng.huang@gmail.com (cid:128) https://hifvla.github.io https://github.com/OpenHelix-Team/HiF-VLA "
[11.12.2025 08:33] Response: ```python
[
    "Westlake University",
    "Zhejiang University",
    "HKUST(GZ)",
    "Nanjing University",
    "Westlake Robotics"
]
```
[11.12.2025 08:33] Deleting PDF ./assets/pdf/2512.09928.pdf.
[11.12.2025 08:33] Success.
[11.12.2025 08:33] Downloading and parsing paper https://huggingface.co/papers/2512.09247.
[11.12.2025 08:33] Downloading paper 2512.09247 from https://arxiv.org/pdf/2512.09247v1...
[11.12.2025 08:33] Extracting affiliations from text.
[11.12.2025 08:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"OmniPSD: Layered PSD Generation with Diffusion Transformer Cheng Liu1, * Yiren Song1, * Haofan Wang2 Mike Zheng Shou1 1National University of Singapore 2Lovart AI 5 2 0 2 0 ] . [ 1 7 4 2 9 0 . 2 1 5 2 : r Figure 1. OmniPSD is Diffusion-Transformer framework that generates layered PSD files with transparent alpha channels. Our system supports both Text-to-PSD multi-layer synthesis and Image-to-PSD reconstruction, producing editable layers that preserve structure, transparency, and semantic consistency. "
[11.12.2025 08:33] Response: ```python
[
    "National University of Singapore",
    "Lovart AI"
]
```
[11.12.2025 08:33] Deleting PDF ./assets/pdf/2512.09247.pdf.
[11.12.2025 08:33] Success.
[11.12.2025 08:33] Downloading and parsing paper https://huggingface.co/papers/2512.04753.
[11.12.2025 08:33] Downloading paper 2512.04753 from https://arxiv.org/pdf/2512.04753v1...
[11.12.2025 08:33] Extracting affiliations from text.
[11.12.2025 08:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"EtCon: Edit-then-Consolidate for Reliable Knowledge Editing ETCON: EDIT-THEN-CONSOLIDATE FOR RELIABLE KNOWLEDGE EDITING Ruilin Li1,2, Yibin Wang2,3, Wenhong Zhu2,4, Chenglin Li2, Jinghao Zhang2,5, Chenliang Li,1 Junchi Yan,2,4 Jiaqi Wang2 1Wuhan University 4Shanghai Jiao Tong University https://github.com/RlinL/EtCon 2Shanghai Innovation Institute 5University of Science and Technology of China 3Fudan University 5 2 0 2 4 ] . [ 1 3 5 7 4 0 . 2 1 5 2 : r a "
[11.12.2025 08:33] Response: ```python
[
    "Wuhan University",
    "Shanghai Innovation Institute",
    "Fudan University",
    "Shanghai Jiao Tong University",
    "University of Science and Technology of China"
]
```
[11.12.2025 08:33] Deleting PDF ./assets/pdf/2512.04753.pdf.
[11.12.2025 08:33] Success.
[11.12.2025 08:33] Downloading and parsing paper https://huggingface.co/papers/2512.09106.
[11.12.2025 08:33] Downloading paper 2512.09106 from https://arxiv.org/pdf/2512.09106v1...
[11.12.2025 08:33] Extracting affiliations from text.
[11.12.2025 08:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 ] . [ 1 6 0 1 9 0 . 2 1 5 2 : r Learning Unmasking Policies for Diffusion Language Models Metod Jazbec, Theo X. Olausson, Louis Béthune, Pierre Ablin, Michael Kirchhof, Joao Monterio, Victor Turrisi, Jason Ramapuram, Marco Cuturi Apple, University of Amsterdam, Massachusetts Institute of Technology Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which buffer filled with special mask tokens is progressively replaced with tokens sampled from the models vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as Markov decision process in which the dLLM serves as the environment, and propose lightweight policy architecture based on single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of stateof-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we al"
[11.12.2025 08:33] Response: ```python
["Apple", "University of Amsterdam", "Massachusetts Institute of Technology"]
```
[11.12.2025 08:33] Deleting PDF ./assets/pdf/2512.09106.pdf.
[11.12.2025 08:33] Success.
[11.12.2025 08:33] Downloading and parsing paper https://huggingface.co/papers/2512.02892.
[11.12.2025 08:33] Downloading paper 2512.02892 from https://arxiv.org/pdf/2512.02892v1...
[11.12.2025 08:33] Extracting affiliations from text.
[11.12.2025 08:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 ] . [ 1 2 9 8 2 0 . 2 1 5 2 : r FAST-DECODING DIFFUSION LANGUAGE MODELS VIA PROGRESS-AWARE CONFIDENCE SCHEDULES Amr Mohamed1,2, Yang Zhang2, Michalis Vazirgiannis1,2, Guokan Shang1 1MBZUAI, 2Ecole Polytechnique "
[11.12.2025 08:33] Response: ```python
['MBZUAI', 'Ecole Polytechnique']
```
[11.12.2025 08:33] Deleting PDF ./assets/pdf/2512.02892.pdf.
[11.12.2025 08:33] Success.
[11.12.2025 08:33] Downloading and parsing paper https://huggingface.co/papers/2512.09164.
[11.12.2025 08:33] Downloading paper 2512.09164 from https://arxiv.org/pdf/2512.09164v1...
[11.12.2025 08:33] Extracting affiliations from text.
[11.12.2025 08:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"WonderZoom: Multi-Scale 3D World Generation Jin Cao* Hong-Xing Yu* https://wonderzoom.github.io/ 5 2 0 2 9 ] . [ 1 4 6 1 9 0 . 2 1 5 2 : r Figure 1. Multi-scale 3D world generation from single image. WonderZoom enables interactive exploration across spatial scales. Users can zoom into any region and specify prompts to generate new fine-scale content while maintaining cross-scale consistency. Here we show three zoom-in sequences. We attach an interactive viewer in the supplmentary material. "
[11.12.2025 08:33] Response: ```python
[]
```
[11.12.2025 08:33] Extracting affiliations from text.
[11.12.2025 08:33] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"WonderZoom: Multi-Scale 3D World Generation Jin Cao* Hong-Xing Yu*https://wonderzoom.github.io/ 5 2 0 2 9 ] . [ 1 4 6 1 9 0 . 2 1 5 2 : r Figure 1. Multi-scale 3D world generation from single image. WonderZoom enables interactive exploration across spatial scales. Users can zoom into any region and specify prompts to generate new fine-scale content while maintaining cross-scale consistency. Here we show three zoom-in sequences. We attach an interactive viewer in the supplmentary material.We present WonderZoom, novel approach to generating 3D scenes with contents across multiple spatial scales from single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users *Equal contribution. to zoom into 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multiscale 3D world creation from single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/. 1. Introduction 3D world generation has emerged as transformative capability in computer vision, enabling the synthesis of immersive environments from minimal input [7, 9, 13, 24, 50, 51]. However, despite the inherently multi-scale nature of realworld scenes, existing approaches remain fundamentally constrained to single-scale generation. They can produce landscape-level environments and room-scale scenes, but fail to synthesize coherent content across multiple spatial scales, e.g., tiny ladybug lying on sunflower in vast field. This limitation prevents existing approaches from generating rich, detailed worlds that span from panoramic vistas down to microscopic surface details, restricting their applicability for interactive exploration and content creation. The fundamental challenge underlying this limitation is the absence of scale-adaptive 3D representation suitable for scene generation. Traditional Level-of-Detail (LoD) representations [26] were designed for efficiently rendering pre-existing graphics content, where all geometric details are known in advance. Recent hierarchical representations like Hierarchical 3D Gaussian Splatting [17] and Mip-NeRF [1] extend these principles to neural reconstruction, efficiently encoding scenes at multiple scales. But critically, they still assume access to complete multi-scale image data upfront for one-pass optimization. Both paradigms, rendering and reconstruction, fundamentally conflict with generation, where images do not exist priori and must be synthesized progressively. In generation, we must create coarse-scale content first, then iteratively synthesize finer details conditioned on both the coarser structure and user-specified prompt and regions of interest. This requires representations that can grow dynamically as new fine-scale content is generated, not static hierarchies optimized with complete supervision. Current generation methods [50, 51] sidestep this challenge entirely by restricting themselves to single scales, while naive application of existing hierarchical representations would demand generating all scales simultaneously, which is computationally intractable approach that violates the inherent coarse-to-fine nature of multi-scale synthesis. To address this challenge, we propose WonderZoom, novel framework for multi-scale 3D world generation from single image. Our approach introduces two key technical innovations: (1) scale-adaptive Gaussian surfels, dynamically updatable hierarchical representation that, unlike existing multi-scale methods, supports incremental refinement as new content is generated. It allows adding arbitrary levels of detail without re-optimization, and (2) progressive detail synthesizer that iteratively generates fine-grained 3D structures conditioned on both coarser scales and userspecified regions and viewpoints. These components work synergistically: the scale-adaptive representation provides persistent 3D canvas that grows in detail over time, while the synthesizer produces coherent multi-scale content through controlled coarse-to-fine generation process. By enabling dynamic updates to the 3D representation as new scales are synthesized, WonderZoom fundamentally shifts from the reconstruction paradigm to multi-scale generation, overcoming the computational and architectural barriers that constrain existing methods to single scales. Our approach enables users to interactively zoom into any region of the generated 3D scene, triggering autoregressive synthesis of previously non-existent details, e.g., from an entire landscape down to microscopic surface features. Unlike traditional multi-resolution rendering that simply reveals pre-existing details, WonderZoom generates new content on-demand, creating coherent structures that were never part of the original input or coarse generation. This capability allows infinite exploration of generated worlds at arbitrary levels of detail. In summary, our contributions are threefold: We propose WonderZoom, the first approach to enable multi-scale 3D world generation from single image, supporting seamless transitions from macro to micro scales. We introduce scale-adaptive Gaussian surfels, dynamically updatable representation that grows incrementally with newly generated finer-scale content, while maintaining real-time rendering performance. We demonstrate and evaluate multi-scale 3D generation across diverse scenarios including natural environments, villages, and urban scenes, achieving consistent quality across scale transitions while significantly outperforming state-of-the-art video and 3D generation models in both perceptual quality and prompt alignment. 2. Related Work 3D World Generation. Early 3D scene generation methods focused on novel view synthesis from single image, constructing renderable representations like layered d"
[11.12.2025 08:33] Mistral response. {"id": "0197c0d3b139479180acad4fdee9de5e", "created": 1765442011, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1246, "total_tokens": 1252, "completion_tokens": 6}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}}]}
[11.12.2025 08:33] Response: ```python
[]
```
[11.12.2025 08:33] Deleting PDF ./assets/pdf/2512.09164.pdf.
[11.12.2025 08:33] Success.
[11.12.2025 08:33] Downloading and parsing paper https://huggingface.co/papers/2512.08560.
[11.12.2025 08:33] Downloading paper 2512.08560 from https://arxiv.org/pdf/2512.08560v1...
[11.12.2025 08:33] Extracting affiliations from text.
[11.12.2025 08:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 ] . [ 1 0 6 5 8 0 . 2 1 5 2 : r BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain Navve Wasserman1, Matias Cosarinsky1, Yuval Golbari1 Aude Oliva2 Antonio Torralba2 Tamar Rott Shaham2 Michal Irani1 1Weizmann Institute of Science 2Massachusetts Institute of Technology Equal contribution "
[11.12.2025 08:33] Response: ```python
[
    "Weizmann Institute of Science",
    "Massachusetts Institute of Technology"
]
```
[11.12.2025 08:33] Deleting PDF ./assets/pdf/2512.08560.pdf.
[11.12.2025 08:33] Success.
[11.12.2025 08:33] Downloading and parsing paper https://huggingface.co/papers/2512.08006.
[11.12.2025 08:33] Downloading paper 2512.08006 from https://arxiv.org/pdf/2512.08006v1...
[11.12.2025 08:33] Extracting affiliations from text.
[11.12.2025 08:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 ] . [ 1 6 0 0 8 0 . 2 1 5 2 : r Beyond Uniﬁed Models: Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS Mahta Fetrat, Donya Navabi, Zahra Dehghanian, Morteza Abolghasemi, Hamid R. Rabiee Sharif University of Technology / Tehran, Iran Correspondence: rabiee@sharif.edu "
[11.12.2025 08:33] Response: ```python
["Sharif University of Technology"]
```
[11.12.2025 08:33] Deleting PDF ./assets/pdf/2512.08006.pdf.
[11.12.2025 08:33] Success.
[11.12.2025 08:33] Downloading and parsing paper https://huggingface.co/papers/2512.07222.
[11.12.2025 08:33] Downloading paper 2512.07222 from https://arxiv.org/pdf/2512.07222v2...
[11.12.2025 08:33] Extracting affiliations from text.
[11.12.2025 08:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 ] . [ 2 2 2 2 7 0 . 2 1 5 2 : r Pay Less Attention to Function Words for Free Robustness of Vision-Language Models Xian Jiaotong University michaeltqw@stu.xjtu.edu.cn, {linchenhao, zhengyu.zhao}@xjtu.edu.cn, chaoshen@mail.xjtu.edu.cn "
[11.12.2025 08:33] Response: ```python
["Xian Jiaotong University"]
```
[11.12.2025 08:33] Deleting PDF ./assets/pdf/2512.07222.pdf.
[11.12.2025 08:33] Success.
[11.12.2025 08:33] Downloading and parsing paper https://huggingface.co/papers/2512.05446.
[11.12.2025 08:33] Downloading paper 2512.05446 from https://arxiv.org/pdf/2512.05446v1...
[11.12.2025 08:33] Extracting affiliations from text.
[11.12.2025 08:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 ] . [ 1 6 4 4 5 0 . 2 1 5 2 : r TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression Cheng-Yuan Ho1, He-Bi Yang1, Jui-Chiu Chiang2, Yu-Lun Liu1, Wen-Hsiao Peng1 1National Yang Ming Chiao Tung University, Taiwan 2National Chung Cheng University, Taiwan {kelvinhe0218.cs12, mrrrimge32.cs13}@nycu.edu.tw, rachel@ccu.edu.tw yulunliu@cs.nycu.edu.tw, wpeng@cs.nycu.edu.tw Figure 1. Overview of TED-4DGS. Left: Qualitative comparison on banana scene. Our TED-4DGS reconstructs the scene with superior rendering quality compared to ADC-GS. It achieves 26% file size reduction while closely matching the ground-truth view. Centre: Temporal duration map. Static background regions reuse long-duration Gaussian primitives, whereas occluded parts of the hand and banana are represented by short-duration primitives, demonstrating the effectiveness of temporal activation. Right-top: Rate-distortion comparison on the HyperNeRF [36] benchmark. Our TED-4DGS attains higher PSNR with smaller file sizes than prior methods. Right-bottom:Illustration of the learnable temporal-activation function, which activates Gaussian primitive from its appearance (as) to disappearance (af ). "
[11.12.2025 08:33] Response: ```python
[
    "National Yang Ming Chiao Tung University, Taiwan",
    "National Chung Cheng University, Taiwan"
]
```
[11.12.2025 08:33] Deleting PDF ./assets/pdf/2512.05446.pdf.
[11.12.2025 08:33] Success.
[11.12.2025 08:33] Downloading and parsing paper https://huggingface.co/papers/2512.04519.
[11.12.2025 08:33] Downloading paper 2512.04519 from https://arxiv.org/pdf/2512.04519v1...
[11.12.2025 08:33] Extracting affiliations from text.
[11.12.2025 08:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 ] . [ 1 9 1 5 4 0 . 2 1 5 2 : r VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory Yifei Yu1,,, Xiaoshan Wu1,, Xinting Hu1, Tao Hu2,, Yang-Tian Sun1, Xiaoyang Lyu1, Bo Wang1, Lin Ma2, Yuewen Ma2,, Zhongrui Wang3,, Xiaojuan Qi1, 1HKU 2PICO, ByteDance 3SUSTech Work done during internship at PICO Equal contribution Project lead Corresponding author Figure 1. We introduce VideoSSM, an AR video diffusion model equipped with novel hybrid memory architecture that combines causal sliding-window local lossless cache with an SSM-based global compressed memory. Compared with prior AR Diffusion Transformers (AR DiTs) that use only window attention, which suffer from quality degradation and temporal drifting, or add sink frames, which reduce drift but cause content repetition and lack of dynamism, our hybrid memory yields videos that remain both long-term consistent and progressively dynamic. Trained via Self Forcing distillation [25] via DMD loss [59] from bidirectional teacher, VideoSSM supports highly stable long video generation and adaptive, interactive prompt-based video generation. "
[11.12.2025 08:33] Response: ```python
[
    "HKU",
    "PICO, ByteDance",
    "SUSTech"
]
```
[11.12.2025 08:33] Deleting PDF ./assets/pdf/2512.04519.pdf.
[11.12.2025 08:33] Success.
[11.12.2025 08:33] Downloading and parsing paper https://huggingface.co/papers/2512.01453.
[11.12.2025 08:33] Downloading paper 2512.01453 from https://arxiv.org/pdf/2512.01453v1...
[11.12.2025 08:34] Extracting affiliations from text.
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Reinventing Clinical Dialogue: Agentic Paradigms for LLM-Enabled Healthcare Communication XIAOQUAN ZHI, College of Management and Economics, Laboratory of Computation and Analytics of Complex Management Systems(CACMS), Tianjin University, China HONGKE ZHAO, College of Management and Economics, Laboratory of Computation and Analytics of Complex Management Systems(CACMS), Tianjin University, China LIKANG WU, College of Management and Economics, Laboratory of Computation and Analytics of Complex Management Systems(CACMS), Tianjin University, China CHUANG ZHAO, College of Management and Economics, Laboratory of Computation and Analytics of Complex Management Systems(CACMS), Tianjin University, China HENGSHU ZHU, Computer Network Information Center, Chinese Academy of Sciences, China Clinical dialogue represents complex duality requiring both the empathetic fluency of natural conversation and the rigorous precision of evidence-based medicine. While Large Language Models possess unprecedented linguistic capabilities, their architectural reliance on reactive and stateless processing often favors probabilistic plausibility over factual veracity. This structural limitation has catalyzed paradigm shift in medical AI from generative text prediction to agentic autonomy, where the model functions as central reasoning engine capable of deliberate planning and persistent memory. Moving beyond existing reviews that primarily catalog downstream applications, this survey provides first-principles analysis of the cognitive architecture underpinning this shift. We introduce novel taxonomy structured along the orthogonal axes of knowledge source and agency objective to delineate the provenance of clinical knowledge against the systems operational scope. This framework facilitates systematic analysis of the intrinsic trade-offs between creativity and reliability by categorizing methods into four archetypes: Latent Space Clinicians, Emergent Planners, Grounded Synthesizers, and Verifiable"
[11.12.2025 08:34] Response: ```python
[
    "College of Management and Economics, Laboratory of Computation and Analytics of Complex Management Systems(CACMS), Tianjin University, China",
    "Computer Network Information Center, Chinese Academy of Sciences, China"
]
```
[11.12.2025 08:34] Deleting PDF ./assets/pdf/2512.01453.pdf.
[11.12.2025 08:34] Success.
[11.12.2025 08:34] Enriching papers with extra data.
[11.12.2025 08:34] ********************************************************************************
[11.12.2025 08:34] Abstract 0. StereoWorld generates high-quality stereo video from monocular input using a pretrained video generator with geometry-aware regularization and spatio-temporal tiling.  					AI-generated summary 				 The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its pr...
[11.12.2025 08:34] ********************************************************************************
[11.12.2025 08:34] Abstract 1. Bind & Compose uses Diffusion Transformers with hierarchical binders and temporal strategies to accurately compose complex visual concepts from images and videos.  					AI-generated summary 				 Visual concept composition, which aims to integrate different elements from images and videos into a sing...
[11.12.2025 08:34] ********************************************************************************
[11.12.2025 08:34] Abstract 2. InfiniteVL, a linear-complexity VLM architecture combining sliding window attention and Gated DeltaNet, achieves competitive performance with less data and faster inference than leading Transformer-based models.  					AI-generated summary 				 Window attention and linear attention represent two prin...
[11.12.2025 08:34] ********************************************************************************
[11.12.2025 08:34] Abstract 3. A unified framework combines vision-language models and video generation to improve autonomous driving in complex scenarios by enhancing reasoning, trajectory planning, and video generation.  					AI-generated summary 				 Autonomous driving (AD) systems struggle in long-tail scenarios due to limite...
[11.12.2025 08:34] ********************************************************************************
[11.12.2025 08:34] Abstract 4. HiF-VLA integrates motion for bidirectional temporal reasoning in VLA models, improving long-horizon manipulation performance with minimal additional latency.  					AI-generated summary 				 Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and lingui...
[11.12.2025 08:34] ********************************************************************************
[11.12.2025 08:34] Abstract 5. OmniPSD, a diffusion framework within the Flux ecosystem, enables text-to-PSD generation and image-to-PSD decomposition, achieving high-fidelity results with transparency awareness.  					AI-generated summary 				 Recent advances in diffusion models have greatly improved image generation and editing...
[11.12.2025 08:34] ********************************************************************************
[11.12.2025 08:34] Abstract 6. A novel knowledge editing framework, Edit-then-Consolidate, addresses overfitting and lack of knowledge integration in large language models through targeted fine-tuning and policy optimization, enhancing reliability and generalization.  					AI-generated summary 				 Knowledge editing aims to updat...
[11.12.2025 08:34] ********************************************************************************
[11.12.2025 08:34] Abstract 7. Reinforcement learning is used to train sampling procedures for masked discrete diffusion language models, improving token throughput and quality compared to heuristic strategies.  					AI-generated summary 				 Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their ...
[11.12.2025 08:34] ********************************************************************************
[11.12.2025 08:34] Abstract 8. SchED, a training-free early-exit algorithm, accelerates diffusion large language model decoding with minimal performance loss across various tasks.  					AI-generated summary 				 Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical uti...
[11.12.2025 08:34] ********************************************************************************
[11.12.2025 08:34] Abstract 9. WonderZoom generates multi-scale 3D scenes from a single image using scale-adaptive Gaussian surfels and a progressive detail synthesizer, outperforming existing models in quality and alignment.  					AI-generated summary 				 We present WonderZoom, a novel approach to generating 3D scenes with cont...
[11.12.2025 08:34] ********************************************************************************
[11.12.2025 08:34] Abstract 10. An automated framework uses unsupervised decomposition and natural language descriptions to identify and explain visual representations in human brain fMRI data.  					AI-generated summary 				 Understanding how the human brain represents visual concepts, and in which brain regions these representat...
[11.12.2025 08:34] ********************************************************************************
[11.12.2025 08:34] Abstract 11. A framework is proposed to improve phonemization quality in TTS systems without sacrificing real-time performance through lightweight context-aware phonemization and a service-oriented architecture.  					AI-generated summary 				 Lightweight, real-time text-to-speech systems are crucial for accessi...
[11.12.2025 08:34] ********************************************************************************
[11.12.2025 08:34] Abstract 12. Function-word De-Attention (FDA) mitigates adversarial attacks on robust VLMs by differentially subtracting function-word cross-attention, improving robustness with minimal performance trade-offs.  					AI-generated summary 				 To address the trade-off between robustness and performance for robust ...
[11.12.2025 08:34] ********************************************************************************
[11.12.2025 08:34] Abstract 13. TED-4DGS combines sparse anchor-based 3D Gaussian Splatting with temporal activation and embedding for efficient and high-quality compression of dynamic 3D scenes.  					AI-generated summary 				 Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension...
[11.12.2025 08:34] ********************************************************************************
[11.12.2025 08:34] Abstract 14. VideoSSM, a hybrid state-space memory model combining AR diffusion, achieves state-of-the-art temporal consistency and motion stability in long-video generation by coordinating short- and long-term context.  					AI-generated summary 				 Autoregressive (AR) diffusion enables streaming, interactive ...
[11.12.2025 08:34] ********************************************************************************
[11.12.2025 08:34] Abstract 15. The survey analyzes the cognitive architecture of medical AI systems, focusing on the shift from generative text prediction to agentic autonomy, and categorizes methods into four archetypes based on knowledge source and agency objective.  					AI-generated summary 				 Clinical dialogue represents a...
[11.12.2025 08:34] Read previous papers.
[11.12.2025 08:34] Generating reviews via LLM API.
[11.12.2025 08:34] Querying the API.
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

StereoWorld generates high-quality stereo video from monocular input using a pretrained video generator with geometry-aware regularization and spatio-temporal tiling.  					AI-generated summary 				 The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.
[11.12.2025 08:34] Response: ```json
{
  "desc": "StereoWorld — это фреймворк для генерации высококачественного стереовидео из монокулярного видео с использованием предобученного генератора видео. Модель одновременно обрабатывает исходное монокулярное видео и использует геометрически-ориентированную регуляризацию для обеспечения структурной точности трёхмерного представления. Для эффективной генерации видео высокого разрешения применяется схема пространственно-временной тилизации. Авторы создали большой датасет из более чем 11 миллионов кадров стереовидео для обучения и оценки качества модели.",
  "emoji": "🎬",
  "title": "Из монокулярного видео в качественное стерео через геометрическую суперзвизию"
}
```
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"StereoWorld generates high-quality stereo video from monocular input using a pretrained video generator with geometry-aware regularization and spatio-temporal tiling.  					AI-generated summary 				 The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/."

[11.12.2025 08:34] Response: ```python
['VIDEO', 'DATASET', '3D', 'MULTIMODAL']
```
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"StereoWorld generates high-quality stereo video from monocular input using a pretrained video generator with geometry-aware regularization and spatio-temporal tiling.  					AI-generated summary 				 The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/."

[11.12.2025 08:34] Response: ```python
['TRANSFER_LEARNING', 'OPEN_SOURCE']
```

**Reasoning:**

- **TRANSFER_LEARNING**: The paper explicitly mentions "repurposes a pretrained video generator," which is a clear example of transfer learning - leveraging a pre-trained model for a new task (monocular-to-stereo video generation).

- **OPEN_SOURCE**: The paper mentions "The project webpage is available at https://ke-xing.github.io/StereoWorld/," indicating the authors are making their work publicly available, which aligns with open-source contribution practices.
[11.12.2025 08:34] Error. Failed to parse JSON from LLM. ["TRANSFER_LEARNING", "OPEN_SOURCE"]


**Reasoning:**

- **TRANSFER_LEARNING**: The paper explicitly mentions "repurposes a pretrained video generator," which is a clear example of transfer learning - leveraging a pre-trained model for a new task (monocular-to-stereo video generation).

- **OPEN_SOURCE**: The paper mentions "The project webpage is available at https://ke-xing.github.io/StereoWorld/," indicating the authors are making their work publicly available, which aligns with open-source contribution practices.
[11.12.2025 08:34] Error getting data: can only concatenate list (not "dict") to list
[11.12.2025 08:34] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#architecture", "#video", "#diffusion"], "emoji": "🎨", "ru": {"title": "Связывание и композиция визуальных концепций через трансформеры диффузии", "desc": "Bind & Compose — это метод для создания сложных визуальных композиций из изображений и видео с использова
[11.12.2025 08:34] Querying the API.
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

InfiniteVL, a linear-complexity VLM architecture combining sliding window attention and Gated DeltaNet, achieves competitive performance with less data and faster inference than leading Transformer-based models.  					AI-generated summary 				 Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.
[11.12.2025 08:34] Response: ```json
{
  "desc": "InfiniteVL — это архитектура визуально-языковой модели с линейной сложностью, которая комбинирует скользящее оконное внимание с механизмом Gated DeltaNet для преодоления ограничений традиционных подходов. Модель решает проблему деградации производительности оконного внимания при длинных последовательностях и недостаточной эффективности линейного внимания на задачах с высокой информационной плотностью, таких как распознавание текста и понимание документов. Благодаря трёхэтапной стратегии обучения с дистилляцией и инструкционной подстройкой, InfiniteVL достигает производительности лучших Transformer-моделей, используя менее 2% их данных для обучения и обеспечивая 3.6-кратное ускорение инференса. Модель также демонстрирует стабильную работу в задачах видеопотока в реальном времени с сохранением долгосрочной памяти контекста.",
  "emoji": "⚡",
  "title": "Бесконечная память при линейной скорости: эффективная мультимодальная модель нового поколения"
}
```
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"InfiniteVL, a linear-complexity VLM architecture combining sliding window attention and Gated DeltaNet, achieves competitive performance with less data and faster inference than leading Transformer-based models.  					AI-generated summary 				 Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL."

[11.12.2025 08:34] Response: ```python
["MULTIMODAL", "ARCHITECTURE", "INFERENCE", "VIDEO"]
```
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"InfiniteVL, a linear-complexity VLM architecture combining sliding window attention and Gated DeltaNet, achieves competitive performance with less data and faster inference than leading Transformer-based models.  					AI-generated summary 				 Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL."

[11.12.2025 08:34] Response: ```python
['LONG_CONTEXT', 'OPTIMIZATION', 'OPEN_SOURCE']
```

**Justification:**

1. **LONG_CONTEXT**: The paper explicitly addresses long context handling through sliding window attention and linear attention mechanisms to handle longer sequences. It mentions "long-sequence SFT" and "effective long-term memory retention," which are core long context techniques.

2. **OPTIMIZATION**: The paper focuses on optimization of Vision-Language Models, specifically addressing quadratic complexity reduction, KV cache efficiency, and achieving 3.6× inference speedup. These are training and inference optimization improvements.

3. **OPEN_SOURCE**: The paper explicitly states "Code and models are available at https://github.com/hustvl/InfiniteVL," indicating the authors are releasing their models and code publicly.
[11.12.2025 08:34] Error. Failed to parse JSON from LLM. ["LONG_CONTEXT", "OPTIMIZATION", "OPEN_SOURCE"]


**Justification:**

1. **LONG_CONTEXT**: The paper explicitly addresses long context handling through sliding window attention and linear attention mechanisms to handle longer sequences. It mentions "long-sequence SFT" and "effective long-term memory retention," which are core long context techniques.

2. **OPTIMIZATION**: The paper focuses on optimization of Vision-Language Models, specifically addressing quadratic complexity reduction, KV cache efficiency, and achieving 3.6× inference speedup. These are training and inference optimization improvements.

3. **OPEN_SOURCE**: The paper explicitly states "Code and models are available at https://github.com/hustvl/InfiniteVL," indicating the authors are releasing their models and code publicly.
[11.12.2025 08:34] Error getting data: can only concatenate list (not "dict") to list
[11.12.2025 08:34] Querying the API.
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A unified framework combines vision-language models and video generation to improve autonomous driving in complex scenarios by enhancing reasoning, trajectory planning, and video generation.  					AI-generated summary 				 Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.
[11.12.2025 08:34] Response: ```json
{
  "desc": "Статья предлагает унифицированный фреймворк UniUGP, который объединяет видео-языковые модели и генеративные модели видео для улучшения автономного вождения в сложных сценариях. Система использует гибридную архитектуру экспертов, которая синергирует рассуждение о сцене, генерацию будущих видеокадров и планирование траектории движения. На входе модель принимает многокадровые наблюдения и текстовые инструкции, на выходе генерирует интерпретируемое цепное рассуждение, физически корректные траектории и согласованные видеопредсказания. Четырёхэтапная стратегия обучения позволяет системе эффективно обобщаться на редкие и сложные дорожные ситуации.",
  "emoji": "🚗",
  "title": "Единый фреймворк для понимания, генерации и планирования в автономном вождении"
}
```
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A unified framework combines vision-language models and video generation to improve autonomous driving in complex scenarios by enhancing reasoning, trajectory planning, and video generation.  					AI-generated summary 				 Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations."

[11.12.2025 08:34] Response: ```python
['DATASET', 'VIDEO', 'MULTIMODAL', 'ROBOTICS', 'TRAINING']
```
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A unified framework combines vision-language models and video generation to improve autonomous driving in complex scenarios by enhancing reasoning, trajectory planning, and video generation.  					AI-generated summary 				 Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations."

[11.12.2025 08:34] Response: ```python
['REASONING', 'SYNTHETIC']
```

**Justification:**

- **REASONING**: The paper explicitly discusses enhancing reasoning capabilities through "scene reasoning," "chain-of-thought reasoning," and integrating large language models for reasoning in autonomous driving scenarios.

- **SYNTHETIC**: The paper mentions constructing "multiple specialized datasets" and proposes a framework that generates "future videos" and uses "video generation models," which involves creating synthetic data for training autonomous driving systems.
[11.12.2025 08:34] Error. Failed to parse JSON from LLM. ["REASONING", "SYNTHETIC"]


**Justification:**

- **REASONING**: The paper explicitly discusses enhancing reasoning capabilities through "scene reasoning," "chain-of-thought reasoning," and integrating large language models for reasoning in autonomous driving scenarios.

- **SYNTHETIC**: The paper mentions constructing "multiple specialized datasets" and proposes a framework that generates "future videos" and uses "video generation models," which involves creating synthetic data for training autonomous driving systems.
[11.12.2025 08:34] Error getting data: can only concatenate list (not "dict") to list
[11.12.2025 08:34] Querying the API.
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

HiF-VLA integrates motion for bidirectional temporal reasoning in VLA models, improving long-horizon manipulation performance with minimal additional latency.  					AI-generated summary 				 Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.
[11.12.2025 08:34] Response: ```json
{
  "desc": "В работе предложена HiF-VLA — модель vision-language-action (зрение-язык-действие), которая решает проблему марковского предположения в робототехнике, используя информацию о движении для двунаправленного временного рассуждения. Модель кодирует прошлую динамику через ретроспективные приоры, предсказывает будущее движение через предвидящее рассуждение и объединяет оба подхода для выполнения долгогоризонтных манипуляционных задач. Благодаря такой архитектуре HiF-VLA значительно превосходит базовые методы на бенчмарках LIBERO-Long и CALVIN ABC-D, при этом добавляя минимальные вычислительные затраты. Модель демонстрирует существенные улучшения в реальных задачах манипуляции с увеличенным горизонтом планирования.",
  "emoji": "🤖",
  "title": "Движение как ключ к памяти: временное рассуждение в робототехнике"
}
```
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HiF-VLA integrates motion for bidirectional temporal reasoning in VLA models, improving long-horizon manipulation performance with minimal additional latency.  					AI-generated summary 				 Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings."

[11.12.2025 08:34] Response: ```python
['ROBOTICS', 'MULTIMODAL', 'ARCHITECTURE', 'BENCHMARK']
```
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HiF-VLA integrates motion for bidirectional temporal reasoning in VLA models, improving long-horizon manipulation performance with minimal additional latency.  					AI-generated summary 				 Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings."

[11.12.2025 08:34] Response: ```python
['REASONING']
```

The paper focuses on enhancing temporal reasoning capabilities in Vision-Language-Action models through bidirectional temporal reasoning (hindsight and foresight), which directly relates to improving logical reasoning for long-horizon manipulation tasks.
[11.12.2025 08:34] Error. Failed to parse JSON from LLM. ["REASONING"]


The paper focuses on enhancing temporal reasoning capabilities in Vision-Language-Action models through bidirectional temporal reasoning (hindsight and foresight), which directly relates to improving logical reasoning for long-horizon manipulation tasks.
[11.12.2025 08:34] Error getting data: can only concatenate list (not "dict") to list
[11.12.2025 08:34] Using data from previous issue: {"categories": ["#open_source", "#cv", "#multimodal", "#architecture", "#diffusion", "#dataset"], "emoji": "🎨", "ru": {"title": "Диффузионная генерация многослойных дизайнов с полной поддержкой прозрачности", "desc": "OmniPSD — это унифицированная фреймворк диффузионных моделей, построенная на основ
[11.12.2025 08:34] Using data from previous issue: {"categories": ["#rlhf", "#optimization", "#training", "#alignment"], "emoji": "✏️", "ru": {"title": "От редактирования знаний к их интеграции в языковые модели", "desc": "Статья предлагает инновационный фреймворк Edit-then-Consolidate для редактирования знаний в больших языковых моделях. Метод реша
[11.12.2025 08:34] Using data from previous issue: {"categories": ["#rl", "#training", "#architecture", "#optimization", "#transfer_learning", "#diffusion"], "emoji": "🎯", "ru": {"title": "Обучение с подкреплением для оптимальной выборки токенов в диффузионных языковых моделях", "desc": "В работе предлагается использовать обучение с подкреплением дл
[11.12.2025 08:34] Using data from previous issue: {"categories": ["#inference", "#training", "#translation", "#optimization", "#diffusion"], "emoji": "⚡", "ru": {"title": "Быстрое декодирование диффузионных моделей через уверенную остановку", "desc": "SchED — это алгоритм ранней остановки, который работает без обучения и ускоряет декодирование дифф
[11.12.2025 08:34] Querying the API.
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

WonderZoom generates multi-scale 3D scenes from a single image using scale-adaptive Gaussian surfels and a progressive detail synthesizer, outperforming existing models in quality and alignment.  					AI-generated summary 				 We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to "zoom into" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/
[11.12.2025 08:34] Response: ```json
{
  "desc": "WonderZoom представляет инновационный подход к синтезу трёхмерных сцен с деталями на разных масштабах из единственного изображения. Основной вклад работы заключается в разработке масштабно-адаптивного представления на основе гауссовых сюрфелей, позволяющего генерировать и отображать контент с существенно различающимися пространственными размерами. Модель включает прогрессивный синтезатор деталей, который итеративно генерирует трёхмерный контент всё более высокого разрешения, от ландшафтов до микроскопических характеристик. Экспериментальные результаты показывают, что предложенный метод превосходит существующие видео и трёхмерные модели по качеству и соответствию исходному изображению.",
  "emoji": "🔍",
  "title": "Масштабируемый синтез деталей 3D-сцен из одного изображения"
}
```
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"WonderZoom generates multi-scale 3D scenes from a single image using scale-adaptive Gaussian surfels and a progressive detail synthesizer, outperforming existing models in quality and alignment.  					AI-generated summary 				 We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to "zoom into" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/"

[11.12.2025 08:34] Response: ```python
["3D", "CV"]
```
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"WonderZoom generates multi-scale 3D scenes from a single image using scale-adaptive Gaussian surfels and a progressive detail synthesizer, outperforming existing models in quality and alignment.  					AI-generated summary 				 We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to "zoom into" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/"

[11.12.2025 08:34] Response: ```python
[]
```

The paper is about 3D scene generation from images using Gaussian surfels and progressive synthesis techniques. It does not directly relate to any of the topics in the provided list, which focus on language models, machine learning optimization, AI ethics, games, reasoning, translation, and other specific ML domains. This paper is primarily about 3D computer vision and generative modeling for 3D scenes, which are not covered by the given topic categories.
[11.12.2025 08:34] Error. Failed to parse JSON from LLM. []


The paper is about 3D scene generation from images using Gaussian surfels and progressive synthesis techniques. It does not directly relate to any of the topics in the provided list, which focus on language models, machine learning optimization, AI ethics, games, reasoning, translation, and other specific ML domains. This paper is primarily about 3D computer vision and generative modeling for 3D scenes, which are not covered by the given topic categories.
[11.12.2025 08:34] Error getting data: can only concatenate list (not "dict") to list
[11.12.2025 08:34] Querying the API.
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

An automated framework uses unsupervised decomposition and natural language descriptions to identify and explain visual representations in human brain fMRI data.  					AI-generated summary 				 Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.
[11.12.2025 08:34] Response: ```json
{
  "desc": "Исследователи разработали автоматизированную систему для обнаружения и интерпретации визуальных представлений в данных фМРТ человеческого мозга. Метод использует неконтролируемую декомпозицию активности мозга для выделения интерпретируемых паттернов, а затем генерирует естественно-языковые описания выявленных концепций. Система автоматически тестирует множество кандидатских объяснений, оценивает их надежность и выбирает наиболее согласованное описание для каждого паттерна вокселя. Фреймворк позволил выявить тысячи интерпретируемых паттернов, соответствующих разнообразным визуальным концепциям, включая ранее не описанные тонкие представления.",
  "emoji": "🧠",
  "title": "Автоматическое открытие и объяснение визуальных кодов мозга"
}
```
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"An automated framework uses unsupervised decomposition and natural language descriptions to identify and explain visual representations in human brain fMRI data.  					AI-generated summary 				 Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported."

[11.12.2025 08:34] Response: ```python
["CV", "HEALTHCARE", "MULTIMODAL"]
```

**Justification:**
- **CV**: The paper focuses on visual representations and visual processing in the brain, analyzing how visual concepts are encoded and represented.
- **HEALTHCARE**: The paper applies machine learning methods to understand human brain fMRI data, which is a medical/neuroscience application domain.
- **MULTIMODAL**: The paper combines multiple modalities - visual fMRI brain signals with natural language descriptions to explain visual representations.
[11.12.2025 08:34] Error. Failed to parse JSON from LLM. ["CV", "HEALTHCARE", "MULTIMODAL"]


**Justification:**
- **CV**: The paper focuses on visual representations and visual processing in the brain, analyzing how visual concepts are encoded and represented.
- **HEALTHCARE**: The paper applies machine learning methods to understand human brain fMRI data, which is a medical/neuroscience application domain.
- **MULTIMODAL**: The paper combines multiple modalities - visual fMRI brain signals with natural language descriptions to explain visual representations.
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"An automated framework uses unsupervised decomposition and natural language descriptions to identify and explain visual representations in human brain fMRI data.  					AI-generated summary 				 Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported."

[11.12.2025 08:34] Response: ```python
['INTERPRETABILITY']
```
[11.12.2025 08:34] Error getting data: unsupported operand type(s) for +: 'dict' and 'list'
[11.12.2025 08:34] Querying the API.
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A framework is proposed to improve phonemization quality in TTS systems without sacrificing real-time performance through lightweight context-aware phonemization and a service-oriented architecture.  					AI-generated summary 				 Lightweight, real-time text-to-speech systems are crucial for accessibility. However, the most efficient TTS models often rely on lightweight phonemizers that struggle with context-dependent challenges. In contrast, more advanced phonemizers with a deeper linguistic understanding typically incur high computational costs, which prevents real-time performance.   This paper examines the trade-off between phonemization quality and inference speed in G2P-aided TTS systems, introducing a practical framework to bridge this gap. We propose lightweight strategies for context-aware phonemization and a service-oriented TTS architecture that executes these modules as independent services. This design decouples heavy context-aware components from the core TTS engine, effectively breaking the latency barrier and enabling real-time use of high-quality phonemization models. Experimental results confirm that the proposed system improves pronunciation soundness and linguistic accuracy while maintaining real-time responsiveness, making it well-suited for offline and end-device TTS applications.
[11.12.2025 08:34] Response: ```json
{
  "desc": "В работе предложена архитектура для улучшения качества фонемизации в системах синтеза речи без потери скорости обработки в реальном времени. Авторы исследуют компромисс между качеством фонемизации и скоростью инференса в системах TTS с использованием моделей графема-в-фонему (G2P). Предложенный подход использует облегченные стратегии контекстно-зависимой фонемизации и сервис-ориентированную архитектуру, которая выполняет компоненты обработки как независимые микросервисы. Экспериментальные результаты демонстрируют, что система улучшает качество произношения и лингвистическую точность при сохранении возможности работы в реальном времени.",
  "emoji": "🎤",
  "title": "Контекстная фонемизация без снижения скорости синтеза речи"
}
```
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A framework is proposed to improve phonemization quality in TTS systems without sacrificing real-time performance through lightweight context-aware phonemization and a service-oriented architecture.  					AI-generated summary 				 Lightweight, real-time text-to-speech systems are crucial for accessibility. However, the most efficient TTS models often rely on lightweight phonemizers that struggle with context-dependent challenges. In contrast, more advanced phonemizers with a deeper linguistic understanding typically incur high computational costs, which prevents real-time performance.   This paper examines the trade-off between phonemization quality and inference speed in G2P-aided TTS systems, introducing a practical framework to bridge this gap. We propose lightweight strategies for context-aware phonemization and a service-oriented TTS architecture that executes these modules as independent services. This design decouples heavy context-aware components from the core TTS engine, effectively breaking the latency barrier and enabling real-time use of high-quality phonemization models. Experimental results confirm that the proposed system improves pronunciation soundness and linguistic accuracy while maintaining real-time responsiveness, making it well-suited for offline and end-device TTS applications."

[11.12.2025 08:34] Response: ```python
["AUDIO", "INFERENCE", "ARCHITECTURE"]
```
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A framework is proposed to improve phonemization quality in TTS systems without sacrificing real-time performance through lightweight context-aware phonemization and a service-oriented architecture.  					AI-generated summary 				 Lightweight, real-time text-to-speech systems are crucial for accessibility. However, the most efficient TTS models often rely on lightweight phonemizers that struggle with context-dependent challenges. In contrast, more advanced phonemizers with a deeper linguistic understanding typically incur high computational costs, which prevents real-time performance.   This paper examines the trade-off between phonemization quality and inference speed in G2P-aided TTS systems, introducing a practical framework to bridge this gap. We propose lightweight strategies for context-aware phonemization and a service-oriented TTS architecture that executes these modules as independent services. This design decouples heavy context-aware components from the core TTS engine, effectively breaking the latency barrier and enabling real-time use of high-quality phonemization models. Experimental results confirm that the proposed system improves pronunciation soundness and linguistic accuracy while maintaining real-time responsiveness, making it well-suited for offline and end-device TTS applications."

[11.12.2025 08:34] Response: ```python
['OPTIMIZATION']
```

The paper focuses on improving the efficiency and performance trade-offs in text-to-speech systems through lightweight strategies and architectural design choices that maintain real-time performance while improving quality. This directly relates to optimization of training and inference methods.
[11.12.2025 08:34] Error. Failed to parse JSON from LLM. ["OPTIMIZATION"]


The paper focuses on improving the efficiency and performance trade-offs in text-to-speech systems through lightweight strategies and architectural design choices that maintain real-time performance while improving quality. This directly relates to optimization of training and inference methods.
[11.12.2025 08:34] Error getting data: can only concatenate list (not "dict") to list
[11.12.2025 08:34] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#security", "#architecture"], "emoji": "🛡️", "ru": {"title": "Избирательное подавление служебных слов для защиты мультимодальных моделей", "desc": "Статья посвящена методу Function-word De-Attention (FDA), который повышает устойчивость мультимодальных 
[11.12.2025 08:34] Querying the API.
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

TED-4DGS combines sparse anchor-based 3D Gaussian Splatting with temporal activation and embedding for efficient and high-quality compression of dynamic 3D scenes.  					AI-generated summary 				 Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations.
[11.12.2025 08:34] Response: ```json
{
  "desc": "В статье представляется метод TED-4DGS для сжатия динамических 3D-сцен, который объединяет разреженное представление якорных точек на основе 3D Gaussian Splatting с временной активацией и встроенными деформациями. Каждый канонический якорь получает обучаемые параметры временной активации для управления его появлением и исчезновением во времени, а также лёгкие временные встраивания для запроса общего банка деформаций. Для оптимизации компрессии по критерию искажения-скорости передачи используются неявные нейросетевые представления как гиперприоры и канальные авторегрессионные модели. Предложенный подход достигает лучших результатов в компромиссе между качеством и размером на реальных датасетах динамических сцен.",
  "emoji": "🎬",
  "title": "Сжатие динамических 3D-сцен через временно активированные якоря Gaussian Splatting"
}
```
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"TED-4DGS combines sparse anchor-based 3D Gaussian Splatting with temporal activation and embedding for efficient and high-quality compression of dynamic 3D scenes.  					AI-generated summary 				 Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations."

[11.12.2025 08:34] Response: ```python
["3D", "VIDEO"]
```
[11.12.2025 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"TED-4DGS combines sparse anchor-based 3D Gaussian Splatting with temporal activation and embedding for efficient and high-quality compression of dynamic 3D scenes.  					AI-generated summary 				 Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations."

[11.12.2025 08:34] Response: ```python
["OPTIMIZATION"]
```

The paper focuses on rate-distortion-optimized compression strategies for dynamic 3D Gaussian Splatting representations, which is fundamentally an optimization problem aimed at improving compression efficiency and quality trade-offs.
[11.12.2025 08:34] Error. Failed to parse JSON from LLM. ["OPTIMIZATION"]


The paper focuses on rate-distortion-optimized compression strategies for dynamic 3D Gaussian Splatting representations, which is fundamentally an optimization problem aimed at improving compression efficiency and quality trade-offs.
[11.12.2025 08:34] Error getting data: can only concatenate list (not "dict") to list
[11.12.2025 08:34] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#architecture", "#video", "#diffusion"], "emoji": "🎬", "ru": {"title": "Гибридная память для стабильной генерации видео любой длины", "desc": "VideoSSM — это гибридная модель на основе state-space memory, которая комбинирует авторегрессивную диффузию д
[11.12.2025 08:34] Using data from previous issue: {"categories": ["#healthcare", "#agents", "#survey", "#science", "#architecture", "#reasoning"], "emoji": "🏥", "ru": {"title": "От текстового предсказания к думающему врачебному помощнику", "desc": "В статье анализируется когнитивная архитектура медицинских AI систем, отслеживая переход от простого 
[11.12.2025 08:34] Renaming data file.
[11.12.2025 08:34] Renaming previous data. hf_papers.json to ./d/2025-12-11.json
[11.12.2025 08:34] Saving new data file.
[11.12.2025 08:34] Generating page.
[11.12.2025 08:34] Renaming previous page.
[11.12.2025 08:34] Renaming previous data. index.html to ./d/2025-12-11.html
[11.12.2025 08:34] Writing result.
[11.12.2025 08:34] Renaming log file.
[11.12.2025 08:34] Renaming previous data. log.txt to ./logs/2025-12-11_last_log.txt
