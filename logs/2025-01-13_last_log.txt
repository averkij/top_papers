[13.01.2025 21:09] Read previous papers.
[13.01.2025 21:09] Generating top page (month).
[13.01.2025 21:09] Writing top page (month).
[13.01.2025 22:09] Read previous papers.
[13.01.2025 22:09] Get feed.
[13.01.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.05874
[13.01.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.03841
[13.01.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06186
[13.01.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.05510
[13.01.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.05727
[13.01.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.05452
[13.01.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.04698
[13.01.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.05707
[13.01.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.04961
[13.01.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06187
[13.01.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.05542
[13.01.2025 22:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.01.2025 22:09] No deleted papers detected.
[13.01.2025 22:09] Downloading and parsing papers (pdf, html). Total: 11.
[13.01.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.05874.
[13.01.2025 22:09] Extra JSON file exists (./assets/json/2501.05874.json), skip PDF parsing.
[13.01.2025 22:09] Paper image links file exists (./assets/img_data/2501.05874.json), skip HTML parsing.
[13.01.2025 22:09] Success.
[13.01.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.03841.
[13.01.2025 22:09] Extra JSON file exists (./assets/json/2501.03841.json), skip PDF parsing.
[13.01.2025 22:09] Paper image links file exists (./assets/img_data/2501.03841.json), skip HTML parsing.
[13.01.2025 22:09] Success.
[13.01.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.06186.
[13.01.2025 22:09] Extra JSON file exists (./assets/json/2501.06186.json), skip PDF parsing.
[13.01.2025 22:09] Paper image links file exists (./assets/img_data/2501.06186.json), skip HTML parsing.
[13.01.2025 22:09] Success.
[13.01.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.05510.
[13.01.2025 22:09] Extra JSON file exists (./assets/json/2501.05510.json), skip PDF parsing.
[13.01.2025 22:09] Paper image links file exists (./assets/img_data/2501.05510.json), skip HTML parsing.
[13.01.2025 22:09] Success.
[13.01.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.05727.
[13.01.2025 22:09] Extra JSON file exists (./assets/json/2501.05727.json), skip PDF parsing.
[13.01.2025 22:09] Paper image links file exists (./assets/img_data/2501.05727.json), skip HTML parsing.
[13.01.2025 22:09] Success.
[13.01.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.05452.
[13.01.2025 22:09] Extra JSON file exists (./assets/json/2501.05452.json), skip PDF parsing.
[13.01.2025 22:09] Paper image links file exists (./assets/img_data/2501.05452.json), skip HTML parsing.
[13.01.2025 22:09] Success.
[13.01.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.04698.
[13.01.2025 22:09] Extra JSON file exists (./assets/json/2501.04698.json), skip PDF parsing.
[13.01.2025 22:09] Paper image links file exists (./assets/img_data/2501.04698.json), skip HTML parsing.
[13.01.2025 22:09] Success.
[13.01.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.05707.
[13.01.2025 22:09] Extra JSON file exists (./assets/json/2501.05707.json), skip PDF parsing.
[13.01.2025 22:09] Paper image links file exists (./assets/img_data/2501.05707.json), skip HTML parsing.
[13.01.2025 22:09] Success.
[13.01.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.04961.
[13.01.2025 22:09] Extra JSON file exists (./assets/json/2501.04961.json), skip PDF parsing.
[13.01.2025 22:09] Paper image links file exists (./assets/img_data/2501.04961.json), skip HTML parsing.
[13.01.2025 22:09] Success.
[13.01.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.06187.
[13.01.2025 22:09] Extra JSON file exists (./assets/json/2501.06187.json), skip PDF parsing.
[13.01.2025 22:09] Paper image links file exists (./assets/img_data/2501.06187.json), skip HTML parsing.
[13.01.2025 22:09] Success.
[13.01.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.05542.
[13.01.2025 22:09] Extra JSON file exists (./assets/json/2501.05542.json), skip PDF parsing.
[13.01.2025 22:09] Paper image links file exists (./assets/img_data/2501.05542.json), skip HTML parsing.
[13.01.2025 22:09] Success.
[13.01.2025 22:09] Enriching papers with extra data.
[13.01.2025 22:09] ********************************************************************************
[13.01.2025 22:09] Abstract 0. Retrieval-Augmented Generation (RAG) is a powerful strategy to address the issue of generating factually incorrect outputs in foundation models by retrieving external knowledge relevant to queries and incorporating it into their generation process. However, existing RAG approaches have primarily foc...
[13.01.2025 22:09] ********************************************************************************
[13.01.2025 22:09] Abstract 1. The development of general robotic systems capable of manipulating in unstructured environments is a significant challenge. While Vision-Language Models(VLM) excel in high-level commonsense reasoning, they lack the fine-grained 3D spatial understanding required for precise manipulation tasks. Fine-t...
[13.01.2025 22:09] ********************************************************************************
[13.01.2025 22:09] Abstract 2. Reasoning is a fundamental capability for solving complex multi-step problems, particularly in visual contexts where sequential step-wise understanding is essential. Existing approaches lack a comprehensive framework for evaluating visual reasoning and do not emphasize step-wise problem-solving. To ...
[13.01.2025 22:09] ********************************************************************************
[13.01.2025 22:09] Abstract 3. Temporal Awareness, the ability to reason dynamically based on the timestamp when a question is raised, is the key distinction between offline and online video LLMs. Unlike offline models, which rely on complete videos for static, post hoc analysis, online models process video streams incrementally ...
[13.01.2025 22:09] ********************************************************************************
[13.01.2025 22:09] Abstract 4. Despite their remarkable performance, the development of Large Language Models (LLMs) faces a critical challenge in scalable oversight: providing effective feedback for tasks where human evaluation is difficult or where LLMs outperform humans. While there is growing interest in using LLMs for critiq...
[13.01.2025 22:09] ********************************************************************************
[13.01.2025 22:09] Abstract 5. Structured image understanding, such as interpreting tables and charts, requires strategically refocusing across various structures and texts within an image, forming a reasoning sequence to arrive at the final answer. However, current multimodal large language models (LLMs) lack this multihop selec...
[13.01.2025 22:09] ********************************************************************************
[13.01.2025 22:09] Abstract 6. Text-to-video generation has made remarkable advancements through diffusion models. However, Multi-Concept Video Customization (MCVC) remains a significant challenge. We identify two key challenges in this task: 1) the identity decoupling problem, where directly adopting existing customization metho...
[13.01.2025 22:09] ********************************************************************************
[13.01.2025 22:09] Abstract 7. Large language models (LLMs) have achieved remarkable performance in recent years but are fundamentally limited by the underlying training data. To improve models beyond the training data, recent works have explored how LLMs can be used to generate synthetic data for autonomous self-improvement. How...
[13.01.2025 22:09] ********************************************************************************
[13.01.2025 22:09] Abstract 8. Domain-adaptive post-training of large language models (LLMs) has emerged as a promising approach for specialized domains such as medicine and finance. However, significant challenges remain in identifying optimal adaptation criteria and training strategies across varying data and model configuratio...
[13.01.2025 22:09] ********************************************************************************
[13.01.2025 22:09] Abstract 9. Video personalization methods allow us to synthesize videos with specific concepts such as people, pets, and places. However, existing methods often focus on limited domains, require time-consuming optimization per subject, or support only a single subject. We present Video Alchemist - a video model...
[13.01.2025 22:09] ********************************************************************************
[13.01.2025 22:09] Abstract 10. This study demonstrates a novel approach to testing the security boundaries of Vision-Large Language Model (VLM/ LLM) using the EICAR test file embedded within JPEG images. We successfully executed four distinct protocols across multiple LLM platforms, including OpenAI GPT-4o, Microsoft Copilot, Goo...
[13.01.2025 22:09] Read previous papers.
[13.01.2025 22:09] Generating reviews via LLM API.
[13.01.2025 22:09] Using data from previous issue: {"categories": ["#multimodal", "#rag", "#interpretability", "#hallucinations", "#video"], "emoji": "üé•", "ru": {"title": "VideoRAG: –û–±–æ–≥–∞—â–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "VideoRAG - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç
[13.01.2025 22:09] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#robotics", "#3d", "#transfer_learning", "#agi"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–∫—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º VLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
[13.01.2025 22:09] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#training", "#multimodal", "#open_source", "#reasoning"], "emoji": "üß†", "ru": {"title": "–®–∞–≥ –∑–∞ —à–∞–≥–æ–º –∫ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–º—É –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –ø–æ—à–∞–≥–æ–≤–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[13.01.2025 22:09] Using data from previous issue: {"categories": ["#benchmark", "#survey", "#video", "#reasoning"], "emoji": "‚è±Ô∏è", "ru": {"title": "–í—Ä–µ–º–µ–Ω–Ω–∞—è –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç—å –∫–∞–∫ –∫–ª—é—á –∫ –æ–Ω–ª–∞–π–Ω-–∞–Ω–∞–ª–∏–∑—É –≤–∏–¥–µ–æ –¥–ª—è LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ OVO-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ-LLM –º–æ–¥–µ–ª–µ–π –∫ –æ–Ω–ª–∞–π–Ω-–∞–Ω–∞–ª–∏–∑—É –≤–∏–¥–µ–æ —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–Ω
[13.01.2025 22:09] Using data from previous issue: {"categories": ["#training", "#benchmark", "#optimization", "#rlhf", "#synthetic"], "emoji": "üî¨", "ru": {"title": "SCRIT: –°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É—é—â–∏–π—Å—è –∫—Ä–∏—Ç–∏–∫ –¥–ª—è LLM", "desc": "SCRIT - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫–µ –±–µ–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –Ω–∞–¥–∑–æ—Ä–∞. –û–Ω–∞ –∏—Å–ø–æ–ª
[13.01.2025 22:09] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#dataset", "#reasoning", "#training", "#cv"], "emoji": "üîç", "ru": {"title": "ReFocus: –£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è LLM —á–µ—Ä–µ–∑ —É–ø—Ä–∞–≤–ª—è–µ–º–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ReFocus - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–¥–µ–ª—è–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª
[13.01.2025 22:09] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#data", "#video", "#dataset"], "emoji": "üé¨", "ru": {"title": "ConceptMaster: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ConceptMaster - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ü–µ–ø—Ç–∞–º–∏. 
[13.01.2025 22:09] Using data from previous issue: {"categories": ["#synthetic", "#reasoning", "#training", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ —É–ª—É—á—à–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø
[13.01.2025 22:09] Using data from previous issue: {"categories": ["#optimization", "#rlhf", "#healthcare", "#transfer_learning", "#training"], "emoji": "üíπ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è LLM –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤: –æ—Ç –∞–Ω–∞–ª–∏–∑–∞ –¥–æ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FINDAP - —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–æ–º–µ–Ω–Ω–æ-–∞–¥–∞–ø—Ç–∏–≤–Ω–æ–º—É –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ
[13.01.2025 22:09] Using data from previous issue: {"categories": ["#diffusion", "#synthetic", "#benchmark", "#data", "#optimization", "#video", "#dataset"], "emoji": "üé≠", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –≤–∏–¥–µ–æ –±–µ–∑ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Video Alchemist - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å
[13.01.2025 22:09] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#data", "#security"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ù–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ VLM/LLM —Å –ø–æ–º–æ—â—å—é EICAR", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –≥—Ä–∞–Ω–∏—Ü –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Ç–∏–ø–∞ Vision-Large Language Model (VLM/LLM)
[13.01.2025 22:09] Loading Chinese text from previous data.
[13.01.2025 22:09] Renaming data file.
[13.01.2025 22:09] Renaming previous data. hf_papers.json to ./d/2025-01-13.json
[13.01.2025 22:09] Saving new data file.
[13.01.2025 22:09] Generating page.
[13.01.2025 22:09] Renaming previous page.
[13.01.2025 22:09] Renaming previous data. index.html to ./d/2025-01-13.html
[13.01.2025 22:09] [Experimental] Generating Chinese page for reading.
[13.01.2025 22:09] Chinese vocab [{'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generate'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'}, {'word': 'ÈîôËØØ', 'pinyin': 'cu√≤w√π', 'trans': 'error'}, {'word': '‰ø°ÊÅØ', 'pinyin': 'x√¨nxƒ´', 'trans': 'information'}, {'word': 'Ê£ÄÁ¥¢', 'pinyin': 'ji«énsu«í', 'trans': 'retrieve'}, {'word': 'Êü•ËØ¢', 'pinyin': 'ch√°x√∫n', 'trans': 'query'}, {'word': 'Áõ∏ÂÖ≥', 'pinyin': 'xiƒÅngguƒÅn', 'trans': 'related'}, {'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ju√©', 'trans': 'visual'}, {'word': 'Âà©Áî®', 'pinyin': 'l√¨y√≤ng', 'trans': 'utilize'}, {'word': 'ËßÜÈ¢ë', 'pinyin': 'sh√¨p√≠n', 'trans': 'video'}, {'word': 'ÊñáÊú¨', 'pinyin': 'w√©nbƒõn', 'trans': 'text'}, {'word': 'ËæìÂá∫', 'pinyin': 'sh≈´ch≈´', 'trans': 'output'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'}, {'word': 'Âü∫‰∫é', 'pinyin': 'jƒ´y√∫', 'trans': 'based on'}, {'word': 'Â§ßÂûã', 'pinyin': 'd√†x√≠ng', 'trans': 'large-scale'}, {'word': 'ËØ≠Ë®Ä', 'pinyin': 'y«îy√°n', 'trans': 'language'}, {'word': 'Â§ÑÁêÜ', 'pinyin': 'ch«îl«ê', 'trans': 'process'}, {'word': 'Ë°®Á§∫', 'pinyin': 'bi«éosh√¨', 'trans': 'represent'}, {'word': 'ÂÜÖÂÆπ', 'pinyin': 'n√®ir√≥ng', 'trans': 'content'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'}, {'word': 'ËØÅÊòé', 'pinyin': 'zh√®ngm√≠ng', 'trans': 'prove'}, {'word': 'ÊúâÊïà', 'pinyin': 'y«íuxi√†o', 'trans': 'effective'}]
[13.01.2025 22:09] Renaming previous Chinese page.
[13.01.2025 22:09] Renaming previous data. zh.html to ./d/2025-01-12_zh_reading_task.html
[13.01.2025 22:09] Writing Chinese reading task.
[13.01.2025 22:09] Writing result.
[13.01.2025 22:09] Renaming log file.
[13.01.2025 22:09] Renaming previous data. log.txt to ./logs/2025-01-13_last_log.txt
