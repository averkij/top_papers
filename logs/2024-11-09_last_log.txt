[09.11.2024 08:14] Read previous papers.
[09.11.2024 08:14] Generating top page (month).
[09.11.2024 08:14] Writing top page (month).
[09.11.2024 10:11] Read previous papers.
[09.11.2024 10:11] Get feed.
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04905
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.05003
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04965
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04928
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04996
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04709
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04496
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.05000
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.05001
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04952
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04923
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.05007
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04999
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.05005
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04989
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04752
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04075
[09.11.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04335
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 0. Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems.While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scienti...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 1. Recently, breakthroughs in video modeling have allowed for controllable camera trajectories in generated videos. However, these methods cannot be directly applied to user-provided videos that are not generated by a video model. In this paper, we present ReCapture, a method for generating new videos ...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 2. Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 3. In this paper, we introduce DimensionX, a framework designed to generate photorealistic 3D and 4D scenes from just a single image with video diffusion. Our approach begins with the insight that both the spatial structure of a 3D scene and the temporal evolution of a 4D scene can be effectively repre...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 4. The development of large language models (LLMs) has expanded to multi-modal systems capable of processing text, images, and speech within a unified framework. Training these models demands significantly larger datasets and computational resources compared to text-only LLMs. To address the scaling ch...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 5. Video generation models are revolutionizing content creation, with image-to-video models drawing increasing attention due to their enhanced controllability, visual consistency, and practical applications. However, despite their popularity, these models rely on user-provided text and image prompts, a...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 6. To increase social bonding with interlocutors, humans naturally acquire the ability to respond appropriately in a given situation by considering which conversational skill is most suitable for the response - a process we call skill-of-mind. For large language model (LLM)-based conversational agents,...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 7. As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 8. With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learnin...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 9. Document visual question answering (DocVQA) pipelines that answer questions from documents have broad applications. Existing methods focus on handling single-page documents with multi-modal language models (MLMs), or rely on text-based retrieval-augmented generation (RAG) that uses text extraction t...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 10. Fine-grained alignment between videos and text is challenging due to complex spatial and temporal dynamics in videos. Existing video-based Large Multimodal Models (LMMs) handle basic conversations but struggle with precise pixel-level grounding in videos. To address this, we introduce VideoGLaMM, a ...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 11. Diffusion models have been proven highly effective at generating high-quality images. However, as these models grow larger, they require significantly more memory and suffer from higher latency, posing substantial challenges for deployment. In this work, we aim to accelerate diffusion models by quan...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 12. Significant progress has been made in open-vocabulary mobile manipulation, where the goal is for a robot to perform tasks in any environment given a natural language description. However, most current systems assume a static environment, which limits the system's applicability in real-world scenario...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 13. Beyond high-fidelity image synthesis, diffusion models have recently exhibited promising results in dense visual perception tasks. However, most existing work treats diffusion models as a standalone component for perception tasks, employing them either solely for off-the-shelf data augmentation or a...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 14. Methods for image-to-video generation have achieved impressive, photo-realistic quality. However, adjusting specific elements in generated videos, such as object motion or camera movement, is often a tedious process of trial and error, e.g., involving re-generating videos with different random seeds...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 15. Code-mixing, the integration of lexical and grammatical elements from multiple languages within a single sentence, is a widespread linguistic phenomenon, particularly prevalent in multilingual societies. In India, social media users frequently engage in code-mixed conversations using the Roman scrip...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 16. Existing benchmarks for evaluating foundation models mainly focus on single-document, text-only tasks. However, they often fail to fully capture the complexity of research workflows, which typically involve interpreting non-textual data and gathering information across multiple documents. To address...
[09.11.2024 10:11] ********************************************************************************
[09.11.2024 10:11] Abstract 17. We present GazeGen, a user interaction system that generates visual content (images and videos) for locations indicated by the user's eye gaze. GazeGen allows intuitive manipulation of visual content by targeting regions of interest with gaze. Using advanced techniques in object detection and genera...
[09.11.2024 10:11] Read previous papers.
[09.11.2024 10:11] Generating reviews via LLM API.
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#dataset", "#data", "#training", "#synthetic", "#agents", "#plp"], "emoji": "ğŸ§‘â€ğŸ’»", "ru": {"title": "OpenCoder: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ĞºĞ½Ğ¸Ğ³Ğ° Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ğ¿Ğ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ´Ğ°", "desc": "OpenCoder - ÑÑ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ°Ñ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#video", "#diffusion", "#hallucinations"], "emoji": "ğŸ¥", "ru": {"title": "ĞŸĞµÑ€ĞµÑĞ½Ğ¸Ğ¼Ğ°ĞµĞ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ĞºÑƒÑ€ÑÑ‹ Ğ´Ğ»Ñ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "ReCapture - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ³ĞµĞ½ĞµÑ€Ğ¸
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#inference", "#optimization", "#architecture"], "emoji": "ğŸ§ ", "ru": {"title": "BitNet a4.8: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ¸Ñ€Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BitNet a4.8 - ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ 1-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 4-Ğ±Ğ¸Ñ‚Ğ½Ñ‹Ğµ Ğ°ĞºÑ‚
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#3d", "#video", "#synthetic"], "emoji": "ğŸ­", "ru": {"title": "ĞÑ‚ 2D Ğº 4D: DimensionX Ñ€Ğ°Ğ·Ğ´Ğ²Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "DimensionX - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D Ğ¸ 4D ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚Ñ
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#architecture", "#multimodal", "#training", "#optimization"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ° Ğ¶Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Mixture-of-Transformers (MoT) - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… 
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#dataset", "#video"], "emoji": "ğŸ¬", "ru": {"title": "TIP-I2V: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ TIP-I2V - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 1,70 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ 
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#dataset", "#agents", "#multimodal"], "emoji": "ğŸ—£ï¸", "ru": {"title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ñƒ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Multifaceted Skill-of-Mind, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ†Ğµ
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset", "#long_context"], "emoji": "ğŸ§µ", "ru": {"title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ»Ğ°Ğ±Ğ¸Ñ€Ğ¸Ğ½Ñ‚Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°: Ğ½Ğ¸Ñ‚Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#interpretability", "#alignment"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸: Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¦Ğ¸Ğ¿Ñ„Ğ¾Ğ¼ Ğ¸ Ñ…Ğ°Ğ¾ÑĞ¾Ğ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#benchmark", "#rag", "#dataset", "#multimodal", "#games", "#long_context"], "emoji": "ğŸ“„", "ru": {"title": "M3DocRAG: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ M3DocRAG - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ´Ğ¾Ğº
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#architecture", "#games", "#alignment", "#cv"], "emoji": "ğŸ¥", "ru": {"title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜", "desc": "VideoGLaMM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#inference", "#optimization", "#diffusion"], "emoji": "ğŸš€", "ru": {"title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SVDQuant Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#robotics", "#3d", "#multimodal", "#agents"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑÑ‰ĞµĞ¼ÑÑ Ğ¼Ğ¸Ñ€Ğµ", "desc": "DynaMem - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞµĞ¼Ğ°Ğ½
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#training", "#diffusion", "#data", "#multimodal", "#optimization", "#cv"], "emoji": "ğŸ”„", "ru": {"title": "Diff-2-in-1: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#video", "#diffusion", "#dataset"], "emoji": "ğŸ¬", "ru": {"title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ", "desc": "SG-I2V - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#data"], "emoji": "ğŸ—£ï¸", "ru": {"title": "Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ² Ğ² ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#reasoning", "#science"], "emoji": "ğŸ§ ", "ru": {"title": "M3SciQA: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°", "desc": "M3SciQA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°
[09.11.2024 10:11] Using data from previous issue: {"categories": ["#agents", "#cv", "#video", "#edge_computing", "#training"], "emoji": "ğŸ‘ï¸", "ru": {"title": "Ğ’Ğ·Ğ³Ğ»ÑĞ´Ğ¾Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞ¹: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ GazeGen, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾
[09.11.2024 10:11] Trying to get texts in Chinese.
[09.11.2024 10:11] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems.While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an ``open cookbook'' for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI.
[09.11.2024 10:11] Mistral response. {"id": "27a89edba905450bb34b8810131b7304", "object": "chat.completion", "created": 1731147073, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u3001\u63a8\u7406\u4efb\u52a1\u548c\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u3002\u867d\u7136\u5f00\u653e\u8bbf\u95ee\u7684\u4ee3\u7801LLMs\u6027\u80fd\u8d8a\u6765\u8d8a\u63a5\u8fd1\u4e13\u6709\u6a21\u578b\uff0c\u4f46\u9002\u5408\u4e25\u683c\u79d1\u5b66\u7814\u7a76\u7684\u9ad8\u8d28\u91cf\u4ee3\u7801LLMs\u4ecd\u7136\u6709\u9650\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f5c\u8005\u5f15\u5165\u4e86OpenCoder\uff0c\u4e00\u4e2a\u9876\u5c16\u7684\u4ee3\u7801LLMs\uff0c\u4e0d\u4ec5\u6027\u80fd\u5ab2\u7f8e\u9886\u5148\u6a21\u578b\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u6570\u636e\u5904\u7406\u6d41\u6c34\u7ebf\u548c\u900f\u660e\u7684\u8bad\u7ec3\u534f\u8bae\u3002\u901a\u8fc7\u53d1\u5e03\u6a21\u578b\u6743\u91cd\u3001\u63a8\u7406\u4ee3\u7801\u3001\u53ef\u590d\u73b0\u7684\u8bad\u7ec3\u6570\u636e\u548c\u8be6\u7ec6\u7684\u8bad\u7ec3\u534f\u8bae\uff0cOpenCoder\u65e8\u5728\u52a0\u901f\u7814\u7a76\u5e76\u5b9e\u73b0\u4ee3\u7801AI\u7684\u53ef\u590d\u73b0\u8fdb\u5c55\u3002", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 360, "total_tokens": 565, "completion_tokens": 205}}
[09.11.2024 10:11] Response: è¿™ç¯‡æ–‡ç« ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆã€æ¨ç†ä»»åŠ¡å’Œä»£ç†ç³»ç»Ÿä¸­çš„é‡è¦æ€§ã€‚è™½ç„¶å¼€æ”¾è®¿é—®çš„ä»£ç LLMsæ€§èƒ½è¶Šæ¥è¶Šæ¥è¿‘ä¸“æœ‰æ¨¡å‹ï¼Œä½†é€‚åˆä¸¥æ ¼ç§‘å­¦ç ”ç©¶çš„é«˜è´¨é‡ä»£ç LLMsä»ç„¶æœ‰é™ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œä½œè€…å¼•å…¥äº†OpenCoderï¼Œä¸€ä¸ªé¡¶å°–çš„ä»£ç LLMsï¼Œä¸ä»…æ€§èƒ½åª²ç¾é¢†å…ˆæ¨¡å‹ï¼Œè¿˜æä¾›äº†å®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿å’Œé€æ˜çš„è®­ç»ƒåè®®ã€‚é€šè¿‡å‘å¸ƒæ¨¡å‹æƒé‡ã€æ¨ç†ä»£ç ã€å¯å¤ç°çš„è®­ç»ƒæ•°æ®å’Œè¯¦ç»†çš„è®­ç»ƒåè®®ï¼ŒOpenCoderæ—¨åœ¨åŠ é€Ÿç ”ç©¶å¹¶å®ç°ä»£ç AIçš„å¯å¤ç°è¿›å±•ã€‚
[09.11.2024 10:11] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆã€æ¨ç†ä»»åŠ¡å’Œä»£ç†ç³»ç»Ÿä¸­çš„é‡è¦æ€§ã€‚è™½ç„¶å¼€æ”¾è®¿é—®çš„ä»£ç LLMsæ€§èƒ½è¶Šæ¥è¶Šæ¥è¿‘ä¸“æœ‰æ¨¡å‹ï¼Œä½†é€‚åˆä¸¥æ ¼ç§‘å­¦ç ”ç©¶çš„é«˜è´¨é‡ä»£ç LLMsä»ç„¶æœ‰é™ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œä½œè€…å¼•å…¥äº†OpenCoderï¼Œä¸€ä¸ªé¡¶å°–çš„ä»£ç LLMsï¼Œä¸ä»…æ€§èƒ½åª²ç¾é¢†å…ˆæ¨¡å‹ï¼Œè¿˜æä¾›äº†å®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿å’Œé€æ˜çš„è®­ç»ƒåè®®ã€‚é€šè¿‡å‘å¸ƒæ¨¡å‹æƒé‡ã€æ¨ç†ä»£ç ã€å¯å¤ç°çš„è®­ç»ƒæ•°æ®å’Œè¯¦ç»†çš„è®­ç»ƒåè®®ï¼ŒOpenCoderæ—¨åœ¨åŠ é€Ÿç ”ç©¶å¹¶å®ç°ä»£ç AIçš„å¯å¤ç°è¿›å±•ã€‚
[09.11.2024 10:11] Mistral response. {"id": "6b5d259235b64fcb941d8571abf21cc9", "object": "chat.completion", "created": 1731147082, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "Zh\u00e8 pi\u0101n w\u00e9nzh\u0101ng ji\u00e8sh\u00e0o le d\u00e0x\u00edng y\u01d4y\u00e1n m\u00f3x\u00edng (LLMs) z\u00e0i d\u00e0im\u01ce sh\u0113ngch\u00e9ng, tu\u012bl\u01d0 r\u00e8nw\u00f9 h\u00e9 d\u00e0il\u01d0 x\u00ect\u01d2ng zh\u014dng de zh\u00f2ngy\u00e0ox\u00ecng. Su\u012br\u00e1n k\u0101if\u00e0ng f\u01cengw\u00e8n de d\u00e0im\u01ce LLMs x\u00ecngn\u00e9ng yu\u00e8l\u00e1iyu\u00e8 j\u00ecnj\u00ecn zhu\u0101nh\u01cen m\u00f3x\u00edng, d\u00e0n sh\u00ec sh\u00ech\u00e9 y\u00e1nge k\u0113xu\u00e9 y\u00e1nji\u016b de g\u0101o zh\u00ecli\u00e0ng d\u00e0im\u01ce LLMs r\u00e9ngr\u00e1n y\u01d2uxi\u00e0n. W\u00e8ile ti\u00e1nb\u01d4 zh\u00e8 y\u012b k\u00f2ngb\u00e1i, zu\u00f2zh\u011b y\u01d0n r\u00f9le OpenCoder, y\u012bg\u00e8 d\u01d0ngji\u0101n de d\u00e0im\u01ce LLMs, b\u00f9j\u01d0n x\u00ecngn\u00e9ng j\u00ecm\u01d0 l\u01d0ngxi\u0101n m\u00f3x\u00edng, h\u00e1i t\u00edg\u014dngle w\u00e1nzh\u011bng de sh\u00f9j\u00f9 ch\u01d4l\u01d0 li\u00fashu\u01d0xi\u00e0n h\u00e9 t\u00f2um\u00ecng de x\u00f9nli\u00e0n xi\u00e9d\u00ecng. T\u014dnggu\u00f2 f\u0101b\u00f9 m\u00f3x\u00edng qu\u00e1nzh\u00f2ng, tu\u012bl\u01d0 d\u00e0im\u01ce, k\u011b f\u00f9ch\u00e1ng de x\u00f9nli\u00e0n sh\u00f9j\u00f9 h\u00e9 xi\u00e1ngx\u00ec de x\u00f9nli\u00e0n xi\u00e9d\u00ecng, OpenCoder zh\u01d0 y\u00fa ji\u0101s\u00f9 y\u00e1nji\u016b b\u00ecng sh\u00edxi\u00e0n d\u00e0im\u01ce AI de k\u011b f\u00f9ch\u00e1ng j\u00ecnb\u00f9.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 221, "total_tokens": 642, "completion_tokens": 421}}
[09.11.2024 10:11] Response: ZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zÃ i dÃ imÇ shÄ“ngchÃ©ng, tuÄ«lÇ rÃ¨nwÃ¹ hÃ© dÃ ilÇ xÃ¬tÇ’ng zhÅng de zhÃ²ngyÃ oxÃ¬ng. SuÄ«rÃ¡n kÄifÃ ng fÇngwÃ¨n de dÃ imÇ LLMs xÃ¬ngnÃ©ng yuÃ¨lÃ¡iyuÃ¨ jÃ¬njÃ¬n zhuÄnhÇn mÃ³xÃ­ng, dÃ n shÃ¬ shÃ¬hÃ© yÃ¡nge kÄ“xuÃ© yÃ¡njiÅ« de gÄo zhÃ¬liÃ ng dÃ imÇ LLMs rÃ©ngrÃ¡n yÇ’uxiÃ n. WÃ¨ile tiÃ¡nbÇ” zhÃ¨ yÄ« kÃ²ngbÃ¡i, zuÃ²zhÄ› yÇn rÃ¹le OpenCoder, yÄ«gÃ¨ dÇngjiÄn de dÃ imÇ LLMs, bÃ¹jÇn xÃ¬ngnÃ©ng jÃ¬mÇ lÇngxiÄn mÃ³xÃ­ng, hÃ¡i tÃ­gÅngle wÃ¡nzhÄ›ng de shÃ¹jÃ¹ chÇ”lÇ liÃºshuÇxiÃ n hÃ© tÃ²umÃ¬ng de xÃ¹nliÃ n xiÃ©dÃ¬ng. TÅngguÃ² fÄbÃ¹ mÃ³xÃ­ng quÃ¡nzhÃ²ng, tuÄ«lÇ dÃ imÇ, kÄ› fÃ¹chÃ¡ng de xÃ¹nliÃ n shÃ¹jÃ¹ hÃ© xiÃ¡ngxÃ¬ de xÃ¹nliÃ n xiÃ©dÃ¬ng, OpenCoder zhÇ yÃº jiÄsÃ¹ yÃ¡njiÅ« bÃ¬ng shÃ­xiÃ n dÃ imÇ AI de kÄ› fÃ¹chÃ¡ng jÃ¬nbÃ¹.
[09.11.2024 10:11] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆã€æ¨ç†ä»»åŠ¡å’Œä»£ç†ç³»ç»Ÿä¸­çš„é‡è¦æ€§ã€‚è™½ç„¶å¼€æ”¾è®¿é—®çš„ä»£ç LLMsæ€§èƒ½è¶Šæ¥è¶Šæ¥è¿‘ä¸“æœ‰æ¨¡å‹ï¼Œä½†é€‚åˆä¸¥æ ¼ç§‘å­¦ç ”ç©¶çš„é«˜è´¨é‡ä»£ç LLMsä»ç„¶æœ‰é™ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œä½œè€…å¼•å…¥äº†OpenCoderï¼Œä¸€ä¸ªé¡¶å°–çš„ä»£ç LLMsï¼Œä¸ä»…æ€§èƒ½åª²ç¾é¢†å…ˆæ¨¡å‹ï¼Œè¿˜æä¾›äº†å®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿å’Œé€æ˜çš„è®­ç»ƒåè®®ã€‚é€šè¿‡å‘å¸ƒæ¨¡å‹æƒé‡ã€æ¨ç†ä»£ç ã€å¯å¤ç°çš„è®­ç»ƒæ•°æ®å’Œè¯¦ç»†çš„è®­ç»ƒåè®®ï¼ŒOpenCoderæ—¨åœ¨åŠ é€Ÿç ”ç©¶å¹¶å®ç°ä»£ç AIçš„å¯å¤ç°è¿›å±•ã€‚
[09.11.2024 10:12] Mistral response. {"id": "e56125a3faff4054b2ede17dc8e14e62", "object": "chat.completion", "created": 1731147096, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "[\n    {\"word\": \"\u4ee3\u7801\u751f\u6210\", \"pinyin\": \"d\u00e0i m\u01ce sh\u0113ng ch\u011bng\", \"trans\": \"code generation\"},\n    {\"word\": \"\u63a8\u7406\u4efb\u52a1\", \"pinyin\": \"tu\u012b l\u01d0 r\u00e8n w\u00f9\", \"trans\": \"reasoning tasks\"},\n    {\"word\": \"\u4ee3\u7406\u7cfb\u7edf\", \"pinyin\": \"d\u00e0i l\u01d0 x\u00ec t\u01d2ng\", \"trans\": \"agent systems\"},\n    {\"word\": \"\u5f00\u653e\u8bbf\u95ee\", \"pinyin\": \"k\u0101i f\u00e0ng f\u01ceng w\u00e8n\", \"trans\": \"open access\"},\n    {\"word\": \"\u4e13\u6709\u6a21\u578b\", \"pinyin\": \"zhu\u0101n y\u01d2u m\u00f3 x\u00edng\", \"trans\": \"proprietary models\"},\n    {\"word\": \"\u4e25\u683c\u79d1\u5b66\u7814\u7a76\", \"pinyin\": \"y\u00e1n g\u0113 k\u0113 xu\u00e9 y\u00e1n ji\u016b\", \"trans\": \"rigorous scientific research\"},\n    {\"word\": \"\u9ad8\u8d28\u91cf\", \"pinyin\": \"g\u0101o zh\u00ec li\u00e0ng\", \"trans\": \"high quality\"},\n    {\"word\": \"\u586b\u8865\", \"pinyin\": \"ti\u00e1n b\u01d4\", \"trans\": \"fill\"},\n    {\"word\": \"\u7a7a\u767d\", \"pinyin\": \"k\u00f2ng b\u00e1i\", \"trans\": \"gap\"},\n    {\"word\": \"\u5f15\u5165\", \"pinyin\": \"y\u01d0n r\u00f9\", \"trans\": \"introduce\"},\n    {\"word\": \"\u9876\u5c16\", \"pinyin\": \"d\u01d0ng ji\u0101n\", \"trans\": \"top-notch\"},\n    {\"word\": \"\u5ab2\u7f8e\", \"pinyin\": \"p\u00ec m\u011bi\", \"trans\": \"rival\"},\n    {\"word\": \"\u9886\u5148\u6a21\u578b\", \"pinyin\": \"l\u01d0ng xi\u0101n m\u00f3 x\u00edng\", \"trans\": \"leading models\"},\n    {\"word\": \"\u5b8c\u6574\", \"pinyin\": \"w\u00e1n zh\u011bng\", \"trans\": \"complete\"},\n    {\"word\": \"\u6570\u636e\u5904\u7406\u6d41\u6c34\u7ebf\", \"pinyin\": \"sh\u00f9 j\u00f9 ch\u01d4 l\u01d0 li\u00fa shu\u01d0 xi\u00e0n\", \"trans\": \"data processing pipeline\"},\n    {\"word\": \"\u900f\u660e\", \"pinyin\": \"t\u00f2u m\u00edng\", \"trans\": \"transparent\"},\n    {\"word\": \"\u8bad\u7ec3\u534f\u8bae\", \"pinyin\": \"x\u00f9n li\u00e0n xi\u00e9 y\u00ec\", \"trans\": \"training protocol\"},\n    {\"word\": \"\u53d1\u5e03\", \"pinyin\": \"f\u0101 b\u00f9\", \"trans\": \"release\"},\n    {\"word\": \"\u6a21\u578b\u6743\u91cd\", \"pinyin\": \"m\u00f3 x\u00edng qu\u00e1n zh\u00f2ng\", \"trans\": \"model weights\"},\n    {\"word\": \"\u63a8\u7406\u4ee3\u7801\", \"pinyin\": \"tu\u012b l\u01d0 d\u00e0i m\u01ce\", \"trans\": \"inference code\"},\n    {\"word\": \"\u53ef\u590d\u73b0\", \"pinyin\": \"k\u011b f\u00f9 xi\u00e0n\", \"trans\": \"reproducible\"},\n    {\"word\": \"\u8bad\u7ec3\u6570\u636e\", \"pinyin\": \"x\u00f9n li\u00e0n sh\u00f9 j\u00f9\", \"trans\": \"training data\"},\n    {\"word\": \"\u8be6\u7ec6\", \"pinyin\": \"xi\u00e1ng x\u00ec\", \"trans\": \"detailed\"},\n    {\"word\": \"\u52a0\u901f\", \"pinyin\": \"ji\u0101 s\u00f9\", \"trans\": \"accelerate\"},\n    {\"word\": \"\u5b9e\u73b0\", \"pinyin\": \"sh\u00ed xi\u00e0n\", \"trans\": \"achieve\"},\n    {\"word\": \"\u4ee3\u7801AI\", \"pinyin\": \"d\u00e0i m\u01ce AI\", \"trans\": \"code AI\"},\n    {\"word\": \"\u8fdb\u5c55\", \"pinyin\": \"j\u00ecn zh\u01cen\", \"trans\": \"progress\"}\n]", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 253, "total_tokens": 1143, "completion_tokens": 890}}
[09.11.2024 10:12] Response: [
    {"word": "ä»£ç ç”Ÿæˆ", "pinyin": "dÃ i mÇ shÄ“ng chÄ›ng", "trans": "code generation"},
    {"word": "æ¨ç†ä»»åŠ¡", "pinyin": "tuÄ« lÇ rÃ¨n wÃ¹", "trans": "reasoning tasks"},
    {"word": "ä»£ç†ç³»ç»Ÿ", "pinyin": "dÃ i lÇ xÃ¬ tÇ’ng", "trans": "agent systems"},
    {"word": "å¼€æ”¾è®¿é—®", "pinyin": "kÄi fÃ ng fÇng wÃ¨n", "trans": "open access"},
    {"word": "ä¸“æœ‰æ¨¡å‹", "pinyin": "zhuÄn yÇ’u mÃ³ xÃ­ng", "trans": "proprietary models"},
    {"word": "ä¸¥æ ¼ç§‘å­¦ç ”ç©¶", "pinyin": "yÃ¡n gÄ“ kÄ“ xuÃ© yÃ¡n jiÅ«", "trans": "rigorous scientific research"},
    {"word": "é«˜è´¨é‡", "pinyin": "gÄo zhÃ¬ liÃ ng", "trans": "high quality"},
    {"word": "å¡«è¡¥", "pinyin": "tiÃ¡n bÇ”", "trans": "fill"},
    {"word": "ç©ºç™½", "pinyin": "kÃ²ng bÃ¡i", "trans": "gap"},
    {"word": "å¼•å…¥", "pinyin": "yÇn rÃ¹", "trans": "introduce"},
    {"word": "é¡¶å°–", "pinyin": "dÇng jiÄn", "trans": "top-notch"},
    {"word": "åª²ç¾", "pinyin": "pÃ¬ mÄ›i", "trans": "rival"},
    {"word": "é¢†å…ˆæ¨¡å‹", "pinyin": "lÇng xiÄn mÃ³ xÃ­ng", "trans": "leading models"},
    {"word": "å®Œæ•´", "pinyin": "wÃ¡n zhÄ›ng", "trans": "complete"},
    {"word": "æ•°æ®å¤„ç†æµæ°´çº¿", "pinyin": "shÃ¹ jÃ¹ chÇ” lÇ liÃº shuÇ xiÃ n", "trans": "data processing pipeline"},
    {"word": "é€æ˜", "pinyin": "tÃ²u mÃ­ng", "trans": "transparent"},
    {"word": "è®­ç»ƒåè®®", "pinyin": "xÃ¹n liÃ n xiÃ© yÃ¬", "trans": "training protocol"},
    {"word": "å‘å¸ƒ", "pinyin": "fÄ bÃ¹", "trans": "release"},
    {"word": "æ¨¡å‹æƒé‡", "pinyin": "mÃ³ xÃ­ng quÃ¡n zhÃ²ng", "trans": "model weights"},
    {"word": "æ¨ç†ä»£ç ", "pinyin": "tuÄ« lÇ dÃ i mÇ", "trans": "inference code"},
    {"word": "å¯å¤ç°", "pinyin": "kÄ› fÃ¹ xiÃ n", "trans": "reproducible"},
    {"word": "è®­ç»ƒæ•°æ®", "pinyin": "xÃ¹n liÃ n shÃ¹ jÃ¹", "trans": "training data"},
    {"word": "è¯¦ç»†", "pinyin": "xiÃ¡ng xÃ¬", "trans": "detailed"},
    {"word": "åŠ é€Ÿ", "pinyin": "jiÄ sÃ¹", "trans": "accelerate"},
    {"word": "å®ç°", "pinyin": "shÃ­ xiÃ n", "trans": "achieve"},
    {"word": "ä»£ç AI", "pinyin": "dÃ i mÇ AI", "trans": "code AI"},
    {"word": "è¿›å±•", "pinyin": "jÃ¬n zhÇn", "trans": "progress"}
]
[09.11.2024 10:12] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆã€æ¨ç†ä»»åŠ¡å’Œä»£ç†ç³»ç»Ÿä¸­çš„é‡è¦æ€§ã€‚è™½ç„¶å¼€æ”¾è®¿é—®çš„ä»£ç LLMsæ€§èƒ½è¶Šæ¥è¶Šæ¥è¿‘ä¸“æœ‰æ¨¡å‹ï¼Œä½†é€‚åˆä¸¥æ ¼ç§‘å­¦ç ”ç©¶çš„é«˜è´¨é‡ä»£ç LLMsä»ç„¶æœ‰é™ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œä½œè€…å¼•å…¥äº†OpenCoderï¼Œä¸€ä¸ªé¡¶å°–çš„ä»£ç LLMsï¼Œä¸ä»…æ€§èƒ½åª²ç¾é¢†å…ˆæ¨¡å‹ï¼Œè¿˜æä¾›äº†å®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿å’Œé€æ˜çš„è®­ç»ƒåè®®ã€‚é€šè¿‡å‘å¸ƒæ¨¡å‹æƒé‡ã€æ¨ç†ä»£ç ã€å¯å¤ç°çš„è®­ç»ƒæ•°æ®å’Œè¯¦ç»†çš„è®­ç»ƒåè®®ï¼ŒOpenCoderæ—¨åœ¨åŠ é€Ÿç ”ç©¶å¹¶å®ç°ä»£ç AIçš„å¯å¤ç°è¿›å±•ã€‚
[09.11.2024 10:12] Mistral response. {"id": "4fee5eb27b83440f9ed57a6a58cbab3f", "object": "chat.completion", "created": 1731147121, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "This article discusses the importance of large language models (LLMs) in code generation, reasoning tasks, and agent systems. While the performance of open-access code LLMs is increasingly approaching that of proprietary models, high-quality code LLMs suitable for rigorous scientific research remain limited. To address this gap, the authors introduce OpenCoder, a top-tier code LLM that not only matches the performance of leading models but also provides a complete data processing pipeline and transparent training protocols. By releasing model weights, inference code, reproducible training data, and detailed training protocols, OpenCoder aims to accelerate research and achieve reproducible progress in code AI.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 219, "total_tokens": 362, "completion_tokens": 143}}
[09.11.2024 10:12] Response: This article discusses the importance of large language models (LLMs) in code generation, reasoning tasks, and agent systems. While the performance of open-access code LLMs is increasingly approaching that of proprietary models, high-quality code LLMs suitable for rigorous scientific research remain limited. To address this gap, the authors introduce OpenCoder, a top-tier code LLM that not only matches the performance of leading models but also provides a complete data processing pipeline and transparent training protocols. By releasing model weights, inference code, reproducible training data, and detailed training protocols, OpenCoder aims to accelerate research and achieve reproducible progress in code AI.
[09.11.2024 10:12] Renaming data file.
[09.11.2024 10:12] Renaming previous data. hf_papers.json to ./d/2024-11-08.json
[09.11.2024 10:12] Saving new data file.
[09.11.2024 10:12] Generating page.
[09.11.2024 10:12] Renaming previous page.
[09.11.2024 10:12] Renaming previous data. index.html to ./d/2024-11-08.html
[09.11.2024 10:12] [Experimental] Generating Chinese page for reading.
[09.11.2024 10:12] Chinese vocab [{'word': 'ä»£ç ç”Ÿæˆ', 'pinyin': 'dÃ i mÇ shÄ“ng chÄ›ng', 'trans': 'code generation'}, {'word': 'æ¨ç†ä»»åŠ¡', 'pinyin': 'tuÄ« lÇ rÃ¨n wÃ¹', 'trans': 'reasoning tasks'}, {'word': 'ä»£ç†ç³»ç»Ÿ', 'pinyin': 'dÃ i lÇ xÃ¬ tÇ’ng', 'trans': 'agent systems'}, {'word': 'å¼€æ”¾è®¿é—®', 'pinyin': 'kÄi fÃ ng fÇng wÃ¨n', 'trans': 'open access'}, {'word': 'ä¸“æœ‰æ¨¡å‹', 'pinyin': 'zhuÄn yÇ’u mÃ³ xÃ­ng', 'trans': 'proprietary models'}, {'word': 'ä¸¥æ ¼ç§‘å­¦ç ”ç©¶', 'pinyin': 'yÃ¡n gÄ“ kÄ“ xuÃ© yÃ¡n jiÅ«', 'trans': 'rigorous scientific research'}, {'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬ liÃ ng', 'trans': 'high quality'}, {'word': 'å¡«è¡¥', 'pinyin': 'tiÃ¡n bÇ”', 'trans': 'fill'}, {'word': 'ç©ºç™½', 'pinyin': 'kÃ²ng bÃ¡i', 'trans': 'gap'}, {'word': 'å¼•å…¥', 'pinyin': 'yÇn rÃ¹', 'trans': 'introduce'}, {'word': 'é¡¶å°–', 'pinyin': 'dÇng jiÄn', 'trans': 'top-notch'}, {'word': 'åª²ç¾', 'pinyin': 'pÃ¬ mÄ›i', 'trans': 'rival'}, {'word': 'é¢†å…ˆæ¨¡å‹', 'pinyin': 'lÇng xiÄn mÃ³ xÃ­ng', 'trans': 'leading models'}, {'word': 'å®Œæ•´', 'pinyin': 'wÃ¡n zhÄ›ng', 'trans': 'complete'}, {'word': 'æ•°æ®å¤„ç†æµæ°´çº¿', 'pinyin': 'shÃ¹ jÃ¹ chÇ” lÇ liÃº shuÇ xiÃ n', 'trans': 'data processing pipeline'}, {'word': 'é€æ˜', 'pinyin': 'tÃ²u mÃ­ng', 'trans': 'transparent'}, {'word': 'è®­ç»ƒåè®®', 'pinyin': 'xÃ¹n liÃ n xiÃ© yÃ¬', 'trans': 'training protocol'}, {'word': 'å‘å¸ƒ', 'pinyin': 'fÄ bÃ¹', 'trans': 'release'}, {'word': 'æ¨¡å‹æƒé‡', 'pinyin': 'mÃ³ xÃ­ng quÃ¡n zhÃ²ng', 'trans': 'model weights'}, {'word': 'æ¨ç†ä»£ç ', 'pinyin': 'tuÄ« lÇ dÃ i mÇ', 'trans': 'inference code'}, {'word': 'å¯å¤ç°', 'pinyin': 'kÄ› fÃ¹ xiÃ n', 'trans': 'reproducible'}, {'word': 'è®­ç»ƒæ•°æ®', 'pinyin': 'xÃ¹n liÃ n shÃ¹ jÃ¹', 'trans': 'training data'}, {'word': 'è¯¦ç»†', 'pinyin': 'xiÃ¡ng xÃ¬', 'trans': 'detailed'}, {'word': 'åŠ é€Ÿ', 'pinyin': 'jiÄ sÃ¹', 'trans': 'accelerate'}, {'word': 'å®ç°', 'pinyin': 'shÃ­ xiÃ n', 'trans': 'achieve'}, {'word': 'ä»£ç AI', 'pinyin': 'dÃ i mÇ AI', 'trans': 'code AI'}, {'word': 'è¿›å±•', 'pinyin': 'jÃ¬n zhÇn', 'trans': 'progress'}]
[09.11.2024 10:12] Renaming previous Chinese page.
[09.11.2024 10:12] Renaming previous data. zh.html to ./d/2024-11-08_zh_reading_task.html
[09.11.2024 10:12] Writing Chinese reading task.
[09.11.2024 10:12] Writing result.
[09.11.2024 10:12] Renaming log file.
[09.11.2024 10:12] Renaming previous data. log.txt to ./logs/2024-11-09_last_log.txt
