[08.11.2024 22:11] Read previous papers.
[08.11.2024 22:11] Generating top page (month).
[08.11.2024 22:11] Writing top page (month).
[09.11.2024 00:55] Read previous papers.
[09.11.2024 00:55] Get feed.
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04905
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.05003
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04965
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04928
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04996
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04709
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04496
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.05000
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04923
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.05007
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04989
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04999
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04335
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04752
[09.11.2024 00:55] Extract page data from URL. URL: https://huggingface.co/papers/2411.05001
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04075
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04952
[09.11.2024 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2411.05005
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 0. Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems.While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scienti...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 1. Recently, breakthroughs in video modeling have allowed for controllable camera trajectories in generated videos. However, these methods cannot be directly applied to user-provided videos that are not generated by a video model. In this paper, we present ReCapture, a method for generating new videos ...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 2. Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 3. In this paper, we introduce DimensionX, a framework designed to generate photorealistic 3D and 4D scenes from just a single image with video diffusion. Our approach begins with the insight that both the spatial structure of a 3D scene and the temporal evolution of a 4D scene can be effectively repre...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 4. The development of large language models (LLMs) has expanded to multi-modal systems capable of processing text, images, and speech within a unified framework. Training these models demands significantly larger datasets and computational resources compared to text-only LLMs. To address the scaling ch...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 5. Video generation models are revolutionizing content creation, with image-to-video models drawing increasing attention due to their enhanced controllability, visual consistency, and practical applications. However, despite their popularity, these models rely on user-provided text and image prompts, a...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 6. To increase social bonding with interlocutors, humans naturally acquire the ability to respond appropriately in a given situation by considering which conversational skill is most suitable for the response - a process we call skill-of-mind. For large language model (LLM)-based conversational agents,...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 7. As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 8. Fine-grained alignment between videos and text is challenging due to complex spatial and temporal dynamics in videos. Existing video-based Large Multimodal Models (LMMs) handle basic conversations but struggle with precise pixel-level grounding in videos. To address this, we introduce VideoGLaMM, a ...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 9. Diffusion models have been proven highly effective at generating high-quality images. However, as these models grow larger, they require significantly more memory and suffer from higher latency, posing substantial challenges for deployment. In this work, we aim to accelerate diffusion models by quan...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 10. Methods for image-to-video generation have achieved impressive, photo-realistic quality. However, adjusting specific elements in generated videos, such as object motion or camera movement, is often a tedious process of trial and error, e.g., involving re-generating videos with different random seeds...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 11. Significant progress has been made in open-vocabulary mobile manipulation, where the goal is for a robot to perform tasks in any environment given a natural language description. However, most current systems assume a static environment, which limits the system's applicability in real-world scenario...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 12. We present GazeGen, a user interaction system that generates visual content (images and videos) for locations indicated by the user's eye gaze. GazeGen allows intuitive manipulation of visual content by targeting regions of interest with gaze. Using advanced techniques in object detection and genera...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 13. Code-mixing, the integration of lexical and grammatical elements from multiple languages within a single sentence, is a widespread linguistic phenomenon, particularly prevalent in multilingual societies. In India, social media users frequently engage in code-mixed conversations using the Roman scrip...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 14. With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learnin...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 15. Existing benchmarks for evaluating foundation models mainly focus on single-document, text-only tasks. However, they often fail to fully capture the complexity of research workflows, which typically involve interpreting non-textual data and gathering information across multiple documents. To address...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 16. Document visual question answering (DocVQA) pipelines that answer questions from documents have broad applications. Existing methods focus on handling single-page documents with multi-modal language models (MLMs), or rely on text-based retrieval-augmented generation (RAG) that uses text extraction t...
[09.11.2024 00:55] ********************************************************************************
[09.11.2024 00:55] Abstract 17. Beyond high-fidelity image synthesis, diffusion models have recently exhibited promising results in dense visual perception tasks. However, most existing work treats diffusion models as a standalone component for perception tasks, employing them either solely for off-the-shelf data augmentation or a...
[09.11.2024 00:55] Read previous papers.
[09.11.2024 00:55] Generating reviews via LLM API.
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#dataset", "#data", "#training", "#synthetic", "#agents", "#plp"], "emoji": "🧑‍💻", "ru": {"title": "OpenCoder: открытая книга рецептов для создания топовых языковых моделей кода", "desc": "OpenCoder - это высококачественная языковая модель для работы с кодом, сопоставимая по произво
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#video", "#diffusion", "#hallucinations"], "emoji": "🎥", "ru": {"title": "Переснимаем реальность: новые ракурсы для любого видео", "desc": "ReCapture - это метод генерации новых видео с измененными траекториями камеры на основе одного пользовательского видео. Он позволяет перегенери
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#inference", "#optimization", "#architecture"], "emoji": "🧠", "ru": {"title": "BitNet a4.8: Эффективность и производительность в мире языковых моделей", "desc": "Статья представляет BitNet a4.8 - усовершенствованную версию 1-битной языковой модели. Эта модель использует 4-битные акт
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#3d", "#video", "#synthetic"], "emoji": "🎭", "ru": {"title": "От 2D к 4D: DimensionX раздвигает границы генеративных моделей", "desc": "DimensionX - это фреймворк для генерации фотореалистичных 3D и 4D сцен из одного изображения с помощью видео-диффузии. Ключевым компонентом являетс
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#architecture", "#multimodal", "#training", "#optimization"], "emoji": "🧠", "ru": {"title": "Эффективные мультимодальные трансформеры: меньше вычислений, та же мощность", "desc": "Эта статья представляет Mixture-of-Transformers (MoT) - новую архитектуру для мультимодальных языковых 
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#dataset", "#video"], "emoji": "🎬", "ru": {"title": "TIP-I2V: Революция в изучении промптов для генерации видео из изображений", "desc": "Исследователи представили TIP-I2V - первый крупномасштабный набор данных, содержащий более 1,70 миллиона уникальных пользовательских текстовых и 
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#dataset", "#agents", "#multimodal"], "emoji": "🗣️", "ru": {"title": "Языковые модели учатся искусству общения", "desc": "Исследователи представили новый набор данных под названием Multifaceted Skill-of-Mind, содержащий аннотации навыков ведения диалога в различных интерактивных сце
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset", "#long_context"], "emoji": "🧵", "ru": {"title": "Языковые модели в лабиринте длинного контекста: нити информации и границы понимания", "desc": "Исследование посвящено анализу способностей современных языковых моделей (LLM) эффективно использов
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#architecture", "#games", "#alignment", "#cv"], "emoji": "🎥", "ru": {"title": "Точная локализация в видео с помощью мультимодального ИИ", "desc": "VideoGLaMM - это новая модель крупномасштабного мультимодального обучения, разработанная для точной пиксельно
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#inference", "#optimization", "#diffusion"], "emoji": "🚀", "ru": {"title": "Ускорение диффузионных моделей с помощью 4-битного квантования", "desc": "Статья представляет новый метод квантования под названием SVDQuant для ускорения диффузионных моделей. Метод использует низкоранговую
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#video", "#diffusion", "#dataset"], "emoji": "🎬", "ru": {"title": "Управляемая генерация видео без дополнительного обучения", "desc": "SG-I2V - это новый фреймворк для управляемой генерации видео из изображений. Он использует предобученную диффузионную модель без необходимости допол
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#robotics", "#3d", "#multimodal", "#agents"], "emoji": "🤖", "ru": {"title": "Динамическая память для роботов в изменяющемся мире", "desc": "DynaMem - новый подход к мобильной манипуляции с открытым словарем в динамических средах. Система использует динамическую пространственно-семан
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#agents", "#cv", "#video", "#edge_computing", "#training"], "emoji": "👁️", "ru": {"title": "Взглядом управляй: революция в генерации визуального контента", "desc": "Исследователи представляют систему GazeGen, которая генерирует визуальный контент на основе направления взгляда пользо
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#data"], "emoji": "🗣️", "ru": {"title": "Извлечение информации из многоязычных разговоров: новый подход к обработке смешанных текстов", "desc": "Статья посвящена проблеме извлечения релевантной информации из смешанных языковых разговоров в социальных сет
[09.11.2024 00:55] Querying the API.
[09.11.2024 00:55] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learning joint alignments between visual and human languages. However, little is known about the statistical behavior of these visual languages - whether they follow similar frequency distributions, grammatical structures, or topologies as natural languages. In this paper, we take a natural-language-centric approach to analyzing discrete visual languages and uncover striking similarities and fundamental differences. We demonstrate that, although visual languages adhere to Zipfian distributions, higher token innovation drives greater entropy and lower compression, with tokens predominantly representing object parts, indicating intermediate granularity. We also show that visual languages lack cohesive grammatical structures, leading to higher perplexity and weaker hierarchical organization compared to natural languages. Finally, we demonstrate that, while vision models align more closely with natural languages than other models, this alignment remains significantly weaker than the cohesion found within natural languages. Through these experiments, we demonstrate how understanding the statistical properties of discrete visual languages can inform the design of more effective computer vision models.
[09.11.2024 00:55] Response: {
  "desc": "Статья исследует статистические свойства дискретных визуальных языков, используемых в современных моделях компьютерного зрения. Авторы обнаружили, что визуальные языки следуют распределению Ципфа, но имеют более высокую энтропию и меньшую сжимаемость по сравнению с естественными языками. Визуальные токены в основном представляют части объектов, а не целые объекты или сцены. Исследование также показало, что визуальным языкам не хватает связной грамматической структуры и иерархической организации, характерной для естественных языков.",

  "emoji": "🖼️",

  "title": "Визуальные языки: между Ципфом и хаосом"
}
[09.11.2024 00:55] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learning joint alignments between visual and human languages. However, little is known about the statistical behavior of these visual languages - whether they follow similar frequency distributions, grammatical structures, or topologies as natural languages. In this paper, we take a natural-language-centric approach to analyzing discrete visual languages and uncover striking similarities and fundamental differences. We demonstrate that, although visual languages adhere to Zipfian distributions, higher token innovation drives greater entropy and lower compression, with tokens predominantly representing object parts, indicating intermediate granularity. We also show that visual languages lack cohesive grammatical structures, leading to higher perplexity and weaker hierarchical organization compared to natural languages. Finally, we demonstrate that, while vision models align more closely with natural languages than other models, this alignment remains significantly weaker than the cohesion found within natural languages. Through these experiments, we demonstrate how understanding the statistical properties of discrete visual languages can inform the design of more effective computer vision models."

[09.11.2024 00:55] Response: ```python
['CV', 'MULTIMODAL']
```
[09.11.2024 00:55] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learning joint alignments between visual and human languages. However, little is known about the statistical behavior of these visual languages - whether they follow similar frequency distributions, grammatical structures, or topologies as natural languages. In this paper, we take a natural-language-centric approach to analyzing discrete visual languages and uncover striking similarities and fundamental differences. We demonstrate that, although visual languages adhere to Zipfian distributions, higher token innovation drives greater entropy and lower compression, with tokens predominantly representing object parts, indicating intermediate granularity. We also show that visual languages lack cohesive grammatical structures, leading to higher perplexity and weaker hierarchical organization compared to natural languages. Finally, we demonstrate that, while vision models align more closely with natural languages than other models, this alignment remains significantly weaker than the cohesion found within natural languages. Through these experiments, we demonstrate how understanding the statistical properties of discrete visual languages can inform the design of more effective computer vision models."

[09.11.2024 00:55] Response: ```python
["ALIGNMENT", "INTERPRETABILITY"]
```
[09.11.2024 00:55] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores the statistical properties of discrete visual languages used in transformer-based models for vision and language tasks. It reveals that these visual languages exhibit Zipfian distributions, but their token innovation leads to higher entropy and lower compression, primarily representing object parts. The study finds that visual languages lack cohesive grammatical structures, resulting in higher perplexity and less hierarchical organization compared to natural languages. Ultimately, the research highlights the importance of understanding these properties to improve the design of computer vision models.","title":"Unlocking the Secrets of Visual Languages in AI"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper explores the statistical properties of discrete visual languages used in transformer-based models for vision and language tasks. It reveals that these visual languages exhibit Zipfian distributions, but their token innovation leads to higher entropy and lower compression, primarily representing object parts. The study finds that visual languages lack cohesive grammatical structures, resulting in higher perplexity and less hierarchical organization compared to natural languages. Ultimately, the research highlights the importance of understanding these properties to improve the design of computer vision models.', title='Unlocking the Secrets of Visual Languages in AI'))
[09.11.2024 00:55] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文探讨了基于变换器的视觉和语言任务模型（如LLaVA和Chameleon）中图像的离散标记表示。研究发现，视觉语言虽然遵循Zipf分布，但更高的标记创新会导致更大的熵和更低的压缩率，标记主要代表物体部分，显示出中间粒度。与自然语言相比，视觉语言缺乏连贯的语法结构，导致更高的困惑度和较弱的层次组织。通过这些实验，我们揭示了离散视觉语言的统计特性如何影响计算机视觉模型的设计。","title":"揭示视觉语言的统计特性"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='本文探讨了基于变换器的视觉和语言任务模型（如LLaVA和Chameleon）中图像的离散标记表示。研究发现，视觉语言虽然遵循Zipf分布，但更高的标记创新会导致更大的熵和更低的压缩率，标记主要代表物体部分，显示出中间粒度。与自然语言相比，视觉语言缺乏连贯的语法结构，导致更高的困惑度和较弱的层次组织。通过这些实验，我们揭示了离散视觉语言的统计特性如何影响计算机视觉模型的设计。', title='揭示视觉语言的统计特性'))
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#reasoning", "#science"], "emoji": "🧠", "ru": {"title": "M3SciQA: Новый рубеж в оценке фундаментальных моделей для научного анализа", "desc": "M3SciQA - это новый бенчмарк для оценки фундаментальных моделей в области научных вопросов и ответов. Он включа
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#benchmark", "#rag", "#dataset", "#multimodal", "#games", "#long_context"], "emoji": "📄", "ru": {"title": "M3DocRAG: Мультимодальные ответы на вопросы по документам нового поколения", "desc": "Статья представляет M3DocRAG - новую мультимодальную систему для ответов на вопросы по док
[09.11.2024 00:55] Using data from previous issue: {"categories": ["#training", "#diffusion", "#data", "#multimodal", "#optimization", "#cv"], "emoji": "🔄", "ru": {"title": "Diff-2-in-1: Объединение генерации и восприятия в диффузионных моделях", "desc": "Статья представляет новый универсальный фреймворк на основе диффузионных моделей под названием 
[09.11.2024 00:55] Loading Chinese text from previous data.
[09.11.2024 00:55] Renaming data file.
[09.11.2024 00:55] Renaming previous data. hf_papers.json to ./d/2024-11-08.json
[09.11.2024 00:55] Saving new data file.
[09.11.2024 00:55] Generating page.
[09.11.2024 00:55] Renaming previous page.
[09.11.2024 00:55] Renaming previous data. index.html to ./d/2024-11-08.html
[09.11.2024 00:55] [Experimental] Generating Chinese page for reading.
[09.11.2024 00:55] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '大型', 'pinyin': 'dà xíng', 'trans': 'large-scale'}, {'word': '语言模型', 'pinyin': 'yǔ yán mó xíng', 'trans': 'language model'}, {'word': '代码生成', 'pinyin': 'dài mǎ shēng chéng', 'trans': 'code generation'}, {'word': '推理任务', 'pinyin': 'tuī lǐ rèn wù', 'trans': 'reasoning tasks'}, {'word': '代理系统', 'pinyin': 'dài lǐ xì tǒng', 'trans': 'proxy system'}, {'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'}, {'word': '开放访问', 'pinyin': 'kāi fàng fǎng wèn', 'trans': 'open access'}, {'word': '专有模型', 'pinyin': 'zhuān yǒu mó xíng', 'trans': 'proprietary model'}, {'word': '严谨', 'pinyin': 'yán jǐn', 'trans': 'rigorous'}, {'word': '科学研究', 'pinyin': 'kē xué yán jiū', 'trans': 'scientific research'}, {'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'}, {'word': '有限', 'pinyin': 'yǒu xiàn', 'trans': 'limited'}, {'word': '填补', 'pinyin': 'tián bǔ', 'trans': 'fill'}, {'word': '空白', 'pinyin': 'kòng bái', 'trans': 'gap'}, {'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'}, {'word': '顶尖', 'pinyin': 'dǐng jiān', 'trans': 'top-notch'}, {'word': '堪比', 'pinyin': 'kān bǐ', 'trans': 'comparable'}, {'word': '领先模型', 'pinyin': 'lǐng xiān mó xíng', 'trans': 'leading model'}, {'word': '食谱', 'pinyin': 'shí pǔ', 'trans': 'recipe'}, {'word': '模型权重', 'pinyin': 'mó xíng quán zhòng', 'trans': 'model weights'}, {'word': '推理代码', 'pinyin': 'tuī lǐ dài mǎ', 'trans': 'inference code'}, {'word': '可重复', 'pinyin': 'kě chóng fù', 'trans': 'reproducible'}, {'word': '训练数据', 'pinyin': 'xùn liàn shù jù', 'trans': 'training data'}, {'word': '完整', 'pinyin': 'wán zhěng', 'trans': 'complete'}, {'word': '数据处理流水线', 'pinyin': 'shù jù chǔ lǐ liú shuǐ xiàn', 'trans': 'data processing pipeline'}, {'word': '严格', 'pinyin': 'yán gé', 'trans': 'strict'}, {'word': '实验消融结果', 'pinyin': 'shí yàn xiāo róng jié guǒ', 'trans': 'experimental ablation results'}, {'word': '详细', 'pinyin': 'xiáng xì', 'trans': 'detailed'}, {'word': '训练协议', 'pinyin': 'xùn liàn xié yì', 'trans': 'training protocol'}, {'word': '促进', 'pinyin': 'cù jìn', 'trans': 'promote'}, {'word': '开放', 'pinyin': 'kāi fàng', 'trans': 'open'}, {'word': '可重复性', 'pinyin': 'kě chóng fù xìng', 'trans': 'reproducibility'}]
[09.11.2024 00:55] Renaming previous Chinese page.
[09.11.2024 00:55] Renaming previous data. zh.html to ./d/2024-11-08_zh_reading_task.html
[09.11.2024 00:55] Writing Chinese reading task.
[09.11.2024 00:55] Writing result.
[09.11.2024 00:55] Renaming log file.
[09.11.2024 00:55] Renaming previous data. log.txt to ./logs/2024-11-09_last_log.txt
