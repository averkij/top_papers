[23.12.2025 04:42] Read previous papers.
[23.12.2025 04:42] Generating top page (month).
[23.12.2025 04:42] Writing top page (month).
[23.12.2025 05:25] Read previous papers.
[23.12.2025 05:25] Get feed.
[23.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16676
[23.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19134
[23.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.18880
[23.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19693
[23.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19682
[23.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17650
[23.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19678
[23.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19539
[23.12.2025 05:25] Extract page data from URL. URL: https://huggingface.co/papers/2512.17206
[23.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19432
[23.12.2025 05:25] Extract page data from URL. URL: https://huggingface.co/papers/2512.19402
[23.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.18003
[23.12.2025 05:25] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.12.2025 05:25] No deleted papers detected.
[23.12.2025 05:25] Downloading and parsing papers (pdf, html). Total: 12.
[23.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.16676.
[23.12.2025 05:25] Extra JSON file exists (./assets/json/2512.16676.json), skip PDF parsing.
[23.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.16676.json), skip HTML parsing.
[23.12.2025 05:25] Success.
[23.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.19134.
[23.12.2025 05:25] Extra JSON file exists (./assets/json/2512.19134.json), skip PDF parsing.
[23.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.19134.json), skip HTML parsing.
[23.12.2025 05:25] Success.
[23.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.18880.
[23.12.2025 05:25] Extra JSON file exists (./assets/json/2512.18880.json), skip PDF parsing.
[23.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.18880.json), skip HTML parsing.
[23.12.2025 05:25] Success.
[23.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.19693.
[23.12.2025 05:25] Extra JSON file exists (./assets/json/2512.19693.json), skip PDF parsing.
[23.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.19693.json), skip HTML parsing.
[23.12.2025 05:25] Success.
[23.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.19682.
[23.12.2025 05:25] Extra JSON file exists (./assets/json/2512.19682.json), skip PDF parsing.
[23.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.19682.json), skip HTML parsing.
[23.12.2025 05:25] Success.
[23.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.17650.
[23.12.2025 05:25] Extra JSON file exists (./assets/json/2512.17650.json), skip PDF parsing.
[23.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.17650.json), skip HTML parsing.
[23.12.2025 05:25] Success.
[23.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.19678.
[23.12.2025 05:25] Extra JSON file exists (./assets/json/2512.19678.json), skip PDF parsing.
[23.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.19678.json), skip HTML parsing.
[23.12.2025 05:25] Success.
[23.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.19539.
[23.12.2025 05:25] Extra JSON file exists (./assets/json/2512.19539.json), skip PDF parsing.
[23.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.19539.json), skip HTML parsing.
[23.12.2025 05:25] Success.
[23.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.17206.
[23.12.2025 05:25] Downloading paper 2512.17206 from https://arxiv.org/pdf/2512.17206v1...
[23.12.2025 05:25] Extracting affiliations from text.
[23.12.2025 05:25] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs Rujiao Long1* Yang Li1,2* Xingyao Zhang1 Weixun Wang1 Tianqianjin Lin1,3 Xi Zhao1 Yuchi Xu1 Wenbo Su1 1Alibaba Group 2Shanghai Jiao Tong University Junchi Yan2 Bo Zheng1 3Zhejiang University 5 2 0 2 9 1 ] . [ 1 6 0 2 7 1 . 2 1 5 2 : r a "
[23.12.2025 05:25] Response: ```python
[
    "Alibaba Group",
    "Shanghai Jiao Tong University",
    "Zhejiang University"
]
```
[23.12.2025 05:25] Deleting PDF ./assets/pdf/2512.17206.pdf.
[23.12.2025 05:25] Success.
[23.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.19432.
[23.12.2025 05:25] Extra JSON file exists (./assets/json/2512.19432.json), skip PDF parsing.
[23.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.19432.json), skip HTML parsing.
[23.12.2025 05:25] Success.
[23.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.19402.
[23.12.2025 05:25] Downloading paper 2512.19402 from https://arxiv.org/pdf/2512.19402v1...
[23.12.2025 05:25] Extracting affiliations from text.
[23.12.2025 05:25] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 2 ] . [ 1 2 0 4 9 1 . 2 1 5 2 : r Real2Edit2Real: Generating Robotic Demonstrations via 3D Control Interface Yujie Zhao1,2 Hongwei Fan1,2 Di Chen3 Shengcong Chen3 Liliang Chen3 Xiaoqi Li1,2 Guanghui Ren3 Hao Dong1,2 1CFCS, School of Computer Science, Peking University 2PKU-AgiBot Lab 3AgiBot https://real2edit2real.github.io/ Figure 1. The overview of Real2Edit2Real. Real2Edit2Real generates diverse robotic demonstrations, featuring 10-50 improvement on data efficiency compared with real-world collection across four tasks. sistent depth, which serves as reliable condition for synthesizing new demonstrations. Finally, we propose multiconditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 15 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50. Moreover, experimental results on height and texture editing demonstrate the frameworks flexibility and extensibility, indicating its potential to serve as unified data generation framework. "
[23.12.2025 05:25] Response: ```python
[
    "CFCS, School of Computer Science, Peking University",
    "PKU-AgiBot Lab",
    "AgiBot"
]
```
[23.12.2025 05:25] Deleting PDF ./assets/pdf/2512.19402.pdf.
[23.12.2025 05:25] Success.
[23.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.18003.
[23.12.2025 05:25] Extra JSON file exists (./assets/json/2512.18003.json), skip PDF parsing.
[23.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.18003.json), skip HTML parsing.
[23.12.2025 05:25] Success.
[23.12.2025 05:25] Enriching papers with extra data.
[23.12.2025 05:25] ********************************************************************************
[23.12.2025 05:25] Abstract 0. DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.  					AI-generated summary 				 The rapidly growing demand for high-quality data in Large Language Models (LLMs) ha...
[23.12.2025 05:25] ********************************************************************************
[23.12.2025 05:25] Abstract 1. QuCo-RAG uses objective corpus statistics to mitigate hallucinations in large language models during generation, improving accuracy across various benchmarks.  					AI-generated summary 				 Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate h...
[23.12.2025 05:25] ********************************************************************************
[23.12.2025 05:25] Abstract 2. Large Language Models struggle to accurately estimate human cognitive difficulty due to a misalignment with human perceptions and a lack of introspection regarding their own limitations.  					AI-generated summary 				 Accurate estimation of item (question or task) difficulty is critical for educati...
[23.12.2025 05:25] ********************************************************************************
[23.12.2025 05:25] Abstract 3. Unified Autoencoding combines semantic and pixel-level information through a frequency-band modulator, resulting in a latent space with state-of-the-art performance on image benchmarks.  					AI-generated summary 				 Deep representations across modalities are inherently intertwined. In this paper, ...
[23.12.2025 05:25] ********************************************************************************
[23.12.2025 05:25] Abstract 4. GenEnv, a framework using a co-evolutionary game with a generative environment simulator, enhances LLM agent performance by 40.3% over 7B baselines and uses less data than offline augmentation.  					AI-generated summary 				 Training capable Large Language Model (LLM) agents is critically bottlenec...
[23.12.2025 05:25] ********************************************************************************
[23.12.2025 05:25] Abstract 5. ReCo is a novel instructional video editing paradigm that enhances accuracy and reduces token interference by incorporating constraint modeling and regularization techniques during in-context generation.  					AI-generated summary 				 The In-context generation paradigm recently has demonstrated str...
[23.12.2025 05:25] ********************************************************************************
[23.12.2025 05:25] Abstract 6. WorldWarp addresses the challenge of generating consistent long-range videos by integrating a 3D geometric cache with a spatio-temporal diffusion model, ensuring structural consistency and textural refinement.  					AI-generated summary 				 Generating long-range, geometrically consistent video pres...
[23.12.2025 05:25] ********************************************************************************
[23.12.2025 05:25] Abstract 7. StoryMem enhances multi-shot video generation with cinematic quality and long-range consistency using a memory bank and pre-trained single-shot video diffusion models.  					AI-generated summary 				 Visual storytelling requires generating multi-shot videos with cinematic quality and long-range cons...
[23.12.2025 05:25] ********************************************************************************
[23.12.2025 05:25] Abstract 8. Reasoning Palette enhances large language models by using a latent-modulation framework to guide internal planning and improve both inference and reinforcement learning performance.  					AI-generated summary 				 Exploration capacity shapes both inference-time performance and reinforcement learning...
[23.12.2025 05:25] ********************************************************************************
[23.12.2025 05:25] Abstract 9. MobileWorld, a more challenging benchmark than AndroidWorld, includes diverse real-world mobile tasks and interactions, revealing significant gaps in current model capabilities.  					AI-generated summary 				 Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benc...
[23.12.2025 05:25] ********************************************************************************
[23.12.2025 05:25] Abstract 10. A framework called Real2Edit2Real generates new manipulation demonstrations by using 3D reconstruction, editing, and video synthesis, improving data efficiency in robot learning.  					AI-generated summary 				 Recent progress in robot learning has been driven by large-scale datasets and powerful vi...
[23.12.2025 05:25] ********************************************************************************
[23.12.2025 05:25] Abstract 11. ALIGN-Parts addresses semantic 3D part segmentation by aligning implicit 3D part representations with part descriptions using geometric, appearance, and semantic cues, supporting open-vocabulary part naming and creating a unified ontology for multiple datasets.  					AI-generated summary 				 We add...
[23.12.2025 05:25] Read previous papers.
[23.12.2025 05:25] Generating reviews via LLM API.
[23.12.2025 05:25] Using data from previous issue: {"categories": ["#open_source", "#synthetic", "#optimization", "#data", "#rag", "#dataset", "#training", "#agents"], "emoji": "ğŸ”§", "ru": {"title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "DataFlow â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ±
[23.12.2025 05:25] Using data from previous issue: {"categories": ["#open_source", "#hallucinations", "#benchmark", "#rag"], "emoji": "ğŸ”", "ru": {"title": "ĞÑ‚ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞµ Ğ² RAG", "desc": "QuCo-RAG â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚
[23.12.2025 05:25] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#alignment"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ£Ğ¼Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ĞµĞ¹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 20 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ…
[23.12.2025 05:25] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#architecture", "#benchmark"], "emoji": "ğŸŒˆ", "ru": {"title": "Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ", "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚
[23.12.2025 05:25] Using data from previous issue: {"categories": ["#training", "#small_models", "#agents", "#benchmark"], "emoji": "ğŸ®", "ru": {"title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ curriculum learning Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¸Ğ³Ñ€Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°", "desc": "GenEnv â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¸Ğ³Ñ€Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¾ĞºÑ€
[23.12.2025 05:25] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#video", "#dataset", "#training"], "emoji": "ğŸ¬", "ru": {"title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ", "desc": "ReCo â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²
[23.12.2025 05:25] Using data from previous issue: {"categories": ["#video", "#3d", "#multimodal"], "emoji": "ğŸ¬", "ru": {"title": "3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ: ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· ĞºÑÑˆĞ°", "desc": "WorldWarp Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ 3D ĞºÑÑˆ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Gaussian Splatting
[23.12.2025 05:25] Using data from previous issue: {"categories": ["#story_generation", "#diffusion", "#benchmark", "#video", "#dataset", "#training", "#long_context"], "emoji": "ğŸ¬", "ru": {"title": "ĞŸĞ°Ğ¼ÑÑ‚ÑŒ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ°ÑÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ", "desc": "StoryMem â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¸Ğ½
[23.12.2025 05:25] Querying the API.
[23.12.2025 05:25] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Reasoning Palette enhances large language models by using a latent-modulation framework to guide internal planning and improve both inference and reinforcement learning performance.  					AI-generated summary 				 Exploration capacity shapes both inference-time performance and reinforcement learning (RL) training for large (vision-) language models, as stochastic sampling often yields redundant reasoning paths with little high-level diversity. This paper proposes Reasoning Palette, a novel latent-modulation framework that endows the model with a stochastic latent variable for strategic contextualization, guiding its internal planning prior to token generation. This latent context is inferred from the mean-pooled embedding of a question-answer pair via a variational autoencoder (VAE), where each sampled latent potentially encodes a distinct reasoning context. During inference, a sampled latent is decoded into learnable token prefixes and prepended to the input prompt, modulating the model's internal reasoning trajectory. In this way, the model performs internal sampling over reasoning strategies prior to output generation, which shapes the style and structure of the entire response sequence. A brief supervised fine-tuning (SFT) warm-up phase allows the model to adapt to this latent conditioning. Within RL optimization, Reasoning Palette facilitates structured exploration by enabling on-demand injection for diverse reasoning modes, significantly enhancing exploration efficiency and sustained learning capability. Experiments across multiple reasoning benchmarks demonstrate that our method enables interpretable and controllable control over the (vision-) language model's strategic behavior, thereby achieving consistent performance gains over standard RL methods.
[23.12.2025 05:25] Response: ```json
{
  "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Reasoning Palette, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ, Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€ Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑÑ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸Ñ€ÑƒÑ Ğ²ÑÑ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.",
  "emoji": "ğŸ¨",
  "title": "ĞŸĞ°Ğ»Ğ¸Ñ‚Ñ€Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ"
}
```
[23.12.2025 05:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reasoning Palette enhances large language models by using a latent-modulation framework to guide internal planning and improve both inference and reinforcement learning performance.  					AI-generated summary 				 Exploration capacity shapes both inference-time performance and reinforcement learning (RL) training for large (vision-) language models, as stochastic sampling often yields redundant reasoning paths with little high-level diversity. This paper proposes Reasoning Palette, a novel latent-modulation framework that endows the model with a stochastic latent variable for strategic contextualization, guiding its internal planning prior to token generation. This latent context is inferred from the mean-pooled embedding of a question-answer pair via a variational autoencoder (VAE), where each sampled latent potentially encodes a distinct reasoning context. During inference, a sampled latent is decoded into learnable token prefixes and prepended to the input prompt, modulating the model's internal reasoning trajectory. In this way, the model performs internal sampling over reasoning strategies prior to output generation, which shapes the style and structure of the entire response sequence. A brief supervised fine-tuning (SFT) warm-up phase allows the model to adapt to this latent conditioning. Within RL optimization, Reasoning Palette facilitates structured exploration by enabling on-demand injection for diverse reasoning modes, significantly enhancing exploration efficiency and sustained learning capability. Experiments across multiple reasoning benchmarks demonstrate that our method enables interpretable and controllable control over the (vision-) language model's strategic behavior, thereby achieving consistent performance gains over standard RL methods."

[23.12.2025 05:25] Response: ```python
["RL", "RLHF", "TRAINING", "ARCHITECTURE", "BENCHMARK"]
```
[23.12.2025 05:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reasoning Palette enhances large language models by using a latent-modulation framework to guide internal planning and improve both inference and reinforcement learning performance.  					AI-generated summary 				 Exploration capacity shapes both inference-time performance and reinforcement learning (RL) training for large (vision-) language models, as stochastic sampling often yields redundant reasoning paths with little high-level diversity. This paper proposes Reasoning Palette, a novel latent-modulation framework that endows the model with a stochastic latent variable for strategic contextualization, guiding its internal planning prior to token generation. This latent context is inferred from the mean-pooled embedding of a question-answer pair via a variational autoencoder (VAE), where each sampled latent potentially encodes a distinct reasoning context. During inference, a sampled latent is decoded into learnable token prefixes and prepended to the input prompt, modulating the model's internal reasoning trajectory. In this way, the model performs internal sampling over reasoning strategies prior to output generation, which shapes the style and structure of the entire response sequence. A brief supervised fine-tuning (SFT) warm-up phase allows the model to adapt to this latent conditioning. Within RL optimization, Reasoning Palette facilitates structured exploration by enabling on-demand injection for diverse reasoning modes, significantly enhancing exploration efficiency and sustained learning capability. Experiments across multiple reasoning benchmarks demonstrate that our method enables interpretable and controllable control over the (vision-) language model's strategic behavior, thereby achieving consistent performance gains over standard RL methods."

[23.12.2025 05:26] Response: ```python
["REASONING", "OPTIMIZATION", "INTERPRETABILITY"]
```
[23.12.2025 05:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces Reasoning Palette, a framework that improves large language models by using a latent-modulation approach. This method incorporates a stochastic latent variable that helps the model plan its reasoning before generating responses. By utilizing a variational autoencoder, the model can sample different reasoning contexts, which enhances its ability to explore diverse strategies during inference and reinforcement learning. The results show that this framework leads to better performance and more interpretable behavior in language models compared to traditional reinforcement learning techniques.","title":"Enhancing Reasoning Diversity in Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces Reasoning Palette, a framework that improves large language models by using a latent-modulation approach. This method incorporates a stochastic latent variable that helps the model plan its reasoning before generating responses. By utilizing a variational autoencoder, the model can sample different reasoning contexts, which enhances its ability to explore diverse strategies during inference and reinforcement learning. The results show that this framework leads to better performance and more interpretable behavior in language models compared to traditional reinforcement learning techniques.', title='Enhancing Reasoning Diversity in Language Models'))
[23.12.2025 05:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Reasoning Palette æ˜¯ä¸€ç§æ–°é¢–çš„æ½œåœ¨è°ƒåˆ¶æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡å¼•å…¥éšæœºæ½œåœ¨å˜é‡æ¥æŒ‡å¯¼æ¨¡å‹çš„å†…éƒ¨è§„åˆ’ï¼Œä»è€Œæ”¹å–„æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ä»é—®ç­”å¯¹çš„å‡å€¼æ± åµŒå…¥ä¸­æ¨æ–­æ½œåœ¨ä¸Šä¸‹æ–‡ï¼Œå¸®åŠ©æ¨¡å‹åœ¨ç”Ÿæˆè¾“å‡ºä¹‹å‰è¿›è¡Œå†…éƒ¨é‡‡æ ·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReasoning Palette èƒ½å¤Ÿæé«˜æ¨¡å‹çš„æ¢ç´¢æ•ˆç‡å’Œå­¦ä¹ èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚","title":"æå‡æ¨ç†èƒ½åŠ›çš„æ½œåœ¨è°ƒåˆ¶æ¡†æ¶"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Reasoning Palette æ˜¯ä¸€ç§æ–°é¢–çš„æ½œåœ¨è°ƒåˆ¶æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡å¼•å…¥éšæœºæ½œåœ¨å˜é‡æ¥æŒ‡å¯¼æ¨¡å‹çš„å†…éƒ¨è§„åˆ’ï¼Œä»è€Œæ”¹å–„æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ä»é—®ç­”å¯¹çš„å‡å€¼æ± åµŒå…¥ä¸­æ¨æ–­æ½œåœ¨ä¸Šä¸‹æ–‡ï¼Œå¸®åŠ©æ¨¡å‹åœ¨ç”Ÿæˆè¾“å‡ºä¹‹å‰è¿›è¡Œå†…éƒ¨é‡‡æ ·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReasoning Palette èƒ½å¤Ÿæé«˜æ¨¡å‹çš„æ¢ç´¢æ•ˆç‡å’Œå­¦ä¹ èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚', title='æå‡æ¨ç†èƒ½åŠ›çš„æ½œåœ¨è°ƒåˆ¶æ¡†æ¶'))
[23.12.2025 05:26] Using data from previous issue: {"categories": ["#dataset", "#games", "#agents", "#benchmark"], "emoji": "ğŸ“±", "ru": {"title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²", "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MobileWorld Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°
[23.12.2025 05:26] Querying the API.
[23.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A framework called Real2Edit2Real generates new manipulation demonstrations by using 3D reconstruction, editing, and video synthesis, improving data efficiency in robot learning.  					AI-generated summary 				 Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.
[23.12.2025 05:26] Response: ```json
{
  "desc": "Real2Edit2Real â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¸Ğ· RGB-Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ·Ñ‹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ°Ñ€Ñ‚Ñ‹, Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 1-5 Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° 50 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ…, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 10-50 Ñ€Ğ°Ğ·. Ğ“Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾Ñ‚Ñ‹ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ĞµĞ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….",
  "emoji": "ğŸ¤–",
  "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· 3D Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ¸Ğ½Ñ‚ĞµĞ·"
}
```
[23.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A framework called Real2Edit2Real generates new manipulation demonstrations by using 3D reconstruction, editing, and video synthesis, improving data efficiency in robot learning.  					AI-generated summary 				 Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework."

[23.12.2025 05:26] Response: ```python
['ROBOTICS', '3D', 'VIDEO', 'TRAINING', 'DATASET']
```
[23.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A framework called Real2Edit2Real generates new manipulation demonstrations by using 3D reconstruction, editing, and video synthesis, improving data efficiency in robot learning.  					AI-generated summary 				 Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework."

[23.12.2025 05:26] Response: ```python
['SYNTHETIC', 'OPTIMIZATION']
```

**Justification:**

- **SYNTHETIC**: The paper is fundamentally about generating synthetic data (new manipulation demonstrations) through 3D reconstruction, editing, and video synthesis to train robot learning models. This directly addresses using artificial/synthetic data for training purposes.

- **OPTIMIZATION**: The paper addresses improving data efficiency in robot learning, reducing the need for large-scale real-world data collection by achieving comparable performance with significantly fewer source demonstrations (1-5 vs 50). This is an optimization of the training process and data requirements.
[23.12.2025 05:26] Error. Failed to parse JSON from LLM. ["SYNTHETIC", "OPTIMIZATION"]


**Justification:**

- **SYNTHETIC**: The paper is fundamentally about generating synthetic data (new manipulation demonstrations) through 3D reconstruction, editing, and video synthesis to train robot learning models. This directly addresses using artificial/synthetic data for training purposes.

- **OPTIMIZATION**: The paper addresses improving data efficiency in robot learning, reducing the need for large-scale real-world data collection by achieving comparable performance with significantly fewer source demonstrations (1-5 vs 50). This is an optimization of the training process and data requirements.
[23.12.2025 05:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces Real2Edit2Real, a novel framework designed to enhance robot learning by generating new manipulation demonstrations. It utilizes 3D reconstruction to create a detailed geometric model from 2D images, allowing for depth-reliable editing of point clouds. This enables the generation of new manipulation trajectories while ensuring that robot poses remain physically consistent. The framework significantly improves data efficiency, allowing policies trained on a few demonstrations to perform as well as those trained on many, showcasing its potential for broad applications in robot learning.","title":"Revolutionizing Robot Learning with Efficient Data Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces Real2Edit2Real, a novel framework designed to enhance robot learning by generating new manipulation demonstrations. It utilizes 3D reconstruction to create a detailed geometric model from 2D images, allowing for depth-reliable editing of point clouds. This enables the generation of new manipulation trajectories while ensuring that robot poses remain physically consistent. The framework significantly improves data efficiency, allowing policies trained on a few demonstrations to perform as well as those trained on many, showcasing its potential for broad applications in robot learning.', title='Revolutionizing Robot Learning with Efficient Data Generation'))
[23.12.2025 05:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Real2Edit2Realæ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œé€šè¿‡3Dé‡å»ºã€ç¼–è¾‘å’Œè§†é¢‘åˆæˆç”Ÿæˆæ–°çš„æ“ä½œæ¼”ç¤ºï¼Œæå‡äº†æœºå™¨äººå­¦ä¹ ä¸­çš„æ•°æ®æ•ˆç‡ã€‚è¯¥æ–¹æ³•é¦–å…ˆä»å¤šè§†è§’RGBè§‚å¯Ÿä¸­é‡å»ºåœºæ™¯å‡ ä½•ï¼Œç„¶ååœ¨ç‚¹äº‘ä¸Šè¿›è¡Œæ·±åº¦å¯é çš„3Dç¼–è¾‘ï¼Œç”Ÿæˆæ–°çš„æ“ä½œè½¨è¿¹ã€‚æ¥ç€ï¼Œåˆ©ç”¨æ·±åº¦ä½œä¸ºä¸»è¦æ§åˆ¶ä¿¡å·ï¼Œç»“åˆåŠ¨ä½œã€è¾¹ç¼˜å’Œå…‰çº¿å›¾ï¼Œåˆæˆç©ºé—´å¢å¼ºçš„å¤šè§†è§’æ“ä½œè§†é¢‘ã€‚å®éªŒè¡¨æ˜ï¼Œä»…ç”¨1-5ä¸ªæºæ¼”ç¤ºç”Ÿæˆçš„æ•°æ®ï¼Œè®­ç»ƒå‡ºçš„ç­–ç•¥å¯ä»¥ä¸ä½¿ç”¨50ä¸ªçœŸå®æ¼”ç¤ºçš„æ•°æ®ç›¸åª²ç¾ï¼Œæ•°æ®æ•ˆç‡æé«˜äº†10-50å€ã€‚","title":"æå‡æœºå™¨äººå­¦ä¹ çš„æ•°æ®æ•ˆç‡"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Real2Edit2Realæ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œé€šè¿‡3Dé‡å»ºã€ç¼–è¾‘å’Œè§†é¢‘åˆæˆç”Ÿæˆæ–°çš„æ“ä½œæ¼”ç¤ºï¼Œæå‡äº†æœºå™¨äººå­¦ä¹ ä¸­çš„æ•°æ®æ•ˆç‡ã€‚è¯¥æ–¹æ³•é¦–å…ˆä»å¤šè§†è§’RGBè§‚å¯Ÿä¸­é‡å»ºåœºæ™¯å‡ ä½•ï¼Œç„¶ååœ¨ç‚¹äº‘ä¸Šè¿›è¡Œæ·±åº¦å¯é çš„3Dç¼–è¾‘ï¼Œç”Ÿæˆæ–°çš„æ“ä½œè½¨è¿¹ã€‚æ¥ç€ï¼Œåˆ©ç”¨æ·±åº¦ä½œä¸ºä¸»è¦æ§åˆ¶ä¿¡å·ï¼Œç»“åˆåŠ¨ä½œã€è¾¹ç¼˜å’Œå…‰çº¿å›¾ï¼Œåˆæˆç©ºé—´å¢å¼ºçš„å¤šè§†è§’æ“ä½œè§†é¢‘ã€‚å®éªŒè¡¨æ˜ï¼Œä»…ç”¨1-5ä¸ªæºæ¼”ç¤ºç”Ÿæˆçš„æ•°æ®ï¼Œè®­ç»ƒå‡ºçš„ç­–ç•¥å¯ä»¥ä¸ä½¿ç”¨50ä¸ªçœŸå®æ¼”ç¤ºçš„æ•°æ®ç›¸åª²ç¾ï¼Œæ•°æ®æ•ˆç‡æé«˜äº†10-50å€ã€‚', title='æå‡æœºå™¨äººå­¦ä¹ çš„æ•°æ®æ•ˆç‡'))
[23.12.2025 05:26] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#3d", "#interpretability", "#multimodal"], "emoji": "ğŸ§©", "ru": {"title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ALIGN-Parts â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… 
[23.12.2025 05:26] Renaming data file.
[23.12.2025 05:26] Renaming previous data. hf_papers.json to ./d/2025-12-23.json
[23.12.2025 05:26] Saving new data file.
[23.12.2025 05:26] Generating page.
[23.12.2025 05:26] Renaming previous page.
[23.12.2025 05:26] Renaming previous data. index.html to ./d/2025-12-23.html
[23.12.2025 05:26] Writing result.
[23.12.2025 05:26] Renaming log file.
[23.12.2025 05:26] Renaming previous data. log.txt to ./logs/2025-12-23_last_log.txt
