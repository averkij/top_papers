[23.12.2025 05:26] Read previous papers.
[23.12.2025 05:26] Generating top page (month).
[23.12.2025 05:26] Writing top page (month).
[23.12.2025 06:36] Read previous papers.
[23.12.2025 06:36] Get feed.
[23.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16676
[23.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19693
[23.12.2025 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2512.17040
[23.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19134
[23.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.18880
[23.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17650
[23.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19678
[23.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19682
[23.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17206
[23.12.2025 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2512.17385
[23.12.2025 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2512.19629
[23.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19539
[23.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19432
[23.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19402
[23.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.18003
[23.12.2025 06:36] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.12.2025 06:36] No deleted papers detected.
[23.12.2025 06:36] Downloading and parsing papers (pdf, html). Total: 15.
[23.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.16676.
[23.12.2025 06:36] Extra JSON file exists (./assets/json/2512.16676.json), skip PDF parsing.
[23.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.16676.json), skip HTML parsing.
[23.12.2025 06:36] Success.
[23.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.19693.
[23.12.2025 06:36] Extra JSON file exists (./assets/json/2512.19693.json), skip PDF parsing.
[23.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.19693.json), skip HTML parsing.
[23.12.2025 06:36] Success.
[23.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.17040.
[23.12.2025 06:36] Downloading paper 2512.17040 from https://arxiv.org/pdf/2512.17040v1...
[23.12.2025 06:36] Extracting affiliations from text.
[23.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 0 4 0 7 1 . 2 1 5 2 : r Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation Min-Jung Kim Jeongho Kim Hoiyeong Jin KAIST AI {emjay73,rlawjdghek,hy.jin,sharpeeee,jchoo}@kaist.ac.kr Equal contribution Figure 1. InfCam Results. Given video and target camera trajectory, InfCam generates video that faithfully follows the specified camera path. The world coordinate origin is defined by the first frames camera pose (highlighted in red). The leftmost column visualizes the backward, arc, and rotational camera trajectories, and the right side shows inputgenerated video pairs corresponding to each trajectory. The rotational trajectory is generated with shorter focal length to illustrate wide field-of-view generation. The black dashed box in the last row indicates the original field-of-view of the input video. "
[23.12.2025 06:36] Response: ```python
["KAIST AI"]
```
[23.12.2025 06:36] Deleting PDF ./assets/pdf/2512.17040.pdf.
[23.12.2025 06:36] Success.
[23.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.19134.
[23.12.2025 06:36] Extra JSON file exists (./assets/json/2512.19134.json), skip PDF parsing.
[23.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.19134.json), skip HTML parsing.
[23.12.2025 06:36] Success.
[23.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.18880.
[23.12.2025 06:36] Extra JSON file exists (./assets/json/2512.18880.json), skip PDF parsing.
[23.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.18880.json), skip HTML parsing.
[23.12.2025 06:36] Success.
[23.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.17650.
[23.12.2025 06:36] Extra JSON file exists (./assets/json/2512.17650.json), skip PDF parsing.
[23.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.17650.json), skip HTML parsing.
[23.12.2025 06:36] Success.
[23.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.19678.
[23.12.2025 06:36] Extra JSON file exists (./assets/json/2512.19678.json), skip PDF parsing.
[23.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.19678.json), skip HTML parsing.
[23.12.2025 06:36] Success.
[23.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.19682.
[23.12.2025 06:36] Extra JSON file exists (./assets/json/2512.19682.json), skip PDF parsing.
[23.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.19682.json), skip HTML parsing.
[23.12.2025 06:36] Success.
[23.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.17206.
[23.12.2025 06:36] Extra JSON file exists (./assets/json/2512.17206.json), skip PDF parsing.
[23.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.17206.json), skip HTML parsing.
[23.12.2025 06:36] Success.
[23.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.17385.
[23.12.2025 06:36] Downloading paper 2512.17385 from https://arxiv.org/pdf/2512.17385v1...
[23.12.2025 06:37] Extracting affiliations from text.
[23.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models Jiajun Wu1, Jian Yang1, Wei Zhang1, Lin Jing1, Yuqing Ma2, Ensheng Shi2, Yuchi Ma2, Zhoujun Li2, Xianglong Liu1 1Beihang University; 2Huawei; {wuyuverse,jiayang}@buaa.edu.cn 5 2 0 2 9 1 ] . [ 1 5 8 3 7 1 . 2 1 5 2 : r a "
[23.12.2025 06:37] Response: ```python
["Beihang University", "Huawei"]
```
[23.12.2025 06:37] Deleting PDF ./assets/pdf/2512.17385.pdf.
[23.12.2025 06:37] Success.
[23.12.2025 06:37] Downloading and parsing paper https://huggingface.co/papers/2512.19629.
[23.12.2025 06:37] Downloading paper 2512.19629 from https://arxiv.org/pdf/2512.19629v1...
[23.12.2025 06:37] Extracting affiliations from text.
[23.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry Jiaqi Peng,1,2, Wenzhe Cai,2, Yuqiang Yang,2, Tai Wang2,, Yuan Shen1,2, and Jiangmiao Pang2 5 2 0 2 2 2 ] . [ 1 9 2 6 9 1 . 2 1 5 2 : r Abstract Trajectory planning in unstructured environments is fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation. We evaluate LoGoPlanner in both simulation and real-world settings, where its fully endto-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than 27.3% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the project page. I. INTRODUCTION Autonomous navigation, requiring robots to reliably reach specified goals"
[23.12.2025 06:37] Response: ```python
[
    "Tsinghua University",
    "OpenGVLab"
]
```
[23.12.2025 06:37] Deleting PDF ./assets/pdf/2512.19629.pdf.
[23.12.2025 06:37] Success.
[23.12.2025 06:37] Downloading and parsing paper https://huggingface.co/papers/2512.19539.
[23.12.2025 06:37] Extra JSON file exists (./assets/json/2512.19539.json), skip PDF parsing.
[23.12.2025 06:37] Paper image links file exists (./assets/img_data/2512.19539.json), skip HTML parsing.
[23.12.2025 06:37] Success.
[23.12.2025 06:37] Downloading and parsing paper https://huggingface.co/papers/2512.19432.
[23.12.2025 06:37] Extra JSON file exists (./assets/json/2512.19432.json), skip PDF parsing.
[23.12.2025 06:37] Paper image links file exists (./assets/img_data/2512.19432.json), skip HTML parsing.
[23.12.2025 06:37] Success.
[23.12.2025 06:37] Downloading and parsing paper https://huggingface.co/papers/2512.19402.
[23.12.2025 06:37] Extra JSON file exists (./assets/json/2512.19402.json), skip PDF parsing.
[23.12.2025 06:37] Paper image links file exists (./assets/img_data/2512.19402.json), skip HTML parsing.
[23.12.2025 06:37] Success.
[23.12.2025 06:37] Downloading and parsing paper https://huggingface.co/papers/2512.18003.
[23.12.2025 06:37] Extra JSON file exists (./assets/json/2512.18003.json), skip PDF parsing.
[23.12.2025 06:37] Paper image links file exists (./assets/img_data/2512.18003.json), skip HTML parsing.
[23.12.2025 06:37] Success.
[23.12.2025 06:37] Enriching papers with extra data.
[23.12.2025 06:37] ********************************************************************************
[23.12.2025 06:37] Abstract 0. DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.  					AI-generated summary 				 The rapidly growing demand for high-quality data in Large Language Models (LLMs) ha...
[23.12.2025 06:37] ********************************************************************************
[23.12.2025 06:37] Abstract 1. Unified Autoencoding combines semantic and pixel-level information through a frequency-band modulator, resulting in a latent space with state-of-the-art performance on image benchmarks.  					AI-generated summary 				 Deep representations across modalities are inherently intertwined. In this paper, ...
[23.12.2025 06:37] ********************************************************************************
[23.12.2025 06:37] Abstract 2. InfCam generates high-fidelity videos with accurate camera poses by using infinite homography warping and augmenting synthetic datasets with diverse trajectories.  					AI-generated summary 				 Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view vi...
[23.12.2025 06:37] ********************************************************************************
[23.12.2025 06:37] Abstract 3. QuCo-RAG uses objective corpus statistics to mitigate hallucinations in large language models during generation, improving accuracy across various benchmarks.  					AI-generated summary 				 Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate h...
[23.12.2025 06:37] ********************************************************************************
[23.12.2025 06:37] Abstract 4. Large Language Models struggle to accurately estimate human cognitive difficulty due to a misalignment with human perceptions and a lack of introspection regarding their own limitations.  					AI-generated summary 				 Accurate estimation of item (question or task) difficulty is critical for educati...
[23.12.2025 06:37] ********************************************************************************
[23.12.2025 06:37] Abstract 5. ReCo is a novel instructional video editing paradigm that enhances accuracy and reduces token interference by incorporating constraint modeling and regularization techniques during in-context generation.  					AI-generated summary 				 The In-context generation paradigm recently has demonstrated str...
[23.12.2025 06:37] ********************************************************************************
[23.12.2025 06:37] Abstract 6. WorldWarp addresses the challenge of generating consistent long-range videos by integrating a 3D geometric cache with a spatio-temporal diffusion model, ensuring structural consistency and textural refinement.  					AI-generated summary 				 Generating long-range, geometrically consistent video pres...
[23.12.2025 06:37] ********************************************************************************
[23.12.2025 06:37] Abstract 7. GenEnv, a framework using a co-evolutionary game with a generative environment simulator, enhances LLM agent performance by 40.3% over 7B baselines and uses less data than offline augmentation.  					AI-generated summary 				 Training capable Large Language Model (LLM) agents is critically bottlenec...
[23.12.2025 06:37] ********************************************************************************
[23.12.2025 06:37] Abstract 8. Reasoning Palette enhances large language models by using a latent-modulation framework to guide internal planning and improve both inference and reinforcement learning performance.  					AI-generated summary 				 Exploration capacity shapes both inference-time performance and reinforcement learning...
[23.12.2025 06:37] ********************************************************************************
[23.12.2025 06:37] Abstract 9. IPC is an unsupervised framework that uses internal probing of large language models to generate code without labeled datasets, achieving competitive performance with reduced resource dependency.  					AI-generated summary 				 Large language models (LLMs) have demonstrated remarkable capabilities i...
[23.12.2025 06:37] ********************************************************************************
[23.12.2025 06:37] Abstract 10. LoGoPlanner is an end-to-end navigation framework that improves trajectory planning in unstructured environments by integrating localization, scene geometry reconstruction, and policy conditioning.  					AI-generated summary 				 Trajectory planning in unstructured environments is a fundamental and ...
[23.12.2025 06:37] ********************************************************************************
[23.12.2025 06:37] Abstract 11. StoryMem enhances multi-shot video generation with cinematic quality and long-range consistency using a memory bank and pre-trained single-shot video diffusion models.  					AI-generated summary 				 Visual storytelling requires generating multi-shot videos with cinematic quality and long-range cons...
[23.12.2025 06:37] ********************************************************************************
[23.12.2025 06:37] Abstract 12. MobileWorld, a more challenging benchmark than AndroidWorld, includes diverse real-world mobile tasks and interactions, revealing significant gaps in current model capabilities.  					AI-generated summary 				 Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benc...
[23.12.2025 06:37] ********************************************************************************
[23.12.2025 06:37] Abstract 13. A framework called Real2Edit2Real generates new manipulation demonstrations by using 3D reconstruction, editing, and video synthesis, improving data efficiency in robot learning.  					AI-generated summary 				 Recent progress in robot learning has been driven by large-scale datasets and powerful vi...
[23.12.2025 06:37] ********************************************************************************
[23.12.2025 06:37] Abstract 14. ALIGN-Parts addresses semantic 3D part segmentation by aligning implicit 3D part representations with part descriptions using geometric, appearance, and semantic cues, supporting open-vocabulary part naming and creating a unified ontology for multiple datasets.  					AI-generated summary 				 We add...
[23.12.2025 06:37] Read previous papers.
[23.12.2025 06:37] Generating reviews via LLM API.
[23.12.2025 06:37] Using data from previous issue: {"categories": ["#open_source", "#synthetic", "#optimization", "#data", "#rag", "#dataset", "#training", "#agents"], "emoji": "üîß", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "DataFlow ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö, —É–ø—Ä–∞–≤–ª—è–µ–º—ã–π –±
[23.12.2025 06:37] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#architecture", "#benchmark"], "emoji": "üåà", "ru": {"title": "–ï–¥–∏–Ω–æ–µ –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ: –≥–∞—Ä–º–æ–Ω–∏—è –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏ –∏ –¥–µ—Ç–∞–ª–µ–π —á–µ—Ä–µ–∑ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –∏—Å—Å–ª–µ–¥—É—é—Ç —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –∏ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç
[23.12.2025 06:37] Querying the API.
[23.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

InfCam generates high-fidelity videos with accurate camera poses by using infinite homography warping and augmenting synthetic datasets with diverse trajectories.  					AI-generated summary 				 Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production. A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations. To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions. Nevertheless, existing methods struggle to generate camera-pose-faithful, high-quality videos for two main reasons: (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and (2) the limited diversity of camera trajectories in existing datasets restricts learned models. To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity. The framework integrates two key components: (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths. Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data. Link to our project page:https://emjay73.github.io/InfCam/
[23.12.2025 06:37] Response: ```json
{
  "desc": "InfCam ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–∞–º–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω—É—é –≥–æ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫—É—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è 3D —Ä–æ—Ç–∞—Ü–∏–π –∫–∞–º–µ—Ä—ã –≤ 2D –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã. –ú–µ—Ç–æ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –ø–∞—Ä–∞–ª–ª–∞–∫—Å —á–µ—Ä–µ–∑ —Å–∫–≤–æ–∑–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∑–∞–¥–∞–Ω–Ω–æ–π –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ –∫–∞–º–µ—Ä—ã. –î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫–æ–Ω–≤–µ–π–µ—Ä –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –º—É–ª—å—Ç–∏–≤—å—é–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ –∏ —Ñ–æ–∫—É—Å–Ω—ã–º–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ InfCam –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ–∑–∏—Ü–∏–∏ –∫–∞–º–µ—Ä—ã –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π —á–µ—Ç–∫–æ—Å—Ç–∏, —Ö–æ—Ä–æ—à–æ –æ–±–æ–±—â–∞—è—Å—å —Å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –≤–∏–¥–µ–æ.",
  "emoji": "üé¨",
  "title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å —Ç–æ—á–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–∞–º–µ—Ä—ã —á–µ—Ä–µ–∑ –≥–æ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫—É—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –±–µ–∑ –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã"
}
```
[23.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"InfCam generates high-fidelity videos with accurate camera poses by using infinite homography warping and augmenting synthetic datasets with diverse trajectories.  					AI-generated summary 				 Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production. A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations. To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions. Nevertheless, existing methods struggle to generate camera-pose-faithful, high-quality videos for two main reasons: (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and (2) the limited diversity of camera trajectories in existing datasets restricts learned models. To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity. The framework integrates two key components: (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths. Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data. Link to our project page:https://emjay73.github.io/InfCam/"

[23.12.2025 06:37] Response: ```python
['VIDEO', 'DATASET', 'MULTIMODAL']
```
[23.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"InfCam generates high-fidelity videos with accurate camera poses by using infinite homography warping and augmenting synthetic datasets with diverse trajectories.  					AI-generated summary 				 Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production. A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations. To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions. Nevertheless, existing methods struggle to generate camera-pose-faithful, high-quality videos for two main reasons: (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and (2) the limited diversity of camera trajectories in existing datasets restricts learned models. To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity. The framework integrates two key components: (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths. Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data. Link to our project page:https://emjay73.github.io/InfCam/"

[23.12.2025 06:37] Response: ```python
['DIFFUSION', 'SYNTHETIC']
```
[23.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"InfCam is a novel framework for generating high-quality videos that accurately reflect specified camera poses without relying on depth estimation. It utilizes infinite homography warping to directly incorporate 3D camera rotations into the 2D latent space of a video diffusion model, enhancing pose fidelity. Additionally, it employs a data augmentation strategy to create diverse camera trajectories from existing synthetic datasets, addressing the limitations of previous methods. Experimental results show that InfCam significantly improves camera-pose accuracy and visual quality, effectively bridging the gap between synthetic and real-world video generation.","title":"InfCam: High-Fidelity Video Generation with Accurate Camera Control"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='InfCam is a novel framework for generating high-quality videos that accurately reflect specified camera poses without relying on depth estimation. It utilizes infinite homography warping to directly incorporate 3D camera rotations into the 2D latent space of a video diffusion model, enhancing pose fidelity. Additionally, it employs a data augmentation strategy to create diverse camera trajectories from existing synthetic datasets, addressing the limitations of previous methods. Experimental results show that InfCam significantly improves camera-pose accuracy and visual quality, effectively bridging the gap between synthetic and real-world video generation.', title='InfCam: High-Fidelity Video Generation with Accurate Camera Control'))
[23.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"InfCam ÊòØ‰∏ÄÁßçÊó†Ê∑±Â∫¶„ÄÅÁõ∏Êú∫ÊéßÂà∂ÁöÑËßÜÈ¢ëÁîüÊàêÊ°ÜÊû∂ÔºåËÉΩÂ§üÁîüÊàêÈ´ò‰øùÁúüÂ∫¶ÁöÑËßÜÈ¢ëÂπ∂ÂáÜÁ°ÆÊéßÂà∂Áõ∏Êú∫ÂßøÊÄÅ„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®Êó†ÈôêÂçïÂ∫îÊÄßÂèòÊç¢ÔºåÂ∞Ü‰∏âÁª¥Áõ∏Êú∫ÊóãËΩ¨‰ø°ÊÅØÁõ¥Êé•ÁºñÁ†ÅÂà∞ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁöÑ‰∫åÁª¥ÊΩúÂú®Á©∫Èó¥‰∏≠„ÄÇÈÄöËøáÁ´ØÂà∞Á´ØËÆ≠ÁªÉÔºåÈ¢ÑÊµãÊÆãÂ∑ÆËßÜÂ∑ÆÈ°πÔºå‰ª•ÂÆûÁé∞È´òÁõ∏Êú∫ÂßøÊÄÅ‰øùÁúüÂ∫¶„ÄÇÂêåÊó∂ÔºåÊï∞ÊçÆÂ¢ûÂº∫ÁÆ°ÈÅìÂ∞ÜÁé∞ÊúâÁöÑÂêàÊàêÂ§öËßÜÂõæÊï∞ÊçÆÈõÜËΩ¨Âåñ‰∏∫ÂÖ∑ÊúâÂ§öÊ†∑ÂåñËΩ®ËøπÂíåÁÑ¶Ë∑ùÁöÑÂ∫èÂàó„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåInfCam Âú®Áõ∏Êú∫ÂßøÊÄÅÂáÜÁ°ÆÊÄßÂíåËßÜËßâ‰øùÁúüÂ∫¶ÊñπÈù¢‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåÂπ∂ËÉΩÂæàÂ•ΩÂú∞‰ªéÂêàÊàêÊï∞ÊçÆÊé®ÂπøÂà∞ÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆ„ÄÇ","title":"È´ò‰øùÁúüËßÜÈ¢ëÁîüÊàê‰∏éÁõ∏Êú∫ÊéßÂà∂ÁöÑÂàõÊñ∞‰πãË∑Ø"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='InfCam ÊòØ‰∏ÄÁßçÊó†Ê∑±Â∫¶„ÄÅÁõ∏Êú∫ÊéßÂà∂ÁöÑËßÜÈ¢ëÁîüÊàêÊ°ÜÊû∂ÔºåËÉΩÂ§üÁîüÊàêÈ´ò‰øùÁúüÂ∫¶ÁöÑËßÜÈ¢ëÂπ∂ÂáÜÁ°ÆÊéßÂà∂Áõ∏Êú∫ÂßøÊÄÅ„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®Êó†ÈôêÂçïÂ∫îÊÄßÂèòÊç¢ÔºåÂ∞Ü‰∏âÁª¥Áõ∏Êú∫ÊóãËΩ¨‰ø°ÊÅØÁõ¥Êé•ÁºñÁ†ÅÂà∞ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁöÑ‰∫åÁª¥ÊΩúÂú®Á©∫Èó¥‰∏≠„ÄÇÈÄöËøáÁ´ØÂà∞Á´ØËÆ≠ÁªÉÔºåÈ¢ÑÊµãÊÆãÂ∑ÆËßÜÂ∑ÆÈ°πÔºå‰ª•ÂÆûÁé∞È´òÁõ∏Êú∫ÂßøÊÄÅ‰øùÁúüÂ∫¶„ÄÇÂêåÊó∂ÔºåÊï∞ÊçÆÂ¢ûÂº∫ÁÆ°ÈÅìÂ∞ÜÁé∞ÊúâÁöÑÂêàÊàêÂ§öËßÜÂõæÊï∞ÊçÆÈõÜËΩ¨Âåñ‰∏∫ÂÖ∑ÊúâÂ§öÊ†∑ÂåñËΩ®ËøπÂíåÁÑ¶Ë∑ùÁöÑÂ∫èÂàó„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåInfCam Âú®Áõ∏Êú∫ÂßøÊÄÅÂáÜÁ°ÆÊÄßÂíåËßÜËßâ‰øùÁúüÂ∫¶ÊñπÈù¢‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåÂπ∂ËÉΩÂæàÂ•ΩÂú∞‰ªéÂêàÊàêÊï∞ÊçÆÊé®ÂπøÂà∞ÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆ„ÄÇ', title='È´ò‰øùÁúüËßÜÈ¢ëÁîüÊàê‰∏éÁõ∏Êú∫ÊéßÂà∂ÁöÑÂàõÊñ∞‰πãË∑Ø'))
[23.12.2025 06:37] Using data from previous issue: {"categories": ["#open_source", "#hallucinations", "#benchmark", "#rag"], "emoji": "üîç", "ru": {"title": "–û—Ç —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∫ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ–π –∫–æ—Ä–ø—É—Å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ –≤ RAG", "desc": "QuCo-RAG ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—ä–µ–∫—Ç–∏–≤–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç
[23.12.2025 06:37] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#alignment"], "emoji": "üß†", "ru": {"title": "–£–º–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –ø–æ–Ω–∏–º–∞—é—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö —Ç—Ä—É–¥–Ω–æ—Å—Ç–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ LLM —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –±–æ–ª–µ–µ 20 –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö
[23.12.2025 06:37] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#video", "#dataset", "#training"], "emoji": "üé¨", "ru": {"title": "–¢–æ—á–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é", "desc": "ReCo ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –ø–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤
[23.12.2025 06:37] Using data from previous issue: {"categories": ["#video", "#3d", "#multimodal"], "emoji": "üé¨", "ru": {"title": "3D –≥–µ–æ–º–µ—Ç—Ä–∏—è –≤—Å—Ç—Ä–µ—á–∞–µ—Ç –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏—é: —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–µ –≤–∏–¥–µ–æ –∏–∑ –∫—ç—à–∞", "desc": "WorldWarp —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏, –∫–æ–º–±–∏–Ω–∏—Ä—É—è 3D –∫—ç—à –Ω–∞ –æ—Å–Ω–æ–≤–µ Gaussian Splatting
[23.12.2025 06:37] Using data from previous issue: {"categories": ["#training", "#small_models", "#agents", "#benchmark"], "emoji": "üéÆ", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π curriculum learning —á–µ—Ä–µ–∑ —Å–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—É—é –∏–≥—Ä—É –∞–≥–µ–Ω—Ç–∞ –∏ —Å–∏–º—É–ª—è—Ç–æ—Ä–∞", "desc": "GenEnv ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—É—é –∏–≥—Ä—É –º–µ–∂–¥—É –∞–≥–µ–Ω—Ç–æ–º –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º —Å–∏–º—É–ª—è—Ç–æ—Ä–æ–º –æ–∫—Ä
[23.12.2025 06:37] Using data from previous issue: {"categories": ["#optimization", "#training", "#benchmark", "#rlhf", "#rl", "#interpretability", "#reasoning", "#architecture"], "emoji": "üé®", "ru": {"title": "–ü–∞–ª–∏—Ç—Ä–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ –º—ã—à–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ –ª–∞—Ç–µ–Ω—Ç–Ω—É—é –º–æ–¥—É–ª—è—Ü–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Reasoning Palette, –∫–æ
[23.12.2025 06:37] Querying the API.
[23.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

IPC is an unsupervised framework that uses internal probing of large language models to generate code without labeled datasets, achieving competitive performance with reduced resource dependency.  					AI-generated summary 				 Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.
[23.12.2025 06:37] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ IPC ‚Äî –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –∑–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö. –ü–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ—Ö–Ω–∏–∫: –∑–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–æ–±–ª–µ–º, –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤, –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Ä–µ—à–µ–Ω–∏–π –∏ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—é –∑–Ω–∞–Ω–∏–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –º–æ–¥–µ–ª–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç –º–µ—Ö–∞–Ω–∏–∑–º—ã —Å–∞–º–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è UCoder ‚Äî –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ —Å –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–º –æ–±—É—á–µ–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, –ø—Ä–∏ —ç—Ç–æ–º –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.",
  "emoji": "üîç",
  "title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –±–µ–∑ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –∑–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏–µ LLM"
}
```
[23.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"IPC is an unsupervised framework that uses internal probing of large language models to generate code without labeled datasets, achieving competitive performance with reduced resource dependency.  					AI-generated summary 				 Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios."

[23.12.2025 06:37] Response: ```python
["PLP", "TRAINING", "BENCHMARK"]
```
[23.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"IPC is an unsupervised framework that uses internal probing of large language models to generate code without labeled datasets, achieving competitive performance with reduced resource dependency.  					AI-generated summary 				 Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios."

[23.12.2025 06:37] Response: ```python
['OPTIMIZATION', 'INTERPRETABILITY']
```

**Justification:**

1. **OPTIMIZATION**: The paper focuses on reducing resource dependency and computational costs for training code generation models through an unsupervised framework, which is a core optimization concern.

2. **INTERPRETABILITY**: The paper explicitly analyzes internal model states and probes the internal knowledge and confidence patterns of LLMs ("internal probing," "internal model states contain rich signals"), which directly relates to understanding and explaining model behavior.
[23.12.2025 06:37] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "INTERPRETABILITY"]


**Justification:**

1. **OPTIMIZATION**: The paper focuses on reducing resource dependency and computational costs for training code generation models through an unsupervised framework, which is a core optimization concern.

2. **INTERPRETABILITY**: The paper explicitly analyzes internal model states and probes the internal knowledge and confidence patterns of LLMs ("internal probing," "internal model states contain rich signals"), which directly relates to understanding and explaining model behavior.
[23.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents IPC, an unsupervised framework that enables code generation using large language models (LLMs) without the need for labeled datasets. It explores various probing techniques to extract internal knowledge and confidence patterns from LLMs, allowing for effective code generation. By employing self-consistency mechanisms and representation-based quality estimation, IPC identifies reliable code candidates and trains a model called UCoder. The results show that IPC can achieve competitive performance in code generation tasks while significantly reducing reliance on labeled data and computational resources.","title":"Unleashing Code Generation: IPC\'s Unsupervised Approach"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents IPC, an unsupervised framework that enables code generation using large language models (LLMs) without the need for labeled datasets. It explores various probing techniques to extract internal knowledge and confidence patterns from LLMs, allowing for effective code generation. By employing self-consistency mechanisms and representation-based quality estimation, IPC identifies reliable code candidates and trains a model called UCoder. The results show that IPC can achieve competitive performance in code generation tasks while significantly reducing reliance on labeled data and computational resources.', title="Unleashing Code Generation: IPC's Unsupervised Approach"))
[23.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫IPCÁöÑÊó†ÁõëÁù£Ê°ÜÊû∂ÔºåÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÜÖÈÉ®Êé¢ÊµãÊù•ÁîüÊàê‰ª£Á†ÅÔºåËÄåÊó†ÈúÄ‰æùËµñÊ†áËÆ∞Êï∞ÊçÆÈõÜ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÈóÆÈ¢òÁ©∫Èó¥Êé¢Êµã„ÄÅÁêÜËß£Êé¢Êµã„ÄÅËß£ÂÜ≥ÊñπÊ°àÁ©∫Èó¥Êé¢ÊµãÂíåÁü•ËØÜÂ∑©Âõ∫Á≠âÊäÄÊúØÔºåÊåñÊéòÊ®°ÂûãÂÜÖÈÉ®ÁöÑÁü•ËØÜÂíå‰ø°ÂøÉÊ®°Âºè„ÄÇIPCÈÄöËøáËá™‰∏ÄËá¥ÊÄßÊú∫Âà∂ÂíåÂü∫‰∫éË°®Á§∫ÁöÑË¥®ÈáèËØÑ‰º∞Êù•ËØÜÂà´ÂèØÈù†ÁöÑ‰ª£Á†ÅÂÄôÈÄâÔºå‰ªéËÄåËÆ≠ÁªÉÊó†ÁõëÁù£Â≠¶‰π†ÁöÑÁºñÁ†ÅÂô®UCoder„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåIPCÂú®Â§ö‰∏™‰ª£Á†ÅÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫‰∏éÁõëÁù£ÊñπÊ≥ïÁõ∏ÂΩìÁöÑÊÄßËÉΩÔºåÂêåÊó∂ÊòæËëóÂáèÂ∞ë‰∫ÜÂØπÊ†áËÆ∞Êï∞ÊçÆÂíåËÆ°ÁÆóËµÑÊ∫êÁöÑ‰æùËµñ„ÄÇ","title":"Êó†ÁõëÁù£Â≠¶‰π†Ôºö‰ª£Á†ÅÁîüÊàêÁöÑÊñ∞ÊñπÂêë"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫IPCÁöÑÊó†ÁõëÁù£Ê°ÜÊû∂ÔºåÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÜÖÈÉ®Êé¢ÊµãÊù•ÁîüÊàê‰ª£Á†ÅÔºåËÄåÊó†ÈúÄ‰æùËµñÊ†áËÆ∞Êï∞ÊçÆÈõÜ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÈóÆÈ¢òÁ©∫Èó¥Êé¢Êµã„ÄÅÁêÜËß£Êé¢Êµã„ÄÅËß£ÂÜ≥ÊñπÊ°àÁ©∫Èó¥Êé¢ÊµãÂíåÁü•ËØÜÂ∑©Âõ∫Á≠âÊäÄÊúØÔºåÊåñÊéòÊ®°ÂûãÂÜÖÈÉ®ÁöÑÁü•ËØÜÂíå‰ø°ÂøÉÊ®°Âºè„ÄÇIPCÈÄöËøáËá™‰∏ÄËá¥ÊÄßÊú∫Âà∂ÂíåÂü∫‰∫éË°®Á§∫ÁöÑË¥®ÈáèËØÑ‰º∞Êù•ËØÜÂà´ÂèØÈù†ÁöÑ‰ª£Á†ÅÂÄôÈÄâÔºå‰ªéËÄåËÆ≠ÁªÉÊó†ÁõëÁù£Â≠¶‰π†ÁöÑÁºñÁ†ÅÂô®UCoder„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåIPCÂú®Â§ö‰∏™‰ª£Á†ÅÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫‰∏éÁõëÁù£ÊñπÊ≥ïÁõ∏ÂΩìÁöÑÊÄßËÉΩÔºåÂêåÊó∂ÊòæËëóÂáèÂ∞ë‰∫ÜÂØπÊ†áËÆ∞Êï∞ÊçÆÂíåËÆ°ÁÆóËµÑÊ∫êÁöÑ‰æùËµñ„ÄÇ', title='Êó†ÁõëÁù£Â≠¶‰π†Ôºö‰ª£Á†ÅÁîüÊàêÁöÑÊñ∞ÊñπÂêë'))
[23.12.2025 06:37] Querying the API.
[23.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LoGoPlanner is an end-to-end navigation framework that improves trajectory planning in unstructured environments by integrating localization, scene geometry reconstruction, and policy conditioning.  					AI-generated summary 				 Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the https://steinate.github.io/logoplanner.github.io/{project page}.
[23.12.2025 06:37] Response: ```json
{
  "desc": "LoGoPlanner –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–∫–≤–æ–∑–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –º–æ–±–∏–ª—å–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤ –≤ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö. –°–∏—Å—Ç–µ–º–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é, —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –≥–µ–æ–º–µ—Ç—Ä–∏–∏ —Å—Ü–µ–Ω—ã –∏ –æ–±—É—Å–ª–æ–≤–ª–∏–≤–∞–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –µ–¥–∏–Ω—ã–π –∫–æ–Ω–µ—Ü-–≤-–∫–æ–Ω–µ—Ü –æ–±—É—á–∞–µ–º—ã–π –ø—Ä–æ—Ü–µ—Å—Å. –ö–ª—é—á–µ–≤–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏-–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ-–≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –æ—Å–Ω–æ–≤—ã –¥–ª—è –Ω–µ—è–≤–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ –ø–ª–æ—Ç–Ω–æ–π –∫–∞—Ä—Ç—ã –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–π, —á—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤ –∫–∞—Å–∫–∞–¥–Ω—ã—Ö –º–æ–¥—É–ª—è—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 27.3% –∏ —Ö–æ—Ä–æ—à—É—é –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —Ä–æ–±–æ—Ç–æ–≤ –∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è.",
  "emoji": "ü§ñ",
  "title": "–ï–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Å –Ω–µ—è–≤–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–µ–π —á–µ—Ä–µ–∑ –≥–µ–æ–º–µ—Ç—Ä–∏—é —Å—Ü–µ–Ω—ã"
}
```
[23.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LoGoPlanner is an end-to-end navigation framework that improves trajectory planning in unstructured environments by integrating localization, scene geometry reconstruction, and policy conditioning.  					AI-generated summary 				 Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the https://steinate.github.io/logoplanner.github.io/{project page}."

[23.12.2025 06:37] Response: ```python
["ROBOTICS", "AGENTS", "3D"]
```
[23.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LoGoPlanner is an end-to-end navigation framework that improves trajectory planning in unstructured environments by integrating localization, scene geometry reconstruction, and policy conditioning.  					AI-generated summary 				 Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the https://steinate.github.io/logoplanner.github.io/{project page}."

[23.12.2025 06:37] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```

**Justification:**

- **OPTIMIZATION**: The paper focuses on improving trajectory planning efficiency and performance through an end-to-end learning framework that reduces latency and cascading errors compared to traditional modular pipelines. The optimization of the navigation system through auxiliary tasks (localization grounding, geometry reconstruction, policy conditioning) is central to the contribution.

- **OPEN_SOURCE**: The paper explicitly states "The code and models have been made publicly available on the https://steinate.github.io/logoplanner.github.io/{project page}," indicating the authors are releasing their framework, models, and code to the public.
[23.12.2025 06:37] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "OPEN_SOURCE"]


**Justification:**

- **OPTIMIZATION**: The paper focuses on improving trajectory planning efficiency and performance through an end-to-end learning framework that reduces latency and cascading errors compared to traditional modular pipelines. The optimization of the navigation system through auxiliary tasks (localization grounding, geometry reconstruction, policy conditioning) is central to the contribution.

- **OPEN_SOURCE**: The paper explicitly states "The code and models have been made publicly available on the https://steinate.github.io/logoplanner.github.io/{project page}," indicating the authors are releasing their framework, models, and code to the public.
[23.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LoGoPlanner is a novel navigation framework designed for mobile robots operating in unstructured environments. It integrates localization, scene geometry reconstruction, and policy conditioning into a single end-to-end system, which helps to minimize errors and improve efficiency. By fine-tuning a visual-geometry backbone and reconstructing scene geometry, LoGoPlanner enhances obstacle avoidance and provides accurate localization without relying on separate modules. The framework has shown significant performance improvements in both simulations and real-world tests, outperforming traditional methods by over 27.3%.","title":"Revolutionizing Robot Navigation with LoGoPlanner"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LoGoPlanner is a novel navigation framework designed for mobile robots operating in unstructured environments. It integrates localization, scene geometry reconstruction, and policy conditioning into a single end-to-end system, which helps to minimize errors and improve efficiency. By fine-tuning a visual-geometry backbone and reconstructing scene geometry, LoGoPlanner enhances obstacle avoidance and provides accurate localization without relying on separate modules. The framework has shown significant performance improvements in both simulations and real-world tests, outperforming traditional methods by over 27.3%.', title='Revolutionizing Robot Navigation with LoGoPlanner'))
[23.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LoGoPlanner ÊòØ‰∏Ä‰∏™Á´ØÂà∞Á´ØÁöÑÂØºËà™Ê°ÜÊû∂ÔºåÊó®Âú®ÊîπÂñÑÂú®ÈùûÁªìÊûÑÂåñÁéØÂ¢É‰∏≠ÁöÑËΩ®ËøπËßÑÂàí„ÄÇÂÆÉÈÄöËøáÊï¥ÂêàÂÆö‰Ωç„ÄÅÂú∫ÊôØÂá†‰ΩïÈáçÂª∫ÂíåÁ≠ñÁï•Êù°‰ª∂ÂåñÔºåËß£ÂÜ≥‰∫Ü‰º†ÁªüÊ®°ÂùóÂåñÁÆ°ÈÅì‰∏≠ÁöÑÂª∂ËøüÂíåÈîôËØØ‰º†Êí≠ÈóÆÈ¢ò„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂæÆË∞ÉËßÜËßâÂá†‰ΩïÈ™®Âπ≤ÁΩëÁªúÔºåÂÆûÁé∞‰∫ÜÂáÜÁ°ÆÁöÑÂÆö‰ΩçÂíåÁéØÂ¢ÉÊÑüÁü•Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÈöúÁ¢çÁâ©ËßÑÈÅøÁöÑÂèØÈù†ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLoGoPlanner Âú®Ê®°ÊãüÂíåÁúüÂÆûÁéØÂ¢É‰∏≠ÂùáË°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëóÊèêÈ´ò‰∫ÜËßÑÂàíÁöÑ‰∏ÄËá¥ÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ","title":"LoGoPlannerÔºöÊèêÂçáÈùûÁªìÊûÑÂåñÁéØÂ¢É‰∏≠ÁöÑÂØºËà™ËÉΩÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LoGoPlanner ÊòØ‰∏Ä‰∏™Á´ØÂà∞Á´ØÁöÑÂØºËà™Ê°ÜÊû∂ÔºåÊó®Âú®ÊîπÂñÑÂú®ÈùûÁªìÊûÑÂåñÁéØÂ¢É‰∏≠ÁöÑËΩ®ËøπËßÑÂàí„ÄÇÂÆÉÈÄöËøáÊï¥ÂêàÂÆö‰Ωç„ÄÅÂú∫ÊôØÂá†‰ΩïÈáçÂª∫ÂíåÁ≠ñÁï•Êù°‰ª∂ÂåñÔºåËß£ÂÜ≥‰∫Ü‰º†ÁªüÊ®°ÂùóÂåñÁÆ°ÈÅì‰∏≠ÁöÑÂª∂ËøüÂíåÈîôËØØ‰º†Êí≠ÈóÆÈ¢ò„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂæÆË∞ÉËßÜËßâÂá†‰ΩïÈ™®Âπ≤ÁΩëÁªúÔºåÂÆûÁé∞‰∫ÜÂáÜÁ°ÆÁöÑÂÆö‰ΩçÂíåÁéØÂ¢ÉÊÑüÁü•Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÈöúÁ¢çÁâ©ËßÑÈÅøÁöÑÂèØÈù†ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLoGoPlanner Âú®Ê®°ÊãüÂíåÁúüÂÆûÁéØÂ¢É‰∏≠ÂùáË°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëóÊèêÈ´ò‰∫ÜËßÑÂàíÁöÑ‰∏ÄËá¥ÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ', title='LoGoPlannerÔºöÊèêÂçáÈùûÁªìÊûÑÂåñÁéØÂ¢É‰∏≠ÁöÑÂØºËà™ËÉΩÂäõ'))
[23.12.2025 06:37] Using data from previous issue: {"categories": ["#story_generation", "#diffusion", "#benchmark", "#video", "#dataset", "#training", "#long_context"], "emoji": "üé¨", "ru": {"title": "–ü–∞–º—è—Ç—å –∫–∞–∫ –∫–ª—é—á –∫ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–º—É –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–º—É –≤–∏–¥–µ–æ—Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–Ω–∏—é", "desc": "StoryMem ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ–∫–∞–¥—Ä–æ–≤—ã—Ö –≤–∏–¥–µ–æ —Å –∫–∏–Ω
[23.12.2025 06:37] Using data from previous issue: {"categories": ["#dataset", "#games", "#agents", "#benchmark"], "emoji": "üì±", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –Ω–∞—Å—ã—â–µ–Ω–∏—è: –Ω–æ–≤—ã–π —Å–ª–æ–∂–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö AI-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MobileWorld –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
[23.12.2025 06:37] Using data from previous issue: {"categories": ["#dataset", "#3d", "#training", "#video", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π —á–µ—Ä–µ–∑ 3D —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–∏–¥–µ–æ—Å–∏–Ω—Ç–µ–∑", "desc": "Real2Edit2Real ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π —Ä–æ–±–æ—Ç–æ–º, –∫–æ—Ç–æ—Ä—ã
[23.12.2025 06:37] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#3d", "#interpretability", "#multimodal"], "emoji": "üß©", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –Ω–µ—è–≤–Ω—ã—Ö 3D-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —á–∞—Å—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ALIGN-Parts ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö 
[23.12.2025 06:37] Renaming data file.
[23.12.2025 06:37] Renaming previous data. hf_papers.json to ./d/2025-12-23.json
[23.12.2025 06:37] Saving new data file.
[23.12.2025 06:37] Generating page.
[23.12.2025 06:37] Renaming previous page.
[23.12.2025 06:37] Renaming previous data. index.html to ./d/2025-12-23.html
[23.12.2025 06:37] Writing result.
[23.12.2025 06:37] Renaming log file.
[23.12.2025 06:37] Renaming previous data. log.txt to ./logs/2025-12-23_last_log.txt
