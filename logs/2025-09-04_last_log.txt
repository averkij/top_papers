[04.09.2025 21:10] Read previous papers.
[04.09.2025 21:10] Generating top page (month).
[04.09.2025 21:10] Writing top page (month).
[04.09.2025 22:11] Read previous papers.
[04.09.2025 22:11] Get feed.
[04.09.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01106
[04.09.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00375
[04.09.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.03405
[04.09.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01977
[04.09.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00428
[04.09.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02722
[04.09.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02530
[04.09.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00930
[04.09.2025 22:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[04.09.2025 22:11] No deleted papers detected.
[04.09.2025 22:11] Downloading and parsing papers (pdf, html). Total: 8.
[04.09.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2509.01106.
[04.09.2025 22:11] Extra JSON file exists (./assets/json/2509.01106.json), skip PDF parsing.
[04.09.2025 22:11] Paper image links file exists (./assets/img_data/2509.01106.json), skip HTML parsing.
[04.09.2025 22:11] Success.
[04.09.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2509.00375.
[04.09.2025 22:11] Extra JSON file exists (./assets/json/2509.00375.json), skip PDF parsing.
[04.09.2025 22:11] Paper image links file exists (./assets/img_data/2509.00375.json), skip HTML parsing.
[04.09.2025 22:11] Success.
[04.09.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2509.03405.
[04.09.2025 22:11] Extra JSON file exists (./assets/json/2509.03405.json), skip PDF parsing.
[04.09.2025 22:11] Paper image links file exists (./assets/img_data/2509.03405.json), skip HTML parsing.
[04.09.2025 22:11] Success.
[04.09.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2509.01977.
[04.09.2025 22:11] Extra JSON file exists (./assets/json/2509.01977.json), skip PDF parsing.
[04.09.2025 22:11] Paper image links file exists (./assets/img_data/2509.01977.json), skip HTML parsing.
[04.09.2025 22:11] Success.
[04.09.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2509.00428.
[04.09.2025 22:11] Extra JSON file exists (./assets/json/2509.00428.json), skip PDF parsing.
[04.09.2025 22:11] Paper image links file exists (./assets/img_data/2509.00428.json), skip HTML parsing.
[04.09.2025 22:11] Success.
[04.09.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2509.02722.
[04.09.2025 22:11] Extra JSON file exists (./assets/json/2509.02722.json), skip PDF parsing.
[04.09.2025 22:11] Paper image links file exists (./assets/img_data/2509.02722.json), skip HTML parsing.
[04.09.2025 22:11] Success.
[04.09.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2509.02530.
[04.09.2025 22:11] Extra JSON file exists (./assets/json/2509.02530.json), skip PDF parsing.
[04.09.2025 22:11] Paper image links file exists (./assets/img_data/2509.02530.json), skip HTML parsing.
[04.09.2025 22:11] Success.
[04.09.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2509.00930.
[04.09.2025 22:11] Extra JSON file exists (./assets/json/2509.00930.json), skip PDF parsing.
[04.09.2025 22:11] Paper image links file exists (./assets/img_data/2509.00930.json), skip HTML parsing.
[04.09.2025 22:11] Success.
[04.09.2025 22:11] Enriching papers with extra data.
[04.09.2025 22:11] ********************************************************************************
[04.09.2025 22:11] Abstract 0. Robix, a unified vision-language model, integrates robot reasoning, task planning, and natural language interaction, demonstrating superior performance in interactive task execution through chain-of-thought reasoning and a three-stage training strategy.  					AI-generated summary 				 We introduce R...
[04.09.2025 22:11] ********************************************************************************
[04.09.2025 22:11] Abstract 1. InfoSeek is a scalable framework for generating complex Deep Research tasks by synthesizing hierarchical constraint satisfaction problems, enabling models to outperform larger baselines on challenging benchmarks.  					AI-generated summary 				 Large language models (LLMs) are increasingly expected ...
[04.09.2025 22:11] ********************************************************************************
[04.09.2025 22:11] Abstract 2. LMEnt is a suite for analyzing knowledge acquisition in language models during pretraining, providing annotated corpora, retrieval methods, and pretrained models to study knowledge representations and learning dynamics.  					AI-generated summary 				 Language models (LMs) increasingly drive real-wo...
[04.09.2025 22:11] ********************************************************************************
[04.09.2025 22:11] Abstract 3. MOSAIC framework enhances multi-subject image generation by ensuring precise semantic alignment and orthogonal feature disentanglement, achieving high fidelity even with multiple references.  					AI-generated summary 				 Multi-subject personalized generation presents unique challenges in maintaini...
[04.09.2025 22:11] ********************************************************************************
[04.09.2025 22:11] Abstract 4. Face-MoGLE, a novel framework using Diffusion Transformers, achieves high-quality, controllable face generation through semantic-decoupled latent modeling, expert specialization, and dynamic gating.  					AI-generated summary 				 Controllable face generation poses critical challenges in generative ...
[04.09.2025 22:11] ********************************************************************************
[04.09.2025 22:11] Abstract 5. The Vision Language World Model (VLWM) achieves state-of-the-art performance in visual planning by integrating language-based world modeling, action policy learning, and dynamics modeling with semantic and temporal abstraction.  					AI-generated summary 				 Effective planning requires strong world...
[04.09.2025 22:11] ********************************************************************************
[04.09.2025 22:11] Abstract 6. Camera Depth Models (CDMs) enhance depth camera accuracy by denoising and improving metric depth prediction, enabling better generalization of robotic manipulation policies from simulation to real-world tasks.  					AI-generated summary 				 Modern robotic manipulation primarily relies on visual obs...
[04.09.2025 22:11] ********************************************************************************
[04.09.2025 22:11] Abstract 7. SATQuest evaluates and enhances LLM logical reasoning by generating diverse SAT-based problems, offering insights into reasoning performance and enabling effective fine-tuning.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) have demonstrated remarkable general reason...
[04.09.2025 22:11] Read previous papers.
[04.09.2025 22:11] Generating reviews via LLM API.
[04.09.2025 22:11] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#alignment", "#robotics", "#multimodal", "#agents", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "Robix: –ï–¥–∏–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "Robix - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Ä–æ–±–æ
[04.09.2025 22:11] Using data from previous issue: {"categories": ["#training", "#dataset", "#synthetic", "#reasoning", "#benchmark", "#multimodal", "#optimization"], "emoji": "üîç", "ru": {"title": "InfoSeek: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "InfoSeek - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –≥–ª—É–±–æ–∫–æ–≥–æ
[04.09.2025 22:11] Using data from previous issue: {"categories": ["#data", "#open_source", "#interpretability", "#dataset", "#training", "#benchmark", "#science", "#multimodal"], "emoji": "üß†", "ru": {"title": "LMEnt: –ó–∞–≥–ª—è–¥—ã–≤–∞—è –≤–Ω—É—Ç—Ä—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "LMEnt - —ç—Ç–æ –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —É—Å–≤–æ–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å
[04.09.2025 22:11] Using data from previous issue: {"categories": ["#dataset", "#cv", "#synthetic", "#benchmark"], "emoji": "üß©", "ru": {"title": "–¢–æ—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "MOSAIC - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ç–æ—á–Ω–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏ –æ—Ä—Ç–æ–≥–æ–Ω
[04.09.2025 22:11] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#multimodal", "#architecture", "#security"], "emoji": "üé≠", "ru": {"title": "–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –ª–∏—Ü —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤", "desc": "Face-MoGLE - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª–∏—Ü, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã. –û–Ω–∞ –æ–±–µ
[04.09.2025 22:11] Using data from previous issue: {"categories": ["#agents", "#rl", "#benchmark", "#cv", "#optimization", "#multimodal", "#reasoning"], "emoji": "üß†", "ru": {"title": "VLWM: –í–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "VLWM (Vision Language World Model) - —ç—Ç–æ –º–æ–¥–µ–ª—å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —è–∑—ã–∫–æ–≤–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∏—Ä–∞
[04.09.2025 22:11] Using data from previous issue: {"categories": ["#optimization", "#3d", "#transfer_learning", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–¢–æ—á–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –≥–ª—É–±–∏–Ω—ã –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏: –æ—Ç —Å–∏–º—É–ª—è—Ü–∏–∏ –∫ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Camera Depth Models (CDM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–∞–º–µ—Ä –≥–ª—É–±–∏–Ω—ã –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ. CDM –æ–±
[04.09.2025 22:11] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#benchmark", "#training", "#dataset"], "emoji": "üß†", "ru": {"title": "SATQuest: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –ª–æ–≥–∏–∫–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "SATQuest - —ç—Ç–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 
[04.09.2025 22:11] Renaming data file.
[04.09.2025 22:11] Renaming previous data. hf_papers.json to ./d/2025-09-04.json
[04.09.2025 22:11] Saving new data file.
[04.09.2025 22:11] Generating page.
[04.09.2025 22:11] Renaming previous page.
[04.09.2025 22:11] Renaming previous data. index.html to ./d/2025-09-04.html
[04.09.2025 22:11] Writing result.
[04.09.2025 22:11] Renaming log file.
[04.09.2025 22:11] Renaming previous data. log.txt to ./logs/2025-09-04_last_log.txt
