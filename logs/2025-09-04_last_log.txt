[04.09.2025 04:13] Read previous papers.
[04.09.2025 04:13] Generating top page (month).
[04.09.2025 04:13] Writing top page (month).
[04.09.2025 05:11] Read previous papers.
[04.09.2025 05:11] Get feed.
[04.09.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01106
[04.09.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00375
[04.09.2025 05:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[04.09.2025 05:11] No deleted papers detected.
[04.09.2025 05:11] Downloading and parsing papers (pdf, html). Total: 2.
[04.09.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2509.01106.
[04.09.2025 05:11] Extra JSON file exists (./assets/json/2509.01106.json), skip PDF parsing.
[04.09.2025 05:11] Paper image links file exists (./assets/img_data/2509.01106.json), skip HTML parsing.
[04.09.2025 05:11] Success.
[04.09.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2509.00375.
[04.09.2025 05:11] Downloading paper 2509.00375 from http://arxiv.org/pdf/2509.00375v1...
[04.09.2025 05:11] Extracting affiliations from text.
[04.09.2025 05:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Technical report Kun Luo Ziyi Xia BAAI {ziyixia85,luokun695,chienqhj,zhengliu1026}@gmail.com Hongjin Qian Zheng Liu "
[04.09.2025 05:11] Response: []
[04.09.2025 05:11] Extracting affiliations from text.
[04.09.2025 05:11] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Technical reportKun Luo Ziyi Xia BAAI {ziyixia85,luokun695,chienqhj,zhengliu1026}@gmail.com Hongjin Qian Zheng LiuLarge language models (LLMs) are increasingly expected to go beyond simple factual queries toward Deep Researchtasks that require decomposing questions into sub-problems, coordinating multi-step reasoning, and synthesizing evidence from diverse sources. We formalize Deep Research tasks with verifiable answers as Hierarchical Constraint Satisfaction Problems (HCSPs), which are fundamentally different from single-constraint, multi-hop, or flat CSP formulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA) fail to capture this complexity, while recent synthetic datasets often introduce shortcut reasoning, knowledge leakage, or lack sufficient structural depth. To address this gap, we introduce InfoSeek, scalable framework for synthesizing complex Deep Research tasks. InfoSeek uses dual-agent system to recursively build Research Tree from large-scale webpages, blurring intermediate nodes into valid sub-problems, and converting these trees into natural language questions that require traversing the full hierarchy. It also enables rapid scaling, yielding over 50K training examples, curated test set, and reasoning trajectories generated via reject sampling. Experiments show that models trained on InfoSeek consistently outperform strong baselines. On challenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass much larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash), while achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro). By preserving meta-information such as intermediate steps and retrieval labels, InfoSeek further supports advanced optimization strategies, including compound reward design and trajectory-level exploration. We provide our codes and datasets in this repository. 5 2 0 2 0 3 ] . [ 1 5 7 3 0 0 . 9 0 5 2 : r Figure 1: Performance comparison on the BrowseComp-Plus benchmark. InfoSeeker-3B, compact LLM trained with the InfoSeek dataset, significantly outperforms Qwen3-32B and achieves performance on par with leading commercial LLMs (ordered in API prices), highlighting the strong potential of InfoSeek for advancing Deep Research tasks. Notably, the InfoSeek data synthesis framework is fully open-source, enabling convenient and scalable dataset construction. Equal contribution. Corresponding Author 1 Technical report Table 1: Comparison of the open-source status of classical QA datasets and recent data synthesis approaches for Deep Research. While prior datasets either lack structural depth or remain limited in scale, InfoSeek provides the first large-scale dataset dedicated to Deep Research scenarios, capable of generating hierarchical constraint satisfaction problems with controllable complexity, and supporting easy scalability for diverse research needs. Problem Data Source QA pairs Trajectories Framework Name NQ HotpotQA Single-hop Multi-hop Wiki Wiki Multi-hop WebWalkerQA InForage Multi-hop SimpleDeepSearcher Multi-hop Multi-hop Pangu DeepDiver Multi-hop Wiki&Web WebDancer Complex WebShaper HCSP InfoSeek Wiki Wiki&Web Web Web Web 300k+ 100k+ 14.3k 200 500 50k+ 871 200 16 .5k Open OpenRecently, large language models (LLMs) have transformed AI by generating and interpreting language with unprecedented fluency and contextual depth (OpenAI, 2023; Gemini Team, 2025). Beyond simple factual queries, emerging advances point to new frontier: Deep Researchwhere models must decompose complex tasks, generate sub-queries, and reason across diverse sources of information (OpenAI, 2025). Unlike conventional information seeking, which suffices for straightforward retrieval, Deep Research demands synthesizing heterogeneous evidence, coordinating multi-step reasoning, and often interacting with external tools (Wu et al., 2025c; Zhang et al., 2024). Such capabilities are essential for domains like scientific discovery and policy analysis, where problems are open-ended and knowledge landscapes continuously evolve. Consequently, Deep Research is increasingly viewed as cornerstone for the next generation of LLMs, shifting them from conversational assistants to autonomous knowledge engines (Li et al., 2025c). deep research question goes beyond simple factual lookup (OpenAI, 2025). It requires navigating multiple layers of knowledge integration, and is best understood in contrast with several simpler problem types. constraint satisfaction problem is solved by combining several independent conditions to narrow the candidate set. multi-hop problem demands sequence of dependent inferences and search (Yang et al., 2018; Zhao et al., 2024; Qian et al., 2025). Deep research questions extend beyond both by involving hierarchy of interdependent constraints that intertwine both parallel conditions and sequential steps. The solution emerges only through progressively resolving this hierarchy of sub-questions. When the final answer is unique and verifiable, the reasoning process can be naturally represented as tree, with intermediate vertices denoting sub-questions and branches encoding their logical dependencies. In this work, rather than long-form open-ended tasks such as report writing, we focus on deep research questions that yield unique and verifiable answer. Some recent approaches propose carefully designed workflows for planning and tool use (Li et al., 2025b; Wu et al., 2025c; Soni et al., 2025; Zhang et al., 2025). While effective in narrow domains, these workflows lack the flexibility required for diverse Deep Research tasks (Li et al., 2025c). Another line of work enhances models reasoning and search capabilities through supervised finetuning (Sun et al., 2025b) or reinforcement learning (Jin et al., 2025; Song et al., 2025a; Zheng et al., 2025). Although such methods achieve gains on traditional and multi-hop QA benchmarks, their training still depends heavily on datasets like Natural Questions (Kwiatkowski et al., 2019) and HotpotQA (Yang et al., 2018), which remain far simpler than real Deep Research scenarios. Other efforts (Wu et al., 2025b; Shi et al., 2025a) attempt to construct open-source QA datasets or trajectories, but these remain focused on multi-hop QA. More recent studies (Qian & Liu, 2025; Shi et al., 2025a; Wu et al., 2025a; Tao et al., 2025) explore harder question types involving web pages or Wikipedia, yet neither their datasets nor their workflows are publicly released. Table 1 2"
[04.09.2025 05:11] Mistral response. {"id": "4ef7a08e4f9a4f08b2d9ad632dc8c387", "created": 1756962679, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1503, "total_tokens": 1515, "completion_tokens": 12}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"BAAI\"\n]\n```"}}]}
[04.09.2025 05:11] Response: ```python
[
    "BAAI"
]
```
[04.09.2025 05:11] Deleting PDF ./assets/pdf/2509.00375.pdf.
[04.09.2025 05:11] Success.
[04.09.2025 05:11] Enriching papers with extra data.
[04.09.2025 05:11] ********************************************************************************
[04.09.2025 05:11] Abstract 0. Robix, a unified vision-language model, integrates robot reasoning, task planning, and natural language interaction, demonstrating superior performance in interactive task execution through chain-of-thought reasoning and a three-stage training strategy.  					AI-generated summary 				 We introduce R...
[04.09.2025 05:11] ********************************************************************************
[04.09.2025 05:11] Abstract 1. InfoSeek is a scalable framework for generating complex Deep Research tasks by synthesizing hierarchical constraint satisfaction problems, enabling models to outperform larger baselines on challenging benchmarks.  					AI-generated summary 				 Large language models (LLMs) are increasingly expected ...
[04.09.2025 05:11] Read previous papers.
[04.09.2025 05:11] Generating reviews via LLM API.
[04.09.2025 05:11] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#alignment", "#robotics", "#multimodal", "#agents", "#optimization"], "emoji": "🤖", "ru": {"title": "Robix: Единый интеллект для роботов нового поколения", "desc": "Robix - это унифицированная модель машинного обучения, объединяющая рассуждения робо
[04.09.2025 05:11] Using data from previous issue: {"categories": ["#training", "#dataset", "#synthetic", "#reasoning", "#benchmark", "#multimodal", "#optimization"], "emoji": "🔍", "ru": {"title": "InfoSeek: Новый уровень глубокого исследования для языковых моделей", "desc": "InfoSeek - это масштабируемая система для создания сложных задач глубокого
[04.09.2025 05:11] Renaming data file.
[04.09.2025 05:11] Renaming previous data. hf_papers.json to ./d/2025-09-04.json
[04.09.2025 05:11] Saving new data file.
[04.09.2025 05:11] Generating page.
[04.09.2025 05:11] Renaming previous page.
[04.09.2025 05:11] Renaming previous data. index.html to ./d/2025-09-04.html
[04.09.2025 05:11] Writing result.
[04.09.2025 05:11] Renaming log file.
[04.09.2025 05:11] Renaming previous data. log.txt to ./logs/2025-09-04_last_log.txt
