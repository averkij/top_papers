[19.12.2025 11:20] Read previous papers.
[19.12.2025 11:20] Generating top page (month).
[19.12.2025 11:20] Writing top page (month).
[19.12.2025 12:43] Read previous papers.
[19.12.2025 12:43] Get feed.
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16776
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15745
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16922
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16915
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13507
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16913
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16923
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16625
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16301
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16905
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16636
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16561
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16924
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16918
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16912
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16864
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16900
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16501
[19.12.2025 12:43] Extract page data from URL. URL: https://huggingface.co/papers/2512.16649
[19.12.2025 12:43] Extract page data from URL. URL: https://huggingface.co/papers/2512.16378
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16921
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16899
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11251
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16767
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12576
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16670
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16615
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14884
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15907
[19.12.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15528
[19.12.2025 12:43] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.12.2025 12:43] No deleted papers detected.
[19.12.2025 12:43] Downloading and parsing papers (pdf, html). Total: 30.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16776.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.16776.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.16776.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.15745.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.15745.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.15745.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16922.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.16922.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.16922.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16915.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.16915.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.16915.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.13507.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.13507.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.13507.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16913.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.16913.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.16913.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16923.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.16923.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.16923.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16625.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.16625.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.16625.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16301.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.16301.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.16301.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16905.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.16905.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.16905.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16636.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.16636.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.16636.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16561.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.16561.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.16561.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16924.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.16924.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.16924.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16918.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.16918.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.16918.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16912.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.16912.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.16912.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16864.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.16864.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.16864.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16900.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.16900.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.16900.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16501.
[19.12.2025 12:43] Extra JSON file exists (./assets/json/2512.16501.json), skip PDF parsing.
[19.12.2025 12:43] Paper image links file exists (./assets/img_data/2512.16501.json), skip HTML parsing.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16649.
[19.12.2025 12:43] Downloading paper 2512.16649 from https://arxiv.org/pdf/2512.16649v1...
[19.12.2025 12:43] Extracting affiliations from text.
[19.12.2025 12:43] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"JustRL: Scaling 1.5B LLM with Simple RL Recipe 2025-12-19 JustRL: Scaling 1.5B LLM with Simple RL Recipe Bingxiang He1, Zekai Qu1 , Zeyuan Liu1 , Yinghao Chen1, Yuxin Zuo1, Cheng Qian2, Kaiyan Zhang1, Weize Chen1, Chaojun Xiao1, Ganqu Cui3, Ning Ding1, Zhiyuan Liu1 1Tsinghua University Corresponding Authors. # hebx24@mails.tsinghua.edu.cn 2University of Illinois Urbana-Champaign 3Shanghai AI Lab https://huggingface.co/collections/hbx/justrl https://github.com/thunlp/JustRL Abstract Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises fundamental question: Is this complexity necessary? We present JustRL, minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9% and 64.3% average accuracy across nine mathematical benchmarks) while using 2 less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding standard tricks like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with stable, scaled-up baseline. We release our models and code to establish simple, validated baseline for the community. Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away. Antoine de Saint-Exup√©ry, Airmans Odyssey 5 2 0 2 8 ] . [ 1 9 4 6 6 1 . 2 1 5 2 : r Figure 1 JustRL achieves substantial performance gains through simple, single-stage training. (a) The AIME24 (avg@32) performance curve for scaling from DeepSeek-R1"
[19.12.2025 12:43] Response: ```python
[
    "Tsinghua University",
    "University of Illinois Urbana-Champaign",
    "Shanghai AI Lab"
]
```
[19.12.2025 12:43] Deleting PDF ./assets/pdf/2512.16649.pdf.
[19.12.2025 12:43] Success.
[19.12.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2512.16378.
[19.12.2025 12:43] Downloading paper 2512.16378 from https://arxiv.org/pdf/2512.16378v1...
[19.12.2025 12:48] Extracting affiliations from text.
[19.12.2025 12:48] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 8 7 3 6 1 . 2 1 5 2 : r Hearing to Translate: 1, Javier Garcia Gilabert 2, Zachary Hopton 3, Vil√©m Zouhar 4, Sara Papi Carlos Escolano5, Gerard I. G√°llego2,5, Jorge Iranzo-S√°nchez6, Ahrii Kim7, Dominik Mach√°Àácek8, Patricia Schmidtova8, Maike Z√ºfle9 1Fondazione Bruno Kessler, 2Barcelona Supercomputing Center, 3University of Zurich, 4ETH Zurich, 5Universitat Polit√®cnica de Catalunya, 6Universitat Polit√®cnica de Val√®ncia, 7AI-Bio Convergence Research Institute, 8Charles University, 9KIT Correspondence: spapi@fbk.eu "
[19.12.2025 12:49] Response: ```python
[
    "Fondazione Bruno Kessler",
    "Barcelona Supercomputing Center",
    "University of Zurich",
    "ETH Zurich",
    "Universitat Polit√®cnica de Catalunya",
    "Universitat Polit√®cnica de Val√®ncia",
    "AI-Bio Convergence Research Institute",
    "Charles University",
    "KIT"
]
```
[19.12.2025 12:49] Deleting PDF ./assets/pdf/2512.16378.pdf.
[19.12.2025 12:49] Failed to download and parse paper https://huggingface.co/papers/2512.16378: Function execution timed out after 300 seconds.
[19.12.2025 12:49] Downloading and parsing paper https://huggingface.co/papers/2512.16921.
[19.12.2025 12:49] Extra JSON file exists (./assets/json/2512.16921.json), skip PDF parsing.
[19.12.2025 12:49] Paper image links file exists (./assets/img_data/2512.16921.json), skip HTML parsing.
[19.12.2025 12:49] Success.
[19.12.2025 12:49] Downloading and parsing paper https://huggingface.co/papers/2512.16899.
[19.12.2025 12:49] Extra JSON file exists (./assets/json/2512.16899.json), skip PDF parsing.
[19.12.2025 12:49] Paper image links file exists (./assets/img_data/2512.16899.json), skip HTML parsing.
[19.12.2025 12:49] Success.
[19.12.2025 12:49] Downloading and parsing paper https://huggingface.co/papers/2512.11251.
[19.12.2025 12:49] Extra JSON file exists (./assets/json/2512.11251.json), skip PDF parsing.
[19.12.2025 12:49] Paper image links file exists (./assets/img_data/2512.11251.json), skip HTML parsing.
[19.12.2025 12:49] Success.
[19.12.2025 12:49] Downloading and parsing paper https://huggingface.co/papers/2512.16767.
[19.12.2025 12:49] Extra JSON file exists (./assets/json/2512.16767.json), skip PDF parsing.
[19.12.2025 12:49] Paper image links file exists (./assets/img_data/2512.16767.json), skip HTML parsing.
[19.12.2025 12:49] Success.
[19.12.2025 12:49] Downloading and parsing paper https://huggingface.co/papers/2512.12576.
[19.12.2025 12:49] Extra JSON file exists (./assets/json/2512.12576.json), skip PDF parsing.
[19.12.2025 12:49] Paper image links file exists (./assets/img_data/2512.12576.json), skip HTML parsing.
[19.12.2025 12:49] Success.
[19.12.2025 12:49] Downloading and parsing paper https://huggingface.co/papers/2512.16670.
[19.12.2025 12:49] Extra JSON file exists (./assets/json/2512.16670.json), skip PDF parsing.
[19.12.2025 12:49] Paper image links file exists (./assets/img_data/2512.16670.json), skip HTML parsing.
[19.12.2025 12:49] Success.
[19.12.2025 12:49] Downloading and parsing paper https://huggingface.co/papers/2512.16615.
[19.12.2025 12:49] Extra JSON file exists (./assets/json/2512.16615.json), skip PDF parsing.
[19.12.2025 12:49] Paper image links file exists (./assets/img_data/2512.16615.json), skip HTML parsing.
[19.12.2025 12:49] Success.
[19.12.2025 12:49] Downloading and parsing paper https://huggingface.co/papers/2512.14884.
[19.12.2025 12:49] Extra JSON file exists (./assets/json/2512.14884.json), skip PDF parsing.
[19.12.2025 12:49] Paper image links file exists (./assets/img_data/2512.14884.json), skip HTML parsing.
[19.12.2025 12:49] Success.
[19.12.2025 12:49] Downloading and parsing paper https://huggingface.co/papers/2512.15907.
[19.12.2025 12:49] Extra JSON file exists (./assets/json/2512.15907.json), skip PDF parsing.
[19.12.2025 12:49] Paper image links file exists (./assets/img_data/2512.15907.json), skip HTML parsing.
[19.12.2025 12:49] Success.
[19.12.2025 12:49] Downloading and parsing paper https://huggingface.co/papers/2512.15528.
[19.12.2025 12:49] Extra JSON file exists (./assets/json/2512.15528.json), skip PDF parsing.
[19.12.2025 12:49] Paper image links file exists (./assets/img_data/2512.15528.json), skip HTML parsing.
[19.12.2025 12:49] Success.
[19.12.2025 12:49] Enriching papers with extra data.
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 0. Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.  					AI-generated summary 				 We present Kling-Omni, a generalist generative framework designed to synthesize high-fidel...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 1. LLaDA2.0 converts auto-regressive models into discrete diffusion large language models using a block-level training scheme, improving efficiency and performance at large scales.  					AI-generated summary 				 This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM)...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 2. Generative pretraining using next embedding prediction outperforms traditional self-supervised methods in visual learning tasks, achieving high accuracy on ImageNet and effective transfer to semantic segmentation.  					AI-generated summary 				 Inspired by the success of generative pretraining in n...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 3. StereoPilot, a feed-forward model leveraging a learnable domain switcher and cycle consistency loss, synthesizes high-quality stereo video directly without depth maps, outperforming existing methods in visual fidelity and computational efficiency.  					AI-generated summary 				 The rapid growth of ...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 4. Seedance 1.5 pro, a dual-branch Diffusion Transformer model, achieves high-quality audio-visual synchronization and generation through cross-modal integration, post-training optimizations, and an acceleration framework.  					AI-generated summary 				 Recent strides in video generation have paved th...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 5. A panoramic metric depth foundation model using DINOv3-Large and a three-stage pseudo-label pipeline achieves robust performance across diverse real-world scenes.  					AI-generated summary 				 In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene ...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 6. Generative Refocusing uses semi-supervised learning with DeblurNet and BokehNet to achieve high-quality single-image refocusing with controllable bokeh and text-guided adjustments.  					AI-generated summary 				 Depth-of-field control is essential in photography, but getting the perfect focus often...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 7. DeContext defends against unauthorized in-context image editing by weakening cross-attention pathways in multimodal attention layers, preserving visual quality while blocking unwanted modifications.  					AI-generated summary 				 In-context diffusion models allow users to modify images with remarka...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 8. This paper presents a framework for agent and tool adaptation in agentic AI systems, clarifying design strategies and identifying open challenges for improving AI capabilities.  					AI-generated summary 				 Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan,...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 9. Alchemist, a meta-gradient-based framework, automatically selects high-quality subsets from large-scale text-image datasets to improve visual quality and training efficiency in Text-to-Image models.  					AI-generated summary 				 Recent advances in Text-to-Image (T2I) generative models, such as Ima...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 10. REGLUE, a unified latent diffusion framework, enhances image synthesis by jointly modeling VAE latents, patch-level VFM semantics, and global tokens, improving semantic supervision and convergence.  					AI-generated summary 				 Latent diffusion models (LDMs) achieve state-of-the-art image synthesi...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 11. N3D-VLM integrates native 3D perception and reasoning in vision-language models, enabling precise 3D localization and spatial understanding with a large-scale dataset.  					AI-generated summary 				 While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D obje...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 12. WorldCanvas generates coherent, controllable world events using a multimodal framework that integrates text, trajectories, and reference images.  					AI-generated summary 				 We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining te...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 13. AdaTooler-V, a multimodal large language model, adaptively uses vision tools based on reinforcement learning, improving performance and reducing unnecessary tool invocations in visual reasoning tasks.  					AI-generated summary 				 Recent advances have shown that multimodal large language models (M...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 14. Reinforcement learning with verifiable rewards improves LLM reasoning through spurious rewards and entropy minimization, despite seemingly paradoxical effects, by reducing clipping bias and policy entropy.  					AI-generated summary 				 This paper examines the exploration-exploitation trade-off in ...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 15. RePlan, a plan-then-execute framework, enhances instruction-based image editing by combining a vision-language planner with a diffusion editor, achieving superior performance in complex and intricate editing tasks using limited data.  					AI-generated summary 				 Instruction-based image editing en...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 16. FlashPortrait is a diffusion-based video transformer for long-portrait animation that ensures ID consistency and achieves 6x acceleration through a dynamic sliding-window scheme and higher-order latent derivatives.  					AI-generated summary 				 Current diffusion-based acceleration methods for long...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 17. VenusBench-GD is a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, offering a hierarchical evaluation framework with extensive data coverage and rich annotations.  					AI-generated summary 				 GUI grounding is a critical component in building capable GUI agents....
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 18. JustRL achieves state-of-the-art performance on reasoning models with minimal complexity, using single-stage training and fixed hyperparameters, outperforming sophisticated approaches in terms of compute and stability.  					AI-generated summary 				 Recent advances in reinforcement learning for lar...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 19. Hearing to Translate benchmarks SpeechLLMs and cascaded systems for speech-to-text translation, finding that cascaded systems are more reliable overall and highlighting the importance of integrating LLMs for high-quality translation.  					AI-generated summary 				 As Large Language Models (LLMs) ex...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 20. AuditDM, an automated framework using reinforcement learning, identifies and rectifies failure modes in multimodal LLMs by generating challenging examples, leading to improved performance across benchmarks.  					AI-generated summary 				 Conventional evaluation methods for multimodal LLMs (MLLMs) l...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 21. Multimodal RewardBench 2 (MMRB2) is a benchmark for reward models on multimodal understanding and generation tasks, featuring expert-annotated preferences and state-of-the-art model evaluations.  					AI-generated summary 				 Reward models (RMs) are essential for training large language models (LLM...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 22. Insight Miner, a large-scale multimodal model, generates high-quality time-series descriptions using a novel agentic workflow and outperforms existing models with the help of the TS-Insights dataset.  					AI-generated summary 				 Time-series data is critical across many scientific and industrial d...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 23. A novel feed-forward framework, Make-It-Poseable, reformulates character posing as a latent-space transformation problem, using a latent posing transformer and dense pose representation to achieve superior posing quality and extend to 3D editing applications.  					AI-generated summary 				 Posing 3...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 24. CoVRL, a hybrid approach combining variational inference and reinforcement learning, enhances language model reasoning by coupling prior and posterior distributions, improving performance and coherence.  					AI-generated summary 				 While reinforcement learning have achieved impressive progress in...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 25. FrameDiffuser is an autoregressive neural rendering framework that generates temporally consistent photorealistic frames using G-buffer data and previous frame outputs, leveraging ControlNet and ControlLoRA for structural and temporal coherence.  					AI-generated summary 				 Neural rendering for i...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 26. Log-linear Sparse Attention (LLSA) improves the efficiency of diffusion transformers by reducing computational costs for long token sequences through a hierarchical structure, enhancing training speed without sacrificing generation quality.  					AI-generated summary 				 Diffusion Transformers (DiT...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 27. Vibe Blending uses Vibe Space, a hierarchical graph manifold, to generate coherent image hybrids by learning geodesics in feature spaces, outperforming current methods in creativity and coherence.  					AI-generated summary 				 Creating new visual concepts often requires connecting distinct ideas t...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 28. TabReX is a reference-less framework using graph-based reasoning to evaluate the quality of tables generated by LLMs, offering structural and factual fidelity scores.  					AI-generated summary 				 Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge:...
[19.12.2025 12:49] ********************************************************************************
[19.12.2025 12:49] Abstract 29. EmoCaliber, a confidence-aware Multimodal Large Language Model, enhances Visual Emotion Comprehension by verbalizing confidence in emotion predictions, leading to improved reliability and accuracy.  					AI-generated summary 				 Visual Emotion Comprehension (VEC) aims to infer sentiment polarities ...
[19.12.2025 12:49] Read previous papers.
[19.12.2025 12:49] Generating reviews via LLM API.
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#reasoning", "#inference", "#training", "#video"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–¥–µ–æ –∏–∑ –ª—é–±—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "Kling-Omni ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∑–¥–∞—ë—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ –∏–∑ —Ä
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#rlhf", "#transfer_learning", "#training", "#diffusion", "#architecture", "#optimization", "#open_source", "#alignment"], "emoji": "üîÑ", "ru": {"title": "–û—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∫ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ LLaDA2
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#training", "#cv", "#architecture"], "emoji": "üîÆ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫–∞–∫ –ø—É—Ç—å –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º—É —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–º—É –æ–±—É—á–µ–Ω–∏—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è NEPA (Next-Embedding Predictive Autoregression), –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ –ø—Ä
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#video", "#dataset", "#benchmark", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ü—Ä—è–º–æ–π —Å–∏–Ω—Ç–µ–∑ —Å—Ç–µ—Ä–µ–æ–≤–∏–¥–µ–æ –±–µ–∑ –∫–∞—Ä—Ç –≥–ª—É–±–∏–Ω—ã", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å StereoPilot, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–µ –≤–∏–¥–µ–æ –≤ —Å—Ç–µ—Ä–µ–æ–≤–∏–¥–µ–æ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –±–µ–∑ —è–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞—Ä—Ç –≥–ª—É–±
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#rlhf", "#open_source", "#video", "#architecture", "#diffusion", "#audio", "#inference", "#multimodal", "#training", "#multilingual"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –¥–≤—É—Ö–≤–µ—Ç–≤–µ–≤–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã", "desc": "Seedance 1.5 pr
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#dataset", "#training", "#benchmark", "#architecture", "#3d", "#cv"], "emoji": "üåç", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –≥–ª—É–±–∏–Ω—ã –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö —Å—Ü–µ–Ω", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è 
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#training", "#dataset", "#cv", "#synthetic", "#optimization"], "emoji": "üì∏", "ru": {"title": "–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –ø–µ—Ä–µ–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ —Ñ–æ–∫—É—Å–∞ —á–µ—Ä–µ–∑ –ø–æ–ª—É—Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ —Å–∏–Ω—Ç–µ–∑ –±–æ–∫–µ", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Generative Refocusing ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –≥–ª—É–±–∏–Ω—ã —Ä–µ–∑–∫–æ—Å—Ç–∏ –Ω–∞ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö 
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#multimodal", "#security", "#diffusion", "#architecture", "#cv"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –æ—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –æ—Å–ª–∞–±–ª–µ–Ω–∏–µ –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ DeContext –¥–ª—è –∑–∞—â–∏—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ—Ç –Ω–µ—Å–∞–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#agents", "#training"], "emoji": "üîß", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ AI —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–ª—è –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑–¥–µ–ª—è
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#training", "#multimodal", "#data", "#dataset"], "emoji": "‚öóÔ∏è", "ru": {"title": "–ú–µ—Ç–∞–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–∞—è –∞–ª—Ö–∏–º–∏—è: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–±–æ—Ä –ª—É—á—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Alchemist ‚Äî —ç—Ç–æ meta-gradient-based —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –≤—ã—Å–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#cv", "#architecture", "#diffusion", "#training", "#open_source"], "emoji": "üé®", "ru": {"title": "–ü–µ—Ä–µ–ø–ª–µ—Ç–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏: —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ VAE, –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ Vision Foundation Models –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "REGLUE ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#data", "#dataset"], "emoji": "üèóÔ∏è", "ru": {"title": "–î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–∏–Ω–Ω–æ–µ 3D –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤ —è–∑—ã–∫–æ–≤–æ-–≤–∏–∑—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏", "desc": "N3D-VLM ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –Ω–∞—Ç–∏–≤–Ω–æ–µ 3D –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –≤ –º–æ–¥–µ–ª–∏ –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞, –ø–æ–∑–≤–æ–ª—è—è –∏–º –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç—ã –≤ —Ç—Ä
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#video", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏—è –º–∏—Ä–∞ —á–µ—Ä–µ–∑ —Ç–µ–∫—Å—Ç, –¥–≤–∏–∂–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–∑—ã", "desc": "WorldCanvas –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å—Ü–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è, —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ 
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#open_source", "#rl", "#benchmark", "#video", "#inference", "#reasoning", "#multimodal", "#optimization", "#dataset"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ AdaTooler-V ‚Äî –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#training", "#rl", "#alignment"], "emoji": "üéØ", "ru": {"title": "–ü–∞—Ä–∞–¥–æ–∫—Å –æ–±—É—á–µ–Ω–∏—è: –∫–∞–∫ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —É–ª—É—á—à–∞—é—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è LLM", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø–∞—Ä–∞–¥–æ–∫—Å–∞–ª—å–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏ (RLV
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#diffusion", "#reasoning", "#multimodal", "#synthetic", "#cv", "#dataset"], "emoji": "üé®", "ru": {"title": "–ü–ª–∞–Ω –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ: –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è —Å–ª–æ–∂–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ vision-language —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ", "desc": "RePlan ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#long_context", "#inference", "#optimization", "#architecture", "#diffusion", "#video"], "emoji": "üé¨", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –ø–æ—Ä—Ç—Ä–µ—Ç–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ª–∏—á–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã", "desc": "FlashPortrait ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π –≤–∏–¥–µ–æ-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–ª–∏—Ç–µ
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#dataset", "#survey", "#multimodal", "#benchmark", "#multilingual", "#agents"], "emoji": "üéØ", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤: –æ—Ç –±–∞–∑–æ–≤—ã—Ö –∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º –∑–∞–¥–∞—á–∞–º", "desc": "VenusBench-GD ‚Äî —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–≤—É—è–∑—ã—á–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ
[19.12.2025 12:49] Querying the API.
[19.12.2025 12:49] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

JustRL achieves state-of-the-art performance on reasoning models with minimal complexity, using single-stage training and fixed hyperparameters, outperforming sophisticated approaches in terms of compute and stability.  					AI-generated summary 				 Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: Is this complexity necessary? We present JustRL, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\% and 64.3\% average accuracy across nine mathematical benchmarks) while using 2times less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.
[19.12.2025 12:49] Response: ```json
{
  "desc": "JustRL –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é reinforcement learning –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è —Å–ª–æ–∂–Ω–∞—è –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–∏–Ω–∏–º–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –æ–¥–Ω–æ–ø—Ä–æ—Ö–æ–¥–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø—Ä–∏ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö. –ú–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –±–µ–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö –∫–æ–ª–ª–∞–ø—Å–æ–≤, –∞ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–Ω–æ—Å—è—Ç—Å—è –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–æ, —á—Ç–æ —á–∞—Å—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–∞—Ö —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å—á–µ–∑–∞—é—Ç –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –±–∞–∑–æ–≤–æ–π –ª–∏–Ω–∏–∏.",
  "emoji": "üéØ",
  "title": "–ü—Ä–æ—Å—Ç–æ—Ç–∞ –ª—É—á—à–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π reinforcement learning –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"
}
```
[19.12.2025 12:49] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"JustRL achieves state-of-the-art performance on reasoning models with minimal complexity, using single-stage training and fixed hyperparameters, outperforming sophisticated approaches in terms of compute and stability.  					AI-generated summary 				 Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: Is this complexity necessary? We present JustRL, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\% and 64.3\% average accuracy across nine mathematical benchmarks) while using 2times less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community."

[19.12.2025 12:49] Response: ```python
['RL', 'TRAINING', 'SMALL_MODELS', 'BENCHMARK']
```
[19.12.2025 12:49] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"JustRL achieves state-of-the-art performance on reasoning models with minimal complexity, using single-stage training and fixed hyperparameters, outperforming sophisticated approaches in terms of compute and stability.  					AI-generated summary 				 Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: Is this complexity necessary? We present JustRL, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\% and 64.3\% average accuracy across nine mathematical benchmarks) while using 2times less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community."

[19.12.2025 12:49] Response: ```python
['REASONING', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[19.12.2025 12:49] Response: ParsedChatCompletionMessage[Article](content='{"desc":"JustRL introduces a simplified approach to reinforcement learning that achieves high performance with minimal complexity. It utilizes single-stage training and fixed hyperparameters, which allows it to outperform more complex methods in both computational efficiency and stability. The method demonstrates consistent improvement across training steps without the common issues of performance collapse. This research challenges the necessity of complex training strategies, suggesting that a stable baseline can yield better results.","title":"Simplifying Success: JustRL\'s Efficient Approach to Reinforcement Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='JustRL introduces a simplified approach to reinforcement learning that achieves high performance with minimal complexity. It utilizes single-stage training and fixed hyperparameters, which allows it to outperform more complex methods in both computational efficiency and stability. The method demonstrates consistent improvement across training steps without the common issues of performance collapse. This research challenges the necessity of complex training strategies, suggesting that a stable baseline can yield better results.', title="Simplifying Success: JustRL's Efficient Approach to Reinforcement Learning"))
[19.12.2025 12:49] Response: ParsedChatCompletionMessage[Article](content='{"desc":"JustRLÊòØ‰∏ÄÁßçÁÆÄÂçïÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÈÄöËøáÂçïÈò∂ÊÆµËÆ≠ÁªÉÂíåÂõ∫ÂÆöË∂ÖÂèÇÊï∞ÔºåÂèñÂæó‰∫ÜÊé®ÁêÜÊ®°ÂûãÁöÑÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇÂÆÉÂú®‰∏§‰∏™1.5‰∫øÂèÇÊï∞ÁöÑÊ®°Âûã‰∏äÂÆûÁé∞‰∫Ü54.9%Âíå64.3%ÁöÑÂπ≥ÂùáÂáÜÁ°ÆÁéáÔºåËÆ°ÁÆóËµÑÊ∫êÊØîÂ§çÊùÇÊñπÊ≥ïÂ∞ë‰∫Ü‰∏§ÂÄç„ÄÇËÆ≠ÁªÉËøáÁ®ãÂπ≥Á®≥ÔºåÈöèÁùÄÊ≠•È™§ÁöÑÂ¢ûÂä†ÔºåÊÄßËÉΩÊåÅÁª≠ÊèêÂçáÔºåÊ≤°ÊúâÂá∫Áé∞Â¥©Ê∫ÉÊàñÂÅúÊªûÁöÑÁé∞Ë±°„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂ¢ûÂä†Â§çÊùÇÊÄßÂπ∂‰∏çÊÄªÊòØÂøÖË¶ÅÁöÑÔºåÁ®≥ÂÆöÁöÑÂü∫Á∫øÂèØËÉΩÊõ¥ÊúâÊïà„ÄÇ","title":"ÁÆÄÂåñÂ§çÊùÇÊÄßÔºåÊèêÂçáÊÄßËÉΩÁöÑJustRL"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='JustRLÊòØ‰∏ÄÁßçÁÆÄÂçïÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÈÄöËøáÂçïÈò∂ÊÆµËÆ≠ÁªÉÂíåÂõ∫ÂÆöË∂ÖÂèÇÊï∞ÔºåÂèñÂæó‰∫ÜÊé®ÁêÜÊ®°ÂûãÁöÑÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇÂÆÉÂú®‰∏§‰∏™1.5‰∫øÂèÇÊï∞ÁöÑÊ®°Âûã‰∏äÂÆûÁé∞‰∫Ü54.9%Âíå64.3%ÁöÑÂπ≥ÂùáÂáÜÁ°ÆÁéáÔºåËÆ°ÁÆóËµÑÊ∫êÊØîÂ§çÊùÇÊñπÊ≥ïÂ∞ë‰∫Ü‰∏§ÂÄç„ÄÇËÆ≠ÁªÉËøáÁ®ãÂπ≥Á®≥ÔºåÈöèÁùÄÊ≠•È™§ÁöÑÂ¢ûÂä†ÔºåÊÄßËÉΩÊåÅÁª≠ÊèêÂçáÔºåÊ≤°ÊúâÂá∫Áé∞Â¥©Ê∫ÉÊàñÂÅúÊªûÁöÑÁé∞Ë±°„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂ¢ûÂä†Â§çÊùÇÊÄßÂπ∂‰∏çÊÄªÊòØÂøÖË¶ÅÁöÑÔºåÁ®≥ÂÆöÁöÑÂü∫Á∫øÂèØËÉΩÊõ¥ÊúâÊïà„ÄÇ', title='ÁÆÄÂåñÂ§çÊùÇÊÄßÔºåÊèêÂçáÊÄßËÉΩÁöÑJustRL'))
[19.12.2025 12:49] Querying the API.
[19.12.2025 12:49] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Hearing to Translate benchmarks SpeechLLMs and cascaded systems for speech-to-text translation, finding that cascaded systems are more reliable overall and highlighting the importance of integrating LLMs for high-quality translation.  					AI-generated summary 				 As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.
[19.12.2025 12:49] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ Hearing to Translate –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä—è–º–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é SpeechLLM –∏ –∫–∞—Å–∫–∞–¥–Ω—ã—Ö —Å–∏—Å—Ç–µ–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç 5 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö SpeechLLM —Å 16 —Å–∏–ª—å–Ω—ã–º–∏ –∫–∞—Å–∫–∞–¥–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç –º–æ–¥–µ–ª–∏ —Ä–µ—á–µ–≤—ã—Ö –æ—Å–Ω–æ–≤ —Å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–º–∏ LLM. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏–ª–∞—Å—å –Ω–∞ 16 –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, 13 —è–∑—ã–∫–æ–≤—ã—Ö –ø–∞—Ä–∞—Ö –∏ 9 —Å–ª–æ–∂–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö, –≤–∫–ª—é—á–∞—è –Ω–µ–≤–Ω—è—Ç–Ω—É—é, —à—É–º–Ω—É—é –∏ –¥–ª–∏–Ω–Ω—É—é —Ä–µ—á—å. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∫–∞—Å–∫–∞–¥–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –æ—Å—Ç–∞—é—Ç—Å—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–º–∏, –∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è LLM –ª–∏–±–æ –≤–Ω—É—Ç—Ä–∏ –º–æ–¥–µ–ª–∏, –ª–∏–±–æ –≤ –∫–æ–Ω–≤–µ–π–µ—Ä–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞ –¥–ª—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ä–µ—á–∏.",
  "emoji": "üé§",
  "title": "–ö–∞—Å–∫–∞–¥–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç SpeechLLM –≤ –ø–µ—Ä–µ–≤–æ–¥–µ —Ä–µ—á–∏"
}
```
[19.12.2025 12:49] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Hearing to Translate benchmarks SpeechLLMs and cascaded systems for speech-to-text translation, finding that cascaded systems are more reliable overall and highlighting the importance of integrating LLMs for high-quality translation.  					AI-generated summary 				 As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation."

[19.12.2025 12:49] Response: ```python
["BENCHMARK", "AUDIO", "MULTIMODAL", "MULTILINGUAL"]
```
[19.12.2025 12:49] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Hearing to Translate benchmarks SpeechLLMs and cascaded systems for speech-to-text translation, finding that cascaded systems are more reliable overall and highlighting the importance of integrating LLMs for high-quality translation.  					AI-generated summary 				 As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation."

[19.12.2025 12:49] Response: ```python
['TRANSLATION', 'BENCHMARK']
```

Wait, let me reconsider. Looking at the provided topic list, 'BENCHMARK' is not included. Let me classify based only on the topics provided:

```python
['TRANSLATION']
```
[19.12.2025 12:49] Error. Failed to parse JSON from LLM. ["TRANSLATION", "BENCHMARK"]


Wait, let me reconsider. Looking at the provided topic list, "BENCHMARK" is not included. Let me classify based only on the topics provided:


["TRANSLATION"]
[19.12.2025 12:49] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper evaluates the performance of SpeechLLMs, which are designed to translate spoken language directly, against traditional cascaded systems that first transcribe speech to text before translation. The study, called Hearing to Translate, benchmarks five advanced SpeechLLMs against sixteen established systems across various languages and challenging audio conditions. The findings reveal that while SpeechLLMs show promise, cascaded systems generally provide more reliable translations overall. The research emphasizes the critical role of integrating Large Language Models (LLMs) to enhance the quality of speech-to-text translation.","title":"Cascaded Systems Outperform SpeechLLMs in Translation Reliability"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper evaluates the performance of SpeechLLMs, which are designed to translate spoken language directly, against traditional cascaded systems that first transcribe speech to text before translation. The study, called Hearing to Translate, benchmarks five advanced SpeechLLMs against sixteen established systems across various languages and challenging audio conditions. The findings reveal that while SpeechLLMs show promise, cascaded systems generally provide more reliable translations overall. The research emphasizes the critical role of integrating Large Language Models (LLMs) to enhance the quality of speech-to-text translation.', title='Cascaded Systems Outperform SpeechLLMs in Translation Reliability'))
[19.12.2025 12:49] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫‚ÄúHearing to Translate‚ÄùÁöÑÂü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞ÊúÄÊñ∞ÁöÑËØ≠Èü≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàSpeechLLMsÔºâ‰∏é‰º†ÁªüÁöÑÁ∫ßËÅîÁ≥ªÁªüÂú®ËØ≠Èü≥Âà∞ÊñáÊú¨ÁøªËØë‰∏≠ÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°ËØ≠Èü≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Êüê‰∫õÁâπÂÆöÊù°‰ª∂‰∏ãË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÊï¥‰Ωì‰∏äÔºåÁ∫ßËÅîÁ≥ªÁªü‰ªçÁÑ∂Êõ¥‰∏∫ÂèØÈù†„ÄÇËÆ∫ÊñáÂº∫Ë∞É‰∫ÜÂ∞ÜÂ§ßËØ≠Ë®ÄÊ®°Âûã‰∏éËØ≠Èü≥Âü∫Á°ÄÊ®°ÂûãÁªìÂêàÁöÑÈáçË¶ÅÊÄßÔºå‰ª•ÊèêÈ´òÁøªËØëË¥®Èáè„ÄÇÈÄöËøáÂØπ16‰∏™Âü∫ÂáÜ„ÄÅ13ÁßçËØ≠Ë®ÄÂØπÂíå9ÁßçÊåëÊàòÊÄßÊù°‰ª∂ÁöÑÂàÜÊûêÔºåÁ†îÁ©∂Êè≠Á§∫‰∫ÜÂΩìÂâçËØ≠Èü≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏äÁöÑ‰∏çË∂≥„ÄÇ","title":"Á∫ßËÅîÁ≥ªÁªüÊõ¥ÂèØÈù†ÔºåËØ≠Èü≥ÁøªËØëÈúÄÊï¥ÂêàÂ§ßËØ≠Ë®ÄÊ®°Âûã"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫‚ÄúHearing to Translate‚ÄùÁöÑÂü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞ÊúÄÊñ∞ÁöÑËØ≠Èü≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàSpeechLLMsÔºâ‰∏é‰º†ÁªüÁöÑÁ∫ßËÅîÁ≥ªÁªüÂú®ËØ≠Èü≥Âà∞ÊñáÊú¨ÁøªËØë‰∏≠ÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°ËØ≠Èü≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Êüê‰∫õÁâπÂÆöÊù°‰ª∂‰∏ãË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÊï¥‰Ωì‰∏äÔºåÁ∫ßËÅîÁ≥ªÁªü‰ªçÁÑ∂Êõ¥‰∏∫ÂèØÈù†„ÄÇËÆ∫ÊñáÂº∫Ë∞É‰∫ÜÂ∞ÜÂ§ßËØ≠Ë®ÄÊ®°Âûã‰∏éËØ≠Èü≥Âü∫Á°ÄÊ®°ÂûãÁªìÂêàÁöÑÈáçË¶ÅÊÄßÔºå‰ª•ÊèêÈ´òÁøªËØëË¥®Èáè„ÄÇÈÄöËøáÂØπ16‰∏™Âü∫ÂáÜ„ÄÅ13ÁßçËØ≠Ë®ÄÂØπÂíå9ÁßçÊåëÊàòÊÄßÊù°‰ª∂ÁöÑÂàÜÊûêÔºåÁ†îÁ©∂Êè≠Á§∫‰∫ÜÂΩìÂâçËØ≠Èü≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏äÁöÑ‰∏çË∂≥„ÄÇ', title='Á∫ßËÅîÁ≥ªÁªüÊõ¥ÂèØÈù†ÔºåËØ≠Èü≥ÁøªËØëÈúÄÊï¥ÂêàÂ§ßËØ≠Ë®ÄÊ®°Âûã'))
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#rl", "#interpretability", "#security", "#multimodal", "#synthetic", "#training", "#small_models"], "emoji": "üîç", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞—É–¥–∏—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ –∏—Ö —Å–ª–∞–±–æ—Å—Ç–µ–π", "desc": "AuditDM ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset"], "emoji": "üéØ", "ru": {"title": "–≠—Ç–∞–ª–æ–Ω–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Multimodal RewardBench 2 (MMRB2) ‚Äî –ø–µ—Ä–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤ –∑
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#agents", "#training", "#dataset", "#multimodal"], "emoji": "üìà", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ–ø–∏—Å–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Insight Miner, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –∏—Å–ø
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#3d", "#architecture"], "emoji": "üé≠", "ru": {"title": "–ü–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —á–µ—Ä–µ–∑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Make-It-Poseable –¥–ª—è –ø–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è 3D –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á—É –∫–∞–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –º–æ
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#optimization", "#training", "#rl", "#reasoning"], "emoji": "üß†", "ru": {"title": "–°–≤—è–∑–∞–Ω–Ω–æ–µ –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ª–æ–≥–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "CoVRL ‚Äî —ç—Ç–æ –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#diffusion", "#video", "#games", "#architecture", "#inference", "#optimization"], "emoji": "üé¨", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –Ω–µ–π—Ä–æ–Ω–Ω—ã–π —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å—é", "desc": "FrameDiffuser ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#cv", "#inference", "#architecture", "#diffusion", "#optimization", "#training", "#long_context", "#open_source"], "emoji": "‚ö°", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Log-linear Sparse Attention (LLSA
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#benchmark"], "emoji": "üé®", "ru": {"title": "–ü–æ–∏—Å–∫ –≥–µ–æ–¥–µ–∑–∏–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –æ–±—Ä–∞–∑–æ–≤ –¥–ª—è —Ç–≤–æ—Ä—á–µ—Å–∫–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Vibe Blending –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≥–∞—Ä–º–æ–Ω–∏—á–Ω—ã—Ö –≥–∏–±—Ä–∏–¥–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø—É—Ç—ë–º –ø–æ–∏—Å–∫–∞ –≥–µ–æ–¥–µ–∑–∏—á–µ—Å–∫–∏—Ö –ª–∏–Ω–∏–π –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏–∑
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#dataset", "#benchmark"], "emoji": "üìä", "ru": {"title": "–ì—Ä–∞—Ñ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–∞–±–ª–∏—Ü –±–µ–∑ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "TabReX ‚Äî —ç—Ç–æ —ç—Ç–∞–ª–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–∞–±–ª–∏—Ü, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä–∞—Ñ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã —Ä
[19.12.2025 12:49] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#training"], "emoji": "üòä", "ru": {"title": "–£–≤–µ—Ä–µ–Ω–Ω–∞—è –≤ —Å–µ–±–µ –º–æ–¥–µ–ª—å: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å AI –≤—ã—Ä–∞–∂–∞—Ç—å —Å–æ–º–Ω–µ–Ω–∏—è –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ —ç–º–æ—Ü–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è EmoCaliber ‚Äî –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π
[19.12.2025 12:49] Renaming data file.
[19.12.2025 12:49] Renaming previous data. hf_papers.json to ./d/2025-12-19.json
[19.12.2025 12:49] Saving new data file.
[19.12.2025 12:49] Generating page.
[19.12.2025 12:49] Renaming previous page.
[19.12.2025 12:49] Renaming previous data. index.html to ./d/2025-12-19.html
[19.12.2025 12:49] Writing result.
[19.12.2025 12:49] Renaming log file.
[19.12.2025 12:49] Renaming previous data. log.txt to ./logs/2025-12-19_last_log.txt
