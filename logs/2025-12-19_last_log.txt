[19.12.2025 15:25] Read previous papers.
[19.12.2025 15:25] Generating top page (month).
[19.12.2025 15:25] Writing top page (month).
[19.12.2025 16:28] Read previous papers.
[19.12.2025 16:28] Get feed.
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16776
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15745
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16922
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16915
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13507
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16301
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16913
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16923
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16625
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16636
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16905
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16561
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16924
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16649
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16918
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16912
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16900
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16899
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16864
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16501
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16378
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16921
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16920
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11251
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16767
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16615
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12576
[19.12.2025 16:28] Extract page data from URL. URL: https://huggingface.co/papers/2512.16909
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16670
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14884
[19.12.2025 16:28] Extract page data from URL. URL: https://huggingface.co/papers/2512.10953
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15907
[19.12.2025 16:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15528
[19.12.2025 16:28] Extract page data from URL. URL: https://huggingface.co/papers/2512.15489
[19.12.2025 16:28] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.12.2025 16:28] No deleted papers detected.
[19.12.2025 16:28] Downloading and parsing papers (pdf, html). Total: 34.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16776.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16776.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16776.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.15745.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.15745.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.15745.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16922.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16922.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16922.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16915.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16915.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16915.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.13507.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.13507.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.13507.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16301.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16301.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16301.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16913.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16913.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16913.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16923.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16923.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16923.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16625.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16625.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16625.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16636.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16636.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16636.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16905.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16905.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16905.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16561.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16561.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16561.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16924.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16924.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16924.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16649.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16649.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16649.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16918.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16918.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16918.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16912.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16912.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16912.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16900.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16900.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16900.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16899.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16899.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16899.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16864.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16864.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16864.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16501.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16501.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16501.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16378.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16378.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16378.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16921.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16921.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16921.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16920.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16920.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16920.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.11251.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.11251.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.11251.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16767.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16767.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16767.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16615.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16615.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16615.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.12576.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.12576.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.12576.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16909.
[19.12.2025 16:28] Downloading paper 2512.16909 from https://arxiv.org/pdf/2512.16909v1...
[19.12.2025 16:28] Extracting affiliations from text.
[19.12.2025 16:28] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 9 0 9 6 1 . 2 1 5 2 : r Preprint. MOMAGRAPH : STATE-AWARE UNIFIED SCENE GRAPHS WITH VISIONLANGUAGE MODEL FOR EMBODIED TASK PLANNING Yuanchen Ju1, Yongyuan Liang2, Yen-Jen Wang1, Nandiraju Gireesh, Yuanliang Ju3 Seungjae Lee2, Qiao Gu3, Elvis Hsieh1, Furong Huang2, Koushil Sreenath1 1University of California, Berkeley 3University of Toronto Project website: https://HybridRobotics.github.io/MomaGraph/ 2University of Maryland, College Park Figure 1: Overview of the MomaGraph. Given task instruction, MomaGraph constructs taskspecific scene graph that highlights relevant objects and parts along with their spatial-functional relationships, enabling the robot to perform spatial understanding and task planning. "
[19.12.2025 16:28] Response: ```python
[
    "University of California, Berkeley",
    "University of Maryland, College Park",
    "University of Toronto"
]
```
[19.12.2025 16:28] Deleting PDF ./assets/pdf/2512.16909.pdf.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.16670.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.16670.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.16670.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.14884.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.14884.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.14884.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.10953.
[19.12.2025 16:28] Downloading paper 2512.10953 from https://arxiv.org/pdf/2512.10953v1...
[19.12.2025 16:28] Extracting affiliations from text.
[19.12.2025 16:28] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Bidirectional Normalizing Flow: From Data to Noise and Back Yiyang Lu1,2,,, Qiao Sun1,, Xianbang Wang1, Zhicheng Jiang1 Hanhong Zhao1 Kaiming He1 Equal technical contribution Project lead 1MIT 2Tsinghua University 5 2 0 2 1 1 ] . [ 1 3 5 9 0 1 . 2 1 5 2 : r a "
[19.12.2025 16:28] Response: ```python
['MIT', 'Tsinghua University']
```
[19.12.2025 16:28] Deleting PDF ./assets/pdf/2512.10953.pdf.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.15907.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.15907.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.15907.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.15528.
[19.12.2025 16:28] Extra JSON file exists (./assets/json/2512.15528.json), skip PDF parsing.
[19.12.2025 16:28] Paper image links file exists (./assets/img_data/2512.15528.json), skip HTML parsing.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Downloading and parsing paper https://huggingface.co/papers/2512.15489.
[19.12.2025 16:28] Downloading paper 2512.15489 from https://arxiv.org/pdf/2512.15489v1...
[19.12.2025 16:28] Extracting affiliations from text.
[19.12.2025 16:28] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 1 ] . [ 1 9 8 4 5 1 . 2 1 5 2 : r 2025-12Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision Wei Du, Shubham Toshniwal, Branislav Kisacanin, Sadegh Mahdavi, Ivan Moshkov, George Armstrong, Stephen Ge, Edgar Minasyan, Feng Chen, Igor Gitman Abstract: High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR). The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the datasets quality. Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems, and incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math-competition benchmarks. To support efficient long-context training, we develop sequential bucketed strategy that accelerates 128K context-length fine-tuning by 23 without significant accuracy loss compared to the full-length training. To verify the scalability of our supervision, we further perform experiments on Qwen3-8B and Qwen3-30B-A3B, showing that both models converge to similar final accuracy under our full context training recipe. Overall, Nemotron-Math provides diverse, high-quality, and scalable reasoning supervision, enabling state-of-the-art performance, including 100% maj@16 accuracy on AIME 2024/2025 for both Qwen3-8B and Qwen3-30B-A3B with Python TIR. 1. Introduction Mathema"
[19.12.2025 16:28] Response: ```python
[]
```
[19.12.2025 16:28] Extracting affiliations from text.
[19.12.2025 16:28] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 1 ] . [ 1 9 8 4 5 1 . 2 1 5 2 : r 2025-12Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision Wei Du, Shubham Toshniwal, Branislav Kisacanin, Sadegh Mahdavi, Ivan Moshkov, George Armstrong, Stephen Ge, Edgar Minasyan, Feng Chen, Igor Gitman Abstract: High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR). The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the datasets quality. Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems, and incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math-competition benchmarks. To support efficient long-context training, we develop sequential bucketed strategy that accelerates 128K context-length fine-tuning by 23 without significant accuracy loss compared to the full-length training. To verify the scalability of our supervision, we further perform experiments on Qwen3-8B and Qwen3-30B-A3B, showing that both models converge to similar final accuracy under our full context training recipe. Overall, Nemotron-Math provides diverse, high-quality, and scalable reasoning supervision, enabling state-of-the-art performance, including 100% maj@16 accuracy on AIME 2024/2025 for both Qwen3-8B and Qwen3-30B-A3B with Python TIR. 1. Introduction Mathematical problem-solving remains key benchmark for evaluating the reasoning capabilities of large language models (LLMs). Unlike general natural language processing tasks, mathematical problems often require multi-step logical deduction, symbolic manipulation, and long-context understanding. Recent large-scale datasets, such as OpenMathInstruct-2 [21], Skywork-MathQA [24], and NuminaMath [11], have substantially advanced the study of mathematical reasoning in LLMs. Building on these efforts, subsequent datasets such as BackMATH [25] and OpenMathReasoning [15] extend this progress toward competition-level and olympiad-style reasoning. Despite these advances, most existing mathematical reasoning datasets are generated by single mode reasoning models, which produce relatively uniform solution styles and limited variation in reasoning depth or tool usage. At the same time, many recent efforts have focused primarily on increasing the difficulty of competition-style mathematical problems. While highly effective for constructing challenging reasoning data, these datasets tend to focus on formal, competition-style problems that cover relatively narrow range of mathematical domains. As result, current mathematical reasoning datasets capture correctness and complexity but only partially reflect the broader spectrum of reasoning behaviors encountered across diverse mathematical problems. The recently released GPT-OSS family of openweight reasoning models, with gpt-oss-120b [4] as the flagship, offers new opportunity to address these limitations. Unlike prior models, it provides three controllable reasoning modes, high, medium, and low, that produce solutions of varying depth and length, and it can generate exceptionally detailed toolintegrated reasoning traces through extensive Python calls. These capabilities make it possible to construct datasets that capture diverse reasoning styles, selfverification behaviors, and tool-usage patterns that were previously inaccessible. Building on this opportunity, we construct Nemotron-Math1, large-scale mathematical reasoning dataset designed to combine rich reasoning diversity with long-context supervision. On the data side, we curate high-quality mathematical problem set by combining structured, competition-style questions from OpenMathReasoning [15] with diverse, community-driven questions from Math Stack 1https://huggingface.co/datasets/nvidia/NemotronMath-v2 2025 NVIDIA. All rights reserved. Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision Exchange [20] and MathOverflow [19], which together constitute the StackExchange-derived portion (StackExchange-Math) of our dataset. Following prior work, we remove proof-style items and further filter out problems that are too easy for gpt-oss-120b: for each question, the model generates 16 solutions with low reasoning mode (8 with and 8 without Python tool-integrated reasoning (TIR)), and any problem with high pass rate (>= 0.8) is discarded as trivial. This filtering reduces the Art of Problem Solving (AoPS) [1] portion, corresponding to the AoPS problems included in [15], from the original 175K problems to 85K, and reduces the StackExchange-Math portion from 651K to 262K, resulting in balanced collection of challenging and nontrivial tasks. From this filtered problem pool, we retain the multimode trajectories generated by gpt-oss-120b under high, medium, and low reasoning modes, each both with and without Python TIR. After removing trajectories that fail to reach the reference answer, the final Nemotron-Math corpus contains 7.5M highquality long-form reasoning traces up to 128K tokens in length, capturing diverse reasoning depths, selfverification styles, and tool-usage behaviors. These properties make Nemotron-Math rich resource for studying long-context and tool-augmented mathematical reasoning. To further enhance the efficiency of long-context fine-tuning on such large-scale data, we propose sequential bucketed training strategy that groups samples by sequence length and trains the model progressively from 16K to 128K tokens. This staged approach enables optimized parallelism configurations at each length scale, significantly improving training throughput and resource utilization. Although it may introduce minor accuracy trade-offs compared to fulllength joint training, the method achieves 23 faster training while maintaining strong overall performance. Building on these developments, our "
[19.12.2025 16:28] Mistral response. {"id": "cf5469bcdc5e47dfb08f8dbc77505a1e", "created": 1766161733, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1413, "total_tokens": 1423, "completion_tokens": 10}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"NVIDIA\"]\n```"}}]}
[19.12.2025 16:28] Response: ```python
["NVIDIA"]
```
[19.12.2025 16:28] Deleting PDF ./assets/pdf/2512.15489.pdf.
[19.12.2025 16:28] Success.
[19.12.2025 16:28] Enriching papers with extra data.
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 0. Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.  					AI-generated summary 				 We present Kling-Omni, a generalist generative framework designed to synthesize high-fidel...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 1. LLaDA2.0 converts auto-regressive models into discrete diffusion large language models using a block-level training scheme, improving efficiency and performance at large scales.  					AI-generated summary 				 This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM)...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 2. Generative pretraining using next embedding prediction outperforms traditional self-supervised methods in visual learning tasks, achieving high accuracy on ImageNet and effective transfer to semantic segmentation.  					AI-generated summary 				 Inspired by the success of generative pretraining in n...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 3. StereoPilot, a feed-forward model leveraging a learnable domain switcher and cycle consistency loss, synthesizes high-quality stereo video directly without depth maps, outperforming existing methods in visual fidelity and computational efficiency.  					AI-generated summary 				 The rapid growth of ...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 4. Seedance 1.5 pro, a dual-branch Diffusion Transformer model, achieves high-quality audio-visual synchronization and generation through cross-modal integration, post-training optimizations, and an acceleration framework.  					AI-generated summary 				 Recent strides in video generation have paved th...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 5. This paper presents a framework for agent and tool adaptation in agentic AI systems, clarifying design strategies and identifying open challenges for improving AI capabilities.  					AI-generated summary 				 Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan,...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 6. A panoramic metric depth foundation model using DINOv3-Large and a three-stage pseudo-label pipeline achieves robust performance across diverse real-world scenes.  					AI-generated summary 				 In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene ...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 7. Generative Refocusing uses semi-supervised learning with DeblurNet and BokehNet to achieve high-quality single-image refocusing with controllable bokeh and text-guided adjustments.  					AI-generated summary 				 Depth-of-field control is essential in photography, but getting the perfect focus often...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 8. DeContext defends against unauthorized in-context image editing by weakening cross-attention pathways in multimodal attention layers, preserving visual quality while blocking unwanted modifications.  					AI-generated summary 				 In-context diffusion models allow users to modify images with remarka...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 9. REGLUE, a unified latent diffusion framework, enhances image synthesis by jointly modeling VAE latents, patch-level VFM semantics, and global tokens, improving semantic supervision and convergence.  					AI-generated summary 				 Latent diffusion models (LDMs) achieve state-of-the-art image synthesi...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 10. Alchemist, a meta-gradient-based framework, automatically selects high-quality subsets from large-scale text-image datasets to improve visual quality and training efficiency in Text-to-Image models.  					AI-generated summary 				 Recent advances in Text-to-Image (T2I) generative models, such as Ima...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 11. N3D-VLM integrates native 3D perception and reasoning in vision-language models, enabling precise 3D localization and spatial understanding with a large-scale dataset.  					AI-generated summary 				 While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D obje...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 12. WorldCanvas generates coherent, controllable world events using a multimodal framework that integrates text, trajectories, and reference images.  					AI-generated summary 				 We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining te...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 13. JustRL achieves state-of-the-art performance on reasoning models with minimal complexity, using single-stage training and fixed hyperparameters, outperforming sophisticated approaches in terms of compute and stability.  					AI-generated summary 				 Recent advances in reinforcement learning for lar...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 14. AdaTooler-V, a multimodal large language model, adaptively uses vision tools based on reinforcement learning, improving performance and reducing unnecessary tool invocations in visual reasoning tasks.  					AI-generated summary 				 Recent advances have shown that multimodal large language models (M...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 15. Reinforcement learning with verifiable rewards improves LLM reasoning through spurious rewards and entropy minimization, despite seemingly paradoxical effects, by reducing clipping bias and policy entropy.  					AI-generated summary 				 This paper examines the exploration-exploitation trade-off in ...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 16. FlashPortrait is a diffusion-based video transformer for long-portrait animation that ensures ID consistency and achieves 6x acceleration through a dynamic sliding-window scheme and higher-order latent derivatives.  					AI-generated summary 				 Current diffusion-based acceleration methods for long...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 17. Multimodal RewardBench 2 (MMRB2) is a benchmark for reward models on multimodal understanding and generation tasks, featuring expert-annotated preferences and state-of-the-art model evaluations.  					AI-generated summary 				 Reward models (RMs) are essential for training large language models (LLM...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 18. RePlan, a plan-then-execute framework, enhances instruction-based image editing by combining a vision-language planner with a diffusion editor, achieving superior performance in complex and intricate editing tasks using limited data.  					AI-generated summary 				 Instruction-based image editing en...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 19. VenusBench-GD is a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, offering a hierarchical evaluation framework with extensive data coverage and rich annotations.  					AI-generated summary 				 GUI grounding is a critical component in building capable GUI agents....
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 20. Hearing to Translate benchmarks SpeechLLMs and cascaded systems for speech-to-text translation, finding that cascaded systems are more reliable overall and highlighting the importance of integrating LLMs for high-quality translation.  					AI-generated summary 				 As Large Language Models (LLMs) ex...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 21. AuditDM, an automated framework using reinforcement learning, identifies and rectifies failure modes in multimodal LLMs by generating challenging examples, leading to improved performance across benchmarks.  					AI-generated summary 				 Conventional evaluation methods for multimodal LLMs (MLLMs) l...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 22. EasyV2V framework enhances video editing by combining diverse data sources, leveraging pretrained text-to-video models with LoRA fine-tuning, and implementing unified spatiotemporal control, achieving top results.  					AI-generated summary 				 While image editing has advanced rapidly, video editin...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 23. Insight Miner, a large-scale multimodal model, generates high-quality time-series descriptions using a novel agentic workflow and outperforms existing models with the help of the TS-Insights dataset.  					AI-generated summary 				 Time-series data is critical across many scientific and industrial d...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 24. A novel feed-forward framework, Make-It-Poseable, reformulates character posing as a latent-space transformation problem, using a latent posing transformer and dense pose representation to achieve superior posing quality and extend to 3D editing applications.  					AI-generated summary 				 Posing 3...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 25. Log-linear Sparse Attention (LLSA) improves the efficiency of diffusion transformers by reducing computational costs for long token sequences through a hierarchical structure, enhancing training speed without sacrificing generation quality.  					AI-generated summary 				 Diffusion Transformers (DiT...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 26. CoVRL, a hybrid approach combining variational inference and reinforcement learning, enhances language model reasoning by coupling prior and posterior distributions, improving performance and coherence.  					AI-generated summary 				 While reinforcement learning have achieved impressive progress in...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 27. MomaGraph-R1, a vision-language model trained with reinforcement learning, achieves state-of-the-art performance in predicting task-oriented scene graphs and zero-shot task planning in household environments using the MomaGraph-Scenes dataset and MomaGraph-Bench evaluation suite.  					AI-generated ...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 28. FrameDiffuser is an autoregressive neural rendering framework that generates temporally consistent photorealistic frames using G-buffer data and previous frame outputs, leveraging ControlNet and ControlLoRA for structural and temporal coherence.  					AI-generated summary 				 Neural rendering for i...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 29. Vibe Blending uses Vibe Space, a hierarchical graph manifold, to generate coherent image hybrids by learning geodesics in feature spaces, outperforming current methods in creativity and coherence.  					AI-generated summary 				 Creating new visual concepts often requires connecting distinct ideas t...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 30. Bidirectional Normalizing Flow (BiFlow) enhances generative modeling by approximating the noise-to-data inverse mapping, significantly improving generation quality and sampling speed compared to standard Normalizing Flows.  					AI-generated summary 				 Normalizing Flows (NFs) have been established...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 31. TabReX is a reference-less framework using graph-based reasoning to evaluate the quality of tables generated by LLMs, offering structural and factual fidelity scores.  					AI-generated summary 				 Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge:...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 32. EmoCaliber, a confidence-aware Multimodal Large Language Model, enhances Visual Emotion Comprehension by verbalizing confidence in emotion predictions, leading to improved reliability and accuracy.  					AI-generated summary 				 Visual Emotion Comprehension (VEC) aims to infer sentiment polarities ...
[19.12.2025 16:28] ********************************************************************************
[19.12.2025 16:28] Abstract 33. Nemotron-Math, a large-scale mathematical reasoning dataset, enhances performance and robustness through diverse problem integration and efficient long-context training strategies.  					AI-generated summary 				 High-quality mathematical reasoning supervision requires diverse reasoning styles, long...
[19.12.2025 16:28] Read previous papers.
[19.12.2025 16:28] Generating reviews via LLM API.
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#reasoning", "#inference", "#training", "#video"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–¥–µ–æ –∏–∑ –ª—é–±—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "Kling-Omni ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∑–¥–∞—ë—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ –∏–∑ —Ä
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#rlhf", "#transfer_learning", "#training", "#diffusion", "#architecture", "#optimization", "#open_source", "#alignment"], "emoji": "üîÑ", "ru": {"title": "–û—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∫ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ LLaDA2
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#training", "#cv", "#architecture"], "emoji": "üîÆ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫–∞–∫ –ø—É—Ç—å –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º—É —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–º—É –æ–±—É—á–µ–Ω–∏—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è NEPA (Next-Embedding Predictive Autoregression), –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ –ø—Ä
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#video", "#dataset", "#benchmark", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ü—Ä—è–º–æ–π —Å–∏–Ω—Ç–µ–∑ —Å—Ç–µ—Ä–µ–æ–≤–∏–¥–µ–æ –±–µ–∑ –∫–∞—Ä—Ç –≥–ª—É–±–∏–Ω—ã", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å StereoPilot, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–µ –≤–∏–¥–µ–æ –≤ —Å—Ç–µ—Ä–µ–æ–≤–∏–¥–µ–æ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –±–µ–∑ —è–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞—Ä—Ç –≥–ª—É–±
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#rlhf", "#open_source", "#video", "#architecture", "#diffusion", "#audio", "#inference", "#multimodal", "#training", "#multilingual"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –¥–≤—É—Ö–≤–µ—Ç–≤–µ–≤–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã", "desc": "Seedance 1.5 pr
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#agents", "#training"], "emoji": "üîß", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ AI —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–ª—è –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑–¥–µ–ª—è
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#dataset", "#training", "#benchmark", "#architecture", "#3d", "#cv"], "emoji": "üåç", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –≥–ª—É–±–∏–Ω—ã –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö —Å—Ü–µ–Ω", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è 
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#training", "#dataset", "#cv", "#synthetic", "#optimization"], "emoji": "üì∏", "ru": {"title": "–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –ø–µ—Ä–µ–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ —Ñ–æ–∫—É—Å–∞ —á–µ—Ä–µ–∑ –ø–æ–ª—É—Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ —Å–∏–Ω—Ç–µ–∑ –±–æ–∫–µ", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Generative Refocusing ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –≥–ª—É–±–∏–Ω—ã —Ä–µ–∑–∫–æ—Å—Ç–∏ –Ω–∞ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö 
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#multimodal", "#security", "#diffusion", "#architecture", "#cv"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –æ—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –æ—Å–ª–∞–±–ª–µ–Ω–∏–µ –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ DeContext –¥–ª—è –∑–∞—â–∏—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ—Ç –Ω–µ—Å–∞–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#cv", "#architecture", "#diffusion", "#training", "#open_source"], "emoji": "üé®", "ru": {"title": "–ü–µ—Ä–µ–ø–ª–µ—Ç–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏: —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ VAE, –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ Vision Foundation Models –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "REGLUE ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#training", "#multimodal", "#data", "#dataset"], "emoji": "‚öóÔ∏è", "ru": {"title": "–ú–µ—Ç–∞–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–∞—è –∞–ª—Ö–∏–º–∏—è: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–±–æ—Ä –ª—É—á—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Alchemist ‚Äî —ç—Ç–æ meta-gradient-based —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –≤—ã—Å–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#data", "#dataset"], "emoji": "üèóÔ∏è", "ru": {"title": "–î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–∏–Ω–Ω–æ–µ 3D –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤ —è–∑—ã–∫–æ–≤–æ-–≤–∏–∑—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏", "desc": "N3D-VLM ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –Ω–∞—Ç–∏–≤–Ω–æ–µ 3D –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –≤ –º–æ–¥–µ–ª–∏ –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞, –ø–æ–∑–≤–æ–ª—è—è –∏–º –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç—ã –≤ —Ç—Ä
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#video", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏—è –º–∏—Ä–∞ —á–µ—Ä–µ–∑ —Ç–µ–∫—Å—Ç, –¥–≤–∏–∂–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–∑—ã", "desc": "WorldCanvas –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å—Ü–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è, —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ 
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#training", "#benchmark", "#small_models", "#rl", "#optimization", "#reasoning", "#open_source"], "emoji": "üéØ", "ru": {"title": "–ü—Ä–æ—Å—Ç–æ—Ç–∞ –ª—É—á—à–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π reinforcement learning –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "JustRL –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#open_source", "#rl", "#benchmark", "#video", "#inference", "#reasoning", "#multimodal", "#optimization", "#dataset"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ AdaTooler-V ‚Äî –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#training", "#rl", "#alignment"], "emoji": "üéØ", "ru": {"title": "–ü–∞—Ä–∞–¥–æ–∫—Å –æ–±—É—á–µ–Ω–∏—è: –∫–∞–∫ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —É–ª—É—á—à–∞—é—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è LLM", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø–∞—Ä–∞–¥–æ–∫—Å–∞–ª—å–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏ (RLV
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#long_context", "#inference", "#optimization", "#architecture", "#diffusion", "#video"], "emoji": "üé¨", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –ø–æ—Ä—Ç—Ä–µ—Ç–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ª–∏—á–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã", "desc": "FlashPortrait ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π –≤–∏–¥–µ–æ-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–ª–∏—Ç–µ
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset"], "emoji": "üéØ", "ru": {"title": "–≠—Ç–∞–ª–æ–Ω–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Multimodal RewardBench 2 (MMRB2) ‚Äî –ø–µ—Ä–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤ –∑
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#diffusion", "#reasoning", "#multimodal", "#synthetic", "#cv", "#dataset"], "emoji": "üé®", "ru": {"title": "–ü–ª–∞–Ω –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ: –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è —Å–ª–æ–∂–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ vision-language —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ", "desc": "RePlan ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#dataset", "#survey", "#multimodal", "#benchmark", "#multilingual", "#agents"], "emoji": "üéØ", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤: –æ—Ç –±–∞–∑–æ–≤—ã—Ö –∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º –∑–∞–¥–∞—á–∞–º", "desc": "VenusBench-GD ‚Äî —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–≤—É—è–∑—ã—á–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#multilingual", "#multimodal", "#benchmark", "#audio"], "emoji": "üé§", "ru": {"title": "–ö–∞—Å–∫–∞–¥–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç SpeechLLM –≤ –ø–µ—Ä–µ–≤–æ–¥–µ —Ä–µ—á–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ Hearing to Translate –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä—è–º–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é SpeechLLM –∏ –∫–∞—Å–∫–∞–¥–Ω—ã—Ö 
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#rl", "#interpretability", "#security", "#multimodal", "#synthetic", "#training", "#small_models"], "emoji": "üîç", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞—É–¥–∏—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ –∏—Ö —Å–ª–∞–±–æ—Å—Ç–µ–π", "desc": "AuditDM ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#training", "#dataset", "#video", "#architecture", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ü—Ä–æ—Å—Ç–æ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –ª—ë–≥–∫—É—é –¥–æ–ø–æ–¥–≥–æ—Ç–æ–≤–∫—É", "desc": "EasyV2V ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –∫–æ—Ç–æ—Ä—ã–π —Ä
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#agents", "#training", "#dataset", "#multimodal"], "emoji": "üìà", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ–ø–∏—Å–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Insight Miner, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –∏—Å–ø
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#3d", "#architecture"], "emoji": "üé≠", "ru": {"title": "–ü–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —á–µ—Ä–µ–∑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Make-It-Poseable –¥–ª—è –ø–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è 3D –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á—É –∫–∞–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –º–æ
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#cv", "#inference", "#architecture", "#diffusion", "#optimization", "#training", "#long_context", "#open_source"], "emoji": "‚ö°", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Log-linear Sparse Attention (LLSA
[19.12.2025 16:28] Using data from previous issue: {"categories": ["#optimization", "#training", "#rl", "#reasoning"], "emoji": "üß†", "ru": {"title": "–°–≤—è–∑–∞–Ω–Ω–æ–µ –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ª–æ–≥–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "CoVRL ‚Äî —ç—Ç–æ –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª
[19.12.2025 16:28] Querying the API.
[19.12.2025 16:28] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MomaGraph-R1, a vision-language model trained with reinforcement learning, achieves state-of-the-art performance in predicting task-oriented scene graphs and zero-shot task planning in household environments using the MomaGraph-Scenes dataset and MomaGraph-Bench evaluation suite.  					AI-generated summary 				 Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.
[19.12.2025 16:28] Response: ```json
{
  "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç MomaGraph ‚Äî —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—Ü–µ–Ω—ã –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä–æ–µ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏ —Å —É—á—ë—Ç–æ–º –∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ MomaGraph-Scenes ‚Äî –ø–µ—Ä–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≥—Ä–∞—Ñ–∞–º–∏ —Å—Ü–µ–Ω –≤ –¥–æ–º–∞—à–Ω–∏—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö –∏ MomaGraph-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å—Ü–µ–Ω—ã. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ MomaGraph-R1 ‚Äî vision-language –º–æ–¥–µ–ª—å —Ä–∞–∑–º–µ—Ä–æ–º 7B, –æ–±—É—á–µ–Ω–Ω–∞—è —Å –ø–æ–º–æ—â—å—é reinforcement learning, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –∑–∞–¥–∞—á–∏ –≥—Ä–∞—Ñ—ã —Å—Ü–µ–Ω –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–ª–∞ 71.6% —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ –∏ —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—Å—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ —Ä–æ–±–æ—Ç–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã.",
  "emoji": "ü§ñ",
  "title": "–ì—Ä–∞—Ñ–∏–∫ —Å—Ü–µ–Ω—ã –¥–ª—è —É–º–Ω–æ–≥–æ —Ä–æ–±–æ—Ç–∞: –æ—Ç –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∫ –¥–µ–π—Å—Ç–≤–∏—é"
}
```
[19.12.2025 16:28] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MomaGraph-R1, a vision-language model trained with reinforcement learning, achieves state-of-the-art performance in predicting task-oriented scene graphs and zero-shot task planning in household environments using the MomaGraph-Scenes dataset and MomaGraph-Bench evaluation suite.  					AI-generated summary 				 Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments."

[19.12.2025 16:28] Response: ```python
['DATASET', 'BENCHMARK', 'CV', 'RL', 'MULTIMODAL', 'ROBOTICS']
```
[19.12.2025 16:28] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MomaGraph-R1, a vision-language model trained with reinforcement learning, achieves state-of-the-art performance in predicting task-oriented scene graphs and zero-shot task planning in household environments using the MomaGraph-Scenes dataset and MomaGraph-Bench evaluation suite.  					AI-generated summary 				 Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments."

[19.12.2025 16:28] Response: ```python
['GRAPHS', 'REASONING', 'OPEN_SOURCE']
```
[19.12.2025 16:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MomaGraph-R1 is a vision-language model that uses reinforcement learning to excel in predicting task-oriented scene graphs and performing zero-shot task planning in household settings. It introduces a unified scene representation that combines spatial and functional relationships, addressing limitations of previous models that treated scenes as static and overlooked dynamic interactions. The model is trained on the MomaGraph-Scenes dataset, which is the first large-scale collection of annotated scene graphs designed for task-driven applications. MomaGraph-R1 achieves state-of-the-art performance, significantly improving accuracy and demonstrating effective generalization to real-world robotic tasks.","title":"MomaGraph-R1: Revolutionizing Task Planning with Unified Scene Representation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MomaGraph-R1 is a vision-language model that uses reinforcement learning to excel in predicting task-oriented scene graphs and performing zero-shot task planning in household settings. It introduces a unified scene representation that combines spatial and functional relationships, addressing limitations of previous models that treated scenes as static and overlooked dynamic interactions. The model is trained on the MomaGraph-Scenes dataset, which is the first large-scale collection of annotated scene graphs designed for task-driven applications. MomaGraph-R1 achieves state-of-the-art performance, significantly improving accuracy and demonstrating effective generalization to real-world robotic tasks.', title='MomaGraph-R1: Revolutionizing Task Planning with Unified Scene Representation'))
[19.12.2025 16:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MomaGraph-R1ÊòØ‰∏ÄÁßçÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§üÂú®ÂÆ∂Â∫≠ÁéØÂ¢É‰∏≠È¢ÑÊµã‰ªªÂä°ÂØºÂêëÁöÑÂú∫ÊôØÂõæÂíåËøõË°åÈõ∂-shot‰ªªÂä°ËßÑÂàí„ÄÇËØ•Ê®°Âûã‰ΩøÁî®MomaGraph-ScenesÊï∞ÊçÆÈõÜÂíåMomaGraph-BenchËØÑ‰º∞Â•ó‰ª∂ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®‰ªªÂä°ÂØºÂêëÂú∫ÊôØÂõæÈ¢ÑÊµãÊñπÈù¢ÁöÑÂÖàËøõÊÄßËÉΩ„ÄÇMomaGraphÈÄöËøáÊï¥ÂêàÁ©∫Èó¥ÂíåÂäüËÉΩÂÖ≥Á≥ªÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑÂú∫ÊôØË°®Á§∫ÔºåÂÖãÊúç‰∫Ü‰ª•ÂæÄÁ†îÁ©∂‰∏≠ÂØπÂú∫ÊôØÈùôÊÄÅÂø´ÁÖßÁöÑÂ±ÄÈôê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMomaGraph-R1Âú®ÂºÄÊîæÊ∫ê‰ª£Á†ÅÊ®°Âûã‰∏≠ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÂáÜÁ°ÆÁéáÔºå‰∏îÂú®ÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™å‰∏≠‰πüË°®Áé∞Âá∫ËâØÂ•ΩÁöÑËøÅÁßªËÉΩÂäõ„ÄÇ","title":"MomaGraph-R1ÔºöÂÆ∂Â∫≠ÁéØÂ¢É‰∏≠ÁöÑÊô∫ËÉΩ‰ªªÂä°ËßÑÂàíÊñ∞Á™ÅÁ†¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MomaGraph-R1ÊòØ‰∏ÄÁßçÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§üÂú®ÂÆ∂Â∫≠ÁéØÂ¢É‰∏≠È¢ÑÊµã‰ªªÂä°ÂØºÂêëÁöÑÂú∫ÊôØÂõæÂíåËøõË°åÈõ∂-shot‰ªªÂä°ËßÑÂàí„ÄÇËØ•Ê®°Âûã‰ΩøÁî®MomaGraph-ScenesÊï∞ÊçÆÈõÜÂíåMomaGraph-BenchËØÑ‰º∞Â•ó‰ª∂ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®‰ªªÂä°ÂØºÂêëÂú∫ÊôØÂõæÈ¢ÑÊµãÊñπÈù¢ÁöÑÂÖàËøõÊÄßËÉΩ„ÄÇMomaGraphÈÄöËøáÊï¥ÂêàÁ©∫Èó¥ÂíåÂäüËÉΩÂÖ≥Á≥ªÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑÂú∫ÊôØË°®Á§∫ÔºåÂÖãÊúç‰∫Ü‰ª•ÂæÄÁ†îÁ©∂‰∏≠ÂØπÂú∫ÊôØÈùôÊÄÅÂø´ÁÖßÁöÑÂ±ÄÈôê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMomaGraph-R1Âú®ÂºÄÊîæÊ∫ê‰ª£Á†ÅÊ®°Âûã‰∏≠ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÂáÜÁ°ÆÁéáÔºå‰∏îÂú®ÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™å‰∏≠‰πüË°®Áé∞Âá∫ËâØÂ•ΩÁöÑËøÅÁßªËÉΩÂäõ„ÄÇ', title='MomaGraph-R1ÔºöÂÆ∂Â∫≠ÁéØÂ¢É‰∏≠ÁöÑÊô∫ËÉΩ‰ªªÂä°ËßÑÂàíÊñ∞Á™ÅÁ†¥'))
[19.12.2025 16:29] Using data from previous issue: {"categories": ["#diffusion", "#video", "#games", "#architecture", "#inference", "#optimization"], "emoji": "üé¨", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –Ω–µ–π—Ä–æ–Ω–Ω—ã–π —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å—é", "desc": "FrameDiffuser ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á
[19.12.2025 16:29] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#benchmark"], "emoji": "üé®", "ru": {"title": "–ü–æ–∏—Å–∫ –≥–µ–æ–¥–µ–∑–∏–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –æ–±—Ä–∞–∑–æ–≤ –¥–ª—è —Ç–≤–æ—Ä—á–µ—Å–∫–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Vibe Blending –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≥–∞—Ä–º–æ–Ω–∏—á–Ω—ã—Ö –≥–∏–±—Ä–∏–¥–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø—É—Ç—ë–º –ø–æ–∏—Å–∫–∞ –≥–µ–æ–¥–µ–∑–∏—á–µ—Å–∫–∏—Ö –ª–∏–Ω–∏–π –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏–∑
[19.12.2025 16:29] Querying the API.
[19.12.2025 16:29] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Bidirectional Normalizing Flow (BiFlow) enhances generative modeling by approximating the noise-to-data inverse mapping, significantly improving generation quality and sampling speed compared to standard Normalizing Flows.  					AI-generated summary 				 Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow (BiFlow), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation ("1-NFE") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.
[19.12.2025 16:29] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Bidirectional Normalizing Flow (BiFlow) ‚Äî –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—â–∏—Ö –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ—Ç–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –æ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è. BiFlow –æ–±—É—á–∞–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É–µ—Ç –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ —à—É–º–∞ –≤ –¥–∞–Ω–Ω—ã–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –≥–∏–±–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ ImageNet –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ BiFlow –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —É—Å–∫–æ—Ä—è–µ—Ç —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –¥–≤–∞ –ø–æ—Ä—è–¥–∫–∞ –≤–µ–ª–∏—á–∏–Ω—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–∏—á–∏–Ω–Ω—ã–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏—Å–∫—É—Å—Å—Ç–≤–∞ —Å—Ä–µ–¥–∏ –º–µ—Ç–æ–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—â–∏—Ö –ø–æ—Ç–æ–∫–æ–≤ –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –º–µ—Ç–æ–¥–æ–≤ —Å –æ–¥–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π.",
  "emoji": "üîÑ",
  "title": "–î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—â–∏–µ –ø–æ—Ç–æ–∫–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è"
}
```
[19.12.2025 16:29] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Bidirectional Normalizing Flow (BiFlow) enhances generative modeling by approximating the noise-to-data inverse mapping, significantly improving generation quality and sampling speed compared to standard Normalizing Flows.  					AI-generated summary 				 Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow (BiFlow), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation ("1-NFE") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm."

[19.12.2025 16:29] Response: ```python
["ARCHITECTURE", "TRAINING"]
```

**Justification:**

- **ARCHITECTURE**: The paper proposes Bidirectional Normalizing Flow (BiFlow), a novel neural architecture that modifies the standard normalizing flow framework by removing the requirement for exact analytic inverses and introducing a learned reverse model. This is a core architectural contribution.

- **TRAINING**: The paper discusses improvements to the training/learning methodology of normalizing flows, including more flexible loss functions and the learning of a reverse model to approximate the noise-to-data mapping, which relates to training improvements.
[19.12.2025 16:29] Error. Failed to parse JSON from LLM. ["ARCHITECTURE", "TRAINING"]


**Justification:**

- **ARCHITECTURE**: The paper proposes Bidirectional Normalizing Flow (BiFlow), a novel neural architecture that modifies the standard normalizing flow framework by removing the requirement for exact analytic inverses and introducing a learned reverse model. This is a core architectural contribution.

- **TRAINING**: The paper discusses improvements to the training/learning methodology of normalizing flows, including more flexible loss functions and the learning of a reverse model to approximate the noise-to-data mapping, which relates to training improvements.
[19.12.2025 16:29] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Bidirectional Normalizing Flow (BiFlow) enhances generative modeling by approximating the noise-to-data inverse mapping, significantly improving generation quality and sampling speed compared to standard Normalizing Flows.  					AI-generated summary 				 Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow (BiFlow), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation ("1-NFE") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm."

[19.12.2025 16:29] Response: ```python
['DIFFUSION', 'OPTIMIZATION']
```
[19.12.2025 16:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Bidirectional Normalizing Flow (BiFlow) is a new approach in generative modeling that improves the process of generating data from noise. Unlike traditional Normalizing Flows, which require a strict inverse mapping, BiFlow learns an approximate reverse model, allowing for more flexibility in its architecture and loss functions. This innovation leads to better quality in generated samples and significantly faster sampling speeds, making it much more efficient. Experiments show that BiFlow outperforms existing methods, achieving state-of-the-art results in generative tasks.","title":"BiFlow: Revolutionizing Generative Modeling with Flexibility and Speed"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Bidirectional Normalizing Flow (BiFlow) is a new approach in generative modeling that improves the process of generating data from noise. Unlike traditional Normalizing Flows, which require a strict inverse mapping, BiFlow learns an approximate reverse model, allowing for more flexibility in its architecture and loss functions. This innovation leads to better quality in generated samples and significantly faster sampling speeds, making it much more efficient. Experiments show that BiFlow outperforms existing methods, achieving state-of-the-art results in generative tasks.', title='BiFlow: Revolutionizing Generative Modeling with Flexibility and Speed'))
[19.12.2025 16:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ÂèåÂêëÂΩí‰∏ÄÂåñÊµÅÔºàBiFlowÔºâÈÄöËøáËøë‰ººÂô™Â£∞Âà∞Êï∞ÊçÆÁöÑÈÄÜÊò†Â∞ÑÔºåÂ¢ûÂº∫‰∫ÜÁîüÊàêÂª∫Ê®°ÁöÑËÉΩÂäõ„ÄÇ‰∏éÊ†áÂáÜÂΩí‰∏ÄÂåñÊµÅÁõ∏ÊØîÔºåBiFlowÊòæËëóÊèêÈ´ò‰∫ÜÁîüÊàêË¥®ÈáèÂíåÈááÊ†∑ÈÄüÂ∫¶„ÄÇËØ•ÊñπÊ≥ï‰∏çÂÜçÈúÄË¶ÅÁ≤æÁ°ÆÁöÑËß£ÊûêÈÄÜÔºåËÄåÊòØÂ≠¶‰π†‰∏Ä‰∏™ÈÄÜÊ®°ÂûãÔºå‰ΩøÂæóÊçüÂ§±ÂáΩÊï∞ÂíåÊû∂ÊûÑÊõ¥Âä†ÁÅµÊ¥ª„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåBiFlowÂú®ÁîüÊàêË¥®ÈáèÂíåÈááÊ†∑ÊïàÁéá‰∏äÂùá‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂΩí‰∏ÄÂåñÊµÅÊñπÊ≥ï‰∏≠ÁöÑÈ¢ÜÂÖàÂú∞‰Ωç„ÄÇ","title":"ÂèåÂêëÂΩí‰∏ÄÂåñÊµÅÔºöÊèêÂçáÁîüÊàêË¥®Èáè‰∏éÈááÊ†∑ÈÄüÂ∫¶ÁöÑÂàõÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ÂèåÂêëÂΩí‰∏ÄÂåñÊµÅÔºàBiFlowÔºâÈÄöËøáËøë‰ººÂô™Â£∞Âà∞Êï∞ÊçÆÁöÑÈÄÜÊò†Â∞ÑÔºåÂ¢ûÂº∫‰∫ÜÁîüÊàêÂª∫Ê®°ÁöÑËÉΩÂäõ„ÄÇ‰∏éÊ†áÂáÜÂΩí‰∏ÄÂåñÊµÅÁõ∏ÊØîÔºåBiFlowÊòæËëóÊèêÈ´ò‰∫ÜÁîüÊàêË¥®ÈáèÂíåÈááÊ†∑ÈÄüÂ∫¶„ÄÇËØ•ÊñπÊ≥ï‰∏çÂÜçÈúÄË¶ÅÁ≤æÁ°ÆÁöÑËß£ÊûêÈÄÜÔºåËÄåÊòØÂ≠¶‰π†‰∏Ä‰∏™ÈÄÜÊ®°ÂûãÔºå‰ΩøÂæóÊçüÂ§±ÂáΩÊï∞ÂíåÊû∂ÊûÑÊõ¥Âä†ÁÅµÊ¥ª„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåBiFlowÂú®ÁîüÊàêË¥®ÈáèÂíåÈááÊ†∑ÊïàÁéá‰∏äÂùá‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂΩí‰∏ÄÂåñÊµÅÊñπÊ≥ï‰∏≠ÁöÑÈ¢ÜÂÖàÂú∞‰Ωç„ÄÇ', title='ÂèåÂêëÂΩí‰∏ÄÂåñÊµÅÔºöÊèêÂçáÁîüÊàêË¥®Èáè‰∏éÈááÊ†∑ÈÄüÂ∫¶ÁöÑÂàõÊñ∞ÊñπÊ≥ï'))
[19.12.2025 16:29] Using data from previous issue: {"categories": ["#dataset", "#benchmark"], "emoji": "üìä", "ru": {"title": "–ì—Ä–∞—Ñ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–∞–±–ª–∏—Ü –±–µ–∑ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "TabReX ‚Äî —ç—Ç–æ —ç—Ç–∞–ª–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–∞–±–ª–∏—Ü, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä–∞—Ñ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã —Ä
[19.12.2025 16:29] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#training"], "emoji": "üòä", "ru": {"title": "–£–≤–µ—Ä–µ–Ω–Ω–∞—è –≤ —Å–µ–±–µ –º–æ–¥–µ–ª—å: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å AI –≤—ã—Ä–∞–∂–∞—Ç—å —Å–æ–º–Ω–µ–Ω–∏—è –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ —ç–º–æ—Ü–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è EmoCaliber ‚Äî –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π
[19.12.2025 16:29] Querying the API.
[19.12.2025 16:29] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Nemotron-Math, a large-scale mathematical reasoning dataset, enhances performance and robustness through diverse problem integration and efficient long-context training strategies.  					AI-generated summary 				 High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).   The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.   Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.   To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3times without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR.
[19.12.2025 16:29] Response: ```json
{
  "desc": "Nemotron-Math ‚Äî —ç—Ç–æ –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 7.5 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ä–µ—à–µ–Ω–∏–π —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç–∏–ª—è–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤—Ä–æ–¥–µ Python. –î–∞—Ç–∞—Å–µ—Ç –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç 85 —Ç—ã—Å—è—á –∑–∞–¥–∞—á —Å –æ–ª–∏–º–ø–∏–∞–¥ AoPS –∏ 262 —Ç—ã—Å—è—á–∏ –∑–∞–¥–∞—á –∏–∑ StackExchange-Math, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∫–∞–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö, –∫–æ—Ç–æ—Ä–∞—è —É—Å–∫–æ—Ä—è–µ—Ç fine-tuning –≤ 2-3 —Ä–∞–∑–∞ –±–µ–∑ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã–¥–∞—é—â–∏—Ö—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –≤–∫–ª—é—á–∞—è 100% —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —ç–∫–∑–∞–º–µ–Ω–µ AIME 2024-2025 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.",
  "emoji": "üßÆ",
  "title": "–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏ –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã"
}
```
[19.12.2025 16:29] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Nemotron-Math, a large-scale mathematical reasoning dataset, enhances performance and robustness through diverse problem integration and efficient long-context training strategies.  					AI-generated summary 				 High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).   The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.   Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.   To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3times without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR."

[19.12.2025 16:29] Response: ```python
["DATASET", "MATH", "TRAINING"]
```
[19.12.2025 16:29] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Nemotron-Math, a large-scale mathematical reasoning dataset, enhances performance and robustness through diverse problem integration and efficient long-context training strategies.  					AI-generated summary 				 High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).   The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.   Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.   To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3times without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR."

[19.12.2025 16:29] Response: ```python
['REASONING', 'SYNTHETIC', 'LONG_CONTEXT', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[19.12.2025 16:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Nemotron-Math is a large-scale dataset designed to improve mathematical reasoning in AI by integrating diverse problem types and efficient training methods. It includes 7.5 million solution traces across various reasoning modes, with options for tool-integrated reasoning using Python. The dataset combines curated competition problems with real-world queries, enhancing the robustness and generalization of AI models. Evaluations show that Nemotron-Math outperforms previous datasets, achieving high accuracy on math competitions while enabling faster training with a novel sequential bucketed strategy.","title":"Revolutionizing Mathematical Reasoning with Nemotron-Math"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Nemotron-Math is a large-scale dataset designed to improve mathematical reasoning in AI by integrating diverse problem types and efficient training methods. It includes 7.5 million solution traces across various reasoning modes, with options for tool-integrated reasoning using Python. The dataset combines curated competition problems with real-world queries, enhancing the robustness and generalization of AI models. Evaluations show that Nemotron-Math outperforms previous datasets, achieving high accuracy on math competitions while enabling faster training with a novel sequential bucketed strategy.', title='Revolutionizing Mathematical Reasoning with Nemotron-Math'))
[19.12.2025 16:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Nemotron-MathÊòØ‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÊï∞Â≠¶Êé®ÁêÜÊï∞ÊçÆÈõÜÔºåÈÄöËøáÂ§öÊ†∑ÂåñÁöÑÈóÆÈ¢òÊï¥ÂêàÂíåÈ´òÊïàÁöÑÈïø‰∏ä‰∏ãÊñáËÆ≠ÁªÉÁ≠ñÁï•Êù•ÊèêÂçáÊÄßËÉΩÂíåÈ≤ÅÊ£íÊÄß„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´750‰∏áÊù°Ëß£ÂÜ≥ÊñπÊ°àËΩ®ËøπÔºåÊ∂µÁõñÈ´ò„ÄÅ‰∏≠„ÄÅ‰ΩéÊé®ÁêÜÊ®°ÂºèÔºåÂπ∂Êèê‰æõ‰∫ÜÈõÜÊàêPythonÂ∑•ÂÖ∑ÁöÑÊé®ÁêÜÂíå‰∏çÈõÜÊàêÁöÑÊé®ÁêÜÁâàÊú¨„ÄÇÂÆÉÁªìÂêà‰∫Ü85,000‰∏™Á≤æÂøÉÁ≠ñÂàíÁöÑAoPSÈóÆÈ¢òÂíå262,000‰∏™Á§æÂå∫Êù•Ê∫êÁöÑStackExchange-MathÈóÆÈ¢òÔºåÊï¥Âêà‰∫ÜÁªìÊûÑÂåñÁöÑÁ´ûËµõ‰ªªÂä°ÂíåÂ§öÊ†∑ÂåñÁöÑÁé∞ÂÆû‰∏ñÁïåÊï∞Â≠¶Êü•ËØ¢„ÄÇÈÄöËøáÊéßÂà∂ËØÑ‰º∞ÔºåÊàë‰ª¨ÂèëÁé∞Nemotron-MathÂú®ÂåπÈÖçÁöÑAoPSÈóÆÈ¢ò‰∏äÂßãÁªà‰ºò‰∫éÂéüÂßãÁöÑOpenMathReasoningÔºå‰∏îÂú®HLE-Math‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ","title":"Nemotron-MathÔºöÊèêÂçáÊï∞Â≠¶Êé®ÁêÜÁöÑÂÖ®Êñ∞Êï∞ÊçÆÈõÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Nemotron-MathÊòØ‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÊï∞Â≠¶Êé®ÁêÜÊï∞ÊçÆÈõÜÔºåÈÄöËøáÂ§öÊ†∑ÂåñÁöÑÈóÆÈ¢òÊï¥ÂêàÂíåÈ´òÊïàÁöÑÈïø‰∏ä‰∏ãÊñáËÆ≠ÁªÉÁ≠ñÁï•Êù•ÊèêÂçáÊÄßËÉΩÂíåÈ≤ÅÊ£íÊÄß„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´750‰∏áÊù°Ëß£ÂÜ≥ÊñπÊ°àËΩ®ËøπÔºåÊ∂µÁõñÈ´ò„ÄÅ‰∏≠„ÄÅ‰ΩéÊé®ÁêÜÊ®°ÂºèÔºåÂπ∂Êèê‰æõ‰∫ÜÈõÜÊàêPythonÂ∑•ÂÖ∑ÁöÑÊé®ÁêÜÂíå‰∏çÈõÜÊàêÁöÑÊé®ÁêÜÁâàÊú¨„ÄÇÂÆÉÁªìÂêà‰∫Ü85,000‰∏™Á≤æÂøÉÁ≠ñÂàíÁöÑAoPSÈóÆÈ¢òÂíå262,000‰∏™Á§æÂå∫Êù•Ê∫êÁöÑStackExchange-MathÈóÆÈ¢òÔºåÊï¥Âêà‰∫ÜÁªìÊûÑÂåñÁöÑÁ´ûËµõ‰ªªÂä°ÂíåÂ§öÊ†∑ÂåñÁöÑÁé∞ÂÆû‰∏ñÁïåÊï∞Â≠¶Êü•ËØ¢„ÄÇÈÄöËøáÊéßÂà∂ËØÑ‰º∞ÔºåÊàë‰ª¨ÂèëÁé∞Nemotron-MathÂú®ÂåπÈÖçÁöÑAoPSÈóÆÈ¢ò‰∏äÂßãÁªà‰ºò‰∫éÂéüÂßãÁöÑOpenMathReasoningÔºå‰∏îÂú®HLE-Math‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ', title='Nemotron-MathÔºöÊèêÂçáÊï∞Â≠¶Êé®ÁêÜÁöÑÂÖ®Êñ∞Êï∞ÊçÆÈõÜ'))
[19.12.2025 16:29] Renaming data file.
[19.12.2025 16:29] Renaming previous data. hf_papers.json to ./d/2025-12-19.json
[19.12.2025 16:29] Saving new data file.
[19.12.2025 16:29] Generating page.
[19.12.2025 16:29] Renaming previous page.
[19.12.2025 16:29] Renaming previous data. index.html to ./d/2025-12-19.html
[19.12.2025 16:29] Writing result.
[19.12.2025 16:29] Renaming log file.
[19.12.2025 16:29] Renaming previous data. log.txt to ./logs/2025-12-19_last_log.txt
