[29.12.2024 06:28] Read previous papers.
[29.12.2024 06:28] Generating top page (month).
[29.12.2024 06:28] Writing top page (month).
[29.12.2024 12:38] Read previous papers.
[29.12.2024 12:38] Get feed.
[29.12.2024 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2412.17743
[29.12.2024 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2412.17483
[29.12.2024 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2412.18176
[29.12.2024 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2412.18072
[29.12.2024 12:38] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[29.12.2024 12:38] No deleted papers detected.
[29.12.2024 12:38] Downloading and parsing papers (pdf, html). Total: 4.
[29.12.2024 12:38] Downloading and parsing paper https://huggingface.co/papers/2412.17743.
[29.12.2024 12:38] Extra JSON file exists (./assets/json/2412.17743.json), skip PDF parsing.
[29.12.2024 12:38] Paper image links file exists (./assets/img_data/2412.17743.json), skip HTML parsing.
[29.12.2024 12:38] Success.
[29.12.2024 12:38] Downloading and parsing paper https://huggingface.co/papers/2412.17483.
[29.12.2024 12:38] Extra JSON file exists (./assets/json/2412.17483.json), skip PDF parsing.
[29.12.2024 12:38] Paper image links file exists (./assets/img_data/2412.17483.json), skip HTML parsing.
[29.12.2024 12:38] Success.
[29.12.2024 12:38] Downloading and parsing paper https://huggingface.co/papers/2412.18176.
[29.12.2024 12:38] Extra JSON file exists (./assets/json/2412.18176.json), skip PDF parsing.
[29.12.2024 12:38] Paper image links file exists (./assets/img_data/2412.18176.json), skip HTML parsing.
[29.12.2024 12:38] Success.
[29.12.2024 12:38] Downloading and parsing paper https://huggingface.co/papers/2412.18072.
[29.12.2024 12:38] Extra JSON file exists (./assets/json/2412.18072.json), skip PDF parsing.
[29.12.2024 12:38] Paper image links file exists (./assets/img_data/2412.18072.json), skip HTML parsing.
[29.12.2024 12:38] Success.
[29.12.2024 12:38] Enriching papers with extra data.
[29.12.2024 12:38] ********************************************************************************
[29.12.2024 12:38] Abstract 0. Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-t...
[29.12.2024 12:38] ********************************************************************************
[29.12.2024 12:38] Abstract 1. In this work, we provide a thorough investigation of gist-based context compression methods to improve long-context processing in large language models. We focus on two key questions: (1) How well can these methods replace full attention models? and (2) What potential failure patterns arise due to c...
[29.12.2024 12:38] ********************************************************************************
[29.12.2024 12:38] Abstract 2. Sequential recommendation (SR) systems have evolved significantly over the past decade, transitioning from traditional collaborative filtering to deep learning approaches and, more recently, to large language models (LLMs). While the adoption of LLMs has driven substantial advancements, these models...
[29.12.2024 12:38] ********************************************************************************
[29.12.2024 12:38] Abstract 3. With advances in foundational and vision-language models, and effective fine-tuning techniques, a large number of both general and special-purpose models have been developed for a variety of visual tasks. Despite the flexibility and accessibility of these models, no single model is able to handle al...
[29.12.2024 12:38] Read previous papers.
[29.12.2024 12:38] Generating reviews via LLM API.
[29.12.2024 12:38] Using data from previous issue: {"categories": ["#long_context", "#open_source", "#optimization", "#small_models", "#data", "#training"], "emoji": "ğŸš€", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ YuLan-Mini - ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 2.42 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´
[29.12.2024 12:38] Using data from previous issue: {"categories": ["#training", "#rag", "#optimization", "#long_context", "#data"], "emoji": "ğŸ—œï¸", "ru": {"title": "Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ¸ÑÑ‚-Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ 
[29.12.2024 12:38] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#games", "#optimization"], "emoji": "ğŸ§ ", "ru": {"title": "Molar: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Molar - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹Ğº
[29.12.2024 12:38] Using data from previous issue: {"categories": ["#benchmark", "#agi", "#optimization", "#agents", "#multimodal"], "emoji": "ğŸ­", "ru": {"title": "MMFactory: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ„Ğ°Ğ±Ñ€Ğ¸ĞºĞ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡", "desc": "MMFactory - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°
[29.12.2024 12:38] Trying to get texts in Chinese.
[29.12.2024 12:38] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-tier performance among models of similar parameter scale. Our pre-training approach focuses on enhancing training efficacy through three key technical contributions: an elaborate data pipeline combines data cleaning with data schedule strategies, a robust optimization method to mitigate training instability, and an effective annealing approach that incorporates targeted data selection and long context training. Remarkably, YuLan-Mini, trained on 1.08T tokens, achieves performance comparable to industry-leading models that require significantly more data. To facilitate reproduction, we release the full details of the data composition for each training phase. Project details can be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.
[29.12.2024 12:38] Mistral response. {"id": "1bc5ac099f75468cb462a7292d08575e", "object": "chat.completion", "created": 1735475905, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578bYuLan-Mini\u3002\u8be5\u6a21\u578b\u67092.42B\u53c2\u6570\uff0c\u6027\u80fd\u5353\u8d8a\u3002\u7814\u7a76\u56e2\u961f\u901a\u8fc7\u6570\u636e\u6e05\u6d17\u3001\u4f18\u5316\u65b9\u6cd5\u548c\u9000\u706b\u6280\u672f\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u679c\u3002YuLan-Mini\u57281.08T tokens\u4e0a\u8bad\u7ec3\uff0c\u6027\u80fd\u5ab2\u7f8e\u9700\u8981\u66f4\u591a\u6570\u636e\u7684\u884c\u4e1a\u9886\u5148\u6a21\u578b\u3002\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u5728GitHub\u4e0a\u627e\u5230\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 249, "total_tokens": 375, "completion_tokens": 126}}
[29.12.2024 12:38] Response: è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªé«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹YuLan-Miniã€‚è¯¥æ¨¡å‹æœ‰2.42Bå‚æ•°ï¼Œæ€§èƒ½å“è¶Šã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ•°æ®æ¸…æ´—ã€ä¼˜åŒ–æ–¹æ³•å’Œé€€ç«æŠ€æœ¯æé«˜äº†è®­ç»ƒæ•ˆæœã€‚YuLan-Miniåœ¨1.08T tokensä¸Šè®­ç»ƒï¼Œæ€§èƒ½åª²ç¾éœ€è¦æ›´å¤šæ•°æ®çš„è¡Œä¸šé¢†å…ˆæ¨¡å‹ã€‚è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚
[29.12.2024 12:38] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªé«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹YuLan-Miniã€‚è¯¥æ¨¡å‹æœ‰2.42Bå‚æ•°ï¼Œæ€§èƒ½å“è¶Šã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ•°æ®æ¸…æ´—ã€ä¼˜åŒ–æ–¹æ³•å’Œé€€ç«æŠ€æœ¯æé«˜äº†è®­ç»ƒæ•ˆæœã€‚YuLan-Miniåœ¨1.08T tokensä¸Šè®­ç»ƒï¼Œæ€§èƒ½åª²ç¾éœ€è¦æ›´å¤šæ•°æ®çš„è¡Œä¸šé¢†å…ˆæ¨¡å‹ã€‚è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚
[29.12.2024 12:38] Mistral response. {"id": "06625116679e4688ade2ceefa62f9c60", "object": "chat.completion", "created": 1735475908, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578bYuLan-Mini\u3002\u8be5\u6a21\u578b\u67092.42B\u53c2\u6570\uff0c\u6027\u80fd\u5353\u8d8a\u3002\u7814\u7a76\u56e2\u961f\u901a\u8fc7\u6570\u636e\u6e05\u6d17\u3001\u4f18\u5316\u65b9\u6cd5\u548c\u9000\u706b\u6280\u672f\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u679c\u3002YuLan-Mini\u57281.08T tokens\u4e0a\u8bad\u7ec3\uff0c\u6027\u80fd\u5ab2\u7f8e\u9700\u8981\u66f4\u591a\u6570\u636e\u7684\u884c\u4e1a\u9886\u5148\u6a21\u578b\u3002\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u5728GitHub\u4e0a\u627e\u5230\u3002\n\nzh\u00e8 pi\u0101n w\u00e9n zh\u0101ng ji\u00e8 sh\u00e0o le y\u012b g\u00e8 g\u0101o xi\u00e0o de d\u00e0 y\u01d4 y\u00e1n m\u00f3 x\u00edng YuLan-Mini. g\u01cei m\u00f3 x\u00edng y\u01d2u 2.42B c\u0101n sh\u01d4, x\u00ecng n\u00e9ng zhu\u00f3 yu\u00e8. y\u00e1n ji\u016b tu\u00e1n du\u012b t\u014dng gu\u00f2 sh\u00f9 j\u00f9 q\u012bng x\u01d0, y\u014du hu\u00e0 f\u0101ng f\u01ce h\u00e9 tu\u00ec hu\u01d2 j\u00ec sh\u00f9 t\u00ed g\u0101o le x\u00f9n li\u00e0n xi\u00e0o gu\u01d2. YuLan-Mini z\u00e0i 1.08T tokens sh\u00e0ng x\u00f9n li\u00e0n, x\u00ecng n\u00e9ng j\u00ec m\u01d0 x\u016b y\u00e0o g\u00e8ng du\u014d sh\u00f9 j\u00f9 de h\u00e1ng y\u00e8 l\u01d0ng xi\u0101n m\u00f3 x\u00edng. xi\u00e1ng x\u00ec x\u00ecn x\u012b k\u011b y\u01d0 z\u00e0i GitHub sh\u00e0ng zh\u01ceo d\u00e0o."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 141, "total_tokens": 513, "completion_tokens": 372}}
[29.12.2024 12:38] Response: è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªé«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹YuLan-Miniã€‚è¯¥æ¨¡å‹æœ‰2.42Bå‚æ•°ï¼Œæ€§èƒ½å“è¶Šã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ•°æ®æ¸…æ´—ã€ä¼˜åŒ–æ–¹æ³•å’Œé€€ç«æŠ€æœ¯æé«˜äº†è®­ç»ƒæ•ˆæœã€‚YuLan-Miniåœ¨1.08T tokensä¸Šè®­ç»ƒï¼Œæ€§èƒ½åª²ç¾éœ€è¦æ›´å¤šæ•°æ®çš„è¡Œä¸šé¢†å…ˆæ¨¡å‹ã€‚è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚

zhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« gÃ¨ gÄo xiÃ o de dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng YuLan-Mini. gÇi mÃ³ xÃ­ng yÇ’u 2.42B cÄn shÇ”, xÃ¬ng nÃ©ng zhuÃ³ yuÃ¨. yÃ¡n jiÅ« tuÃ¡n duÄ« tÅng guÃ² shÃ¹ jÃ¹ qÄ«ng xÇ, yÅu huÃ  fÄng fÇ hÃ© tuÃ¬ huÇ’ jÃ¬ shÃ¹ tÃ­ gÄo le xÃ¹n liÃ n xiÃ o guÇ’. YuLan-Mini zÃ i 1.08T tokens shÃ ng xÃ¹n liÃ n, xÃ¬ng nÃ©ng jÃ¬ mÇ xÅ« yÃ o gÃ¨ng duÅ shÃ¹ jÃ¹ de hÃ¡ng yÃ¨ lÇng xiÄn mÃ³ xÃ­ng. xiÃ¡ng xÃ¬ xÃ¬n xÄ« kÄ› yÇ zÃ i GitHub shÃ ng zhÇo dÃ o.
[29.12.2024 12:38] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªé«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹YuLan-Miniã€‚è¯¥æ¨¡å‹æœ‰2.42Bå‚æ•°ï¼Œæ€§èƒ½å“è¶Šã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ•°æ®æ¸…æ´—ã€ä¼˜åŒ–æ–¹æ³•å’Œé€€ç«æŠ€æœ¯æé«˜äº†è®­ç»ƒæ•ˆæœã€‚YuLan-Miniåœ¨1.08T tokensä¸Šè®­ç»ƒï¼Œæ€§èƒ½åª²ç¾éœ€è¦æ›´å¤šæ•°æ®çš„è¡Œä¸šé¢†å…ˆæ¨¡å‹ã€‚è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚
[29.12.2024 12:38] Mistral response. {"id": "14636cdda4874afa8030864c5b08ea8e", "object": "chat.completion", "created": 1735475916, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u5353\u8d8a\", \"pinyin\": \"zhu\u00f3 yu\u00e8\", \"trans\": \"outstanding\"},\n    {\"word\": \"\u6e05\u6d17\", \"pinyin\": \"q\u012bng x\u01d0\", \"trans\": \"cleaning\"},\n    {\"word\": \"\u4f18\u5316\", \"pinyin\": \"y\u014du hu\u00e0\", \"trans\": \"optimization\"},\n    {\"word\": \"\u9000\u706b\", \"pinyin\": \"tu\u00ec hu\u01d2\", \"trans\": \"annealing\"},\n    {\"word\": \"\u5ab2\u7f8e\", \"pinyin\": \"p\u00ec m\u011bi\", \"trans\": \"rival\"},\n    {\"word\": \"\u9886\u5148\", \"pinyin\": \"l\u01d0ng xi\u0101n\", \"trans\": \"leading\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 173, "total_tokens": 352, "completion_tokens": 179}}
[29.12.2024 12:38] Response: [
    {"word": "å“è¶Š", "pinyin": "zhuÃ³ yuÃ¨", "trans": "outstanding"},
    {"word": "æ¸…æ´—", "pinyin": "qÄ«ng xÇ", "trans": "cleaning"},
    {"word": "ä¼˜åŒ–", "pinyin": "yÅu huÃ ", "trans": "optimization"},
    {"word": "é€€ç«", "pinyin": "tuÃ¬ huÇ’", "trans": "annealing"},
    {"word": "åª²ç¾", "pinyin": "pÃ¬ mÄ›i", "trans": "rival"},
    {"word": "é¢†å…ˆ", "pinyin": "lÇng xiÄn", "trans": "leading"}
]
[29.12.2024 12:38] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªé«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹YuLan-Miniã€‚è¯¥æ¨¡å‹æœ‰2.42Bå‚æ•°ï¼Œæ€§èƒ½å“è¶Šã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ•°æ®æ¸…æ´—ã€ä¼˜åŒ–æ–¹æ³•å’Œé€€ç«æŠ€æœ¯æé«˜äº†è®­ç»ƒæ•ˆæœã€‚YuLan-Miniåœ¨1.08T tokensä¸Šè®­ç»ƒï¼Œæ€§èƒ½åª²ç¾éœ€è¦æ›´å¤šæ•°æ®çš„è¡Œä¸šé¢†å…ˆæ¨¡å‹ã€‚è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚
[29.12.2024 12:38] Mistral response. {"id": "7a6d7b5cda3d49dea605f95ce90bda6f", "object": "chat.completion", "created": 1735475919, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article introduces an efficient large language model called YuLan-Mini. The model has 2.42 billion parameters and delivers outstanding performance. The research team enhanced the training effectiveness through data cleaning, optimization methods, and annealing techniques. YuLan-Mini was trained on 1.08 trillion tokens and performs comparably to industry-leading models that require more data. Detailed information can be found on GitHub."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 139, "total_tokens": 233, "completion_tokens": 94}}
[29.12.2024 12:38] Response: This article introduces an efficient large language model called YuLan-Mini. The model has 2.42 billion parameters and delivers outstanding performance. The research team enhanced the training effectiveness through data cleaning, optimization methods, and annealing techniques. YuLan-Mini was trained on 1.08 trillion tokens and performs comparably to industry-leading models that require more data. Detailed information can be found on GitHub.
[29.12.2024 12:38] Renaming data file.
[29.12.2024 12:38] Renaming previous data. hf_papers.json to ./d/2024-12-27.json
[29.12.2024 12:38] Saving new data file.
[29.12.2024 12:38] Generating page.
[29.12.2024 12:38] Renaming previous page.
[29.12.2024 12:38] Renaming previous data. index.html to ./d/2024-12-27.html
[29.12.2024 12:38] [Experimental] Generating Chinese page for reading.
[29.12.2024 12:38] Chinese vocab [{'word': 'å“è¶Š', 'pinyin': 'zhuÃ³ yuÃ¨', 'trans': 'outstanding'}, {'word': 'æ¸…æ´—', 'pinyin': 'qÄ«ng xÇ', 'trans': 'cleaning'}, {'word': 'ä¼˜åŒ–', 'pinyin': 'yÅu huÃ ', 'trans': 'optimization'}, {'word': 'é€€ç«', 'pinyin': 'tuÃ¬ huÇ’', 'trans': 'annealing'}, {'word': 'åª²ç¾', 'pinyin': 'pÃ¬ mÄ›i', 'trans': 'rival'}, {'word': 'é¢†å…ˆ', 'pinyin': 'lÇng xiÄn', 'trans': 'leading'}]
[29.12.2024 12:38] Renaming previous Chinese page.
[29.12.2024 12:38] Renaming previous data. zh.html to ./d/2024-12-28_zh_reading_task.html
[29.12.2024 12:38] Writing Chinese reading task.
[29.12.2024 12:38] Writing result.
[29.12.2024 12:38] Renaming log file.
[29.12.2024 12:38] Renaming previous data. log.txt to ./logs/2024-12-29_last_log.txt
