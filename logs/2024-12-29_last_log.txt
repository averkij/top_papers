[29.12.2024 06:28] Read previous papers.
[29.12.2024 06:28] Generating top page (month).
[29.12.2024 06:28] Writing top page (month).
[29.12.2024 12:38] Read previous papers.
[29.12.2024 12:38] Get feed.
[29.12.2024 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2412.17743
[29.12.2024 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2412.17483
[29.12.2024 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2412.18176
[29.12.2024 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2412.18072
[29.12.2024 12:38] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[29.12.2024 12:38] No deleted papers detected.
[29.12.2024 12:38] Downloading and parsing papers (pdf, html). Total: 4.
[29.12.2024 12:38] Downloading and parsing paper https://huggingface.co/papers/2412.17743.
[29.12.2024 12:38] Extra JSON file exists (./assets/json/2412.17743.json), skip PDF parsing.
[29.12.2024 12:38] Paper image links file exists (./assets/img_data/2412.17743.json), skip HTML parsing.
[29.12.2024 12:38] Success.
[29.12.2024 12:38] Downloading and parsing paper https://huggingface.co/papers/2412.17483.
[29.12.2024 12:38] Extra JSON file exists (./assets/json/2412.17483.json), skip PDF parsing.
[29.12.2024 12:38] Paper image links file exists (./assets/img_data/2412.17483.json), skip HTML parsing.
[29.12.2024 12:38] Success.
[29.12.2024 12:38] Downloading and parsing paper https://huggingface.co/papers/2412.18176.
[29.12.2024 12:38] Extra JSON file exists (./assets/json/2412.18176.json), skip PDF parsing.
[29.12.2024 12:38] Paper image links file exists (./assets/img_data/2412.18176.json), skip HTML parsing.
[29.12.2024 12:38] Success.
[29.12.2024 12:38] Downloading and parsing paper https://huggingface.co/papers/2412.18072.
[29.12.2024 12:38] Extra JSON file exists (./assets/json/2412.18072.json), skip PDF parsing.
[29.12.2024 12:38] Paper image links file exists (./assets/img_data/2412.18072.json), skip HTML parsing.
[29.12.2024 12:38] Success.
[29.12.2024 12:38] Enriching papers with extra data.
[29.12.2024 12:38] ********************************************************************************
[29.12.2024 12:38] Abstract 0. Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-t...
[29.12.2024 12:38] ********************************************************************************
[29.12.2024 12:38] Abstract 1. In this work, we provide a thorough investigation of gist-based context compression methods to improve long-context processing in large language models. We focus on two key questions: (1) How well can these methods replace full attention models? and (2) What potential failure patterns arise due to c...
[29.12.2024 12:38] ********************************************************************************
[29.12.2024 12:38] Abstract 2. Sequential recommendation (SR) systems have evolved significantly over the past decade, transitioning from traditional collaborative filtering to deep learning approaches and, more recently, to large language models (LLMs). While the adoption of LLMs has driven substantial advancements, these models...
[29.12.2024 12:38] ********************************************************************************
[29.12.2024 12:38] Abstract 3. With advances in foundational and vision-language models, and effective fine-tuning techniques, a large number of both general and special-purpose models have been developed for a variety of visual tasks. Despite the flexibility and accessibility of these models, no single model is able to handle al...
[29.12.2024 12:38] Read previous papers.
[29.12.2024 12:38] Generating reviews via LLM API.
[29.12.2024 12:38] Using data from previous issue: {"categories": ["#long_context", "#open_source", "#optimization", "#small_models", "#data", "#training"], "emoji": "🚀", "ru": {"title": "Эффективное предобучение больших языковых моделей с меньшими ресурсами", "desc": "Статья представляет YuLan-Mini - языковую модель с 2.42 миллиардами параметров, д
[29.12.2024 12:38] Using data from previous issue: {"categories": ["#training", "#rag", "#optimization", "#long_context", "#data"], "emoji": "🗜️", "ru": {"title": "Сжатие без потерь: оптимизация обработки длинного контекста в больших языковых моделях", "desc": "Это исследование посвящено методам сжатия контекста на основе гист-токенов для улучшения 
[29.12.2024 12:38] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#games", "#optimization"], "emoji": "🧠", "ru": {"title": "Molar: Мультимодальные рекомендации с учетом коллаборативной фильтрации", "desc": "Статья представляет Molar - новую систему последовательных рекомендаций, основанную на мультимодальных больших язык
[29.12.2024 12:38] Using data from previous issue: {"categories": ["#benchmark", "#agi", "#optimization", "#agents", "#multimodal"], "emoji": "🏭", "ru": {"title": "MMFactory: универсальная фабрика решений для визуальных задач", "desc": "MMFactory - это универсальная платформа для решения сложных визуальных задач с использованием различных моделей ма
[29.12.2024 12:38] Trying to get texts in Chinese.
[29.12.2024 12:38] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-tier performance among models of similar parameter scale. Our pre-training approach focuses on enhancing training efficacy through three key technical contributions: an elaborate data pipeline combines data cleaning with data schedule strategies, a robust optimization method to mitigate training instability, and an effective annealing approach that incorporates targeted data selection and long context training. Remarkably, YuLan-Mini, trained on 1.08T tokens, achieves performance comparable to industry-leading models that require significantly more data. To facilitate reproduction, we release the full details of the data composition for each training phase. Project details can be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.
[29.12.2024 12:38] Mistral response. {"id": "1bc5ac099f75468cb462a7292d08575e", "object": "chat.completion", "created": 1735475905, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578bYuLan-Mini\u3002\u8be5\u6a21\u578b\u67092.42B\u53c2\u6570\uff0c\u6027\u80fd\u5353\u8d8a\u3002\u7814\u7a76\u56e2\u961f\u901a\u8fc7\u6570\u636e\u6e05\u6d17\u3001\u4f18\u5316\u65b9\u6cd5\u548c\u9000\u706b\u6280\u672f\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u679c\u3002YuLan-Mini\u57281.08T tokens\u4e0a\u8bad\u7ec3\uff0c\u6027\u80fd\u5ab2\u7f8e\u9700\u8981\u66f4\u591a\u6570\u636e\u7684\u884c\u4e1a\u9886\u5148\u6a21\u578b\u3002\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u5728GitHub\u4e0a\u627e\u5230\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 249, "total_tokens": 375, "completion_tokens": 126}}
[29.12.2024 12:38] Response: 这篇文章介绍了一个高效的大语言模型YuLan-Mini。该模型有2.42B参数，性能卓越。研究团队通过数据清洗、优化方法和退火技术提高了训练效果。YuLan-Mini在1.08T tokens上训练，性能媲美需要更多数据的行业领先模型。详细信息可以在GitHub上找到。
[29.12.2024 12:38] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

这篇文章介绍了一个高效的大语言模型YuLan-Mini。该模型有2.42B参数，性能卓越。研究团队通过数据清洗、优化方法和退火技术提高了训练效果。YuLan-Mini在1.08T tokens上训练，性能媲美需要更多数据的行业领先模型。详细信息可以在GitHub上找到。
[29.12.2024 12:38] Mistral response. {"id": "06625116679e4688ade2ceefa62f9c60", "object": "chat.completion", "created": 1735475908, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578bYuLan-Mini\u3002\u8be5\u6a21\u578b\u67092.42B\u53c2\u6570\uff0c\u6027\u80fd\u5353\u8d8a\u3002\u7814\u7a76\u56e2\u961f\u901a\u8fc7\u6570\u636e\u6e05\u6d17\u3001\u4f18\u5316\u65b9\u6cd5\u548c\u9000\u706b\u6280\u672f\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u679c\u3002YuLan-Mini\u57281.08T tokens\u4e0a\u8bad\u7ec3\uff0c\u6027\u80fd\u5ab2\u7f8e\u9700\u8981\u66f4\u591a\u6570\u636e\u7684\u884c\u4e1a\u9886\u5148\u6a21\u578b\u3002\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u5728GitHub\u4e0a\u627e\u5230\u3002\n\nzh\u00e8 pi\u0101n w\u00e9n zh\u0101ng ji\u00e8 sh\u00e0o le y\u012b g\u00e8 g\u0101o xi\u00e0o de d\u00e0 y\u01d4 y\u00e1n m\u00f3 x\u00edng YuLan-Mini. g\u01cei m\u00f3 x\u00edng y\u01d2u 2.42B c\u0101n sh\u01d4, x\u00ecng n\u00e9ng zhu\u00f3 yu\u00e8. y\u00e1n ji\u016b tu\u00e1n du\u012b t\u014dng gu\u00f2 sh\u00f9 j\u00f9 q\u012bng x\u01d0, y\u014du hu\u00e0 f\u0101ng f\u01ce h\u00e9 tu\u00ec hu\u01d2 j\u00ec sh\u00f9 t\u00ed g\u0101o le x\u00f9n li\u00e0n xi\u00e0o gu\u01d2. YuLan-Mini z\u00e0i 1.08T tokens sh\u00e0ng x\u00f9n li\u00e0n, x\u00ecng n\u00e9ng j\u00ec m\u01d0 x\u016b y\u00e0o g\u00e8ng du\u014d sh\u00f9 j\u00f9 de h\u00e1ng y\u00e8 l\u01d0ng xi\u0101n m\u00f3 x\u00edng. xi\u00e1ng x\u00ec x\u00ecn x\u012b k\u011b y\u01d0 z\u00e0i GitHub sh\u00e0ng zh\u01ceo d\u00e0o."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 141, "total_tokens": 513, "completion_tokens": 372}}
[29.12.2024 12:38] Response: 这篇文章介绍了一个高效的大语言模型YuLan-Mini。该模型有2.42B参数，性能卓越。研究团队通过数据清洗、优化方法和退火技术提高了训练效果。YuLan-Mini在1.08T tokens上训练，性能媲美需要更多数据的行业领先模型。详细信息可以在GitHub上找到。

zhè piān wén zhāng jiè shào le yī gè gāo xiào de dà yǔ yán mó xíng YuLan-Mini. gǎi mó xíng yǒu 2.42B cān shǔ, xìng néng zhuó yuè. yán jiū tuán duī tōng guò shù jù qīng xǐ, yōu huà fāng fǎ hé tuì huǒ jì shù tí gāo le xùn liàn xiào guǒ. YuLan-Mini zài 1.08T tokens shàng xùn liàn, xìng néng jì mǐ xū yào gèng duō shù jù de háng yè lǐng xiān mó xíng. xiáng xì xìn xī kě yǐ zài GitHub shàng zhǎo dào.
[29.12.2024 12:38] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

这篇文章介绍了一个高效的大语言模型YuLan-Mini。该模型有2.42B参数，性能卓越。研究团队通过数据清洗、优化方法和退火技术提高了训练效果。YuLan-Mini在1.08T tokens上训练，性能媲美需要更多数据的行业领先模型。详细信息可以在GitHub上找到。
[29.12.2024 12:38] Mistral response. {"id": "14636cdda4874afa8030864c5b08ea8e", "object": "chat.completion", "created": 1735475916, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u5353\u8d8a\", \"pinyin\": \"zhu\u00f3 yu\u00e8\", \"trans\": \"outstanding\"},\n    {\"word\": \"\u6e05\u6d17\", \"pinyin\": \"q\u012bng x\u01d0\", \"trans\": \"cleaning\"},\n    {\"word\": \"\u4f18\u5316\", \"pinyin\": \"y\u014du hu\u00e0\", \"trans\": \"optimization\"},\n    {\"word\": \"\u9000\u706b\", \"pinyin\": \"tu\u00ec hu\u01d2\", \"trans\": \"annealing\"},\n    {\"word\": \"\u5ab2\u7f8e\", \"pinyin\": \"p\u00ec m\u011bi\", \"trans\": \"rival\"},\n    {\"word\": \"\u9886\u5148\", \"pinyin\": \"l\u01d0ng xi\u0101n\", \"trans\": \"leading\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 173, "total_tokens": 352, "completion_tokens": 179}}
[29.12.2024 12:38] Response: [
    {"word": "卓越", "pinyin": "zhuó yuè", "trans": "outstanding"},
    {"word": "清洗", "pinyin": "qīng xǐ", "trans": "cleaning"},
    {"word": "优化", "pinyin": "yōu huà", "trans": "optimization"},
    {"word": "退火", "pinyin": "tuì huǒ", "trans": "annealing"},
    {"word": "媲美", "pinyin": "pì měi", "trans": "rival"},
    {"word": "领先", "pinyin": "lǐng xiān", "trans": "leading"}
]
[29.12.2024 12:38] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

这篇文章介绍了一个高效的大语言模型YuLan-Mini。该模型有2.42B参数，性能卓越。研究团队通过数据清洗、优化方法和退火技术提高了训练效果。YuLan-Mini在1.08T tokens上训练，性能媲美需要更多数据的行业领先模型。详细信息可以在GitHub上找到。
[29.12.2024 12:38] Mistral response. {"id": "7a6d7b5cda3d49dea605f95ce90bda6f", "object": "chat.completion", "created": 1735475919, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article introduces an efficient large language model called YuLan-Mini. The model has 2.42 billion parameters and delivers outstanding performance. The research team enhanced the training effectiveness through data cleaning, optimization methods, and annealing techniques. YuLan-Mini was trained on 1.08 trillion tokens and performs comparably to industry-leading models that require more data. Detailed information can be found on GitHub."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 139, "total_tokens": 233, "completion_tokens": 94}}
[29.12.2024 12:38] Response: This article introduces an efficient large language model called YuLan-Mini. The model has 2.42 billion parameters and delivers outstanding performance. The research team enhanced the training effectiveness through data cleaning, optimization methods, and annealing techniques. YuLan-Mini was trained on 1.08 trillion tokens and performs comparably to industry-leading models that require more data. Detailed information can be found on GitHub.
[29.12.2024 12:38] Renaming data file.
[29.12.2024 12:38] Renaming previous data. hf_papers.json to ./d/2024-12-27.json
[29.12.2024 12:38] Saving new data file.
[29.12.2024 12:38] Generating page.
[29.12.2024 12:38] Renaming previous page.
[29.12.2024 12:38] Renaming previous data. index.html to ./d/2024-12-27.html
[29.12.2024 12:38] [Experimental] Generating Chinese page for reading.
[29.12.2024 12:38] Chinese vocab [{'word': '卓越', 'pinyin': 'zhuó yuè', 'trans': 'outstanding'}, {'word': '清洗', 'pinyin': 'qīng xǐ', 'trans': 'cleaning'}, {'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimization'}, {'word': '退火', 'pinyin': 'tuì huǒ', 'trans': 'annealing'}, {'word': '媲美', 'pinyin': 'pì měi', 'trans': 'rival'}, {'word': '领先', 'pinyin': 'lǐng xiān', 'trans': 'leading'}]
[29.12.2024 12:38] Renaming previous Chinese page.
[29.12.2024 12:38] Renaming previous data. zh.html to ./d/2024-12-28_zh_reading_task.html
[29.12.2024 12:38] Writing Chinese reading task.
[29.12.2024 12:38] Writing result.
[29.12.2024 12:38] Renaming log file.
[29.12.2024 12:38] Renaming previous data. log.txt to ./logs/2024-12-29_last_log.txt
