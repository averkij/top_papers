[02.05.2025 03:36] Read previous papers.
[02.05.2025 03:36] Generating top page (month).
[02.05.2025 03:36] Writing top page (month).
[02.05.2025 04:14] Read previous papers.
[02.05.2025 04:14] Get feed.
[02.05.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.00662
[02.05.2025 04:14] Extract page data from URL. URL: https://huggingface.co/papers/2504.21853
[02.05.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.00703
[02.05.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.21659
[02.05.2025 04:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[02.05.2025 04:14] No deleted papers detected.
[02.05.2025 04:14] Downloading and parsing papers (pdf, html). Total: 4.
[02.05.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2505.00662.
[02.05.2025 04:14] Extra JSON file exists (./assets/json/2505.00662.json), skip PDF parsing.
[02.05.2025 04:14] Paper image links file exists (./assets/img_data/2505.00662.json), skip HTML parsing.
[02.05.2025 04:14] Success.
[02.05.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2504.21853.
[02.05.2025 04:14] Downloading paper 2504.21853 from http://arxiv.org/pdf/2504.21853v1...
[02.05.2025 04:14] Extracting affiliations from text.
[02.05.2025 04:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 3 ] . [ 1 3 5 8 1 2 . 4 0 5 2 : r Jiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Hao Chen, Xihui Liu AbstractInteractive Generative Video (IGV) has emerged as crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications. Index TermsInteractive Generative Video; Video Generation, Video Diffusion Models; Video Game; Embodied AI; Autonomous Driving. The demand for high-quality interactive video has grown significantly in recent years across various domains, from digital entertainment to industrial applications. This trend reflec"
[02.05.2025 04:14] Response: ```python
[]
```
[02.05.2025 04:14] Extracting affiliations from text.
[02.05.2025 04:14] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 3 ] . [ 1 3 5 8 1 2 . 4 0 5 2 : rJiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Hao Chen, Xihui Liu AbstractInteractive Generative Video (IGV) has emerged as crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications. Index TermsInteractive Generative Video; Video Generation, Video Diffusion Models; Video Game; Embodied AI; Autonomous Driving.The demand for high-quality interactive video has grown significantly in recent years across various domains, from digital entertainment to industrial applications. This trend reflects the expanding role of video technology in applications such as simulation, decision-making, and content creation. Meanwhile, video generation technology has witnessed remarkable progress [111], driven by recent advances in generative modeling paradigms, particularly diffusion models [1215] and next-token prediction approaches [1618]. Modern video generation systems can now produce remarkably realistic outputs while offering precise control over the generated content, enabling unprecedented opportunities across different fields. Motivated by these emerging needs and rapid technological developments, we present comprehensive survey of interactive generative video technology in this paper. To establish foundation for our discussion, we first introduce the concept of Interactive Generative Video (IGV), which is characterized by two key aspects. First, it is fundamentally generative, using generative models trained on vast video datasets to produce diverse, high-quality, opendomain video content. Second, it is inherently interactive, enabling online user engagement through control signals and responsive feedback, allowing users to achieve specific tasks or experiences through their interactions. indicates equal contributions. indicates corresponding authors. Jiwen Yu, Yiran Qin, and Xihui Liu are with the The University of Hong Kong, Pok Fu Lam, Hong Kong. Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai are with Kuaishou Technology, Shenzhen, China. Haoxuan Che and Hao Chen are with The Hong Kong University of Science and Technology (HKUST), Hong Kong. Following our definition of IGV, we present in Fig. 1 the evolution and development trajectory of three major IGV applications: Gaming, Embodied AI, and Autonomous Driving. In gaming applications [1935], video games inherently combine visual output with player interaction, aligning perfectly with the core characteristics of IGV. IGV enables the creation of infinitely explorable interactive virtual worlds, where game content can be dynamically generated and personalized based on individual player preferences and skill levels. Moreover, IGVs generative capabilities significantly streamline game development by eliminating manual asset creation processes, substantially reducing development costs and improving efficiency. Notable examples include Oasis [22] and WHAM [26], both of which have released publicly playable versions that, although earlystage, demonstrate the preliminary exploration of IGVs potential in gaming. In the field of embodied AI [3648], IGV plays crucial role in creating realistic and interactive simulations for robotic systems. It enables high-fidelity video sequence generation for task planning and visualization, allowing robots to better understand and interact with their environments. IGV also addresses the challenge of insufficient training data by providing diverse synthetic scenarios, thus improving policy learning and helping robots generalize across different tasks and environments. For autonomous driving [4964], IGV offers advanced simulation capabilities that surpass traditional physicsbased simulators in capturing real-world complexity. It generates high-fidelity video simulations conditioned on various control inputs, enabling comprehensive training across diverse driving scenarios. In addition, IGV enhances realtime decision making by predicting environmental changes and potential hazards, while also providing safe platform to test autonomous systems in rare or dangerous situations. Fig. 1: Evolutionary tree of Interactive Generative Video (IGV) models from 2020 to 2025. This diagram categorizes the development of IGV research from three major application domains: Game Simulation, Embodied AI, and Autonomous Driving, each represented by colored trunk. While IGV has demonstrated promising applications across gaming, embodied AI, and autonomous driving, to better guide its future development in these domains, we propose comprehensive framework that outlines the essential components of an ideal IGV system, as illustrated in Fig. 3. This framework helps identify key challenges and development directions through five fundamental modules: The Generation module focuses on fundamental video generation capabilities; despite significant advances in generation quality, it still faces challenges in achieving realtime performance and frame-by-frame autoregressive generation. The Control module manages user interactions with the virtual world, wit"
[02.05.2025 04:14] Mistral response. {"id": "97838cd206f4474cac12593607fbd46e", "object": "chat.completion", "created": 1746159253, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"The University of Hong Kong, Pok Fu Lam, Hong Kong\", \"Kuaishou Technology, Shenzhen, China\", \"The Hong Kong University of Science and Technology (HKUST), Hong Kong\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1404, "total_tokens": 1456, "completion_tokens": 52}}
[02.05.2025 04:14] Response: ```python
["The University of Hong Kong, Pok Fu Lam, Hong Kong", "Kuaishou Technology, Shenzhen, China", "The Hong Kong University of Science and Technology (HKUST), Hong Kong"]
```
[02.05.2025 04:14] Deleting PDF ./assets/pdf/2504.21853.pdf.
[02.05.2025 04:14] Success.
[02.05.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2505.00703.
[02.05.2025 04:14] Extra JSON file exists (./assets/json/2505.00703.json), skip PDF parsing.
[02.05.2025 04:14] Paper image links file exists (./assets/img_data/2505.00703.json), skip HTML parsing.
[02.05.2025 04:14] Success.
[02.05.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2504.21659.
[02.05.2025 04:14] Extra JSON file exists (./assets/json/2504.21659.json), skip PDF parsing.
[02.05.2025 04:14] Paper image links file exists (./assets/img_data/2504.21659.json), skip HTML parsing.
[02.05.2025 04:14] Success.
[02.05.2025 04:14] Enriching papers with extra data.
[02.05.2025 04:14] ********************************************************************************
[02.05.2025 04:14] Abstract 0. As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhanc...
[02.05.2025 04:14] ********************************************************************************
[02.05.2025 04:14] Abstract 1. Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video conten...
[02.05.2025 04:14] ********************************************************************************
[02.05.2025 04:14] Abstract 2. Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoni...
[02.05.2025 04:14] ********************************************************************************
[02.05.2025 04:14] Abstract 3. Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require e...
[02.05.2025 04:14] Read previous papers.
[02.05.2025 04:14] Generating reviews via LLM API.
[02.05.2025 04:14] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#reasoning", "#rlhf", "#math", "#training"], "emoji": "🧮", "ru": {"title": "Усовершенствование LLM для глубокой критики математических решений", "desc": "Эта статья представляет новый двухэтапный подход к улучшению способностей больших языковых мод
[02.05.2025 04:14] Querying the API.
[02.05.2025 04:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications.
[02.05.2025 04:14] Response: {
  "desc": "Статья представляет обзор технологии Интерактивного Генеративного Видео (IGV), которая сочетает генеративные возможности с интерактивными функциями. Авторы рассматривают применение IGV в трех основных областях: игры, воплощенный ИИ и автономное вождение. Предложена комплексная структура, разбивающая идеальную систему IGV на пять модулей: Генерация, Контроль, Память, Динамика и Интеллект. В работе анализируются технические проблемы и будущие направления развития каждого компонента IGV.",
  "emoji": "🎮",
  "title": "IGV: Будущее интерактивного видеоконтента"
}
[02.05.2025 04:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications."

[02.05.2025 04:14] Response: ```python
['VIDEO', 'AGENTS', 'MULTIMODAL', 'ROBOTICS']
```
[02.05.2025 04:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications."

[02.05.2025 04:14] Response: ```python
['GAMES', 'INTERPRETABILITY', 'OPTIMIZATION', 'SURVEY']
```
[02.05.2025 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Interactive Generative Video (IGV) is a new technology that creates high-quality videos that users can interact with. It combines generative models to produce diverse video content and allows users to engage with it through control signals. The paper explores IGV applications in gaming, embodied AI, and autonomous driving, highlighting its potential for creating immersive experiences and training environments. A framework is proposed to break down IGV systems into five key modules, addressing challenges like real-time generation and accurate physics simulation to guide future advancements in the field.","title":"Empowering Interactive Experiences with Generative Video"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Interactive Generative Video (IGV) is a new technology that creates high-quality videos that users can interact with. It combines generative models to produce diverse video content and allows users to engage with it through control signals. The paper explores IGV applications in gaming, embodied AI, and autonomous driving, highlighting its potential for creating immersive experiences and training environments. A framework is proposed to break down IGV systems into five key modules, addressing challenges like real-time generation and accurate physics simulation to guide future advancements in the field.', title='Empowering Interactive Experiences with Generative Video'))
[02.05.2025 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"互动生成视频（IGV）是一种新兴技术，旨在满足对高质量互动视频内容的需求。本文将IGV定义为一种结合生成能力和互动特性的技术，能够通过控制信号和反馈实现用户参与。我们调查了IGV在游戏、具身人工智能和自动驾驶等三个主要领域的应用，并提出了一个全面的框架，将理想的IGV系统分解为生成、控制、记忆、动态和智能五个模块。通过系统分析技术挑战和未来方向，本文旨在推动IGV领域的研究与发展。","title":"推动互动生成视频技术的未来发展"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='互动生成视频（IGV）是一种新兴技术，旨在满足对高质量互动视频内容的需求。本文将IGV定义为一种结合生成能力和互动特性的技术，能够通过控制信号和反馈实现用户参与。我们调查了IGV在游戏、具身人工智能和自动驾驶等三个主要领域的应用，并提出了一个全面的框架，将理想的IGV系统分解为生成、控制、记忆、动态和智能五个模块。通过系统分析技术挑战和未来方向，本文旨在推动IGV领域的研究与发展。', title='推动互动生成视频技术的未来发展'))
[02.05.2025 04:14] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#optimization", "#rl", "#reasoning", "#training"], "emoji": "🎨", "ru": {"title": "Рассуждения на двух уровнях улучшают генерацию изображений по тексту", "desc": "Данная статья представляет T2I-R1 - новую модель генерации изображений по тексту, улучшенную с помощ
[02.05.2025 04:14] Using data from previous issue: {"categories": ["#reasoning", "#inference", "#math", "#optimization", "#training"], "emoji": "🧠", "ru": {"title": "Адаптивное рассуждение: эффективность без потери качества", "desc": "Статья представляет новый подход к адаптивному и эффективному рассуждению в языковых моделях. Авторы предлагают двух
[02.05.2025 04:14] Loading Chinese text from previous data.
[02.05.2025 04:14] Renaming data file.
[02.05.2025 04:14] Renaming previous data. hf_papers.json to ./d/2025-05-02.json
[02.05.2025 04:14] Saving new data file.
[02.05.2025 04:14] Generating page.
[02.05.2025 04:14] Renaming previous page.
[02.05.2025 04:14] Renaming previous data. index.html to ./d/2025-05-02.html
[02.05.2025 04:14] [Experimental] Generating Chinese page for reading.
[02.05.2025 04:14] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '阿拉伯', 'pinyin': 'ā lā bó', 'trans': 'Arabic'}, {'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'}, {'word': '添加', 'pinyin': 'tiān jiā', 'trans': 'add'}, {'word': '发音', 'pinyin': 'fā yīn', 'trans': 'pronunciation'}, {'word': '符号', 'pinyin': 'fú hào', 'trans': 'symbol'}, {'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '名为', 'pinyin': 'míng wéi', 'trans': 'named'}, {'word': '基于', 'pinyin': 'jī yú', 'trans': 'based on'}, {'word': '微调', 'pinyin': 'wēi tiáo', 'trans': 'fine-tune'}, {'word': '解码器', 'pinyin': 'jiě mǎ qì', 'trans': 'decoder'}, {'word': '语言', 'pinyin': 'yǔ yán', 'trans': 'language'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '精心', 'pinyin': 'jīng xīn', 'trans': 'carefully'}, {'word': '编制', 'pinyin': 'biān zhì', 'trans': 'compile'}, {'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'}, {'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'}, {'word': '进行', 'pinyin': 'jìn xíng', 'trans': 'conduct'}, {'word': '有限', 'pinyin': 'yǒu xiàn', 'trans': 'limited'}, {'word': '计算', 'pinyin': 'jì suàn', 'trans': 'computational'}, {'word': '资源', 'pinyin': 'zī yuán', 'trans': 'resources'}, {'word': '取得', 'pinyin': 'qǔ dé', 'trans': 'achieve'}, {'word': '竞争力', 'pinyin': 'jìng zhēng lì', 'trans': 'competitive'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'results'}, {'word': '此外', 'pinyin': 'cǐ wài', 'trans': 'moreover'}, {'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'}, {'word': '评测', 'pinyin': 'píng cè', 'trans': 'evaluation'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '解决', 'pinyin': 'jiě jué', 'trans': 'address'}, {'word': '当前', 'pinyin': 'dāng qián', 'trans': 'current'}, {'word': '局限性', 'pinyin': 'jú xiàn xìng', 'trans': 'limitations'}, {'word': '这些', 'pinyin': 'zhè xiē', 'trans': 'these'}, {'word': '工具', 'pinyin': 'gōng jù', 'trans': 'tools'}, {'word': '共同', 'pinyin': 'gòng tóng', 'trans': 'jointly'}, {'word': '推动', 'pinyin': 'tuī dòng', 'trans': 'promote'}, {'word': '自然', 'pinyin': 'zì rán', 'trans': 'natural'}, {'word': '处理', 'pinyin': 'chǔ lǐ', 'trans': 'processing'}, {'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'application'}, {'word': '发展', 'pinyin': 'fā zhǎn', 'trans': 'development'}]
[02.05.2025 04:14] Renaming previous Chinese page.
[02.05.2025 04:14] Renaming previous data. zh.html to ./d/2025-05-01_zh_reading_task.html
[02.05.2025 04:14] Writing Chinese reading task.
[02.05.2025 04:14] Writing result.
[02.05.2025 04:14] Renaming log file.
[02.05.2025 04:14] Renaming previous data. log.txt to ./logs/2025-05-02_last_log.txt
