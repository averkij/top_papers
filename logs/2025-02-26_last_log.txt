[26.02.2025 08:14] Read previous papers.
[26.02.2025 08:14] Generating top page (month).
[26.02.2025 08:14] Writing top page (month).
[26.02.2025 09:11] Read previous papers.
[26.02.2025 09:11] Get feed.
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18411
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18137
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17363
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17262
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18449
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15499
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18364
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18356
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16794
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17425
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18461
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17535
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17092
[26.02.2025 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.02.2025 09:11] No deleted papers detected.
[26.02.2025 09:11] Downloading and parsing papers (pdf, html). Total: 13.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18411.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.18411.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.18411.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18137.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.18137.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.18137.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.17363.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.17363.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.17363.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.17262.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.17262.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.17262.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18449.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.18449.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.18449.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.15499.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.15499.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.15499.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18364.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.18364.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.18364.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18356.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.18356.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.18356.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.16794.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.16794.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.16794.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.17425.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.17425.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.17425.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18461.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.18461.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.18461.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.17535.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.17535.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.17535.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.17092.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.17092.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.17092.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Enriching papers with extra data.
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 0. Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featur...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 1. An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the s...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 2. Background consistency remains a significant challenge in image editing tasks. Despite extensive developments, existing works still face a trade-off between maintaining similarity to the original image and generating content that aligns with the target. Here, we propose KV-Edit, a training-free appr...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 3. The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) t...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 4. The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, thi...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 5. Training stability is a persistent challenge in the pre-training of large language models (LLMs), particularly for architectures such as Post-Norm Transformers, which are prone to gradient explosion and dissipation. In this paper, we propose Scale-Distribution Decoupling (SDD), a novel approach that...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 6. Multi-layer image generation is a fundamental task that enables users to isolate, select, and edit specific image layers, thereby revolutionizing interactions with generative models. In this paper, we introduce the Anonymous Region Transformer (ART), which facilitates the direct generation of variab...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 7. We introduce WebGames, a comprehensive benchmark suite designed to evaluate general-purpose web-browsing AI agents through a collection of 50+ interactive challenges. These challenges are specifically crafted to be straightforward for humans while systematically testing the limitations of current AI...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 8. Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existi...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 9. To utilize visual information, Multimodal Large Language Model (MLLM) relies on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. However, MLLM stil...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 10. Recent studies have explored combining different LoRAs to jointly generate learned style and content. However, existing methods either fail to effectively preserve both the original subject and style simultaneously or require additional training. In this paper, we argue that the intrinsic properties...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 11. Motivated by reducing the computational and storage costs of LLMs, model compression and KV cache compression have attracted much attention from researchers. However, current methods predominantly emphasize maintaining the performance of compressed LLMs, as measured by perplexity or simple accuracy ...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 12. We introduce Shakti VLM, a family of vision-language models in the capacity of 1B and 4B parameters designed to address data efficiency challenges in multimodal learning. While recent VLMs achieve strong performance through extensive training data, Shakti models leverage architectural innovations to...
[26.02.2025 09:11] Read previous papers.
[26.02.2025 09:11] Generating reviews via LLM API.
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#alignment", "#training", "#benchmark", "#multimodal", "#dataset", "#open_source"], "emoji": "🤖", "ru": {"title": "Приведение мультимодальных ИИ-моделей в соответствие с человеческими ценностями", "desc": "Статья представляет OmniAlign-V - набор данных для улучшения мультимодальных 
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#architecture", "#inference", "#video", "#optimization", "#cv"], "emoji": "🚀", "ru": {"title": "Универсальное разреженное внимание для ускорения нейросетей", "desc": "В статье представлен новый метод SpargeAttn для оптимизации внимания в нейронных сетях. Он использует двухэтапный он
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#training", "#cv"], "emoji": "🖼️", "ru": {"title": "KV-Edit: Революция в редактировании изображений с сохранением согласованности фона", "desc": "KV-Edit - это новый подход к редактированию изображений, основанный на использовании KV-кэша в DiT-моделях для сохранения согласованности
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#training", "#benchmark"], "emoji": "🔮", "ru": {"title": "Предсказание будущего больших языковых моделей", "desc": "Статья представляет новый метод прогнозирования производительности больших языковых моделей (LLM) на различных задачах. Авторы предла
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#dataset", "#math", "#open_source"], "emoji": "🧠", "ru": {"title": "SWE-RL: Революция в обучении языковых моделей для задач разработки ПО", "desc": "Данная статья представляет SWE-RL - первый подход к масштабированию обучения с подкреплением (RL) дл
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "🚀", "ru": {"title": "Стабильное обучение языковых моделей через разделение масштаба и распределения", "desc": "Статья представляет новый метод Scale-Distribution Decoupling (SDD) для стабилизации обучения больших языковых моде
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#multimodal", "#cv"], "emoji": "🎭", "ru": {"title": "ART: революция в генерации многослойных изображений", "desc": "В статье представлен метод Anonymous Region Transformer (ART) для генерации многослойных прозрачных изображений на основе текстового запроса и анонимной разметки облас
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#games", "#agents"], "emoji": "🌐", "ru": {"title": "WebGames: тестирование AI-агентов в реальных веб-сценариях", "desc": "WebGames - это комплексный набор тестов для оценки AI-агентов общего назначения, предназначенных для веб-браузинга. Он включает бол
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#alignment", "#audio"], "emoji": "👂", "ru": {"title": "Слушать как человек: ИИ с избирательным вниманием", "desc": "Статья представляет новый подход к обработке аудиоданных с использованием больших языковых моделей (LLM), учитывающий избирательное
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#architecture", "#cv"], "emoji": "👁️", "ru": {"title": "Токены визуального восприятия: новый шаг в развитии мультимодальных ИИ", "desc": "В этой статье представлена концепция Токенов Визуального Восприятия для мультимодальных больших языковых моделей (ML
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#training"], "emoji": "🎨", "ru": {"title": "K-LoRA: Эффективное слияние стиля и содержания без дополнительного обучения", "desc": "Статья представляет новый метод K-LoRA для объединения различных LoRA (Low-Rank Adaptation) в генеративн
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#inference", "#optimization", "#reasoning", "#rag", "#training"], "emoji": "🎟️", "ru": {"title": "Лотерейные языковые модели: путь к эффективному сжатию без потери функциональности", "desc": "Статья рассматривает гипотезу лотерейной языковой модели (LLM) в контексте сжатия моделей и
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#reasoning", "#architecture", "#training"], "emoji": "🧠", "ru": {"title": "Эффективные мультимодальные модели без избыточных данных", "desc": "Представлена линейка мультимодальных моделей Shakti VLM с 1B и 4B параметров, нацеленных на повышение эффект
[26.02.2025 09:11] Trying to get texts in Chinese.
[26.02.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs' alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs' alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at https://github.com/PhoenixZ810/OmniAlign-V.
[26.02.2025 09:12] Mistral response. {"id": "67858ea67cec4dfaada31af6dd250b5c", "object": "chat.completion", "created": 1740561117, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6700\u8fd1\u7684\u5f00\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e3b\u8981\u96c6\u4e2d\u5728\u589e\u5f3a\u57fa\u7840\u80fd\u529b\uff0c\u4f46\u5728\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u5dee\u8ddd\u3002\u672c\u6587\u4ecb\u7ecd\u4e86OmniAlign-V\uff0c\u4e00\u4e2a\u5305\u542b20\u4e07\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6837\u672c\u7684\u5168\u9762\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u6837\u5316\u56fe\u50cf\u3001\u590d\u6742\u95ee\u9898\u548c\u591a\u79cd\u56de\u7b54\u683c\u5f0f\uff0c\u4ee5\u63d0\u5347MLLMs\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u3002\u6211\u4eec\u8fd8\u63a8\u51fa\u4e86MM-AlignBench\uff0c\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u8bc4\u4f30MLLMs\u4e0e\u4eba\u7c7b\u4ef7\u503c\u5bf9\u9f50\u7684\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u6216\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5bf9MLLMs\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\uff0c\u540c\u65f6\u5728\u6807\u51c6VQA\u57fa\u51c6\u4e0a\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\uff0c\u4fdd\u7559\u5176\u57fa\u672c\u80fd\u529b\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6\u3001\u4ee3\u7801\u548c\u68c0\u67e5\u70b9\u5df2\u53d1\u5e03\u5728https://github.com/PhoenixZ810/OmniAlign-V\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 242, "total_tokens": 517, "completion_tokens": 275}}
[26.02.2025 09:12] Response: 最近的开源多模态大语言模型（MLLMs）主要集中在增强基础能力，但在与人类偏好对齐方面仍有很大差距。本文介绍了OmniAlign-V，一个包含20万高质量训练样本的全面数据集，涵盖多样化图像、复杂问题和多种回答格式，以提升MLLMs与人类偏好的对齐。我们还推出了MM-AlignBench，一个专门设计用于评估MLLMs与人类价值对齐的人工标注基准。实验结果表明，使用监督微调（SFT）或直接偏好优化（DPO）对MLLMs进行微调，显著提高了人类偏好对齐，同时在标准VQA基准上保持或提升性能，保留其基本能力。我们的数据集、基准、代码和检查点已发布在https://github.com/PhoenixZ810/OmniAlign-V。
[26.02.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

最近的开源多模态大语言模型（MLLMs）主要集中在增强基础能力，但在与人类偏好对齐方面仍有很大差距。本文介绍了OmniAlign-V，一个包含20万高质量训练样本的全面数据集，涵盖多样化图像、复杂问题和多种回答格式，以提升MLLMs与人类偏好的对齐。我们还推出了MM-AlignBench，一个专门设计用于评估MLLMs与人类价值对齐的人工标注基准。实验结果表明，使用监督微调（SFT）或直接偏好优化（DPO）对MLLMs进行微调，显著提高了人类偏好对齐，同时在标准VQA基准上保持或提升性能，保留其基本能力。我们的数据集、基准、代码和检查点已发布在https://github.com/PhoenixZ810/OmniAlign-V。
[26.02.2025 09:12] Mistral response. {"id": "cb4094ac1d444e78ab26e83b293e960c", "object": "chat.completion", "created": 1740561123, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6700\u8fd1\u7684\u5f00\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e3b\u8981\u96c6\u4e2d\u5728\u589e\u5f3a\u57fa\u7840\u80fd\u529b\uff0c\u4f46\u5728\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u5dee\u8ddd\u3002\u672c\u6587\u4ecb\u7ecd\u4e86OmniAlign-V\uff0c\u4e00\u4e2a\u5305\u542b20\u4e07\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6837\u672c\u7684\u5168\u9762\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u6837\u5316\u56fe\u50cf\u3001\u590d\u6742\u95ee\u9898\u548c\u591a\u79cd\u56de\u7b54\u683c\u5f0f\uff0c\u4ee5\u63d0\u5347MLLMs\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u3002\u6211\u4eec\u8fd8\u63a8\u51fa\u4e86MM-AlignBench\uff0c\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u8bc4\u4f30MLLMs\u4e0e\u4eba\u7c7b\u4ef7\u503c\u5bf9\u9f50\u7684\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u6216\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5bf9MLLMs\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\uff0c\u540c\u65f6\u5728\u6807\u51c6VQA\u57fa\u51c6\u4e0a\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\uff0c\u4fdd\u7559\u5176\u57fa\u672c\u80fd\u529b\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6\u3001\u4ee3\u7801\u548c\u68c0\u67e5\u70b9\u5df2\u53d1\u5e03\u5728https://github.com/PhoenixZ810/OmniAlign-V\u3002\n\nZu\u00ecj\u00ecn de k\u0101iyu\u01cen du\u014d m\u00f3sh\u00ec d\u00e0 y\u01d4y\u00e1n m\u00f3x\u00edng (MLLMs) zh\u01d4y\u00e0o j\u00edzh\u014dng z\u00e0i z\u0113ngqi\u00e1ng j\u012bch\u01d4 n\u00e9ngl\u00ec, d\u00e0n z\u00e0i y\u01d4 r\u00e9nl\u00e8i pi\u0101nh\u01ceo du\u00ecq\u00ed f\u0101ngmi\u00e0n r\u00e9ng y\u01d2u h\u011bn d\u00e0 ch\u0101j\u00f9. B\u011bnw\u00e9n ji\u00e8sh\u00e0o le OmniAlign-V, y\u012bg\u00e8 b\u0101oh\u00e1n 20 w\u00e0n g\u0101o zh\u00ecli\u00e0ng x\u00f9nli\u00e0n y\u00e0ngb\u011bn de qu\u00e1nmi\u00e0n sh\u00f9j\u00f9j\u00ed, h\u00e1nji\u0113 du\u014dy\u00e0nghu\u00e0 t\u00faxi\u00e0ng, f\u00f9z\u00e1 xu\u00e1nzh\u00f2ng w\u00e8nt\u00ed h\u00e9 du\u014dzh\u01d2ng hu\u00edd\u00e1 g\u00e9sh\u00ec, y\u01d0 t\u00edsh\u0113ng MLLMs y\u01d4 r\u00e9nl\u00e8i pi\u0101nh\u01ceo de du\u00ecq\u00ed. W\u01d2men h\u00e1i tu\u012bch\u016b le MM-AlignBench, y\u012bg\u00e8 zhu\u0101nm\u00e9n sh\u00e8j\u00ec y\u00f2ngy\u00fa p\u00edngg\u01d4 MLLMs y\u01d4 r\u00e9nl\u00e8i ji\u00e0zh\u00ed du\u00ecq\u00ed de r\u00e9ng\u014dng bi\u0101ozh\u00f9 j\u012bzh\u01d4n. Sh\u00edy\u00e0n ji\u00e9gu\u01d2 bi\u01ceom\u00edng, sh\u01d0y\u00f2ng ji\u00e0nd\u016b w\u0113iti\u00e1o (SFT) hu\u00f2 zh\u00edji\u0113 pi\u0101nh\u01ceo y\u014duhu\u00e0 (DPO) du\u00ec MLLMs j\u00ecnx\u00edng w\u0113iti\u00e1o, xi\u01cenzh\u00f9 t\u00edg\u0101o le r\u00e9nl\u00e8i pi\u0101nh\u01ceo du\u00ecq\u00ed, t\u00f3ngsh\u00ed z\u00e0i bi\u0101ozh\u01d4n VQA j\u012bzh\u01d4n sh\u00e0ng b\u01ceoch\u00ed hu\u00f2 t\u00edsh\u0113ng x\u00ecngn\u00e9ng, b\u01ceoli\u00fa q\u00ed j\u012bb\u011bn n\u00e9ngl\u00ec. W\u01d2men de sh\u00f9j\u00f9j\u00ed, j\u012bzh\u01d4n, d\u00e0im\u01ce h\u00e9 ji\u01cench\u00e1di\u01cen y\u01d0 f\u0101b\u00f9 z\u00e0i https://github.com/PhoenixZ810/OmniAlign-V."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 290, "total_tokens": 1106, "completion_tokens": 816}}
[26.02.2025 09:12] Response: 最近的开源多模态大语言模型（MLLMs）主要集中在增强基础能力，但在与人类偏好对齐方面仍有很大差距。本文介绍了OmniAlign-V，一个包含20万高质量训练样本的全面数据集，涵盖多样化图像、复杂问题和多种回答格式，以提升MLLMs与人类偏好的对齐。我们还推出了MM-AlignBench，一个专门设计用于评估MLLMs与人类价值对齐的人工标注基准。实验结果表明，使用监督微调（SFT）或直接偏好优化（DPO）对MLLMs进行微调，显著提高了人类偏好对齐，同时在标准VQA基准上保持或提升性能，保留其基本能力。我们的数据集、基准、代码和检查点已发布在https://github.com/PhoenixZ810/OmniAlign-V。

Zuìjìn de kāiyuǎn duō móshì dà yǔyán móxíng (MLLMs) zhǔyào jízhōng zài zēngqiáng jīchǔ nénglì, dàn zài yǔ rénlèi piānhǎo duìqí fāngmiàn réng yǒu hěn dà chājù. Běnwén jièshào le OmniAlign-V, yīgè bāohán 20 wàn gāo zhìliàng xùnliàn yàngběn de quánmiàn shùjùjí, hánjiē duōyànghuà túxiàng, fùzá xuánzhòng wèntí hé duōzhǒng huídá géshì, yǐ tíshēng MLLMs yǔ rénlèi piānhǎo de duìqí. Wǒmen hái tuīchū le MM-AlignBench, yīgè zhuānmén shèjì yòngyú pínggǔ MLLMs yǔ rénlèi jiàzhí duìqí de réngōng biāozhù jīzhǔn. Shíyàn jiéguǒ biǎomíng, shǐyòng jiàndū wēitiáo (SFT) huò zhíjiē piānhǎo yōuhuà (DPO) duì MLLMs jìnxíng wēitiáo, xiǎnzhù tígāo le rénlèi piānhǎo duìqí, tóngshí zài biāozhǔn VQA jīzhǔn shàng bǎochí huò tíshēng xìngnéng, bǎoliú qí jīběn nénglì. Wǒmen de shùjùjí, jīzhǔn, dàimǎ hé jiǎnchádiǎn yǐ fābù zài https://github.com/PhoenixZ810/OmniAlign-V.
[26.02.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

最近的开源多模态大语言模型（MLLMs）主要集中在增强基础能力，但在与人类偏好对齐方面仍有很大差距。本文介绍了OmniAlign-V，一个包含20万高质量训练样本的全面数据集，涵盖多样化图像、复杂问题和多种回答格式，以提升MLLMs与人类偏好的对齐。我们还推出了MM-AlignBench，一个专门设计用于评估MLLMs与人类价值对齐的人工标注基准。实验结果表明，使用监督微调（SFT）或直接偏好优化（DPO）对MLLMs进行微调，显著提高了人类偏好对齐，同时在标准VQA基准上保持或提升性能，保留其基本能力。我们的数据集、基准、代码和检查点已发布在https://github.com/PhoenixZ810/OmniAlign-V。
[26.02.2025 09:12] Mistral response. {"id": "2e4ccd4f38e94725bf2888936b02b86b", "object": "chat.completion", "created": 1740561138, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u591a\u6a21\u6001\", \"pinyin\": \"du\u014d m\u00f3 t\u00e0i\", \"trans\": \"multimodal\"},\n    {\"word\": \"\u5927\u8bed\u8a00\u6a21\u578b\", \"pinyin\": \"d\u00e0 y\u01d4 y\u00e1n m\u00f3 x\u00edng\", \"trans\": \"large language model\"},\n    {\"word\": \"\u57fa\u7840\u80fd\u529b\", \"pinyin\": \"j\u012b ch\u01d4 n\u00e9ng l\u00ec\", \"trans\": \"basic capabilities\"},\n    {\"word\": \"\u5bf9\u9f50\", \"pinyin\": \"du\u00ec q\u00ed\", \"trans\": \"alignment\"},\n    {\"word\": \"\u9ad8\u8d28\u91cf\", \"pinyin\": \"g\u0101o zh\u00ec li\u00e0ng\", \"trans\": \"high quality\"},\n    {\"word\": \"\u6837\u672c\", \"pinyin\": \"y\u00e0ng b\u011bn\", \"trans\": \"sample\"},\n    {\"word\": \"\u6db5\u76d6\", \"pinyin\": \"h\u00e1n g\u00e0i\", \"trans\": \"cover\"},\n    {\"word\": \"\u591a\u6837\u5316\", \"pinyin\": \"du\u014d y\u00e0ng hu\u00e0\", \"trans\": \"diversified\"},\n    {\"word\": \"\u590d\u6742\", \"pinyin\": \"f\u00f9 z\u00e1\", \"trans\": \"complex\"},\n    {\"word\": \"\u683c\u5f0f\", \"pinyin\": \"g\u00e9 sh\u00ec\", \"trans\": \"format\"},\n    {\"word\": \"\u63d0\u5347\", \"pinyin\": \"t\u00ed sh\u0113ng\", \"trans\": \"improve\"},\n    {\"word\": \"\u4eba\u5de5\u6807\u6ce8\", \"pinyin\": \"r\u00e9n g\u014dng bi\u0101o zh\u00f9\", \"trans\": \"manual annotation\"},\n    {\"word\": \"\u57fa\u51c6\", \"pinyin\": \"j\u012b zh\u01d4n\", \"trans\": \"benchmark\"},\n    {\"word\": \"\u76d1\u7763\u5fae\u8c03\", \"pinyin\": \"ji\u00e0n d\u016b w\u0113i ti\u00e1o\", \"trans\": \"supervised fine-tuning\"},\n    {\"word\": \"\u76f4\u63a5\u504f\u597d\u4f18\u5316\", \"pinyin\": \"zh\u00ed ji\u0113 pi\u0101n h\u00e0o y\u014du hu\u00e0\", \"trans\": \"direct preference optimization\"},\n    {\"word\": \"\u663e\u8457\", \"pinyin\": \"xi\u01cen zh\u00f9\", \"trans\": \"significant\"},\n    {\"word\": \"\u4fdd\u6301\", \"pinyin\": \"b\u01ceo ch\u00ed\", \"trans\": \"maintain\"},\n    {\"word\": \"\u6807\u51c6\", \"pinyin\": \"bi\u0101o zh\u01d4n\", \"trans\": \"standard\"},\n    {\"word\": \"VQA\", \"pinyin\": \"VQA\", \"trans\": \"Visual Question Answering\"},\n    {\"word\": \"\u68c0\u67e5\u70b9\", \"pinyin\": \"ji\u01cen ch\u00e1 di\u01cen\", \"trans\": \"checkpoint\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 322, "total_tokens": 962, "completion_tokens": 640}}
[26.02.2025 09:12] Response: [
    {"word": "多模态", "pinyin": "duō mó tài", "trans": "multimodal"},
    {"word": "大语言模型", "pinyin": "dà yǔ yán mó xíng", "trans": "large language model"},
    {"word": "基础能力", "pinyin": "jī chǔ néng lì", "trans": "basic capabilities"},
    {"word": "对齐", "pinyin": "duì qí", "trans": "alignment"},
    {"word": "高质量", "pinyin": "gāo zhì liàng", "trans": "high quality"},
    {"word": "样本", "pinyin": "yàng běn", "trans": "sample"},
    {"word": "涵盖", "pinyin": "hán gài", "trans": "cover"},
    {"word": "多样化", "pinyin": "duō yàng huà", "trans": "diversified"},
    {"word": "复杂", "pinyin": "fù zá", "trans": "complex"},
    {"word": "格式", "pinyin": "gé shì", "trans": "format"},
    {"word": "提升", "pinyin": "tí shēng", "trans": "improve"},
    {"word": "人工标注", "pinyin": "rén gōng biāo zhù", "trans": "manual annotation"},
    {"word": "基准", "pinyin": "jī zhǔn", "trans": "benchmark"},
    {"word": "监督微调", "pinyin": "jiàn dū wēi tiáo", "trans": "supervised fine-tuning"},
    {"word": "直接偏好优化", "pinyin": "zhí jiē piān hào yōu huà", "trans": "direct preference optimization"},
    {"word": "显著", "pinyin": "xiǎn zhù", "trans": "significant"},
    {"word": "保持", "pinyin": "bǎo chí", "trans": "maintain"},
    {"word": "标准", "pinyin": "biāo zhǔn", "trans": "standard"},
    {"word": "VQA", "pinyin": "VQA", "trans": "Visual Question Answering"},
    {"word": "检查点", "pinyin": "jiǎn chá diǎn", "trans": "checkpoint"}
]
[26.02.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

最近的开源多模态大语言模型（MLLMs）主要集中在增强基础能力，但在与人类偏好对齐方面仍有很大差距。本文介绍了OmniAlign-V，一个包含20万高质量训练样本的全面数据集，涵盖多样化图像、复杂问题和多种回答格式，以提升MLLMs与人类偏好的对齐。我们还推出了MM-AlignBench，一个专门设计用于评估MLLMs与人类价值对齐的人工标注基准。实验结果表明，使用监督微调（SFT）或直接偏好优化（DPO）对MLLMs进行微调，显著提高了人类偏好对齐，同时在标准VQA基准上保持或提升性能，保留其基本能力。我们的数据集、基准、代码和检查点已发布在https://github.com/PhoenixZ810/OmniAlign-V。
[26.02.2025 09:12] Mistral response. {"id": "afe5c76052714fe19835d615b7991a4b", "object": "chat.completion", "created": 1740561151, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Recent open-source multimodal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, but there remains a significant gap in aligning with human preferences. This paper introduces OmniAlign-V, a comprehensive dataset containing 200,000 high-quality training samples that cover diverse images, complex questions, and various answer formats to improve the alignment of MLLMs with human preferences. We also present MM-AlignBench, a specially designed, human-annotated benchmark for evaluating the alignment of MLLMs with human values. Experimental results demonstrate that fine-tuning MLLMs using supervised fine-tuning (SFT) or direct preference optimization (DPO) significantly improves human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our dataset, benchmark, code, and checkpoints are available at https://github.com/PhoenixZ810/OmniAlign-V."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 288, "total_tokens": 504, "completion_tokens": 216}}
[26.02.2025 09:12] Response: Recent open-source multimodal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, but there remains a significant gap in aligning with human preferences. This paper introduces OmniAlign-V, a comprehensive dataset containing 200,000 high-quality training samples that cover diverse images, complex questions, and various answer formats to improve the alignment of MLLMs with human preferences. We also present MM-AlignBench, a specially designed, human-annotated benchmark for evaluating the alignment of MLLMs with human values. Experimental results demonstrate that fine-tuning MLLMs using supervised fine-tuning (SFT) or direct preference optimization (DPO) significantly improves human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our dataset, benchmark, code, and checkpoints are available at https://github.com/PhoenixZ810/OmniAlign-V.
[26.02.2025 09:12] Renaming data file.
[26.02.2025 09:12] Renaming previous data. hf_papers.json to ./d/2025-02-26.json
[26.02.2025 09:12] Saving new data file.
[26.02.2025 09:12] Generating page.
[26.02.2025 09:12] Renaming previous page.
[26.02.2025 09:12] Renaming previous data. index.html to ./d/2025-02-26.html
[26.02.2025 09:12] [Experimental] Generating Chinese page for reading.
[26.02.2025 09:12] Chinese vocab [{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'}, {'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'}, {'word': '基础能力', 'pinyin': 'jī chǔ néng lì', 'trans': 'basic capabilities'}, {'word': '对齐', 'pinyin': 'duì qí', 'trans': 'alignment'}, {'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'}, {'word': '样本', 'pinyin': 'yàng běn', 'trans': 'sample'}, {'word': '涵盖', 'pinyin': 'hán gài', 'trans': 'cover'}, {'word': '多样化', 'pinyin': 'duō yàng huà', 'trans': 'diversified'}, {'word': '复杂', 'pinyin': 'fù zá', 'trans': 'complex'}, {'word': '格式', 'pinyin': 'gé shì', 'trans': 'format'}, {'word': '提升', 'pinyin': 'tí shēng', 'trans': 'improve'}, {'word': '人工标注', 'pinyin': 'rén gōng biāo zhù', 'trans': 'manual annotation'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '监督微调', 'pinyin': 'jiàn dū wēi tiáo', 'trans': 'supervised fine-tuning'}, {'word': '直接偏好优化', 'pinyin': 'zhí jiē piān hào yōu huà', 'trans': 'direct preference optimization'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '保持', 'pinyin': 'bǎo chí', 'trans': 'maintain'}, {'word': '标准', 'pinyin': 'biāo zhǔn', 'trans': 'standard'}, {'word': 'VQA', 'pinyin': 'VQA', 'trans': 'Visual Question Answering'}, {'word': '检查点', 'pinyin': 'jiǎn chá diǎn', 'trans': 'checkpoint'}]
[26.02.2025 09:12] Renaming previous Chinese page.
[26.02.2025 09:12] Renaming previous data. zh.html to ./d/2025-02-25_zh_reading_task.html
[26.02.2025 09:12] Writing Chinese reading task.
[26.02.2025 09:12] Writing result.
[26.02.2025 09:12] Renaming log file.
[26.02.2025 09:12] Renaming previous data. log.txt to ./logs/2025-02-26_last_log.txt
