[26.02.2025 08:14] Read previous papers.
[26.02.2025 08:14] Generating top page (month).
[26.02.2025 08:14] Writing top page (month).
[26.02.2025 09:11] Read previous papers.
[26.02.2025 09:11] Get feed.
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18411
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18137
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17363
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17262
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18449
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15499
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18364
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18356
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16794
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17425
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18461
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17535
[26.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17092
[26.02.2025 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.02.2025 09:11] No deleted papers detected.
[26.02.2025 09:11] Downloading and parsing papers (pdf, html). Total: 13.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18411.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.18411.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.18411.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18137.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.18137.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.18137.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.17363.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.17363.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.17363.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.17262.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.17262.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.17262.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18449.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.18449.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.18449.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.15499.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.15499.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.15499.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18364.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.18364.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.18364.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18356.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.18356.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.18356.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.16794.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.16794.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.16794.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.17425.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.17425.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.17425.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18461.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.18461.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.18461.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.17535.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.17535.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.17535.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.17092.
[26.02.2025 09:11] Extra JSON file exists (./assets/json/2502.17092.json), skip PDF parsing.
[26.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.17092.json), skip HTML parsing.
[26.02.2025 09:11] Success.
[26.02.2025 09:11] Enriching papers with extra data.
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 0. Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featur...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 1. An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the s...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 2. Background consistency remains a significant challenge in image editing tasks. Despite extensive developments, existing works still face a trade-off between maintaining similarity to the original image and generating content that aligns with the target. Here, we propose KV-Edit, a training-free appr...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 3. The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) t...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 4. The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, thi...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 5. Training stability is a persistent challenge in the pre-training of large language models (LLMs), particularly for architectures such as Post-Norm Transformers, which are prone to gradient explosion and dissipation. In this paper, we propose Scale-Distribution Decoupling (SDD), a novel approach that...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 6. Multi-layer image generation is a fundamental task that enables users to isolate, select, and edit specific image layers, thereby revolutionizing interactions with generative models. In this paper, we introduce the Anonymous Region Transformer (ART), which facilitates the direct generation of variab...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 7. We introduce WebGames, a comprehensive benchmark suite designed to evaluate general-purpose web-browsing AI agents through a collection of 50+ interactive challenges. These challenges are specifically crafted to be straightforward for humans while systematically testing the limitations of current AI...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 8. Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existi...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 9. To utilize visual information, Multimodal Large Language Model (MLLM) relies on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. However, MLLM stil...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 10. Recent studies have explored combining different LoRAs to jointly generate learned style and content. However, existing methods either fail to effectively preserve both the original subject and style simultaneously or require additional training. In this paper, we argue that the intrinsic properties...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 11. Motivated by reducing the computational and storage costs of LLMs, model compression and KV cache compression have attracted much attention from researchers. However, current methods predominantly emphasize maintaining the performance of compressed LLMs, as measured by perplexity or simple accuracy ...
[26.02.2025 09:11] ********************************************************************************
[26.02.2025 09:11] Abstract 12. We introduce Shakti VLM, a family of vision-language models in the capacity of 1B and 4B parameters designed to address data efficiency challenges in multimodal learning. While recent VLMs achieve strong performance through extensive training data, Shakti models leverage architectural innovations to...
[26.02.2025 09:11] Read previous papers.
[26.02.2025 09:11] Generating reviews via LLM API.
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#alignment", "#training", "#benchmark", "#multimodal", "#dataset", "#open_source"], "emoji": "ğŸ¤–", "ru": {"title": "ĞŸÑ€Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OmniAlign-V - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… 
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#architecture", "#inference", "#video", "#optimization", "#cv"], "emoji": "ğŸš€", "ru": {"title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SpargeAttn Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¾Ğ½
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#training", "#cv"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "KV-Edit: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾Ğ½Ğ°", "desc": "KV-Edit - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ KV-ĞºÑÑˆĞ° Ğ² DiT-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#training", "#benchmark"], "emoji": "ğŸ”®", "ru": {"title": "ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#dataset", "#math", "#open_source"], "emoji": "ğŸ§ ", "ru": {"title": "SWE-RL: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ", "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SWE-RL - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "ğŸš€", "ru": {"title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Scale-Distribution Decoupling (SDD) Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğµ
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#multimodal", "#cv"], "emoji": "ğŸ­", "ru": {"title": "ART: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Anonymous Region Transformer (ART) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¾Ğ±Ğ»Ğ°Ñ
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#games", "#agents"], "emoji": "ğŸŒ", "ru": {"title": "WebGames: Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµĞ±-ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…", "desc": "WebGames - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ±Ñ€Ğ°ÑƒĞ·Ğ¸Ğ½Ğ³Ğ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#alignment", "#audio"], "emoji": "ğŸ‘‚", "ru": {"title": "Ğ¡Ğ»ÑƒÑˆĞ°Ñ‚ÑŒ ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº: Ğ˜Ğ˜ Ñ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#architecture", "#cv"], "emoji": "ğŸ‘ï¸", "ru": {"title": "Ğ¢Ğ¾ĞºĞµĞ½Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜", "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¢Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ’Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (ML
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#training"], "emoji": "ğŸ¨", "ru": {"title": "K-LoRA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ K-LoRA Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LoRA (Low-Rank Adaptation) Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#inference", "#optimization", "#reasoning", "#rag", "#training"], "emoji": "ğŸŸï¸", "ru": {"title": "Ğ›Ğ¾Ñ‚ĞµÑ€ĞµĞ¹Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ Ğ»Ğ¾Ñ‚ĞµÑ€ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸
[26.02.2025 09:11] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#reasoning", "#architecture", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ»Ğ¸Ğ½ĞµĞ¹ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Shakti VLM Ñ 1B Ğ¸ 4B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚
[26.02.2025 09:11] Trying to get texts in Chinese.
[26.02.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs' alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs' alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at https://github.com/PhoenixZ810/OmniAlign-V.
[26.02.2025 09:12] Mistral response. {"id": "67858ea67cec4dfaada31af6dd250b5c", "object": "chat.completion", "created": 1740561117, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6700\u8fd1\u7684\u5f00\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e3b\u8981\u96c6\u4e2d\u5728\u589e\u5f3a\u57fa\u7840\u80fd\u529b\uff0c\u4f46\u5728\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u5dee\u8ddd\u3002\u672c\u6587\u4ecb\u7ecd\u4e86OmniAlign-V\uff0c\u4e00\u4e2a\u5305\u542b20\u4e07\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6837\u672c\u7684\u5168\u9762\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u6837\u5316\u56fe\u50cf\u3001\u590d\u6742\u95ee\u9898\u548c\u591a\u79cd\u56de\u7b54\u683c\u5f0f\uff0c\u4ee5\u63d0\u5347MLLMs\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u3002\u6211\u4eec\u8fd8\u63a8\u51fa\u4e86MM-AlignBench\uff0c\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u8bc4\u4f30MLLMs\u4e0e\u4eba\u7c7b\u4ef7\u503c\u5bf9\u9f50\u7684\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u6216\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5bf9MLLMs\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\uff0c\u540c\u65f6\u5728\u6807\u51c6VQA\u57fa\u51c6\u4e0a\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\uff0c\u4fdd\u7559\u5176\u57fa\u672c\u80fd\u529b\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6\u3001\u4ee3\u7801\u548c\u68c0\u67e5\u70b9\u5df2\u53d1\u5e03\u5728https://github.com/PhoenixZ810/OmniAlign-V\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 242, "total_tokens": 517, "completion_tokens": 275}}
[26.02.2025 09:12] Response: æœ€è¿‘çš„å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦é›†ä¸­åœ¨å¢å¼ºåŸºç¡€èƒ½åŠ›ï¼Œä½†åœ¨ä¸äººç±»åå¥½å¯¹é½æ–¹é¢ä»æœ‰å¾ˆå¤§å·®è·ã€‚æœ¬æ–‡ä»‹ç»äº†OmniAlign-Vï¼Œä¸€ä¸ªåŒ…å«20ä¸‡é«˜è´¨é‡è®­ç»ƒæ ·æœ¬çš„å…¨é¢æ•°æ®é›†ï¼Œæ¶µç›–å¤šæ ·åŒ–å›¾åƒã€å¤æ‚é—®é¢˜å’Œå¤šç§å›ç­”æ ¼å¼ï¼Œä»¥æå‡MLLMsä¸äººç±»åå¥½çš„å¯¹é½ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†MM-AlignBenchï¼Œä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMsä¸äººç±»ä»·å€¼å¯¹é½çš„äººå·¥æ ‡æ³¨åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†äººç±»åå¥½å¯¹é½ï¼ŒåŒæ—¶åœ¨æ ‡å‡†VQAåŸºå‡†ä¸Šä¿æŒæˆ–æå‡æ€§èƒ½ï¼Œä¿ç•™å…¶åŸºæœ¬èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€åŸºå‡†ã€ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨https://github.com/PhoenixZ810/OmniAlign-Vã€‚
[26.02.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

æœ€è¿‘çš„å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦é›†ä¸­åœ¨å¢å¼ºåŸºç¡€èƒ½åŠ›ï¼Œä½†åœ¨ä¸äººç±»åå¥½å¯¹é½æ–¹é¢ä»æœ‰å¾ˆå¤§å·®è·ã€‚æœ¬æ–‡ä»‹ç»äº†OmniAlign-Vï¼Œä¸€ä¸ªåŒ…å«20ä¸‡é«˜è´¨é‡è®­ç»ƒæ ·æœ¬çš„å…¨é¢æ•°æ®é›†ï¼Œæ¶µç›–å¤šæ ·åŒ–å›¾åƒã€å¤æ‚é—®é¢˜å’Œå¤šç§å›ç­”æ ¼å¼ï¼Œä»¥æå‡MLLMsä¸äººç±»åå¥½çš„å¯¹é½ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†MM-AlignBenchï¼Œä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMsä¸äººç±»ä»·å€¼å¯¹é½çš„äººå·¥æ ‡æ³¨åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†äººç±»åå¥½å¯¹é½ï¼ŒåŒæ—¶åœ¨æ ‡å‡†VQAåŸºå‡†ä¸Šä¿æŒæˆ–æå‡æ€§èƒ½ï¼Œä¿ç•™å…¶åŸºæœ¬èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€åŸºå‡†ã€ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨https://github.com/PhoenixZ810/OmniAlign-Vã€‚
[26.02.2025 09:12] Mistral response. {"id": "cb4094ac1d444e78ab26e83b293e960c", "object": "chat.completion", "created": 1740561123, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6700\u8fd1\u7684\u5f00\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e3b\u8981\u96c6\u4e2d\u5728\u589e\u5f3a\u57fa\u7840\u80fd\u529b\uff0c\u4f46\u5728\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u5dee\u8ddd\u3002\u672c\u6587\u4ecb\u7ecd\u4e86OmniAlign-V\uff0c\u4e00\u4e2a\u5305\u542b20\u4e07\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6837\u672c\u7684\u5168\u9762\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u6837\u5316\u56fe\u50cf\u3001\u590d\u6742\u95ee\u9898\u548c\u591a\u79cd\u56de\u7b54\u683c\u5f0f\uff0c\u4ee5\u63d0\u5347MLLMs\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u3002\u6211\u4eec\u8fd8\u63a8\u51fa\u4e86MM-AlignBench\uff0c\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u8bc4\u4f30MLLMs\u4e0e\u4eba\u7c7b\u4ef7\u503c\u5bf9\u9f50\u7684\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u6216\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5bf9MLLMs\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\uff0c\u540c\u65f6\u5728\u6807\u51c6VQA\u57fa\u51c6\u4e0a\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\uff0c\u4fdd\u7559\u5176\u57fa\u672c\u80fd\u529b\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6\u3001\u4ee3\u7801\u548c\u68c0\u67e5\u70b9\u5df2\u53d1\u5e03\u5728https://github.com/PhoenixZ810/OmniAlign-V\u3002\n\nZu\u00ecj\u00ecn de k\u0101iyu\u01cen du\u014d m\u00f3sh\u00ec d\u00e0 y\u01d4y\u00e1n m\u00f3x\u00edng (MLLMs) zh\u01d4y\u00e0o j\u00edzh\u014dng z\u00e0i z\u0113ngqi\u00e1ng j\u012bch\u01d4 n\u00e9ngl\u00ec, d\u00e0n z\u00e0i y\u01d4 r\u00e9nl\u00e8i pi\u0101nh\u01ceo du\u00ecq\u00ed f\u0101ngmi\u00e0n r\u00e9ng y\u01d2u h\u011bn d\u00e0 ch\u0101j\u00f9. B\u011bnw\u00e9n ji\u00e8sh\u00e0o le OmniAlign-V, y\u012bg\u00e8 b\u0101oh\u00e1n 20 w\u00e0n g\u0101o zh\u00ecli\u00e0ng x\u00f9nli\u00e0n y\u00e0ngb\u011bn de qu\u00e1nmi\u00e0n sh\u00f9j\u00f9j\u00ed, h\u00e1nji\u0113 du\u014dy\u00e0nghu\u00e0 t\u00faxi\u00e0ng, f\u00f9z\u00e1 xu\u00e1nzh\u00f2ng w\u00e8nt\u00ed h\u00e9 du\u014dzh\u01d2ng hu\u00edd\u00e1 g\u00e9sh\u00ec, y\u01d0 t\u00edsh\u0113ng MLLMs y\u01d4 r\u00e9nl\u00e8i pi\u0101nh\u01ceo de du\u00ecq\u00ed. W\u01d2men h\u00e1i tu\u012bch\u016b le MM-AlignBench, y\u012bg\u00e8 zhu\u0101nm\u00e9n sh\u00e8j\u00ec y\u00f2ngy\u00fa p\u00edngg\u01d4 MLLMs y\u01d4 r\u00e9nl\u00e8i ji\u00e0zh\u00ed du\u00ecq\u00ed de r\u00e9ng\u014dng bi\u0101ozh\u00f9 j\u012bzh\u01d4n. Sh\u00edy\u00e0n ji\u00e9gu\u01d2 bi\u01ceom\u00edng, sh\u01d0y\u00f2ng ji\u00e0nd\u016b w\u0113iti\u00e1o (SFT) hu\u00f2 zh\u00edji\u0113 pi\u0101nh\u01ceo y\u014duhu\u00e0 (DPO) du\u00ec MLLMs j\u00ecnx\u00edng w\u0113iti\u00e1o, xi\u01cenzh\u00f9 t\u00edg\u0101o le r\u00e9nl\u00e8i pi\u0101nh\u01ceo du\u00ecq\u00ed, t\u00f3ngsh\u00ed z\u00e0i bi\u0101ozh\u01d4n VQA j\u012bzh\u01d4n sh\u00e0ng b\u01ceoch\u00ed hu\u00f2 t\u00edsh\u0113ng x\u00ecngn\u00e9ng, b\u01ceoli\u00fa q\u00ed j\u012bb\u011bn n\u00e9ngl\u00ec. W\u01d2men de sh\u00f9j\u00f9j\u00ed, j\u012bzh\u01d4n, d\u00e0im\u01ce h\u00e9 ji\u01cench\u00e1di\u01cen y\u01d0 f\u0101b\u00f9 z\u00e0i https://github.com/PhoenixZ810/OmniAlign-V."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 290, "total_tokens": 1106, "completion_tokens": 816}}
[26.02.2025 09:12] Response: æœ€è¿‘çš„å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦é›†ä¸­åœ¨å¢å¼ºåŸºç¡€èƒ½åŠ›ï¼Œä½†åœ¨ä¸äººç±»åå¥½å¯¹é½æ–¹é¢ä»æœ‰å¾ˆå¤§å·®è·ã€‚æœ¬æ–‡ä»‹ç»äº†OmniAlign-Vï¼Œä¸€ä¸ªåŒ…å«20ä¸‡é«˜è´¨é‡è®­ç»ƒæ ·æœ¬çš„å…¨é¢æ•°æ®é›†ï¼Œæ¶µç›–å¤šæ ·åŒ–å›¾åƒã€å¤æ‚é—®é¢˜å’Œå¤šç§å›ç­”æ ¼å¼ï¼Œä»¥æå‡MLLMsä¸äººç±»åå¥½çš„å¯¹é½ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†MM-AlignBenchï¼Œä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMsä¸äººç±»ä»·å€¼å¯¹é½çš„äººå·¥æ ‡æ³¨åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†äººç±»åå¥½å¯¹é½ï¼ŒåŒæ—¶åœ¨æ ‡å‡†VQAåŸºå‡†ä¸Šä¿æŒæˆ–æå‡æ€§èƒ½ï¼Œä¿ç•™å…¶åŸºæœ¬èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€åŸºå‡†ã€ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨https://github.com/PhoenixZ810/OmniAlign-Vã€‚

ZuÃ¬jÃ¬n de kÄiyuÇn duÅ mÃ³shÃ¬ dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (MLLMs) zhÇ”yÃ o jÃ­zhÅng zÃ i zÄ“ngqiÃ¡ng jÄ«chÇ” nÃ©nglÃ¬, dÃ n zÃ i yÇ” rÃ©nlÃ¨i piÄnhÇo duÃ¬qÃ­ fÄngmiÃ n rÃ©ng yÇ’u hÄ›n dÃ  chÄjÃ¹. BÄ›nwÃ©n jiÃ¨shÃ o le OmniAlign-V, yÄ«gÃ¨ bÄohÃ¡n 20 wÃ n gÄo zhÃ¬liÃ ng xÃ¹nliÃ n yÃ ngbÄ›n de quÃ¡nmiÃ n shÃ¹jÃ¹jÃ­, hÃ¡njiÄ“ duÅyÃ nghuÃ  tÃºxiÃ ng, fÃ¹zÃ¡ xuÃ¡nzhÃ²ng wÃ¨ntÃ­ hÃ© duÅzhÇ’ng huÃ­dÃ¡ gÃ©shÃ¬, yÇ tÃ­shÄ“ng MLLMs yÇ” rÃ©nlÃ¨i piÄnhÇo de duÃ¬qÃ­. WÇ’men hÃ¡i tuÄ«chÅ« le MM-AlignBench, yÄ«gÃ¨ zhuÄnmÃ©n shÃ¨jÃ¬ yÃ²ngyÃº pÃ­nggÇ” MLLMs yÇ” rÃ©nlÃ¨i jiÃ zhÃ­ duÃ¬qÃ­ de rÃ©ngÅng biÄozhÃ¹ jÄ«zhÇ”n. ShÃ­yÃ n jiÃ©guÇ’ biÇomÃ­ng, shÇyÃ²ng jiÃ ndÅ« wÄ“itiÃ¡o (SFT) huÃ² zhÃ­jiÄ“ piÄnhÇo yÅuhuÃ  (DPO) duÃ¬ MLLMs jÃ¬nxÃ­ng wÄ“itiÃ¡o, xiÇnzhÃ¹ tÃ­gÄo le rÃ©nlÃ¨i piÄnhÇo duÃ¬qÃ­, tÃ³ngshÃ­ zÃ i biÄozhÇ”n VQA jÄ«zhÇ”n shÃ ng bÇochÃ­ huÃ² tÃ­shÄ“ng xÃ¬ngnÃ©ng, bÇoliÃº qÃ­ jÄ«bÄ›n nÃ©nglÃ¬. WÇ’men de shÃ¹jÃ¹jÃ­, jÄ«zhÇ”n, dÃ imÇ hÃ© jiÇnchÃ¡diÇn yÇ fÄbÃ¹ zÃ i https://github.com/PhoenixZ810/OmniAlign-V.
[26.02.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

æœ€è¿‘çš„å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦é›†ä¸­åœ¨å¢å¼ºåŸºç¡€èƒ½åŠ›ï¼Œä½†åœ¨ä¸äººç±»åå¥½å¯¹é½æ–¹é¢ä»æœ‰å¾ˆå¤§å·®è·ã€‚æœ¬æ–‡ä»‹ç»äº†OmniAlign-Vï¼Œä¸€ä¸ªåŒ…å«20ä¸‡é«˜è´¨é‡è®­ç»ƒæ ·æœ¬çš„å…¨é¢æ•°æ®é›†ï¼Œæ¶µç›–å¤šæ ·åŒ–å›¾åƒã€å¤æ‚é—®é¢˜å’Œå¤šç§å›ç­”æ ¼å¼ï¼Œä»¥æå‡MLLMsä¸äººç±»åå¥½çš„å¯¹é½ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†MM-AlignBenchï¼Œä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMsä¸äººç±»ä»·å€¼å¯¹é½çš„äººå·¥æ ‡æ³¨åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†äººç±»åå¥½å¯¹é½ï¼ŒåŒæ—¶åœ¨æ ‡å‡†VQAåŸºå‡†ä¸Šä¿æŒæˆ–æå‡æ€§èƒ½ï¼Œä¿ç•™å…¶åŸºæœ¬èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€åŸºå‡†ã€ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨https://github.com/PhoenixZ810/OmniAlign-Vã€‚
[26.02.2025 09:12] Mistral response. {"id": "2e4ccd4f38e94725bf2888936b02b86b", "object": "chat.completion", "created": 1740561138, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u591a\u6a21\u6001\", \"pinyin\": \"du\u014d m\u00f3 t\u00e0i\", \"trans\": \"multimodal\"},\n    {\"word\": \"\u5927\u8bed\u8a00\u6a21\u578b\", \"pinyin\": \"d\u00e0 y\u01d4 y\u00e1n m\u00f3 x\u00edng\", \"trans\": \"large language model\"},\n    {\"word\": \"\u57fa\u7840\u80fd\u529b\", \"pinyin\": \"j\u012b ch\u01d4 n\u00e9ng l\u00ec\", \"trans\": \"basic capabilities\"},\n    {\"word\": \"\u5bf9\u9f50\", \"pinyin\": \"du\u00ec q\u00ed\", \"trans\": \"alignment\"},\n    {\"word\": \"\u9ad8\u8d28\u91cf\", \"pinyin\": \"g\u0101o zh\u00ec li\u00e0ng\", \"trans\": \"high quality\"},\n    {\"word\": \"\u6837\u672c\", \"pinyin\": \"y\u00e0ng b\u011bn\", \"trans\": \"sample\"},\n    {\"word\": \"\u6db5\u76d6\", \"pinyin\": \"h\u00e1n g\u00e0i\", \"trans\": \"cover\"},\n    {\"word\": \"\u591a\u6837\u5316\", \"pinyin\": \"du\u014d y\u00e0ng hu\u00e0\", \"trans\": \"diversified\"},\n    {\"word\": \"\u590d\u6742\", \"pinyin\": \"f\u00f9 z\u00e1\", \"trans\": \"complex\"},\n    {\"word\": \"\u683c\u5f0f\", \"pinyin\": \"g\u00e9 sh\u00ec\", \"trans\": \"format\"},\n    {\"word\": \"\u63d0\u5347\", \"pinyin\": \"t\u00ed sh\u0113ng\", \"trans\": \"improve\"},\n    {\"word\": \"\u4eba\u5de5\u6807\u6ce8\", \"pinyin\": \"r\u00e9n g\u014dng bi\u0101o zh\u00f9\", \"trans\": \"manual annotation\"},\n    {\"word\": \"\u57fa\u51c6\", \"pinyin\": \"j\u012b zh\u01d4n\", \"trans\": \"benchmark\"},\n    {\"word\": \"\u76d1\u7763\u5fae\u8c03\", \"pinyin\": \"ji\u00e0n d\u016b w\u0113i ti\u00e1o\", \"trans\": \"supervised fine-tuning\"},\n    {\"word\": \"\u76f4\u63a5\u504f\u597d\u4f18\u5316\", \"pinyin\": \"zh\u00ed ji\u0113 pi\u0101n h\u00e0o y\u014du hu\u00e0\", \"trans\": \"direct preference optimization\"},\n    {\"word\": \"\u663e\u8457\", \"pinyin\": \"xi\u01cen zh\u00f9\", \"trans\": \"significant\"},\n    {\"word\": \"\u4fdd\u6301\", \"pinyin\": \"b\u01ceo ch\u00ed\", \"trans\": \"maintain\"},\n    {\"word\": \"\u6807\u51c6\", \"pinyin\": \"bi\u0101o zh\u01d4n\", \"trans\": \"standard\"},\n    {\"word\": \"VQA\", \"pinyin\": \"VQA\", \"trans\": \"Visual Question Answering\"},\n    {\"word\": \"\u68c0\u67e5\u70b9\", \"pinyin\": \"ji\u01cen ch\u00e1 di\u01cen\", \"trans\": \"checkpoint\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 322, "total_tokens": 962, "completion_tokens": 640}}
[26.02.2025 09:12] Response: [
    {"word": "å¤šæ¨¡æ€", "pinyin": "duÅ mÃ³ tÃ i", "trans": "multimodal"},
    {"word": "å¤§è¯­è¨€æ¨¡å‹", "pinyin": "dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng", "trans": "large language model"},
    {"word": "åŸºç¡€èƒ½åŠ›", "pinyin": "jÄ« chÇ” nÃ©ng lÃ¬", "trans": "basic capabilities"},
    {"word": "å¯¹é½", "pinyin": "duÃ¬ qÃ­", "trans": "alignment"},
    {"word": "é«˜è´¨é‡", "pinyin": "gÄo zhÃ¬ liÃ ng", "trans": "high quality"},
    {"word": "æ ·æœ¬", "pinyin": "yÃ ng bÄ›n", "trans": "sample"},
    {"word": "æ¶µç›–", "pinyin": "hÃ¡n gÃ i", "trans": "cover"},
    {"word": "å¤šæ ·åŒ–", "pinyin": "duÅ yÃ ng huÃ ", "trans": "diversified"},
    {"word": "å¤æ‚", "pinyin": "fÃ¹ zÃ¡", "trans": "complex"},
    {"word": "æ ¼å¼", "pinyin": "gÃ© shÃ¬", "trans": "format"},
    {"word": "æå‡", "pinyin": "tÃ­ shÄ“ng", "trans": "improve"},
    {"word": "äººå·¥æ ‡æ³¨", "pinyin": "rÃ©n gÅng biÄo zhÃ¹", "trans": "manual annotation"},
    {"word": "åŸºå‡†", "pinyin": "jÄ« zhÇ”n", "trans": "benchmark"},
    {"word": "ç›‘ç£å¾®è°ƒ", "pinyin": "jiÃ n dÅ« wÄ“i tiÃ¡o", "trans": "supervised fine-tuning"},
    {"word": "ç›´æ¥åå¥½ä¼˜åŒ–", "pinyin": "zhÃ­ jiÄ“ piÄn hÃ o yÅu huÃ ", "trans": "direct preference optimization"},
    {"word": "æ˜¾è‘—", "pinyin": "xiÇn zhÃ¹", "trans": "significant"},
    {"word": "ä¿æŒ", "pinyin": "bÇo chÃ­", "trans": "maintain"},
    {"word": "æ ‡å‡†", "pinyin": "biÄo zhÇ”n", "trans": "standard"},
    {"word": "VQA", "pinyin": "VQA", "trans": "Visual Question Answering"},
    {"word": "æ£€æŸ¥ç‚¹", "pinyin": "jiÇn chÃ¡ diÇn", "trans": "checkpoint"}
]
[26.02.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

æœ€è¿‘çš„å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦é›†ä¸­åœ¨å¢å¼ºåŸºç¡€èƒ½åŠ›ï¼Œä½†åœ¨ä¸äººç±»åå¥½å¯¹é½æ–¹é¢ä»æœ‰å¾ˆå¤§å·®è·ã€‚æœ¬æ–‡ä»‹ç»äº†OmniAlign-Vï¼Œä¸€ä¸ªåŒ…å«20ä¸‡é«˜è´¨é‡è®­ç»ƒæ ·æœ¬çš„å…¨é¢æ•°æ®é›†ï¼Œæ¶µç›–å¤šæ ·åŒ–å›¾åƒã€å¤æ‚é—®é¢˜å’Œå¤šç§å›ç­”æ ¼å¼ï¼Œä»¥æå‡MLLMsä¸äººç±»åå¥½çš„å¯¹é½ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†MM-AlignBenchï¼Œä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMsä¸äººç±»ä»·å€¼å¯¹é½çš„äººå·¥æ ‡æ³¨åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†äººç±»åå¥½å¯¹é½ï¼ŒåŒæ—¶åœ¨æ ‡å‡†VQAåŸºå‡†ä¸Šä¿æŒæˆ–æå‡æ€§èƒ½ï¼Œä¿ç•™å…¶åŸºæœ¬èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€åŸºå‡†ã€ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨https://github.com/PhoenixZ810/OmniAlign-Vã€‚
[26.02.2025 09:12] Mistral response. {"id": "afe5c76052714fe19835d615b7991a4b", "object": "chat.completion", "created": 1740561151, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Recent open-source multimodal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, but there remains a significant gap in aligning with human preferences. This paper introduces OmniAlign-V, a comprehensive dataset containing 200,000 high-quality training samples that cover diverse images, complex questions, and various answer formats to improve the alignment of MLLMs with human preferences. We also present MM-AlignBench, a specially designed, human-annotated benchmark for evaluating the alignment of MLLMs with human values. Experimental results demonstrate that fine-tuning MLLMs using supervised fine-tuning (SFT) or direct preference optimization (DPO) significantly improves human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our dataset, benchmark, code, and checkpoints are available at https://github.com/PhoenixZ810/OmniAlign-V."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 288, "total_tokens": 504, "completion_tokens": 216}}
[26.02.2025 09:12] Response: Recent open-source multimodal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, but there remains a significant gap in aligning with human preferences. This paper introduces OmniAlign-V, a comprehensive dataset containing 200,000 high-quality training samples that cover diverse images, complex questions, and various answer formats to improve the alignment of MLLMs with human preferences. We also present MM-AlignBench, a specially designed, human-annotated benchmark for evaluating the alignment of MLLMs with human values. Experimental results demonstrate that fine-tuning MLLMs using supervised fine-tuning (SFT) or direct preference optimization (DPO) significantly improves human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our dataset, benchmark, code, and checkpoints are available at https://github.com/PhoenixZ810/OmniAlign-V.
[26.02.2025 09:12] Renaming data file.
[26.02.2025 09:12] Renaming previous data. hf_papers.json to ./d/2025-02-26.json
[26.02.2025 09:12] Saving new data file.
[26.02.2025 09:12] Generating page.
[26.02.2025 09:12] Renaming previous page.
[26.02.2025 09:12] Renaming previous data. index.html to ./d/2025-02-26.html
[26.02.2025 09:12] [Experimental] Generating Chinese page for reading.
[26.02.2025 09:12] Chinese vocab [{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'}, {'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'}, {'word': 'åŸºç¡€èƒ½åŠ›', 'pinyin': 'jÄ« chÇ” nÃ©ng lÃ¬', 'trans': 'basic capabilities'}, {'word': 'å¯¹é½', 'pinyin': 'duÃ¬ qÃ­', 'trans': 'alignment'}, {'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬ liÃ ng', 'trans': 'high quality'}, {'word': 'æ ·æœ¬', 'pinyin': 'yÃ ng bÄ›n', 'trans': 'sample'}, {'word': 'æ¶µç›–', 'pinyin': 'hÃ¡n gÃ i', 'trans': 'cover'}, {'word': 'å¤šæ ·åŒ–', 'pinyin': 'duÅ yÃ ng huÃ ', 'trans': 'diversified'}, {'word': 'å¤æ‚', 'pinyin': 'fÃ¹ zÃ¡', 'trans': 'complex'}, {'word': 'æ ¼å¼', 'pinyin': 'gÃ© shÃ¬', 'trans': 'format'}, {'word': 'æå‡', 'pinyin': 'tÃ­ shÄ“ng', 'trans': 'improve'}, {'word': 'äººå·¥æ ‡æ³¨', 'pinyin': 'rÃ©n gÅng biÄo zhÃ¹', 'trans': 'manual annotation'}, {'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'}, {'word': 'ç›‘ç£å¾®è°ƒ', 'pinyin': 'jiÃ n dÅ« wÄ“i tiÃ¡o', 'trans': 'supervised fine-tuning'}, {'word': 'ç›´æ¥åå¥½ä¼˜åŒ–', 'pinyin': 'zhÃ­ jiÄ“ piÄn hÃ o yÅu huÃ ', 'trans': 'direct preference optimization'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'}, {'word': 'ä¿æŒ', 'pinyin': 'bÇo chÃ­', 'trans': 'maintain'}, {'word': 'æ ‡å‡†', 'pinyin': 'biÄo zhÇ”n', 'trans': 'standard'}, {'word': 'VQA', 'pinyin': 'VQA', 'trans': 'Visual Question Answering'}, {'word': 'æ£€æŸ¥ç‚¹', 'pinyin': 'jiÇn chÃ¡ diÇn', 'trans': 'checkpoint'}]
[26.02.2025 09:12] Renaming previous Chinese page.
[26.02.2025 09:12] Renaming previous data. zh.html to ./d/2025-02-25_zh_reading_task.html
[26.02.2025 09:12] Writing Chinese reading task.
[26.02.2025 09:12] Writing result.
[26.02.2025 09:12] Renaming log file.
[26.02.2025 09:12] Renaming previous data. log.txt to ./logs/2025-02-26_last_log.txt
