[26.02.2025 12:19] Read previous papers.
[26.02.2025 12:19] Generating top page (month).
[26.02.2025 12:19] Writing top page (month).
[26.02.2025 13:19] Read previous papers.
[26.02.2025 13:19] Get feed.
[26.02.2025 13:19] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18411
[26.02.2025 13:19] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18137
[26.02.2025 13:19] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17363
[26.02.2025 13:19] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18364
[26.02.2025 13:19] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18449
[26.02.2025 13:19] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17262
[26.02.2025 13:19] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15499
[26.02.2025 13:19] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18461
[26.02.2025 13:19] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18356
[26.02.2025 13:19] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17425
[26.02.2025 13:19] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17535
[26.02.2025 13:19] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16794
[26.02.2025 13:19] Extract page data from URL. URL: https://huggingface.co/papers/2502.15612
[26.02.2025 13:19] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17092
[26.02.2025 13:19] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.02.2025 13:19] No deleted papers detected.
[26.02.2025 13:19] Downloading and parsing papers (pdf, html). Total: 14.
[26.02.2025 13:19] Downloading and parsing paper https://huggingface.co/papers/2502.18411.
[26.02.2025 13:19] Extra JSON file exists (./assets/json/2502.18411.json), skip PDF parsing.
[26.02.2025 13:19] Paper image links file exists (./assets/img_data/2502.18411.json), skip HTML parsing.
[26.02.2025 13:19] Success.
[26.02.2025 13:19] Downloading and parsing paper https://huggingface.co/papers/2502.18137.
[26.02.2025 13:19] Extra JSON file exists (./assets/json/2502.18137.json), skip PDF parsing.
[26.02.2025 13:19] Paper image links file exists (./assets/img_data/2502.18137.json), skip HTML parsing.
[26.02.2025 13:19] Success.
[26.02.2025 13:19] Downloading and parsing paper https://huggingface.co/papers/2502.17363.
[26.02.2025 13:19] Extra JSON file exists (./assets/json/2502.17363.json), skip PDF parsing.
[26.02.2025 13:19] Paper image links file exists (./assets/img_data/2502.17363.json), skip HTML parsing.
[26.02.2025 13:19] Success.
[26.02.2025 13:19] Downloading and parsing paper https://huggingface.co/papers/2502.18364.
[26.02.2025 13:19] Extra JSON file exists (./assets/json/2502.18364.json), skip PDF parsing.
[26.02.2025 13:19] Paper image links file exists (./assets/img_data/2502.18364.json), skip HTML parsing.
[26.02.2025 13:19] Success.
[26.02.2025 13:19] Downloading and parsing paper https://huggingface.co/papers/2502.18449.
[26.02.2025 13:19] Extra JSON file exists (./assets/json/2502.18449.json), skip PDF parsing.
[26.02.2025 13:19] Paper image links file exists (./assets/img_data/2502.18449.json), skip HTML parsing.
[26.02.2025 13:19] Success.
[26.02.2025 13:19] Downloading and parsing paper https://huggingface.co/papers/2502.17262.
[26.02.2025 13:19] Extra JSON file exists (./assets/json/2502.17262.json), skip PDF parsing.
[26.02.2025 13:19] Paper image links file exists (./assets/img_data/2502.17262.json), skip HTML parsing.
[26.02.2025 13:19] Success.
[26.02.2025 13:19] Downloading and parsing paper https://huggingface.co/papers/2502.15499.
[26.02.2025 13:19] Extra JSON file exists (./assets/json/2502.15499.json), skip PDF parsing.
[26.02.2025 13:19] Paper image links file exists (./assets/img_data/2502.15499.json), skip HTML parsing.
[26.02.2025 13:19] Success.
[26.02.2025 13:19] Downloading and parsing paper https://huggingface.co/papers/2502.18461.
[26.02.2025 13:19] Extra JSON file exists (./assets/json/2502.18461.json), skip PDF parsing.
[26.02.2025 13:19] Paper image links file exists (./assets/img_data/2502.18461.json), skip HTML parsing.
[26.02.2025 13:19] Success.
[26.02.2025 13:19] Downloading and parsing paper https://huggingface.co/papers/2502.18356.
[26.02.2025 13:19] Extra JSON file exists (./assets/json/2502.18356.json), skip PDF parsing.
[26.02.2025 13:19] Paper image links file exists (./assets/img_data/2502.18356.json), skip HTML parsing.
[26.02.2025 13:19] Success.
[26.02.2025 13:19] Downloading and parsing paper https://huggingface.co/papers/2502.17425.
[26.02.2025 13:19] Extra JSON file exists (./assets/json/2502.17425.json), skip PDF parsing.
[26.02.2025 13:19] Paper image links file exists (./assets/img_data/2502.17425.json), skip HTML parsing.
[26.02.2025 13:19] Success.
[26.02.2025 13:19] Downloading and parsing paper https://huggingface.co/papers/2502.17535.
[26.02.2025 13:19] Extra JSON file exists (./assets/json/2502.17535.json), skip PDF parsing.
[26.02.2025 13:19] Paper image links file exists (./assets/img_data/2502.17535.json), skip HTML parsing.
[26.02.2025 13:19] Success.
[26.02.2025 13:19] Downloading and parsing paper https://huggingface.co/papers/2502.16794.
[26.02.2025 13:19] Extra JSON file exists (./assets/json/2502.16794.json), skip PDF parsing.
[26.02.2025 13:19] Paper image links file exists (./assets/img_data/2502.16794.json), skip HTML parsing.
[26.02.2025 13:19] Success.
[26.02.2025 13:19] Downloading and parsing paper https://huggingface.co/papers/2502.15612.
[26.02.2025 13:19] Downloading paper 2502.15612 from http://arxiv.org/pdf/2502.15612v2...
[26.02.2025 13:19] Extracting affiliations from text.
[26.02.2025 13:19] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LATIM: Measuring Latent Token-to-Token Interactions in Mamba Models Hugo Pitorro, Marcos Treviso Instituto de TelecomunicaÃ§Ãµes, Lisbon hugo.pitorro@gmail.com 5 2 0 2 4 2 ] . [ 2 2 1 6 5 1 . 2 0 5 2 : r a "
[26.02.2025 13:19] Response: ```python
["Instituto de TelecomunicaÃ§Ãµes, Lisbon"]
```
[26.02.2025 13:19] Deleting PDF ./assets/pdf/2502.15612.pdf.
[26.02.2025 13:19] Success.
[26.02.2025 13:19] Downloading and parsing paper https://huggingface.co/papers/2502.17092.
[26.02.2025 13:19] Extra JSON file exists (./assets/json/2502.17092.json), skip PDF parsing.
[26.02.2025 13:19] Paper image links file exists (./assets/img_data/2502.17092.json), skip HTML parsing.
[26.02.2025 13:19] Success.
[26.02.2025 13:19] Enriching papers with extra data.
[26.02.2025 13:19] ********************************************************************************
[26.02.2025 13:19] Abstract 0. Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featur...
[26.02.2025 13:19] ********************************************************************************
[26.02.2025 13:19] Abstract 1. An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the s...
[26.02.2025 13:19] ********************************************************************************
[26.02.2025 13:19] Abstract 2. Background consistency remains a significant challenge in image editing tasks. Despite extensive developments, existing works still face a trade-off between maintaining similarity to the original image and generating content that aligns with the target. Here, we propose KV-Edit, a training-free appr...
[26.02.2025 13:19] ********************************************************************************
[26.02.2025 13:19] Abstract 3. Multi-layer image generation is a fundamental task that enables users to isolate, select, and edit specific image layers, thereby revolutionizing interactions with generative models. In this paper, we introduce the Anonymous Region Transformer (ART), which facilitates the direct generation of variab...
[26.02.2025 13:19] ********************************************************************************
[26.02.2025 13:19] Abstract 4. The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, thi...
[26.02.2025 13:19] ********************************************************************************
[26.02.2025 13:19] Abstract 5. The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) t...
[26.02.2025 13:19] ********************************************************************************
[26.02.2025 13:19] Abstract 6. Training stability is a persistent challenge in the pre-training of large language models (LLMs), particularly for architectures such as Post-Norm Transformers, which are prone to gradient explosion and dissipation. In this paper, we propose Scale-Distribution Decoupling (SDD), a novel approach that...
[26.02.2025 13:19] ********************************************************************************
[26.02.2025 13:19] Abstract 7. Recent studies have explored combining different LoRAs to jointly generate learned style and content. However, existing methods either fail to effectively preserve both the original subject and style simultaneously or require additional training. In this paper, we argue that the intrinsic properties...
[26.02.2025 13:19] ********************************************************************************
[26.02.2025 13:19] Abstract 8. We introduce WebGames, a comprehensive benchmark suite designed to evaluate general-purpose web-browsing AI agents through a collection of 50+ interactive challenges. These challenges are specifically crafted to be straightforward for humans while systematically testing the limitations of current AI...
[26.02.2025 13:19] ********************************************************************************
[26.02.2025 13:19] Abstract 9. To utilize visual information, Multimodal Large Language Model (MLLM) relies on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. However, MLLM stil...
[26.02.2025 13:19] ********************************************************************************
[26.02.2025 13:19] Abstract 10. Motivated by reducing the computational and storage costs of LLMs, model compression and KV cache compression have attracted much attention from researchers. However, current methods predominantly emphasize maintaining the performance of compressed LLMs, as measured by perplexity or simple accuracy ...
[26.02.2025 13:19] ********************************************************************************
[26.02.2025 13:19] Abstract 11. Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existi...
[26.02.2025 13:19] ********************************************************************************
[26.02.2025 13:19] Abstract 12. State space models (SSMs), such as Mamba, have emerged as an efficient alternative to transformers for long-context sequence modeling. However, despite their growing adoption, SSMs lack the interpretability tools that have been crucial for understanding and improving attention-based architectures. W...
[26.02.2025 13:19] ********************************************************************************
[26.02.2025 13:19] Abstract 13. We introduce Shakti VLM, a family of vision-language models in the capacity of 1B and 4B parameters designed to address data efficiency challenges in multimodal learning. While recent VLMs achieve strong performance through extensive training data, Shakti models leverage architectural innovations to...
[26.02.2025 13:19] Read previous papers.
[26.02.2025 13:19] Generating reviews via LLM API.
[26.02.2025 13:19] Using data from previous issue: {"categories": ["#alignment", "#training", "#benchmark", "#multimodal", "#dataset", "#open_source"], "emoji": "ğŸ¤–", "ru": {"title": "ĞŸÑ€Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OmniAlign-V - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… 
[26.02.2025 13:19] Using data from previous issue: {"categories": ["#architecture", "#inference", "#video", "#optimization", "#cv"], "emoji": "ğŸš€", "ru": {"title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SpargeAttn Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¾Ğ½
[26.02.2025 13:19] Using data from previous issue: {"categories": ["#training", "#cv"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "KV-Edit: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾Ğ½Ğ°", "desc": "KV-Edit - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ KV-ĞºÑÑˆĞ° Ğ² DiT-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸
[26.02.2025 13:19] Using data from previous issue: {"categories": ["#multimodal", "#cv"], "emoji": "ğŸ­", "ru": {"title": "ART: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Anonymous Region Transformer (ART) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¾Ğ±Ğ»Ğ°Ñ
[26.02.2025 13:19] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#dataset", "#math", "#open_source"], "emoji": "ğŸ§ ", "ru": {"title": "SWE-RL: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ", "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SWE-RL - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»
[26.02.2025 13:19] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#training", "#benchmark"], "emoji": "ğŸ”®", "ru": {"title": "ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°
[26.02.2025 13:19] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "ğŸš€", "ru": {"title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Scale-Distribution Decoupling (SDD) Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğµ
[26.02.2025 13:19] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#training"], "emoji": "ğŸ¨", "ru": {"title": "K-LoRA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ K-LoRA Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LoRA (Low-Rank Adaptation) Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½
[26.02.2025 13:19] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#games", "#agents"], "emoji": "ğŸŒ", "ru": {"title": "WebGames: Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµĞ±-ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…", "desc": "WebGames - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ±Ñ€Ğ°ÑƒĞ·Ğ¸Ğ½Ğ³Ğ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»
[26.02.2025 13:19] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#architecture", "#cv"], "emoji": "ğŸ‘ï¸", "ru": {"title": "Ğ¢Ğ¾ĞºĞµĞ½Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜", "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¢Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ’Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (ML
[26.02.2025 13:19] Using data from previous issue: {"categories": ["#inference", "#optimization", "#reasoning", "#rag", "#training"], "emoji": "ğŸŸï¸", "ru": {"title": "Ğ›Ğ¾Ñ‚ĞµÑ€ĞµĞ¹Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ Ğ»Ğ¾Ñ‚ĞµÑ€ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸
[26.02.2025 13:19] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#alignment", "#audio"], "emoji": "ğŸ‘‚", "ru": {"title": "Ğ¡Ğ»ÑƒÑˆĞ°Ñ‚ÑŒ ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº: Ğ˜Ğ˜ Ñ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ
[26.02.2025 13:19] Querying the API.
[26.02.2025 13:19] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

State space models (SSMs), such as Mamba, have emerged as an efficient alternative to transformers for long-context sequence modeling. However, despite their growing adoption, SSMs lack the interpretability tools that have been crucial for understanding and improving attention-based architectures. While recent efforts provide insights into Mamba's internal mechanisms, they do not explicitly decompose token-wise contributions, leaving gaps in understanding how Mamba selectively processes sequences across layers. In this work, we introduce LaTIM, a novel token-level decomposition method for both Mamba-1 and Mamba-2 that enables fine-grained interpretability. We extensively evaluate our method across diverse tasks, including machine translation, copying, and retrieval-based generation, demonstrating its effectiveness in revealing Mamba's token-to-token interaction patterns.
[26.02.2025 13:19] Response: {
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ (SSM), Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Mamba, Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ LaTIM - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Mamba-1 Ğ¸ Mamba-2. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ LaTIM Ğ² Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Mamba.",
  "emoji": "ğŸ”",
  "title": "Ğ—Ğ°Ğ³Ğ»ÑĞ½ÑƒÑ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ÑŒ Mamba: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ SSM"
}
[26.02.2025 13:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"State space models (SSMs), such as Mamba, have emerged as an efficient alternative to transformers for long-context sequence modeling. However, despite their growing adoption, SSMs lack the interpretability tools that have been crucial for understanding and improving attention-based architectures. While recent efforts provide insights into Mamba's internal mechanisms, they do not explicitly decompose token-wise contributions, leaving gaps in understanding how Mamba selectively processes sequences across layers. In this work, we introduce LaTIM, a novel token-level decomposition method for both Mamba-1 and Mamba-2 that enables fine-grained interpretability. We extensively evaluate our method across diverse tasks, including machine translation, copying, and retrieval-based generation, demonstrating its effectiveness in revealing Mamba's token-to-token interaction patterns."

[26.02.2025 13:19] Response: ```python
["ARCHITECTURE", "TRAINING", "MULTIMODAL"]
```
[26.02.2025 13:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"State space models (SSMs), such as Mamba, have emerged as an efficient alternative to transformers for long-context sequence modeling. However, despite their growing adoption, SSMs lack the interpretability tools that have been crucial for understanding and improving attention-based architectures. While recent efforts provide insights into Mamba's internal mechanisms, they do not explicitly decompose token-wise contributions, leaving gaps in understanding how Mamba selectively processes sequences across layers. In this work, we introduce LaTIM, a novel token-level decomposition method for both Mamba-1 and Mamba-2 that enables fine-grained interpretability. We extensively evaluate our method across diverse tasks, including machine translation, copying, and retrieval-based generation, demonstrating its effectiveness in revealing Mamba's token-to-token interaction patterns."

[26.02.2025 13:19] Response: ```python
['INTERPRETABILITY', 'LONG_CONTEXT', 'TRANSLATION']
```
[26.02.2025 13:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents LaTIM, a new method designed to improve the interpretability of state space models (SSMs) like Mamba. While SSMs are efficient for handling long sequences, they lack tools to understand how individual tokens contribute to the model\'s decisions. LaTIM allows researchers to break down and analyze the interactions between tokens at a granular level, enhancing our understanding of Mamba\'s processing across different layers. The effectiveness of LaTIM is validated through various tasks, showcasing its ability to reveal intricate token relationships within the model.","title":"Unlocking Interpretability in State Space Models with LaTIM"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents LaTIM, a new method designed to improve the interpretability of state space models (SSMs) like Mamba. While SSMs are efficient for handling long sequences, they lack tools to understand how individual tokens contribute to the model's decisions. LaTIM allows researchers to break down and analyze the interactions between tokens at a granular level, enhancing our understanding of Mamba's processing across different layers. The effectiveness of LaTIM is validated through various tasks, showcasing its ability to reveal intricate token relationships within the model.", title='Unlocking Interpretability in State Space Models with LaTIM'))
[26.02.2025 13:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ï¼Œå¦‚Mambaï¼Œæˆä¸ºäº†é•¿åºåˆ—å»ºæ¨¡ä¸­æ¯”å˜å‹å™¨æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå°½ç®¡å®ƒä»¬çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼ŒSSMsä»ç„¶ç¼ºä¹ç†è§£å’Œæ”¹è¿›åŸºäºæ³¨æ„åŠ›æ¶æ„æ‰€éœ€çš„å¯è§£é‡Šæ€§å·¥å…·ã€‚æœ€è¿‘çš„ç ”ç©¶è™½ç„¶æä¾›äº†å¯¹Mambaå†…éƒ¨æœºåˆ¶çš„è§è§£ï¼Œä½†å¹¶æ²¡æœ‰æ˜ç¡®åˆ†è§£æ¯ä¸ªtokençš„è´¡çŒ®ï¼Œå¯¼è‡´å¯¹Mambaå¦‚ä½•åœ¨å„å±‚ä¸­é€‰æ‹©æ€§å¤„ç†åºåˆ—çš„ç†è§£å­˜åœ¨ç©ºç™½ã€‚æˆ‘ä»¬æå‡ºäº†LaTIMï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„tokençº§åˆ†è§£æ–¹æ³•ï¼Œèƒ½å¤Ÿä¸ºMamba-1å’ŒMamba-2æä¾›ç»†ç²’åº¦çš„å¯è§£é‡Šæ€§ï¼Œå¹¶åœ¨å¤šç§ä»»åŠ¡ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚","title":"æå‡Mambaæ¨¡å‹çš„å¯è§£é‡Šæ€§"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ï¼Œå¦‚Mambaï¼Œæˆä¸ºäº†é•¿åºåˆ—å»ºæ¨¡ä¸­æ¯”å˜å‹å™¨æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå°½ç®¡å®ƒä»¬çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼ŒSSMsä»ç„¶ç¼ºä¹ç†è§£å’Œæ”¹è¿›åŸºäºæ³¨æ„åŠ›æ¶æ„æ‰€éœ€çš„å¯è§£é‡Šæ€§å·¥å…·ã€‚æœ€è¿‘çš„ç ”ç©¶è™½ç„¶æä¾›äº†å¯¹Mambaå†…éƒ¨æœºåˆ¶çš„è§è§£ï¼Œä½†å¹¶æ²¡æœ‰æ˜ç¡®åˆ†è§£æ¯ä¸ªtokençš„è´¡çŒ®ï¼Œå¯¼è‡´å¯¹Mambaå¦‚ä½•åœ¨å„å±‚ä¸­é€‰æ‹©æ€§å¤„ç†åºåˆ—çš„ç†è§£å­˜åœ¨ç©ºç™½ã€‚æˆ‘ä»¬æå‡ºäº†LaTIMï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„tokençº§åˆ†è§£æ–¹æ³•ï¼Œèƒ½å¤Ÿä¸ºMamba-1å’ŒMamba-2æä¾›ç»†ç²’åº¦çš„å¯è§£é‡Šæ€§ï¼Œå¹¶åœ¨å¤šç§ä»»åŠ¡ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚', title='æå‡Mambaæ¨¡å‹çš„å¯è§£é‡Šæ€§'))
[26.02.2025 13:19] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#reasoning", "#architecture", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ»Ğ¸Ğ½ĞµĞ¹ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Shakti VLM Ñ 1B Ğ¸ 4B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚
[26.02.2025 13:19] Loading Chinese text from previous data.
[26.02.2025 13:19] Renaming data file.
[26.02.2025 13:19] Renaming previous data. hf_papers.json to ./d/2025-02-26.json
[26.02.2025 13:19] Saving new data file.
[26.02.2025 13:19] Generating page.
[26.02.2025 13:19] Renaming previous page.
[26.02.2025 13:19] Renaming previous data. index.html to ./d/2025-02-26.html
[26.02.2025 13:19] [Experimental] Generating Chinese page for reading.
[26.02.2025 13:19] Chinese vocab [{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'}, {'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'}, {'word': 'åŸºç¡€èƒ½åŠ›', 'pinyin': 'jÄ« chÇ” nÃ©ng lÃ¬', 'trans': 'basic capabilities'}, {'word': 'å¯¹é½', 'pinyin': 'duÃ¬ qÃ­', 'trans': 'alignment'}, {'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬ liÃ ng', 'trans': 'high quality'}, {'word': 'æ ·æœ¬', 'pinyin': 'yÃ ng bÄ›n', 'trans': 'sample'}, {'word': 'æ¶µç›–', 'pinyin': 'hÃ¡n gÃ i', 'trans': 'cover'}, {'word': 'å¤šæ ·åŒ–', 'pinyin': 'duÅ yÃ ng huÃ ', 'trans': 'diversified'}, {'word': 'å¤æ‚', 'pinyin': 'fÃ¹ zÃ¡', 'trans': 'complex'}, {'word': 'æ ¼å¼', 'pinyin': 'gÃ© shÃ¬', 'trans': 'format'}, {'word': 'æå‡', 'pinyin': 'tÃ­ shÄ“ng', 'trans': 'improve'}, {'word': 'äººå·¥æ ‡æ³¨', 'pinyin': 'rÃ©n gÅng biÄo zhÃ¹', 'trans': 'manual annotation'}, {'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'}, {'word': 'ç›‘ç£å¾®è°ƒ', 'pinyin': 'jiÃ n dÅ« wÄ“i tiÃ¡o', 'trans': 'supervised fine-tuning'}, {'word': 'ç›´æ¥åå¥½ä¼˜åŒ–', 'pinyin': 'zhÃ­ jiÄ“ piÄn hÃ o yÅu huÃ ', 'trans': 'direct preference optimization'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'}, {'word': 'ä¿æŒ', 'pinyin': 'bÇo chÃ­', 'trans': 'maintain'}, {'word': 'æ ‡å‡†', 'pinyin': 'biÄo zhÇ”n', 'trans': 'standard'}, {'word': 'VQA', 'pinyin': 'VQA', 'trans': 'Visual Question Answering'}, {'word': 'æ£€æŸ¥ç‚¹', 'pinyin': 'jiÇn chÃ¡ diÇn', 'trans': 'checkpoint'}]
[26.02.2025 13:19] Renaming previous Chinese page.
[26.02.2025 13:19] Renaming previous data. zh.html to ./d/2025-02-25_zh_reading_task.html
[26.02.2025 13:19] Writing Chinese reading task.
[26.02.2025 13:19] Writing result.
[26.02.2025 13:19] Renaming log file.
[26.02.2025 13:19] Renaming previous data. log.txt to ./logs/2025-02-26_last_log.txt
