[26.02.2025 04:15] Read previous papers.
[26.02.2025 04:15] Generating top page (month).
[26.02.2025 04:15] Writing top page (month).
[26.02.2025 05:10] Read previous papers.
[26.02.2025 05:10] Get feed.
[26.02.2025 05:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18411
[26.02.2025 05:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18137
[26.02.2025 05:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17363
[26.02.2025 05:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17262
[26.02.2025 05:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18449
[26.02.2025 05:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15499
[26.02.2025 05:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16794
[26.02.2025 05:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18364
[26.02.2025 05:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18356
[26.02.2025 05:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.02.2025 05:10] No deleted papers detected.
[26.02.2025 05:10] Downloading and parsing papers (pdf, html). Total: 9.
[26.02.2025 05:10] Downloading and parsing paper https://huggingface.co/papers/2502.18411.
[26.02.2025 05:10] Extra JSON file exists (./assets/json/2502.18411.json), skip PDF parsing.
[26.02.2025 05:10] Paper image links file exists (./assets/img_data/2502.18411.json), skip HTML parsing.
[26.02.2025 05:10] Success.
[26.02.2025 05:10] Downloading and parsing paper https://huggingface.co/papers/2502.18137.
[26.02.2025 05:10] Extra JSON file exists (./assets/json/2502.18137.json), skip PDF parsing.
[26.02.2025 05:10] Paper image links file exists (./assets/img_data/2502.18137.json), skip HTML parsing.
[26.02.2025 05:10] Success.
[26.02.2025 05:10] Downloading and parsing paper https://huggingface.co/papers/2502.17363.
[26.02.2025 05:10] Extra JSON file exists (./assets/json/2502.17363.json), skip PDF parsing.
[26.02.2025 05:10] Paper image links file exists (./assets/img_data/2502.17363.json), skip HTML parsing.
[26.02.2025 05:10] Success.
[26.02.2025 05:10] Downloading and parsing paper https://huggingface.co/papers/2502.17262.
[26.02.2025 05:10] Extra JSON file exists (./assets/json/2502.17262.json), skip PDF parsing.
[26.02.2025 05:10] Paper image links file exists (./assets/img_data/2502.17262.json), skip HTML parsing.
[26.02.2025 05:10] Success.
[26.02.2025 05:10] Downloading and parsing paper https://huggingface.co/papers/2502.18449.
[26.02.2025 05:10] Extra JSON file exists (./assets/json/2502.18449.json), skip PDF parsing.
[26.02.2025 05:10] Paper image links file exists (./assets/img_data/2502.18449.json), skip HTML parsing.
[26.02.2025 05:10] Success.
[26.02.2025 05:10] Downloading and parsing paper https://huggingface.co/papers/2502.15499.
[26.02.2025 05:10] Extra JSON file exists (./assets/json/2502.15499.json), skip PDF parsing.
[26.02.2025 05:10] Paper image links file exists (./assets/img_data/2502.15499.json), skip HTML parsing.
[26.02.2025 05:10] Success.
[26.02.2025 05:10] Downloading and parsing paper https://huggingface.co/papers/2502.16794.
[26.02.2025 05:10] Extra JSON file exists (./assets/json/2502.16794.json), skip PDF parsing.
[26.02.2025 05:10] Paper image links file exists (./assets/img_data/2502.16794.json), skip HTML parsing.
[26.02.2025 05:10] Success.
[26.02.2025 05:10] Downloading and parsing paper https://huggingface.co/papers/2502.18364.
[26.02.2025 05:10] Extra JSON file exists (./assets/json/2502.18364.json), skip PDF parsing.
[26.02.2025 05:10] Paper image links file exists (./assets/img_data/2502.18364.json), skip HTML parsing.
[26.02.2025 05:10] Success.
[26.02.2025 05:10] Downloading and parsing paper https://huggingface.co/papers/2502.18356.
[26.02.2025 05:10] Extra JSON file exists (./assets/json/2502.18356.json), skip PDF parsing.
[26.02.2025 05:10] Paper image links file exists (./assets/img_data/2502.18356.json), skip HTML parsing.
[26.02.2025 05:10] Success.
[26.02.2025 05:10] Enriching papers with extra data.
[26.02.2025 05:10] ********************************************************************************
[26.02.2025 05:10] Abstract 0. Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featur...
[26.02.2025 05:10] ********************************************************************************
[26.02.2025 05:10] Abstract 1. An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the s...
[26.02.2025 05:10] ********************************************************************************
[26.02.2025 05:10] Abstract 2. Background consistency remains a significant challenge in image editing tasks. Despite extensive developments, existing works still face a trade-off between maintaining similarity to the original image and generating content that aligns with the target. Here, we propose KV-Edit, a training-free appr...
[26.02.2025 05:10] ********************************************************************************
[26.02.2025 05:10] Abstract 3. The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) t...
[26.02.2025 05:10] ********************************************************************************
[26.02.2025 05:10] Abstract 4. The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, thi...
[26.02.2025 05:10] ********************************************************************************
[26.02.2025 05:10] Abstract 5. Training stability is a persistent challenge in the pre-training of large language models (LLMs), particularly for architectures such as Post-Norm Transformers, which are prone to gradient explosion and dissipation. In this paper, we propose Scale-Distribution Decoupling (SDD), a novel approach that...
[26.02.2025 05:10] ********************************************************************************
[26.02.2025 05:10] Abstract 6. Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existi...
[26.02.2025 05:10] ********************************************************************************
[26.02.2025 05:10] Abstract 7. Multi-layer image generation is a fundamental task that enables users to isolate, select, and edit specific image layers, thereby revolutionizing interactions with generative models. In this paper, we introduce the Anonymous Region Transformer (ART), which facilitates the direct generation of variab...
[26.02.2025 05:10] ********************************************************************************
[26.02.2025 05:10] Abstract 8. We introduce WebGames, a comprehensive benchmark suite designed to evaluate general-purpose web-browsing AI agents through a collection of 50+ interactive challenges. These challenges are specifically crafted to be straightforward for humans while systematically testing the limitations of current AI...
[26.02.2025 05:10] Read previous papers.
[26.02.2025 05:10] Generating reviews via LLM API.
[26.02.2025 05:10] Using data from previous issue: {"categories": ["#alignment", "#training", "#benchmark", "#multimodal", "#dataset", "#open_source"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ —Ü–µ–Ω–Ω–æ—Å—Ç—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OmniAlign-V - –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö 
[26.02.2025 05:10] Using data from previous issue: {"categories": ["#architecture", "#inference", "#video", "#optimization", "#cv"], "emoji": "üöÄ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ SpargeAttn –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –æ–Ω
[26.02.2025 05:10] Using data from previous issue: {"categories": ["#training", "#cv"], "emoji": "üñºÔ∏è", "ru": {"title": "KV-Edit: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ñ–æ–Ω–∞", "desc": "KV-Edit - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ KV-–∫—ç—à–∞ –≤ DiT-–º–æ–¥–µ–ª—è—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
[26.02.2025 05:10] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#training", "#benchmark"], "emoji": "üîÆ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞
[26.02.2025 05:10] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#dataset", "#math", "#open_source"], "emoji": "üß†", "ru": {"title": "SWE-RL: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ü–û", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SWE-RL - –ø–µ—Ä–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –¥–ª
[26.02.2025 05:10] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "üöÄ", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∞ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Scale-Distribution Decoupling (SDD) –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ
[26.02.2025 05:10] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#alignment", "#audio"], "emoji": "üëÇ", "ru": {"title": "–°–ª—É—à–∞—Ç—å –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫: –ò–ò —Å –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —É—á–∏—Ç—ã–≤–∞—é—â–∏–π –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ
[26.02.2025 05:10] Using data from previous issue: {"categories": ["#multimodal", "#cv"], "emoji": "üé≠", "ru": {"title": "ART: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Anonymous Region Transformer (ART) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö –ø—Ä–æ–∑—Ä–∞—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏ –∞–Ω–æ–Ω–∏–º–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –æ–±–ª–∞—Å
[26.02.2025 05:10] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#games", "#agents"], "emoji": "üåê", "ru": {"title": "WebGames: —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–µ–±-—Å—Ü–µ–Ω–∞—Ä–∏—è—Ö", "desc": "WebGames - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤ –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö –¥–ª—è –≤–µ–±-–±—Ä–∞—É–∑–∏–Ω–≥–∞. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –±–æ–ª
[26.02.2025 05:10] Loading Chinese text from previous data.
[26.02.2025 05:10] Renaming data file.
[26.02.2025 05:10] Renaming previous data. hf_papers.json to ./d/2025-02-26.json
[26.02.2025 05:10] Saving new data file.
[26.02.2025 05:10] Generating page.
[26.02.2025 05:10] Renaming previous page.
[26.02.2025 05:10] Renaming previous data. index.html to ./d/2025-02-26.html
[26.02.2025 05:10] [Experimental] Generating Chinese page for reading.
[26.02.2025 05:10] Chinese vocab [{'word': 'ÂàõÂª∫', 'pinyin': 'chu√†ng ji√†n', 'trans': 'create'}, {'word': 'ËÆ°ÁÆóËµÑÊ∫ê', 'pinyin': 'j√¨ su√†n zƒ´ yu√°n', 'trans': 'computational resources'}, {'word': 'ËÆ≠ÁªÉÊï∞ÊçÆ', 'pinyin': 'x√πn li√†n sh√π j√π', 'trans': 'training data'}, {'word': 'ÊúâÈôê', 'pinyin': 'y«íu xi√†n', 'trans': 'limited'}, {'word': 'Â§ö‰ªªÂä°', 'pinyin': 'du≈ç r√®n w√π', 'trans': 'multi-task'}, {'word': 'ÈÄöÁî®', 'pinyin': 't≈çng y√≤ng', 'trans': 'general-purpose'}, {'word': 'ÊÑüÁü•', 'pinyin': 'g«én zhƒ´', 'trans': 'perception'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥ x√≠ng', 'trans': 'model'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πn li√†n', 'trans': 'pre-trained'}, {'word': 'ÊñáÊú¨Âà∞ÂõæÂÉè', 'pinyin': 'w√©n bƒõn d√†o t√∫ xi√†ng', 'trans': 'text-to-image'}, {'word': 'Êâ©Êï£', 'pinyin': 'ku√≤ s√†n', 'trans': 'diffusion'}, {'word': 'Ë°®Êòé', 'pinyin': 'bi«éo m√≠ng', 'trans': 'indicate'}, {'word': '‰ºòÂºÇ', 'pinyin': 'y≈çu y√¨', 'trans': 'excellent'}, {'word': '‰ªÖ', 'pinyin': 'j«ên', 'trans': 'only'}, {'word': 'Áªü‰∏Ä', 'pinyin': 't«íng yƒ´', 'trans': 'unify'}, {'word': 'ÁºñÁ†Å', 'pinyin': 'biƒÅn m«é', 'trans': 'encoding'}, {'word': 'ÂÖÖÂàÜÂà©Áî®', 'pinyin': 'ch≈çng f√®n l√¨ y√≤ng', 'trans': 'fully utilize'}, {'word': 'ÈÄÇÂ∫î', 'pinyin': 'sh√¨ y√¨ng', 'trans': 'adapt'}, {'word': 'ÂæÆË∞É', 'pinyin': 'wƒìi ti√°o', 'trans': 'fine-tune'}]
[26.02.2025 05:10] Renaming previous Chinese page.
[26.02.2025 05:10] Renaming previous data. zh.html to ./d/2025-02-25_zh_reading_task.html
[26.02.2025 05:10] Writing Chinese reading task.
[26.02.2025 05:10] Writing result.
[26.02.2025 05:10] Renaming log file.
[26.02.2025 05:10] Renaming previous data. log.txt to ./logs/2025-02-26_last_log.txt
