[25.02.2025 23:09] Read previous papers.
[25.02.2025 23:09] Generating top page (month).
[25.02.2025 23:09] Writing top page (month).
[26.02.2025 00:46] Read previous papers.
[26.02.2025 00:46] Get feed.
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17129
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17258
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17157
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15814
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16584
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17435
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16614
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16894
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17407
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16033
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17055
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15894
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17110
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15987
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16707
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16701
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17414
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16922
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15823
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15425
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16810
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14132
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15799
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14247
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14429
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15122
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15920
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16622
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15167
[26.02.2025 00:46] Extract page data from URL. URL: https://huggingface.co/papers/2502.15872
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15919
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17237
[26.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13074
[26.02.2025 00:46] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.02.2025 00:46] No deleted papers detected.
[26.02.2025 00:46] Downloading and parsing papers (pdf, html). Total: 33.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.17129.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.17129.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.17129.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.17258.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.17258.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.17258.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.17157.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.17157.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.17157.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.15814.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.15814.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.15814.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.16584.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.16584.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.16584.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.17435.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.17435.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.17435.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.16614.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.16614.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.16614.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.16894.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.16894.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.16894.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.17407.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.17407.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.17407.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.16033.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.16033.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.16033.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.17055.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.17055.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.17055.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.15894.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.15894.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.15894.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.17110.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.17110.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.17110.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.15987.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.15987.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.15987.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.16707.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.16707.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.16707.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.16701.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.16701.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.16701.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.17414.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.17414.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.17414.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.16922.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.16922.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.16922.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.15823.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.15823.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.15823.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.15425.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.15425.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.15425.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.16810.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.16810.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.16810.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.14132.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.14132.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.14132.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.15799.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.15799.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.15799.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.14247.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.14247.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.14247.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.14429.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.14429.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.14429.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.15122.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.15122.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.15122.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.15920.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.15920.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.15920.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.16622.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.16622.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.16622.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.15167.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.15167.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.15167.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.15872.
[26.02.2025 00:46] Downloading paper 2502.15872 from http://arxiv.org/pdf/2502.15872v1...
[26.02.2025 00:46] Extracting affiliations from text.
[26.02.2025 00:46] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 2 7 8 5 1 . 2 0 5 2 : r MUTAGREP: Execution-Free Repository-Grounded Plan Search for Code-Use Zaid Khan 1 Ali Farhadi 2 Ranjay Krishna 2 Luca Weihs 3 Mohit Bansal 1 Tanmay Gupta "
[26.02.2025 00:46] Response: ```python
[]
```
[26.02.2025 00:46] Extracting affiliations from text.
[26.02.2025 00:46] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 2 7 8 5 1 . 2 0 5 2 : r MUTAGREP: Execution-Free Repository-Grounded Plan Search for Code-Use Zaid Khan 1 Ali Farhadi 2 Ranjay Krishna 2 Luca Weihs 3 Mohit Bansal 1 Tanmay GuptaWhen human requests an LLM to complete coding task using functionality from large code repository, how do we provide context from the repo to the LLM? One approach is to add the entire repo to the LLMs context window. However, most tasks involve only fraction of symbols from repo, longer contexts are detrimental to the LLMs reasoning abilities (Kuratov et al., 2024), and context windows are not unlimited. Alternatively, we could emulate the human ability to navigate large repo, pick out the right functionality, and form plan to solve the task. We propose MUTAGREP (Mutation-guided Grounded Repository Plan Search), an approach to search for plans that decompose user request into natural language steps grounded in the codebase. MUTAGREP performs neural tree search in plan space, exploring by mutating plans and using symbol retriever for grounding. On the challenging LongCodeArena benchmark, our plans use less than 5% of the 128K context window for GPT-4o but rival the coding performance of GPT-4o with context window filled with the repo. Plans produced by MUTAGREP allow Qwen 2.5 Coder 32B and 72B to match the performance of GPT4o with full repo context and enable progress on the hardest LongCodeArena tasks. Project page: zaidkhan.me/MutaGReP Figure 1. MUTAGREP Overview Given user request that requires writing code against specific codebase, we search for realizable plans to solve the users request using LLM-guided tree search. Our search procedure uses symbol retriever to constrain search to plans which are implementable with symbols available in the codebase and explores the search space by mutating plans. Each step of the plan consists of natural language intent and symbols from the codebase that can be used to implement the intent. The user request along with the detailed plan serves as an enriched query that provides necessary context from the codebase to any downstream coding system to convert the plan to code. Our plan search benefits from test-time compute scaling and produces repo-grounded plans without requiring code execution. 1. Introduction Code generation systems powered by LLMs are routinely tasked with writing new code given an existing large codebase. One approach to conditioning an LLMs generation on repository is to utilize its working memory by concatenating all files in the codebase into massive prompt. This is an inefficient use of finite context, because many programming tasks require only small fraction of all symbols (functions, classes, global variables etc) in the codebase. Recent in1University of North Carolina, Chapel Hill 2Allen Institute for Artificial Intelligence (Ai2) 3Vercept AI (work done while at Ai2). Correspondence to: Zaid Khan <zaidkhan@cs.unc.edu>, Tanmay Gupta <tanmayg@allenai.org>. vestigation also shows that leading LLMs are effective at utilizing less than 20% of their context lengths with sharp decline in performance with increasing reasoning complexity (Kuratov et al., 2024). Can we do better? Human programmers are able to understand complex codebases with much smaller biological working memory. They achieve this by decomposing the target task into smaller steps and then using code search tools to identify relevant symbols (functions, classes etc) to use in each step. This is often an iterative process where the programmer interacts with the codebase to develop realizable plan based on the symbols available in the codebase. The realizable plan can then be implemented. 1 MUTAGREP: Execution-Free Repository-Grounded Plan Search for Code-Use In this work, we aim to replicate the ability of human programmers to iteratively search for repo-grounded realizable plan to solve user query given codebase. This plan should wrap all relevant information from the codebase into self-contained prompt which is human readable, editable and can be handed off to any code generation LLM or coding assistant for translation into code. Specifically, given codebase and user query that requires writing code using symbols in the codebase, we aim to find repo-grounded plan with multiple steps. Each step consists of natural language intent and the symbols from the codebase that may be used to implement or realize the plan. good plan is complete, concise, and faithful to the original query. These plans along with the original user query can be viewed as an enriched query that provides all relevant context from the codebase with detailed steps on how to solve the user query. Since the space of all plans is semi-structured and massive, MUTAGREP formulates repo-grounded plan search as an LLM-guided tree-search problem. Each node in the tree represents plan. Each step of the search process involves identifying the most promising node to expand and create children or successors of the node by mutating the plan. The mutation aims to make the successors more accurate, repo-grounded and realizable. When the search budget is exhausted the best plan is returned to the user. Importantly, MUTAGREP does not require executing any code. The design space of MUTAGREP consists of four key components - successor function to mutate plans, symbol retriever to ground intents to symbols in the codebase, treetraversal algorithm that decides the order in which to expand the nodes, and plan ranker to select the most promising node to expand or to identify the best plan from the available nodes. We use the challenging LongCodeArena benchmark (Bogomolov et al., 2024) to thoroughly explore the design space of repo-grounded plan-search. Our contributions include: (i) demonstrating the utility of repo-grounded plans for code-use ( Table 2, Figure 7, Figure 8); (ii) formulating execution-free repo-grounded planning as LLM-guided tree search using an intent to symbol grounding function (Sec. 3); (iii) elucidating and studying the design space of repo-grounded plan search ( Figure 5, Figure 6, Table 3); and (iv) demonstrating that plan search allows code-use to benefit from gains by scaling test-time compute ( Figure 5 and Figure 6). 2. Related Work Repository-grounded code generation. Existing work on repo-level code generation has explored two distinct directions. One line of work focuses on building software engineering agents that can edit real-world codebases to solve Figure 2. repo-grounded plan cr"
[26.02.2025 00:46] Mistral response. {"id": "c2511997c4bd401588114a06bc77da74", "object": "chat.completion", "created": 1740530813, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n['University of North Carolina, Chapel Hill', 'Allen Institute for Artificial Intelligence (Ai2)', 'Vercept AI']\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1600, "total_tokens": 1635, "completion_tokens": 35}}
[26.02.2025 00:46] Response: ```python
['University of North Carolina, Chapel Hill', 'Allen Institute for Artificial Intelligence (Ai2)', 'Vercept AI']
```
[26.02.2025 00:46] Deleting PDF ./assets/pdf/2502.15872.pdf.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.15919.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.15919.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.15919.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.17237.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.17237.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.17237.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.13074.
[26.02.2025 00:46] Extra JSON file exists (./assets/json/2502.13074.json), skip PDF parsing.
[26.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.13074.json), skip HTML parsing.
[26.02.2025 00:46] Success.
[26.02.2025 00:46] Enriching papers with extra data.
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 0. Long context is an important topic in Natural Language Processing (NLP), running through the development of NLP architectures, and offers immense opportunities for Large Language Models (LLMs) giving LLMs the lifelong learning potential akin to humans. Unfortunately, the pursuit of a long context is...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 1. Recent advancements in diffusion models have significantly improved video generation and editing capabilities. However, multi-grained video editing, which encompasses class-level, instance-level, and part-level modifications, remains a formidable challenge. The major difficulties in multi-grained ed...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 2. Our primary goal here is to create a good, generalist perception model that can tackle multiple tasks, within limits on computational resources and training data. To achieve this, we resort to text-to-image diffusion models pre-trained on billions of images. Our exhaustive evaluation metrics demonst...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 3. We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other componen...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 4. Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly unified audio-language models. While inst...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 5. Color constancy methods often struggle to generalize across different camera sensors due to varying spectral sensitivities. We present GCC, which leverages diffusion models to inpaint color checkers into images for illumination estimation. Our key innovations include (1) a single-step deterministic ...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 6. The critique capacity of Large Language Models (LLMs) is essential for reasoning abilities, which can provide necessary suggestions (e.g., detailed analysis and constructive feedback). Therefore, how to evaluate the critique capacity of LLMs has drawn great attention and several critique benchmarks ...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 7. While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singular value decomposition (SVD) subsets, leading to suboptimal leve...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 8. Scaling pre-training compute has proven effective for achieving mulitlinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods-Outcome Reward M...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 9. Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 10. This paper comprehensively evaluates several recently proposed optimizers for 4-bit training, revealing that low-bit precision amplifies sensitivity to learning rates and often causes unstable gradient norms, leading to divergence at higher learning rates. Among these, SPAM, a recent optimizer featu...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 11. Recent advancements in video generation have enabled models to synthesize high-quality, minute-long videos. However, generating even longer videos with temporal coherence remains a major challenge, and existing length extrapolation methods lead to temporal repetition or motion deceleration. In this ...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 12. The rapid increase in mobile device usage necessitates improved automation for seamless task management. However, many AI-driven frameworks struggle due to insufficient operational knowledge. Manually written knowledge helps but is labor-intensive and inefficient. To address these challenges, we int...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 13. As the open-weight AI landscape continues to proliferate-with model development, significant investment, and user interest-it becomes increasingly important to predict which models will ultimately drive innovation and shape AI ecosystems. Building on parallels with citation dynamics in scientific li...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 14. Solving complex long-horizon robotic manipulation problems requires sophisticated high-level planning capabilities, the ability to reason about the physical world, and reactively choose appropriate motor skills. Vision-language models (VLMs) pretrained on Internet data could in principle offer a fra...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 15. Generative AI release decisions determine whether system components are made available, but release does not address many other elements that change how users and stakeholders are able to engage with a system. Beyond release, access to system components informs potential risks and benefits. Access r...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 16. We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize ...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 17. Temporal reasoning is fundamental to human cognition and is crucial for various real-world applications. While recent advances in Large Language Models have demonstrated promising capabilities in temporal reasoning, existing benchmarks primarily rely on rule-based construction, lack contextual depth...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 18. Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 19. Hierarchical organization is fundamental to biological systems and human societies, yet artificial intelligence systems often rely on monolithic architectures that limit adaptability and scalability. Current hierarchical reinforcement learning (HRL) approaches typically restrict hierarchies to two l...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 20. This paper develops an agentic framework that employs large language models (LLMs) to automate the generation of persuasive and grounded marketing content, using real estate listing descriptions as our focal application domain. Our method is designed to align the generated content with user preferen...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 21. Two commonly-employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking or...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 22. Large Language Models (LLMs) have emerged as powerful tools for addressing modern challenges and enabling practical applications. However, their computational expense remains a significant barrier to widespread adoption. Quantization has emerged as a promising technique to democratize access and ena...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 23. This report presents a comprehensive framework for generating high-quality 3D shapes and textures from diverse input prompts, including single images, multi-view images, and text descriptions. The framework consists of 3D shape generation and texture generation. (1). The 3D shape generation pipeline...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 24. Quality estimation is omnipresent in machine translation, for both evaluation and generation. Unfortunately, quality estimation models are often opaque and computationally expensive, making them impractical to be part of large-scale pipelines. In this work, we tackle two connected challenges: (1) re...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 25. We introduce MONSTER-the MONash Scalable Time Series Evaluation Repository-a collection of large datasets for time series classification. The field of time series classification has benefitted from common benchmarks set by the UCR and UEA time series classification repositories. However, the dataset...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 26. Answering complex, long-context questions remains a major challenge for large language models (LLMs) as it requires effective question clarifications and context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a framework designed to enhance an LLM's understanding of such queri...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 27. The COVID-19 pandemic strained healthcare resources and prompted discussion about how machine learning can alleviate physician burdens and contribute to diagnosis. Chest x-rays (CXRs) are used for diagnosis of COVID-19, but few studies predict the severity of a patient's condition from CXRs. In this...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 28. The rapid advancement of AI-generated image (AGI) models has introduced significant challenges in evaluating their quality, which requires considering multiple dimensions such as perceptual quality, prompt correspondence, and authenticity. To address these challenges, we propose M3-AGIQA, a comprehe...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 29. When a human requests an LLM to complete a coding task using functionality from a large code repository, how do we provide context from the repo to the LLM? One approach is to add the entire repo to the LLM's context window. However, most tasks involve only fraction of symbols from a repo, longer co...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 30. As AI chatbots become ubiquitous, voice interaction presents a compelling way to enable rapid, high-bandwidth communication for both semantic and social signals. This has driven research into Large Audio Models (LAMs) to power voice-native experiences. However, aligning LAM development with user goa...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 31. Retrieving images from the same location as a given query is an important component of multiple computer vision tasks, like Visual Place Recognition, Landmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However, existing solutions are built to specifically work for one of these task...
[26.02.2025 00:46] ********************************************************************************
[26.02.2025 00:46] Abstract 32. The Brownian sphere is a random metric space, homeomorphic to the two-dimensional sphere, which arises as the universal scaling limit of many types of random planar maps. The direct construction of the Brownian sphere is via a continuous analogue of the Cori--Vauquelin--Schaeffer (CVS) bijection. Th...
[26.02.2025 00:46] Read previous papers.
[26.02.2025 00:46] Generating reviews via LLM API.
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#benchmark", "#long_context", "#survey", "#training", "#architecture"], "emoji": "üîç", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è –≥—Ä–∞–Ω–∏—Ü—ã: –ø—É—Ç—å –∫ LLM —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#diffusion", "#video", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–¢–æ—á–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "VideoGrain - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–º—É —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#diffusion", "#cv", "#dataset", "#training"], "emoji": "üß†", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏—é —Ç–µ–∫—Å—Ç–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DICEPTION - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–∏
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#audio", "#synthetic", "#data", "#optimization", "#architecture", "#open_source", "#training"], "emoji": "üó£Ô∏è", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Slam - –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#audio", "#dataset"], "emoji": "üéµ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Audio-FLAN - –º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–±–æ—Ç–µ —Å –∞—É–¥–∏–æ. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 80 —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á –≤ –æ–±–ª–∞—Å—Ç–∏ —Ä–µ—á–∏, –º—É–∑—ã–∫–∏ –∏ –∑–≤—É–∫–∞, —Å–æ–¥–µ—Ä–∂
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#inference", "#cv"], "emoji": "üé®", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –æ—Å–≤–µ—â–µ–Ω–∏—è –¥–ª—è –ª—é–±—ã—Ö –∫–∞–º–µ—Ä", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ GCC –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Å–≤–µ—â–µ–Ω–∏—è –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. GCC –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω–æ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#reasoning"], "emoji": "üî¨", "ru": {"title": "CodeCriticBench: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CodeCriticBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫—Ä–∏—Ç–∏
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture", "#reasoning"], "emoji": "üêê", "ru": {"title": "GOAT: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ–ª–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏", "desc": "–≠—Ça —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º GOAT (Great LoRA Mixture-of-Expert) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#low_resource", "#dataset", "#long_context", "#multilingual", "#benchmark", "#math"], "emoji": "üåê", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ø–ú –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞: –≤—ã–∑–æ–≤—ã –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MCLM - –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ —Å –∑–∞–¥–∞—á–∞–º–∏ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#open_source", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ: –æ—Ü–µ–Ω–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ò–ò —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MMIR –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#optimization", "#inference", "#training"], "emoji": "üß†", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Å –Ω–∏–∑–∫–æ–π –±–∏—Ç–Ω–æ—Å—Ç—å—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω—é—é –æ—Ü–µ–Ω–∫—É –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è 4-–±–∏—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ –Ω–∏–∑–∫–æ–±–∏—Ç–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å —É—Å–∏–ª–∏–≤–∞–µ—Ç —á—É
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#synthetic", "#optimization", "#diffusion", "#video"], "emoji": "üé¨", "ru": {"title": "RIFLEx: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ RIFLEx –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Ä–æ
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#video", "#agents"], "emoji": "üì±", "ru": {"title": "–í–∏–¥–µ–æ-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –ò–ò: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤", "desc": "Mobile-Agent-V - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≤–∏–¥–µ–æ–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ 
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#science", "#benchmark", "#open_source", "#training"], "emoji": "üìà", "ru": {"title": "–ò–∑–º–µ—Ä—è–µ–º –≤–ª–∏—è–Ω–∏–µ –ò–ò-–º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –Ω–∞—É—á–Ω—ã—Ö —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç framework –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≤–ª–∏—è–Ω–∏—è open-weight –º–æ–¥–µ–ª–µ–π –≤ —Å—Ñ–µ—Ä–µ –ò–ò. –ê–≤—Ç–æ—Ä—ã –∞–¥–∞–ø—Ç–∏—Ä—É—é—Ç –º–æ–¥–µ–ª—å –í–∞–Ω–≥–∞ 
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#optimization", "#robotics", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–†–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ VLM –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ 
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#ethics", "#open_source"], "emoji": "üîì", "ru": {"title": "–î–æ—Å—Ç—É–ø –∫ –ò–ò: –±–æ–ª—å—à–µ, —á–µ–º –ø—Ä–æ—Å—Ç–æ —Ä–µ–ª–∏–∑", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –¥–æ—Å—Ç—É–ø–∞ –∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –≤—ã—Ö–æ–¥—è—â–∏–µ –∑–∞ —Ä–∞–º–∫–∏ –ø—Ä–æ—Å—Ç–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –æ —Ä–µ–ª–∏–∑–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ—Å—Ç—É
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#architecture", "#video", "#multimodal"], "emoji": "üíÉ", "ru": {"title": "–¢–∞–Ω—Ü—É—é—â–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: –ò–ò –æ–∂–∏–≤–ª—è–µ—Ç —Ñ–æ—Ç–æ –ø–æ–¥ –º—É–∑—ã–∫—É", "desc": "X-Dancer - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∏–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Ç–∞–Ω—Ü–µ–≤ –∏–∑ —Å—Ç–∞—Ç–∏—á–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ–¥ –º—É–∑—ã–∫—É –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#science", "#reasoning", "#multilingual", "#benchmark"], "emoji": "‚è≥", "ru": {"title": "CTM: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–±–ª–∞—Å—Ç–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#science", "#benchmark"], "emoji": "üß†", "ru": {"title": "–ò–Ω–¥—É–∫—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: –∞—Ö–∏–ª–ª–µ—Å–æ–≤–∞ –ø—è—Ç–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ InductionBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –∏–Ω–¥—É–∫—Ç–∏–≤–Ω–æ–º—É –º—ã
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#optimization", "#rl", "#games", "#architecture", "#agents", "#benchmark", "#training"], "emoji": "üå≥", "ru": {"title": "–î–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—è –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç TAME Agent Framework (TAG) - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –ø–æ–ª–Ω–æ—Å—Ç—å—é 
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#science", "#optimization", "#agents", "#multimodal", "#alignment"], "emoji": "üè†", "ru": {"title": "–ò–ò –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —á–µ–ª–æ–≤–µ–∫–∞ –≤ –Ω–∞–ø–∏—Å–∞–Ω–∏–∏ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ 
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#multimodal", "#science", "#ethics", "#data", "#dataset"], "emoji": "üîç", "ru": {"title": "–§–∞–∫—Ç—á–µ–∫–∏–Ω–≥ - –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –º–æ–¥–µ—Ä–∞—Ü–∏–∏", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–æ–º –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –º–æ–¥–µ—Ä–∞—Ü–∏–µ–π –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#benchmark", "#security", "#open_source", "#inference", "#dataset"], "emoji": "üî¨", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ: –ø—É—Ç—å –∫ –¥–æ—Å—Ç—É–ø–Ω—ã–º –∏ –Ω–∞–¥–µ–∂–Ω—ã–º LLM", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#architecture", "#3d"], "emoji": "üé®", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: –æ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞ –∫ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º –æ–±—ä–µ–∫—Ç–∞–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö 3D-—Ñ–æ—Ä–º –∏ —Ç–µ–∫—Å—Ç—É—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è –æ–¥–∏–Ω–æ—á–Ω—ã
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#machine_translation", "#data", "#training", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –∫–∞—á–µ—Å—Ç–≤–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Instant Confidence COMET. –ú–æ–¥–µ–ª
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#benchmark", "#dataset"], "emoji": "ü¶ñ", "ru": {"title": "–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø—Ä–æ—Ä—ã–≤–æ–≤ –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", "desc": "MONSTER - —ç—Ç–æ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –∫—Ä—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤. –û–Ω —Å–æ–∑–¥–∞–Ω –≤ –ø—Ä–æ—Ç–∏–≤–æ–≤–µ—Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –±–µ–Ω—á–º–∞—Ä–∫–∞–º UCR –∏ UEA, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#inference", "#agents", "#multimodal", "#training", "#long_context", "#reasoning"], "emoji": "üß†", "ru": {"title": "AgenticLU: –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ —Å–∞–º–æ—É—Ç–æ—á–Ω–µ–Ω–∏–µ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–µ –∑–∞–∑–µ–º–ª–µ–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AgenticLU - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#healthcare", "#dataset", "#training", "#cv", "#transfer_learning", "#open_source"], "emoji": "ü´Å", "ru": {"title": "–ò–ò –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç—è–∂–µ—Å—Ç—å COVID-19 –ø–æ —Ä–µ–Ω—Ç–≥–µ–Ω—É –Ω–µ —Ö—É–∂–µ –≤—Ä–∞—á–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç—è–∂–µ—Å—Ç–∏ COVID-19 –ø–æ —Ä–µ–Ω—Ç–≥–µ–Ω–æ–≤—Å–∫–∏–º —Å
[26.02.2025 00:46] Using data from previous issue: {"categories": ["#training", "#multimodal", "#optimization", "#benchmark", "#interpretability", "#agi"], "emoji": "üñºÔ∏è", "ru": {"title": "–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ò–ò-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö
[26.02.2025 00:46] Querying the API.
[26.02.2025 00:46] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

When a human requests an LLM to complete a coding task using functionality from a large code repository, how do we provide context from the repo to the LLM? One approach is to add the entire repo to the LLM's context window. However, most tasks involve only fraction of symbols from a repo, longer contexts are detrimental to the LLM's reasoning abilities, and context windows are not unlimited. Alternatively, we could emulate the human ability to navigate a large repo, pick out the right functionality, and form a plan to solve the task. We propose MutaGReP (Mutation-guided Grounded Repository Plan Search), an approach to search for plans that decompose a user request into natural language steps grounded in the codebase. MutaGReP performs neural tree search in plan space, exploring by mutating plans and using a symbol retriever for grounding. On the challenging LongCodeArena benchmark, our plans use less than 5% of the 128K context window for GPT-4o but rival the coding performance of GPT-4o with a context window filled with the repo. Plans produced by MutaGReP allow Qwen 2.5 Coder 32B and 72B to match the performance of GPT-4o with full repo context and enable progress on the hardest LongCodeArena tasks. Project page: zaidkhan.me/MutaGReP
[26.02.2025 00:46] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ MutaGReP –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ –∫–æ–¥–∞ –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç –ø–æ–∏—Å–∫ –ø–ª–∞–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–∑–±–∏–≤–∞—é—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ —à–∞–≥–∏ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∫–æ–¥–æ–≤–æ–π –±–∞–∑–æ–π. MutaGReP –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –¥–µ—Ä–µ–≤—É –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø–ª–∞–Ω–æ–≤, –∏—Å—Å–ª–µ–¥—É—è –º—É—Ç–∞—Ü–∏–∏ –ø–ª–∞–Ω–æ–≤ –∏ –ø—Ä–∏–º–µ–Ω—è—è —Å–∏–º–≤–æ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –∫–æ–¥—É. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–ª–∞–Ω—ã MutaGReP –ø–æ–∑–≤–æ–ª—è—é—Ç –º–æ–¥–µ–ª—è–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω–µ–µ 5% –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞, –Ω–æ –¥–æ—Å—Ç–∏–≥–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–π —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.",
  "emoji": "üß†",
  "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ò–ò-–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –±–æ–ª—å—à–∏–º–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è–º–∏"
}
[26.02.2025 00:46] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"When a human requests an LLM to complete a coding task using functionality from a large code repository, how do we provide context from the repo to the LLM? One approach is to add the entire repo to the LLM's context window. However, most tasks involve only fraction of symbols from a repo, longer contexts are detrimental to the LLM's reasoning abilities, and context windows are not unlimited. Alternatively, we could emulate the human ability to navigate a large repo, pick out the right functionality, and form a plan to solve the task. We propose MutaGReP (Mutation-guided Grounded Repository Plan Search), an approach to search for plans that decompose a user request into natural language steps grounded in the codebase. MutaGReP performs neural tree search in plan space, exploring by mutating plans and using a symbol retriever for grounding. On the challenging LongCodeArena benchmark, our plans use less than 5% of the 128K context window for GPT-4o but rival the coding performance of GPT-4o with a context window filled with the repo. Plans produced by MutaGReP allow Qwen 2.5 Coder 32B and 72B to match the performance of GPT-4o with full repo context and enable progress on the hardest LongCodeArena tasks. Project page: zaidkhan.me/MutaGReP"

[26.02.2025 00:46] Response: ```python
['RAG', 'TRAINING', 'BENCHMARK', 'PLP']
```
[26.02.2025 00:46] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"When a human requests an LLM to complete a coding task using functionality from a large code repository, how do we provide context from the repo to the LLM? One approach is to add the entire repo to the LLM's context window. However, most tasks involve only fraction of symbols from a repo, longer contexts are detrimental to the LLM's reasoning abilities, and context windows are not unlimited. Alternatively, we could emulate the human ability to navigate a large repo, pick out the right functionality, and form a plan to solve the task. We propose MutaGReP (Mutation-guided Grounded Repository Plan Search), an approach to search for plans that decompose a user request into natural language steps grounded in the codebase. MutaGReP performs neural tree search in plan space, exploring by mutating plans and using a symbol retriever for grounding. On the challenging LongCodeArena benchmark, our plans use less than 5% of the 128K context window for GPT-4o but rival the coding performance of GPT-4o with a context window filled with the repo. Plans produced by MutaGReP allow Qwen 2.5 Coder 32B and 72B to match the performance of GPT-4o with full repo context and enable progress on the hardest LongCodeArena tasks. Project page: zaidkhan.me/MutaGReP"

[26.02.2025 00:46] Response: ```python
["LONG_CONTEXT", "REASONING"]
```
[26.02.2025 00:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces MutaGReP, a novel method for efficiently guiding large language models (LLMs) in coding tasks by leveraging a code repository. Instead of overwhelming the LLM with the entire repository, MutaGReP intelligently selects relevant code snippets and formulates a plan in natural language. It employs a neural tree search strategy that mutates potential plans and retrieves symbols from the codebase to ensure grounding. The results demonstrate that MutaGReP can achieve competitive coding performance while using significantly less context, thus enhancing the LLM\'s reasoning capabilities.","title":"Efficient Code Navigation with MutaGReP"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces MutaGReP, a novel method for efficiently guiding large language models (LLMs) in coding tasks by leveraging a code repository. Instead of overwhelming the LLM with the entire repository, MutaGReP intelligently selects relevant code snippets and formulates a plan in natural language. It employs a neural tree search strategy that mutates potential plans and retrieves symbols from the codebase to ensure grounding. The results demonstrate that MutaGReP can achieve competitive coding performance while using significantly less context, thus enhancing the LLM's reasoning capabilities.", title='Efficient Code Navigation with MutaGReP'))
[26.02.2025 00:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫MutaGRePÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÂú®Â§ßÂûã‰ª£Á†ÅÂ∫ì‰∏≠‰∏∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊèê‰æõ‰∏ä‰∏ãÊñá„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂèòÂºÇËÆ°ÂàíÂπ∂‰ΩøÁî®Á¨¶Âè∑Ê£ÄÁ¥¢Âô®Êù•ÂºïÂØºÁî®Êà∑ËØ∑Ê±ÇÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÊ≠•È™§Ôºå‰ªéËÄåÂú®ËÆ°ÂàíÁ©∫Èó¥‰∏≠ËøõË°åÁ•ûÁªèÊ†ëÊêúÁ¥¢„ÄÇMutaGRePÂú®LongCodeArenaÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩøÁî®ÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£‰∏çÂà∞128KÁöÑ5%„ÄÇËØ•ÊñπÊ≥ï‰ΩøÂæóQwen 2.5 CoderËÉΩÂ§üÂú®Â§çÊùÇ‰ªªÂä°‰∏≠‰∏éGPT-4oÁöÑË°®Áé∞Áõ∏Â™≤ÁæéÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®‰ª£Á†ÅÁîüÊàê‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ","title":"MutaGRePÔºöÊô∫ËÉΩÂØºËà™‰ª£Á†ÅÂ∫ìÁöÑËÆ°ÂàíÊêúÁ¥¢"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫MutaGRePÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÂú®Â§ßÂûã‰ª£Á†ÅÂ∫ì‰∏≠‰∏∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊèê‰æõ‰∏ä‰∏ãÊñá„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂèòÂºÇËÆ°ÂàíÂπ∂‰ΩøÁî®Á¨¶Âè∑Ê£ÄÁ¥¢Âô®Êù•ÂºïÂØºÁî®Êà∑ËØ∑Ê±ÇÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÊ≠•È™§Ôºå‰ªéËÄåÂú®ËÆ°ÂàíÁ©∫Èó¥‰∏≠ËøõË°åÁ•ûÁªèÊ†ëÊêúÁ¥¢„ÄÇMutaGRePÂú®LongCodeArenaÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩøÁî®ÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£‰∏çÂà∞128KÁöÑ5%„ÄÇËØ•ÊñπÊ≥ï‰ΩøÂæóQwen 2.5 CoderËÉΩÂ§üÂú®Â§çÊùÇ‰ªªÂä°‰∏≠‰∏éGPT-4oÁöÑË°®Áé∞Áõ∏Â™≤ÁæéÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®‰ª£Á†ÅÁîüÊàê‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ', title='MutaGRePÔºöÊô∫ËÉΩÂØºËà™‰ª£Á†ÅÂ∫ìÁöÑËÆ°ÂàíÊêúÁ¥¢'))
[26.02.2025 00:47] Using data from previous issue: {"categories": ["#benchmark", "#interpretability", "#alignment", "#dataset", "#audio"], "emoji": "üó£Ô∏è", "ru": {"title": "–ì–æ–ª–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤–∞–∂–Ω–µ–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –≤ –æ—Ü–µ–Ω–∫–µ –∞—É–¥–∏–æ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –æ—Ü–µ–Ω–∫—É –ë–æ–ª—å—à–∏—Ö –ê—É–¥–∏–æ –ú–æ–¥–µ–ª–µ–π (LAM) –¥–ª—è –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã —Å–æ–±—Ä–∞–ª–∏ 7500 –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π 
[26.02.2025 00:47] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#cv"], "emoji": "üó∫Ô∏è", "ru": {"title": "MegaLoc: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏", "desc": "MegaLoc - —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è. –û–Ω–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é
[26.02.2025 00:47] Using data from previous issue: {"categories": ["#math"], "emoji": "üåê", "ru": {"title": "–û–±—Ä–∞—Ç–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±—Ä–æ—É–Ω–æ–≤—Å–∫–æ–π —Å—Ñ–µ—Ä—ã –≤ —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ—Ä–µ–≤–æ", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –æ–±—Ä–∞—Ç–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –±–∏–µ–∫—Ü–∏–∏ –ö–æ—Ä–∏-–í–æ–∫–µ–ª–µ–Ω–∞-–®–µ—Ñ—Ñ–µ—Ä–∞ (CVS) –¥–ª—è –±—Ä–æ—É–Ω–æ–≤—Å–∫–æ–π —Å—Ñ–µ—Ä—ã. –ë—Ä–æ—É–Ω–æ–≤—Å–∫–∞—è —Å—Ñ–µ—Ä–∞ - —ç—Ç–æ —Å–ª—É—á–∞–π–Ω–æ–µ –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
[26.02.2025 00:47] Loading Chinese text from previous data.
[26.02.2025 00:47] Renaming data file.
[26.02.2025 00:47] Renaming previous data. hf_papers.json to ./d/2025-02-26.json
[26.02.2025 00:47] Saving new data file.
[26.02.2025 00:47] Generating page.
[26.02.2025 00:47] Renaming previous page.
[26.02.2025 00:47] Renaming previous data. index.html to ./d/2025-02-26.html
[26.02.2025 00:47] [Experimental] Generating Chinese page for reading.
[26.02.2025 00:47] Chinese vocab [{'word': 'ÂàõÂª∫', 'pinyin': 'chu√†ng ji√†n', 'trans': 'create'}, {'word': 'ËÆ°ÁÆóËµÑÊ∫ê', 'pinyin': 'j√¨ su√†n zƒ´ yu√°n', 'trans': 'computational resources'}, {'word': 'ËÆ≠ÁªÉÊï∞ÊçÆ', 'pinyin': 'x√πn li√†n sh√π j√π', 'trans': 'training data'}, {'word': 'ÊúâÈôê', 'pinyin': 'y«íu xi√†n', 'trans': 'limited'}, {'word': 'Â§ö‰ªªÂä°', 'pinyin': 'du≈ç r√®n w√π', 'trans': 'multi-task'}, {'word': 'ÈÄöÁî®', 'pinyin': 't≈çng y√≤ng', 'trans': 'general-purpose'}, {'word': 'ÊÑüÁü•', 'pinyin': 'g«én zhƒ´', 'trans': 'perception'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥ x√≠ng', 'trans': 'model'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πn li√†n', 'trans': 'pre-trained'}, {'word': 'ÊñáÊú¨Âà∞ÂõæÂÉè', 'pinyin': 'w√©n bƒõn d√†o t√∫ xi√†ng', 'trans': 'text-to-image'}, {'word': 'Êâ©Êï£', 'pinyin': 'ku√≤ s√†n', 'trans': 'diffusion'}, {'word': 'Ë°®Êòé', 'pinyin': 'bi«éo m√≠ng', 'trans': 'indicate'}, {'word': '‰ºòÂºÇ', 'pinyin': 'y≈çu y√¨', 'trans': 'excellent'}, {'word': '‰ªÖ', 'pinyin': 'j«ên', 'trans': 'only'}, {'word': 'Áªü‰∏Ä', 'pinyin': 't«íng yƒ´', 'trans': 'unify'}, {'word': 'ÁºñÁ†Å', 'pinyin': 'biƒÅn m«é', 'trans': 'encoding'}, {'word': 'ÂÖÖÂàÜÂà©Áî®', 'pinyin': 'ch≈çng f√®n l√¨ y√≤ng', 'trans': 'fully utilize'}, {'word': 'ÈÄÇÂ∫î', 'pinyin': 'sh√¨ y√¨ng', 'trans': 'adapt'}, {'word': 'ÂæÆË∞É', 'pinyin': 'wƒìi ti√°o', 'trans': 'fine-tune'}]
[26.02.2025 00:47] Renaming previous Chinese page.
[26.02.2025 00:47] Renaming previous data. zh.html to ./d/2025-02-25_zh_reading_task.html
[26.02.2025 00:47] Writing Chinese reading task.
[26.02.2025 00:47] Writing result.
[26.02.2025 00:47] Renaming log file.
[26.02.2025 00:47] Renaming previous data. log.txt to ./logs/2025-02-26_last_log.txt
