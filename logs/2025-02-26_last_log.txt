[26.02.2025 07:10] Read previous papers.
[26.02.2025 07:10] Generating top page (month).
[26.02.2025 07:10] Writing top page (month).
[26.02.2025 08:14] Read previous papers.
[26.02.2025 08:14] Get feed.
[26.02.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18411
[26.02.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18137
[26.02.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17363
[26.02.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17262
[26.02.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18449
[26.02.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15499
[26.02.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18364
[26.02.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18356
[26.02.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16794
[26.02.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18461
[26.02.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17535
[26.02.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17092
[26.02.2025 08:14] Extract page data from URL. URL: https://huggingface.co/papers/2502.17425
[26.02.2025 08:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.02.2025 08:14] No deleted papers detected.
[26.02.2025 08:14] Downloading and parsing papers (pdf, html). Total: 13.
[26.02.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2502.18411.
[26.02.2025 08:14] Extra JSON file exists (./assets/json/2502.18411.json), skip PDF parsing.
[26.02.2025 08:14] Paper image links file exists (./assets/img_data/2502.18411.json), skip HTML parsing.
[26.02.2025 08:14] Success.
[26.02.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2502.18137.
[26.02.2025 08:14] Extra JSON file exists (./assets/json/2502.18137.json), skip PDF parsing.
[26.02.2025 08:14] Paper image links file exists (./assets/img_data/2502.18137.json), skip HTML parsing.
[26.02.2025 08:14] Success.
[26.02.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2502.17363.
[26.02.2025 08:14] Extra JSON file exists (./assets/json/2502.17363.json), skip PDF parsing.
[26.02.2025 08:14] Paper image links file exists (./assets/img_data/2502.17363.json), skip HTML parsing.
[26.02.2025 08:14] Success.
[26.02.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2502.17262.
[26.02.2025 08:14] Extra JSON file exists (./assets/json/2502.17262.json), skip PDF parsing.
[26.02.2025 08:14] Paper image links file exists (./assets/img_data/2502.17262.json), skip HTML parsing.
[26.02.2025 08:14] Success.
[26.02.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2502.18449.
[26.02.2025 08:14] Extra JSON file exists (./assets/json/2502.18449.json), skip PDF parsing.
[26.02.2025 08:14] Paper image links file exists (./assets/img_data/2502.18449.json), skip HTML parsing.
[26.02.2025 08:14] Success.
[26.02.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2502.15499.
[26.02.2025 08:14] Extra JSON file exists (./assets/json/2502.15499.json), skip PDF parsing.
[26.02.2025 08:14] Paper image links file exists (./assets/img_data/2502.15499.json), skip HTML parsing.
[26.02.2025 08:14] Success.
[26.02.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2502.18364.
[26.02.2025 08:14] Extra JSON file exists (./assets/json/2502.18364.json), skip PDF parsing.
[26.02.2025 08:14] Paper image links file exists (./assets/img_data/2502.18364.json), skip HTML parsing.
[26.02.2025 08:14] Success.
[26.02.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2502.18356.
[26.02.2025 08:14] Extra JSON file exists (./assets/json/2502.18356.json), skip PDF parsing.
[26.02.2025 08:14] Paper image links file exists (./assets/img_data/2502.18356.json), skip HTML parsing.
[26.02.2025 08:14] Success.
[26.02.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2502.16794.
[26.02.2025 08:14] Extra JSON file exists (./assets/json/2502.16794.json), skip PDF parsing.
[26.02.2025 08:14] Paper image links file exists (./assets/img_data/2502.16794.json), skip HTML parsing.
[26.02.2025 08:14] Success.
[26.02.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2502.18461.
[26.02.2025 08:14] Extra JSON file exists (./assets/json/2502.18461.json), skip PDF parsing.
[26.02.2025 08:14] Paper image links file exists (./assets/img_data/2502.18461.json), skip HTML parsing.
[26.02.2025 08:14] Success.
[26.02.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2502.17535.
[26.02.2025 08:14] Extra JSON file exists (./assets/json/2502.17535.json), skip PDF parsing.
[26.02.2025 08:14] Paper image links file exists (./assets/img_data/2502.17535.json), skip HTML parsing.
[26.02.2025 08:14] Success.
[26.02.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2502.17092.
[26.02.2025 08:14] Extra JSON file exists (./assets/json/2502.17092.json), skip PDF parsing.
[26.02.2025 08:14] Paper image links file exists (./assets/img_data/2502.17092.json), skip HTML parsing.
[26.02.2025 08:14] Success.
[26.02.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2502.17425.
[26.02.2025 08:14] Downloading paper 2502.17425 from http://arxiv.org/pdf/2502.17425v1...
[26.02.2025 08:14] Extracting affiliations from text.
[26.02.2025 08:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 1 5 2 4 7 1 . 2 0 5 2 : r a Runpeng Yu*, Xinyin Ma* and Xinchao Wang National University of Singapore {r.yu,maxinyin}@u.nus.edu and xinchao@nus.edu.sg (a) Response process of an MLLM equipped with Visual Perception Tokens. (b) Average Performance. Figure 1. The Visual Perception Token aids MLLMs by triggering and controlling additional visual perception processes. Fig. 1a illustrates the response process of MLLMs equipped with the Region Selection Token or Vision Re-Encoding Token. The regions marked in the image are selected by the Region Selection Token. Fig. 1b presents the average performance of 2B model with the Visual Perception Token across various VQA tasks (higher is better, with 1 being the maximum). "
[26.02.2025 08:14] Response: ```python
["National University of Singapore"]
```
[26.02.2025 08:14] Deleting PDF ./assets/pdf/2502.17425.pdf.
[26.02.2025 08:14] Success.
[26.02.2025 08:14] Enriching papers with extra data.
[26.02.2025 08:14] ********************************************************************************
[26.02.2025 08:14] Abstract 0. Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featur...
[26.02.2025 08:14] ********************************************************************************
[26.02.2025 08:14] Abstract 1. An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the s...
[26.02.2025 08:14] ********************************************************************************
[26.02.2025 08:14] Abstract 2. Background consistency remains a significant challenge in image editing tasks. Despite extensive developments, existing works still face a trade-off between maintaining similarity to the original image and generating content that aligns with the target. Here, we propose KV-Edit, a training-free appr...
[26.02.2025 08:14] ********************************************************************************
[26.02.2025 08:14] Abstract 3. The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) t...
[26.02.2025 08:14] ********************************************************************************
[26.02.2025 08:14] Abstract 4. The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, thi...
[26.02.2025 08:14] ********************************************************************************
[26.02.2025 08:14] Abstract 5. Training stability is a persistent challenge in the pre-training of large language models (LLMs), particularly for architectures such as Post-Norm Transformers, which are prone to gradient explosion and dissipation. In this paper, we propose Scale-Distribution Decoupling (SDD), a novel approach that...
[26.02.2025 08:14] ********************************************************************************
[26.02.2025 08:14] Abstract 6. Multi-layer image generation is a fundamental task that enables users to isolate, select, and edit specific image layers, thereby revolutionizing interactions with generative models. In this paper, we introduce the Anonymous Region Transformer (ART), which facilitates the direct generation of variab...
[26.02.2025 08:14] ********************************************************************************
[26.02.2025 08:14] Abstract 7. We introduce WebGames, a comprehensive benchmark suite designed to evaluate general-purpose web-browsing AI agents through a collection of 50+ interactive challenges. These challenges are specifically crafted to be straightforward for humans while systematically testing the limitations of current AI...
[26.02.2025 08:14] ********************************************************************************
[26.02.2025 08:14] Abstract 8. Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existi...
[26.02.2025 08:14] ********************************************************************************
[26.02.2025 08:14] Abstract 9. Recent studies have explored combining different LoRAs to jointly generate learned style and content. However, existing methods either fail to effectively preserve both the original subject and style simultaneously or require additional training. In this paper, we argue that the intrinsic properties...
[26.02.2025 08:14] ********************************************************************************
[26.02.2025 08:14] Abstract 10. Motivated by reducing the computational and storage costs of LLMs, model compression and KV cache compression have attracted much attention from researchers. However, current methods predominantly emphasize maintaining the performance of compressed LLMs, as measured by perplexity or simple accuracy ...
[26.02.2025 08:14] ********************************************************************************
[26.02.2025 08:14] Abstract 11. We introduce Shakti VLM, a family of vision-language models in the capacity of 1B and 4B parameters designed to address data efficiency challenges in multimodal learning. While recent VLMs achieve strong performance through extensive training data, Shakti models leverage architectural innovations to...
[26.02.2025 08:14] ********************************************************************************
[26.02.2025 08:14] Abstract 12. To utilize visual information, Multimodal Large Language Model (MLLM) relies on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. However, MLLM stil...
[26.02.2025 08:14] Read previous papers.
[26.02.2025 08:14] Generating reviews via LLM API.
[26.02.2025 08:14] Using data from previous issue: {"categories": ["#alignment", "#training", "#benchmark", "#multimodal", "#dataset", "#open_source"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ —Ü–µ–Ω–Ω–æ—Å—Ç—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OmniAlign-V - –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö 
[26.02.2025 08:14] Using data from previous issue: {"categories": ["#architecture", "#inference", "#video", "#optimization", "#cv"], "emoji": "üöÄ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ SpargeAttn –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –æ–Ω
[26.02.2025 08:14] Using data from previous issue: {"categories": ["#training", "#cv"], "emoji": "üñºÔ∏è", "ru": {"title": "KV-Edit: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ñ–æ–Ω–∞", "desc": "KV-Edit - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ KV-–∫—ç—à–∞ –≤ DiT-–º–æ–¥–µ–ª—è—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
[26.02.2025 08:14] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#training", "#benchmark"], "emoji": "üîÆ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞
[26.02.2025 08:14] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#dataset", "#math", "#open_source"], "emoji": "üß†", "ru": {"title": "SWE-RL: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ü–û", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SWE-RL - –ø–µ—Ä–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –¥–ª
[26.02.2025 08:14] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "üöÄ", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∞ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Scale-Distribution Decoupling (SDD) –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ
[26.02.2025 08:14] Using data from previous issue: {"categories": ["#multimodal", "#cv"], "emoji": "üé≠", "ru": {"title": "ART: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Anonymous Region Transformer (ART) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö –ø—Ä–æ–∑—Ä–∞—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏ –∞–Ω–æ–Ω–∏–º–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –æ–±–ª–∞—Å
[26.02.2025 08:14] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#games", "#agents"], "emoji": "üåê", "ru": {"title": "WebGames: —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–µ–±-—Å—Ü–µ–Ω–∞—Ä–∏—è—Ö", "desc": "WebGames - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤ –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö –¥–ª—è –≤–µ–±-–±—Ä–∞—É–∑–∏–Ω–≥–∞. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –±–æ–ª
[26.02.2025 08:14] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#alignment", "#audio"], "emoji": "üëÇ", "ru": {"title": "–°–ª—É—à–∞—Ç—å –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫: –ò–ò —Å –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —É—á–∏—Ç—ã–≤–∞—é—â–∏–π –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ
[26.02.2025 08:14] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#training"], "emoji": "üé®", "ru": {"title": "K-LoRA: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ —Å—Ç–∏–ª—è –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ K-LoRA –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LoRA (Low-Rank Adaptation) –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω
[26.02.2025 08:14] Using data from previous issue: {"categories": ["#inference", "#optimization", "#reasoning", "#rag", "#training"], "emoji": "üéüÔ∏è", "ru": {"title": "–õ–æ—Ç–µ—Ä–µ–π–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É —Å–∂–∞—Ç–∏—é –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≥–∏–ø–æ—Ç–µ–∑—É –ª–æ—Ç–µ—Ä–µ–π–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (LLM) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Å–∂–∞—Ç–∏—è –º–æ–¥–µ–ª–µ–π –∏
[26.02.2025 08:14] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#reasoning", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ª–∏–Ω–µ–π–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Shakti VLM —Å 1B –∏ 4B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–∞—Ü–µ–ª–µ–Ω–Ω—ã—Ö –Ω–∞ –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç
[26.02.2025 08:14] Querying the API.
[26.02.2025 08:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

To utilize visual information, Multimodal Large Language Model (MLLM) relies on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. However, MLLM still lacks the autonomous capability to control its own visual perception processes, for example, selectively reviewing specific regions of an image or focusing on information related to specific object categories. In this work, we propose the concept of Visual Perception Token, aiming to empower MLLM with a mechanism to control its visual perception processes. We design two types of Visual Perception Tokens, termed the Region Selection Token and the Vision Re-Encoding Token. MLLMs autonomously generate these tokens, just as they generate text, and use them to trigger additional visual perception actions. The Region Selection Token explicitly identifies specific regions in an image that require further perception, while the Vision Re-Encoding Token uses its hidden states as control signals to guide additional visual perception processes. Extensive experiments demonstrate the advantages of these tokens in handling spatial reasoning, improving fine-grained understanding, and other tasks. On average, the introduction of Visual Perception Tokens improves the performance of a 2B model by 23.6\%, increasing its score from 0.572 to 0.708, and even outperforms a 7B parameter model by 13.4\% (from 0.624). Please check out our repo https://github.com/yu-rp/VisualPerceptionToken
[26.02.2025 08:14] Response: {
  "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è –¢–æ–∫–µ–Ω–æ–≤ –í–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –í–æ—Å–ø—Ä–∏—è—Ç–∏—è –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –≠—Ç–∏ —Ç–æ–∫–µ–Ω—ã –ø–æ–∑–≤–æ–ª—è—é—Ç MLLM –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –≤—ã–±–æ—Ä–æ—á–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ —Ñ–æ–∫—É—Å–∏—Ä—É—è—Å—å –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Å–≤—è–∑–∞–Ω–Ω–æ–π —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –æ–±—ä–µ–∫—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–∞ —Ç–∏–ø–∞ —Ç–æ–∫–µ–Ω–æ–≤: –¢–æ–∫–µ–Ω –í—ã–±–æ—Ä–∞ –†–µ–≥–∏–æ–Ω–∞ –∏ –¢–æ–∫–µ–Ω –ü–µ—Ä–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ó—Ä–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —ç—Ç–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å MLLM –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –∏ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.",
  "emoji": "üëÅÔ∏è",
  "title": "–¢–æ–∫–µ–Ω—ã –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è: –Ω–æ–≤—ã–π —à–∞–≥ –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò"
}
[26.02.2025 08:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"To utilize visual information, Multimodal Large Language Model (MLLM) relies on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. However, MLLM still lacks the autonomous capability to control its own visual perception processes, for example, selectively reviewing specific regions of an image or focusing on information related to specific object categories. In this work, we propose the concept of Visual Perception Token, aiming to empower MLLM with a mechanism to control its visual perception processes. We design two types of Visual Perception Tokens, termed the Region Selection Token and the Vision Re-Encoding Token. MLLMs autonomously generate these tokens, just as they generate text, and use them to trigger additional visual perception actions. The Region Selection Token explicitly identifies specific regions in an image that require further perception, while the Vision Re-Encoding Token uses its hidden states as control signals to guide additional visual perception processes. Extensive experiments demonstrate the advantages of these tokens in handling spatial reasoning, improving fine-grained understanding, and other tasks. On average, the introduction of Visual Perception Tokens improves the performance of a 2B model by 23.6\%, increasing its score from 0.572 to 0.708, and even outperforms a 7B parameter model by 13.4\% (from 0.624). Please check out our repo https://github.com/yu-rp/VisualPerceptionToken"

[26.02.2025 08:14] Response: ```python
['MULTIMODAL', 'CV', 'ARCHITECTURE']
```
[26.02.2025 08:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"To utilize visual information, Multimodal Large Language Model (MLLM) relies on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. However, MLLM still lacks the autonomous capability to control its own visual perception processes, for example, selectively reviewing specific regions of an image or focusing on information related to specific object categories. In this work, we propose the concept of Visual Perception Token, aiming to empower MLLM with a mechanism to control its visual perception processes. We design two types of Visual Perception Tokens, termed the Region Selection Token and the Vision Re-Encoding Token. MLLMs autonomously generate these tokens, just as they generate text, and use them to trigger additional visual perception actions. The Region Selection Token explicitly identifies specific regions in an image that require further perception, while the Vision Re-Encoding Token uses its hidden states as control signals to guide additional visual perception processes. Extensive experiments demonstrate the advantages of these tokens in handling spatial reasoning, improving fine-grained understanding, and other tasks. On average, the introduction of Visual Perception Tokens improves the performance of a 2B model by 23.6\%, increasing its score from 0.572 to 0.708, and even outperforms a 7B parameter model by 13.4\% (from 0.624). Please check out our repo https://github.com/yu-rp/VisualPerceptionToken"

[26.02.2025 08:14] Response: ```python
["REASONING"]
```
[26.02.2025 08:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces the Visual Perception Token concept to enhance the capabilities of Multimodal Large Language Models (MLLMs) in visual perception. The proposed tokens allow MLLMs to autonomously control their visual perception processes, enabling them to focus on specific image regions or object categories. Two types of tokens are designed: the Region Selection Token for identifying areas needing further analysis, and the Vision Re-Encoding Token for guiding additional perception actions. Experimental results show that these tokens significantly improve spatial reasoning and fine-grained understanding, leading to a performance increase of 23.6% in a 2B model compared to previous methods.","title":"Empowering MLLMs with Visual Perception Tokens for Enhanced Understanding"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces the Visual Perception Token concept to enhance the capabilities of Multimodal Large Language Models (MLLMs) in visual perception. The proposed tokens allow MLLMs to autonomously control their visual perception processes, enabling them to focus on specific image regions or object categories. Two types of tokens are designed: the Region Selection Token for identifying areas needing further analysis, and the Vision Re-Encoding Token for guiding additional perception actions. Experimental results show that these tokens significantly improve spatial reasoning and fine-grained understanding, leading to a performance increase of 23.6% in a 2B model compared to previous methods.', title='Empowering MLLMs with Visual Perception Tokens for Enhanced Understanding'))
[26.02.2025 08:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâ‰æùËµñËßÜËßâÁºñÁ†ÅÂô®ÁöÑÊÑüÁü•ËøáÁ®ãÊù•Âà©Áî®ËßÜËßâ‰ø°ÊÅØ„ÄÇËßÜËßâÊÑüÁü•ÁöÑÂÆåÊï¥ÊÄßÂíåÂáÜÁ°ÆÊÄßÂØπÁ©∫Èó¥Êé®ÁêÜÂíåÁªÜÁ≤íÂ∫¶ÁêÜËß£Á≠â‰ªªÂä°ÁöÑÁ≤æÂ∫¶ÊúâÈáçË¶ÅÂΩ±Âìç„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜËßÜËßâÊÑüÁü•‰ª§ÁâåÁöÑÊ¶ÇÂøµÔºå‰ª•Â¢ûÂº∫MLLMÂØπÂÖ∂ËßÜËßâÊÑüÁü•ËøáÁ®ãÁöÑÊéßÂà∂ËÉΩÂäõ„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫ÜÂå∫ÂüüÈÄâÊã©‰ª§ÁâåÂíåËßÜËßâÈáçÊñ∞ÁºñÁ†Å‰ª§Áâå‰∏§ÁßçÁ±ªÂûãÁöÑËßÜËßâÊÑüÁü•‰ª§ÁâåÔºåÂÆûÈ™åË°®ÊòéËøô‰∫õ‰ª§ÁâåÂú®Â§ÑÁêÜÁ©∫Èó¥Êé®ÁêÜÂíåÊèêÈ´òÁªÜÁ≤íÂ∫¶ÁêÜËß£ÊñπÈù¢ÂÖ∑ÊúâÊòæËëó‰ºòÂäø„ÄÇ","title":"ËµãËÉΩMLLMÁöÑËßÜËßâÊÑüÁü•ÊéßÂà∂Êú∫Âà∂"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâ‰æùËµñËßÜËßâÁºñÁ†ÅÂô®ÁöÑÊÑüÁü•ËøáÁ®ãÊù•Âà©Áî®ËßÜËßâ‰ø°ÊÅØ„ÄÇËßÜËßâÊÑüÁü•ÁöÑÂÆåÊï¥ÊÄßÂíåÂáÜÁ°ÆÊÄßÂØπÁ©∫Èó¥Êé®ÁêÜÂíåÁªÜÁ≤íÂ∫¶ÁêÜËß£Á≠â‰ªªÂä°ÁöÑÁ≤æÂ∫¶ÊúâÈáçË¶ÅÂΩ±Âìç„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜËßÜËßâÊÑüÁü•‰ª§ÁâåÁöÑÊ¶ÇÂøµÔºå‰ª•Â¢ûÂº∫MLLMÂØπÂÖ∂ËßÜËßâÊÑüÁü•ËøáÁ®ãÁöÑÊéßÂà∂ËÉΩÂäõ„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫ÜÂå∫ÂüüÈÄâÊã©‰ª§ÁâåÂíåËßÜËßâÈáçÊñ∞ÁºñÁ†Å‰ª§Áâå‰∏§ÁßçÁ±ªÂûãÁöÑËßÜËßâÊÑüÁü•‰ª§ÁâåÔºåÂÆûÈ™åË°®ÊòéËøô‰∫õ‰ª§ÁâåÂú®Â§ÑÁêÜÁ©∫Èó¥Êé®ÁêÜÂíåÊèêÈ´òÁªÜÁ≤íÂ∫¶ÁêÜËß£ÊñπÈù¢ÂÖ∑ÊúâÊòæËëó‰ºòÂäø„ÄÇ', title='ËµãËÉΩMLLMÁöÑËßÜËßâÊÑüÁü•ÊéßÂà∂Êú∫Âà∂'))
[26.02.2025 08:14] Loading Chinese text from previous data.
[26.02.2025 08:14] Renaming data file.
[26.02.2025 08:14] Renaming previous data. hf_papers.json to ./d/2025-02-26.json
[26.02.2025 08:14] Saving new data file.
[26.02.2025 08:14] Generating page.
[26.02.2025 08:14] Renaming previous page.
[26.02.2025 08:14] Renaming previous data. index.html to ./d/2025-02-26.html
[26.02.2025 08:14] [Experimental] Generating Chinese page for reading.
[26.02.2025 08:14] Chinese vocab [{'word': 'ÂàõÂª∫', 'pinyin': 'chu√†ng ji√†n', 'trans': 'create'}, {'word': 'ËÆ°ÁÆóËµÑÊ∫ê', 'pinyin': 'j√¨ su√†n zƒ´ yu√°n', 'trans': 'computational resources'}, {'word': 'ËÆ≠ÁªÉÊï∞ÊçÆ', 'pinyin': 'x√πn li√†n sh√π j√π', 'trans': 'training data'}, {'word': 'ÊúâÈôê', 'pinyin': 'y«íu xi√†n', 'trans': 'limited'}, {'word': 'Â§ö‰ªªÂä°', 'pinyin': 'du≈ç r√®n w√π', 'trans': 'multi-task'}, {'word': 'ÈÄöÁî®', 'pinyin': 't≈çng y√≤ng', 'trans': 'general-purpose'}, {'word': 'ÊÑüÁü•', 'pinyin': 'g«én zhƒ´', 'trans': 'perception'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥ x√≠ng', 'trans': 'model'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πn li√†n', 'trans': 'pre-trained'}, {'word': 'ÊñáÊú¨Âà∞ÂõæÂÉè', 'pinyin': 'w√©n bƒõn d√†o t√∫ xi√†ng', 'trans': 'text-to-image'}, {'word': 'Êâ©Êï£', 'pinyin': 'ku√≤ s√†n', 'trans': 'diffusion'}, {'word': 'Ë°®Êòé', 'pinyin': 'bi«éo m√≠ng', 'trans': 'indicate'}, {'word': '‰ºòÂºÇ', 'pinyin': 'y≈çu y√¨', 'trans': 'excellent'}, {'word': '‰ªÖ', 'pinyin': 'j«ên', 'trans': 'only'}, {'word': 'Áªü‰∏Ä', 'pinyin': 't«íng yƒ´', 'trans': 'unify'}, {'word': 'ÁºñÁ†Å', 'pinyin': 'biƒÅn m«é', 'trans': 'encoding'}, {'word': 'ÂÖÖÂàÜÂà©Áî®', 'pinyin': 'ch≈çng f√®n l√¨ y√≤ng', 'trans': 'fully utilize'}, {'word': 'ÈÄÇÂ∫î', 'pinyin': 'sh√¨ y√¨ng', 'trans': 'adapt'}, {'word': 'ÂæÆË∞É', 'pinyin': 'wƒìi ti√°o', 'trans': 'fine-tune'}]
[26.02.2025 08:14] Renaming previous Chinese page.
[26.02.2025 08:14] Renaming previous data. zh.html to ./d/2025-02-25_zh_reading_task.html
[26.02.2025 08:14] Writing Chinese reading task.
[26.02.2025 08:14] Writing result.
[26.02.2025 08:14] Renaming log file.
[26.02.2025 08:14] Renaming previous data. log.txt to ./logs/2025-02-26_last_log.txt
